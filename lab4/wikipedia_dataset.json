{
  "Information retrieval": {
    "url": "https://en.wikipedia.org/wiki/Information_retrieval",
    "title": "Information retrieval",
    "content": "Information retrieval ( IR ) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need . The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science [ 1 ] of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds. Cross-modal retrieval implies retrieval across modalities. Automated information retrieval systems are used to reduce what has been called information overload . An IR system is a software system that provides access to books, journals and other documents; it also stores and manages those documents. Web search engines are the most visible IR applications. An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval, a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevance . An object is an entity that is represented by information in a content collection or database . User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching. [ 2 ] Depending on the application the data objects may be, for example, text documents, images, [ 3 ] audio, [ 4 ] mind maps [ 5 ] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata . Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. [ 6 ] there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute — J. E. Holmstrom, 1948 The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. [ 7 ] It would appear that Bush was inspired by patents for a 'statistical machine' – filed by Emanuel Goldberg in the 1920s and 1930s – that searched for documents stored on film. [ 8 ] The first description of a computer searching for information was described by Holmstrom in 1948, [ 9 ] detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy Desk Set . In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). [ 7 ] Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s. In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further. By the late 1990s, the rise of the World Wide Web fundamentally transformed information retrieval. While early search engines such as AltaVista (1995) and Yahoo! (1994) offered keyword-based retrieval, they were limited in scale and ranking refinement. The breakthrough came in 1998 with the founding of Google , which introduced the PageRank algorithm, [ 10 ] using the web's hyperlink structure to assess page importance and improve relevance ranking. During the 2000s, web search systems evolved rapidly with the integration of machine learning techniques. These systems began to incorporate user behavior data (e.g., click-through logs), query reformulation, and content-based signals to improve search accuracy and personalization. In 2009, Microsoft launched Bing , introducing features that would later incorporate semantic web technologies through the development of its Satori knowledge base. Academic analysis [ 11 ] have highlighted Bing's semantic capabilities, including structured data use and entity recognition, as part of a broader industry shift toward improving search relevance and understanding user intent through natural language processing. A major leap occurred in 2018, when Google deployed BERT ( B idirectional E ncoder R epresentations from T ransformers) to better understand the contextual meaning of queries and documents. This marked one of the first times deep neural language models were used at scale in real-world retrieval systems. [ 12 ] BERT's bidirectional training enabled a more refined comprehension of word relationships in context, improving the handling of natural language queries. Because of its success, transformer-based models gained traction in academic research and commercial search applications. [ 13 ] Simultaneously, the research community began exploring neural ranking models that outperformed traditional lexical-based methods. Long-standing benchmarks such as the T ext RE trieval C onference ( TREC ), initiated in 1992, and more recent evaluation frameworks Microsoft MARCO( MA chine R eading CO mprehension) (2019) [ 14 ] became central to training and evaluating retrieval systems across multiple tasks and domains. MS MARCO has also been adopted in the TREC Deep Learning Tracks, where it serves as a core dataset for evaluating advances in neural ranking models within a standardized benchmarking environment. [ 15 ] As deep learning became integral to information retrieval systems, researchers began to categorize neural approaches into three broad classes: sparse , dense , and hybrid models. Sparse models, including traditional term-based methods and learned variants like SPLADE, rely on interpretable representations and inverted indexes to enable efficient exact term matching with added semantic signals. [ 16 ] Dense models, such as dual-encoder architectures like ColBERT, use continuous vector embeddings to support semantic similarity beyond keyword overlap. [ 17 ] Hybrid models aim to combine the advantages of both, balancing the lexical (token) precision of sparse methods with the semantic depth of dense models. This way of categorizing models balances scalability, relevance, and efficiency in retrieval systems. [ 18 ] As IR systems increasingly rely on deep learning, concerns around bias, fairness, and explainability have also come to the picture. Research is now focused not just on relevance and efficiency, but on transparency, accountability, and user trust in retrieval algorithms. Areas where information retrieval techniques are employed include (the entries are in alphabetical order within each category): Methods/Techniques in which information retrieval techniques are employed include: In order to effectively retrieve relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model. In addition to the theoretical distinctions, modern information retrieval models are also categorized on how queries and documents are represented and compared, using a practical classification distinguishing between sparse, dense and hybrid models. [ 16 ] This classification has become increasingly common in both academic and the real world applications and is getting widely adopted and used in evaluation benchmarks for Information Retrieval models. [ 18 ] [ 19 ] The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval [ clarification needed ] or top-k retrieval, include precision and recall . All measures assume a ground truth notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevance.",
    "links": [
      "Latent Dirichlet allocation",
      "Information access",
      "Tf–idf",
      "Cross-modal retrieval",
      "Categorization",
      "Digital libraries",
      "European Conference on Information Retrieval",
      "Doi (identifier)",
      "Allen Kent",
      "Dimension reduction",
      "Grateful Med",
      "Automatic summarization",
      "Ricardo Baeza-Yates",
      "Pearl growing",
      "Personal information management",
      "Social information seeking",
      "Alvin Weinberg",
      "International World Wide Web Conference",
      "Text Retrieval Conference",
      "Retrievability",
      "S2CID (identifier)",
      "JSTOR (identifier)",
      "Search engines",
      "Ground truth",
      "Apache Lucene",
      "Video retrieval",
      "Latent semantic analysis",
      "As We May Think",
      "Spam filtering",
      "Xapian",
      "OSTI (identifier)",
      "Tabulating machine",
      "Knowledge organization",
      "Collaborative information seeking",
      "Hdl (identifier)",
      "Information seeking",
      "Nicholas Jardine",
      "Sergey Brin",
      "Yahoo! Inc. (1995–2017)",
      "Theodor Nelson",
      "Science and technology studies",
      "Information retrieval applications",
      "Memory",
      "Computer memory",
      "Information Retrieval Facility",
      "Query understanding",
      "Privacy",
      "Federated search",
      "Probabilistic relevance model",
      "Data modeling",
      "Univac",
      "Enterprise search",
      "Social search",
      "Orthogonality",
      "Semantic Web",
      "Joseph Marie Jacquard",
      "Search engine indexing",
      "Preservation (library and archival science)",
      "Information filtering",
      "Sphinx (search engine)",
      "Information technology",
      "Atlantic Monthly",
      "Software engineering",
      "Terrier Search Engine",
      "Information society",
      "C. J. van Rijsbergen",
      "Taxonomy",
      "Music information retrieval",
      "Web mining",
      "Hypertext",
      "Independence (mathematical logic)",
      "Information needs",
      "Database",
      "Punched cards",
      "Word embedding",
      "Apache Solr",
      "SPLADE",
      "Data mining",
      "CiteSeerX (identifier)",
      "Censorship",
      "Outline of information science",
      "Data retrieval",
      "Don Swanson",
      "Relevance feedback",
      "Learning to rank",
      "Information science",
      "Ill-posed",
      "Computational linguistics",
      "Controlled vocabulary",
      "Vertical search",
      "Information extraction",
      "Communications of the ACM",
      "Language model",
      "PMID (identifier)",
      "Temporal information retrieval",
      "Nicholas J. Belkin",
      "Citation index",
      "Microsoft",
      "Legal information retrieval",
      "Precision and recall",
      "Generalized vector space model",
      "National Bureau of Standards",
      "Desk Set",
      "Computer data storage",
      "Fuzzy retrieval",
      "Desktop search",
      "Lemur Project",
      "Natural language user interface",
      "Library classification",
      "CERN",
      "ArXiv (identifier)",
      "Ranking (information retrieval)",
      "Uncertain inference",
      "Microsoft Bing",
      "Eugene Garfield",
      "Relevance (information retrieval)",
      "Mind maps",
      "Herman Hollerith",
      "OCLC (identifier)",
      "Science",
      "Term Discrimination",
      "World Wide Web",
      "Wikipedia",
      "MEDLARS",
      "Information architecture",
      "Robert R. Korfhage",
      "Co-occurrence",
      "Elasticsearch",
      "Probabilistic relevance model (BM25)",
      "Recommender systems",
      "Vector space model",
      "Information overload",
      "Compound term processing",
      "Information behavior",
      "Multi-document summarization",
      "Bill Maron",
      "XML retrieval",
      "Case Western Reserve University",
      "Computing",
      "SMART Information Retrieval System",
      "Divergence-from-randomness model",
      "Text corpora",
      "Grateful Dead",
      "PageRank",
      "Association for Computing Machinery",
      "National Institute of Standards and Technology",
      "BERT (language model)",
      "3D retrieval",
      "Robert M. Hayes (information scientist)",
      "Subject indexing",
      "AltaVista",
      "Emanuel Goldberg",
      "Quantum information science",
      "Philosophy of information",
      "Topic-based vector space model",
      "Google",
      "Multimedia information retrieval",
      "Library and information science",
      "Karen Spärck Jones Award",
      "J. C. R. Licklider",
      "Intellectual property",
      "Cultural studies",
      "Metadata",
      "Hans Peter Luhn",
      "Latent semantic indexing",
      "Tim Berners-Lee",
      "Calvin Mooers",
      "Knowledge visualization",
      "Jacquard loom",
      "MIT",
      "Vannevar Bush",
      "Adversarial information retrieval",
      "Melvin Earl Maron",
      "Human–computer information retrieval",
      "Special Interest Group on Information Retrieval",
      "Informatics",
      "Cornelis J. van Rijsbergen",
      "Wayback Machine",
      "JASIS",
      "Cyril W. Cleverdon",
      "ISSN (identifier)",
      "Bayes' theorem",
      "Karen Spärck Jones",
      "Image retrieval",
      "Set (mathematics)",
      "Standard Boolean model",
      "Donald A.B. Lindberg",
      "Keypunch",
      "ISBN (identifier)",
      "Web search engine",
      "Question answering",
      "National Library of Medicine",
      "Intellectual freedom",
      "Bibliometrics",
      "Geographic information retrieval",
      "F. Wilfrid Lancaster",
      "Gerard Salton",
      "Document classification",
      "Extended Boolean model",
      "PMC (identifier)",
      "Nearest centroid classifier",
      "Scalability",
      "Ontology (information science)",
      "Gerard Salton Award",
      "Full-text search",
      "Larry Page",
      "Cross-language information retrieval",
      "Information management",
      "Information system",
      "1890 US census",
      "Evaluation measures (information retrieval)",
      "Binary Independence Model",
      "Tony Kent Strix award"
    ]
  },
  "PageRank": {
    "url": "https://en.wikipedia.org/wiki/PageRank",
    "title": "PageRank",
    "content": "PageRank ( PR ) is an algorithm used by Google Search to rank web pages in their search engine results. It is named after both the term \"web page\" and co-founder Larry Page . PageRank is a way of measuring the importance of website pages. According to Google: PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. [ 1 ] Currently, PageRank is not the only algorithm used by Google to order search results, but it is the first algorithm that was used by the company, and it is the best known. [ 2 ] [ 3 ] As of September 24, 2019, all patents associated with PageRank have expired. [ 4 ] PageRank is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web , with the purpose of \"measuring\" its relative importance within the set. The algorithm may be applied to any collection of entities with reciprocal quotations and references. The numerical weight that it assigns to any given element E is referred to as the PageRank of E and denoted by P R ( E ) . {\\displaystyle PR(E).} A PageRank results from a mathematical algorithm based on the Webgraph , created by all World Wide Web pages as nodes and hyperlinks as edges, taking into consideration authority hubs such as cnn.com or mayoclinic.org . The rank value indicates an importance of a particular page. A hyperlink to a page counts as a vote of support. The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it (\" incoming links \"). A page that is linked to by many pages with high PageRank receives a high rank itself. [ 5 ] Numerous academic papers concerning PageRank have been published since Page and Brin's original paper. [ 6 ] In practice, the PageRank concept may be vulnerable to manipulation. Research has been conducted into identifying falsely influenced PageRank rankings. The goal is to find an effective means of ignoring links from documents with falsely influenced PageRank. [ 7 ] Other link-based ranking algorithms for Web pages include the HITS algorithm invented by Jon Kleinberg (used by Teoma and now Ask.com ), the IBM CLEVER project , the TrustRank algorithm, the Hummingbird algorithm, [ 8 ] and the SALSA algorithm . [ 9 ] The eigenvalue problem behind PageRank's algorithm was independently rediscovered and reused in many scoring problems. In 1895, Edmund Landau suggested using it for determining the winner of a chess tournament. [ 10 ] [ 11 ] The eigenvalue problem was also suggested in 1976 by Gabriel Pinski and Francis Narin, who worked on scientometrics ranking scientific journals, [ 12 ] in 1977 by Thomas Saaty in his concept of Analytic Hierarchy Process which weighted alternative choices, [ 13 ] and in 1995 by Bradley Love and Steven Sloman as a cognitive model for concepts, the centrality algorithm. [ 14 ] [ 15 ] A search engine called \" RankDex \" from IDD Information Services, designed by Robin Li in 1996, developed a strategy for site-scoring and page-ranking. [ 16 ] Li referred to his search mechanism as \"link analysis,\" which involved ranking the popularity of a web site based on how many other sites had linked to it. [ 17 ] RankDex, the first search engine with page-ranking and site-scoring algorithms, was launched in 1996. [ 18 ] Li filed a patent for the technology in RankDex in 1997; it was granted in 1999. [ 19 ] He later used it when he founded Baidu in China in 2000. [ 20 ] [ 21 ] Google founder Larry Page referenced Li's work as a citation in some of his U.S. patents for PageRank. [ 22 ] [ 18 ] [ 23 ] Larry Page and Sergey Brin developed PageRank at Stanford University in 1996 as part of a research project about a new kind of search engine. An interview with Héctor García-Molina , Stanford Computer Science professor and advisor to Sergey, [ 24 ] provides background into the development of the page-rank algorithm. [ 25 ] Sergey Brin had the idea that information on the web could be ordered in a hierarchy by \"link popularity\": a page ranks higher as there are more links to it. [ 26 ] The system was developed with the help of Scott Hassan and Alan Steremberg, both of whom were cited by Page and Brin as being critical to the development of Google. [ 6 ] Rajeev Motwani and Terry Winograd co-authored with Page and Brin the first paper about the project, describing PageRank and the initial prototype of the Google search engine , published in 1998. [ 6 ] Shortly after, Page and Brin founded Google Inc. , the company behind the Google search engine. While just one of many factors that determine the ranking of Google search results, PageRank continues to provide the basis for all of Google's web-search tools. [ 27 ] The name \"PageRank\" plays on the name of developer Larry Page, as well as of the concept of a web page . [ 28 ] [ 29 ] The word is a trademark of Google, and the PageRank process has been patented ( U.S. patent 6,285,999 ). However, the patent is assigned to Stanford University and not to Google. Google has exclusive license rights on the patent from Stanford University. The university received 1.8 million shares of Google in exchange for use of the patent; it sold the shares in 2005 for $336 million. [ 30 ] [ 31 ] PageRank was influenced by citation analysis , early developed by Eugene Garfield in the 1950s at the University of Pennsylvania, and by Hyper Search , developed by Massimo Marchiori at the University of Padua . In the same year PageRank was introduced (1998), Jon Kleinberg published his work on HITS . Google's founders cite Garfield, Marchiori, and Kleinberg in their original papers. [ 6 ] [ 32 ] The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. The PageRank computations require several passes, called \"iterations\", through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value. A probability is expressed as a numeric value between 0 and 1. A 0.5 probability is commonly expressed as a \"50% chance\" of something happening. Hence, a document with a PageRank of 0.5 means there is a 50% chance that a person clicking on a random link will be directed to said document. PageRank works on the assumption that a page is important if many other important pages link to it. This means the more quality backlinks a page has, the higher its PageRank score. [1] Assume a small universe of four web pages: A , B , C , and D . Links from a page to itself are ignored. Multiple outbound links from one page to another page are treated as a single link. PageRank is initialized to the same value for all pages. In the original form of PageRank, the sum of PageRank over all pages was the total number of pages on the web at that time, so each page in this example would have an initial value of 1. However, later versions of PageRank, and the remainder of this section, assume a probability distribution between 0 and 1. Hence the initial value for each page in this example is 0.25. The PageRank transferred from a given page to the targets of its outbound links upon the next iteration is divided equally among all outbound links. If the only links in the system were from pages B , C , and D to A , each link would transfer 0.25 PageRank to A upon the next iteration, for a total of 0.75. Suppose instead that page B had a link to pages C and A , page C had a link to page A , and page D had links to all three pages. Thus, upon the first iteration, page B would transfer half of its existing value (0.125) to page A and the other half (0.125) to page C . Page C would transfer all of its existing value (0.25) to the only page it links to, A . Since D had three outbound links, it would transfer one third of its existing value, or approximately 0.083, to A . At the completion of this iteration, page A will have a PageRank of approximately 0.458. In other words, the PageRank conferred by an outbound link is equal to the document's own PageRank score divided by the number of outbound links L( ) . In the general case, the PageRank value for any page u can be expressed as: i.e. the PageRank value for a page u is dependent on the PageRank values for each page v contained in the set B u (the set containing all pages linking to page u ), divided by the number L ( v ) of links from page v . The PageRank theory holds that an imaginary surfer who is randomly clicking on links will eventually stop clicking. The probability, at any step, that the person will continue following links is a damping factor d . The probability that they instead jump to any random page is 1 - d . Various studies have tested different damping factors, but it is generally assumed that the damping factor will be set around 0.85. [ 6 ] The damping factor is subtracted from 1 (and in some variations of the algorithm, the result is divided by the number of documents ( N ) in the collection) and this term is then added to the product of the damping factor and the sum of the incoming PageRank scores. That is, So any page's PageRank is derived in large part from the PageRanks of other pages. The damping factor adjusts the derived value downward. The original paper, however, gave the following formula, which has led to some confusion: The difference between them is that the PageRank values in the first formula sum to one, while in the second formula each PageRank is multiplied by N and the sum becomes N . A statement in Page and Brin's paper that \"the sum of all PageRanks is one\" [ 6 ] and claims by other Google employees [ 33 ] support the first variant of the formula above. Page and Brin confused the two formulas in their most popular paper \"The Anatomy of a Large-Scale Hypertextual Web Search Engine\", where they mistakenly claimed that the latter formula formed a probability distribution over web pages. [ 6 ] Google recalculates PageRank scores each time it crawls the Web and rebuilds its index. As Google increases the number of documents in its collection, the initial approximation of PageRank decreases for all documents. The formula uses a model of a random surfer who reaches their target site after several clicks, then switches to a random page. The PageRank value of a page reflects the chance that the random surfer will land on that page by clicking on a link. It can be understood as a Markov chain in which the states are pages, and the transitions are the links between pages – all of which are all equally probable. If a page has no links to other pages, it becomes a sink and therefore terminates the random surfing process. If the random surfer arrives at a sink page, it picks another URL at random and continues surfing again. When calculating PageRank, pages with no outbound links are assumed to link out to all other pages in the collection. Their PageRank scores are therefore divided evenly among all other pages. In other words, to be fair with pages that are not sinks, these random transitions are added to all nodes in the Web. This residual probability, d , is usually set to 0.85, estimated from the frequency that an average surfer uses his or her browser's bookmark feature. So, the equation is as follows: where p 1 , p 2 , . . . , p N {\\displaystyle p_{1},p_{2},...,p_{N}} are the pages under consideration, M ( p i ) {\\displaystyle M(p_{i})} is the set of pages that link to p i {\\displaystyle p_{i}} , L ( p j ) {\\displaystyle L(p_{j})} is the number of outbound links on page p j {\\displaystyle p_{j}} , and N {\\displaystyle N} is the total number of pages. The PageRank values are the entries of the dominant right eigenvector of the modified adjacency matrix rescaled so that each column adds up to one. This makes PageRank a particularly elegant metric: the eigenvector is where R is the solution of the equation where the adjacency function ℓ ( p i , p j ) {\\displaystyle \\ell (p_{i},p_{j})} is the ratio between number of links outbound from page j to page i to the total number of outbound links of page j. The adjacency function is 0 if page p j {\\displaystyle p_{j}} does not link to p i {\\displaystyle p_{i}} , and normalized such that, for each j i.e. the elements of each column sum up to 1, so the matrix is a stochastic matrix (for more details see the computation section below). Thus this is a variant of the eigenvector centrality measure used commonly in network analysis . Because of the large eigengap of the modified adjacency matrix above, [ 34 ] the values of the PageRank eigenvector can be approximated to within a high degree of accuracy within only a few iterations. Google's founders, in their original paper, [ 32 ] reported that the PageRank algorithm for a network consisting of 322 million links (in-edges and out-edges) converges to within a tolerable limit in 52 iterations. The convergence in a network of half the above size took approximately 45 iterations. Through this data, they concluded the algorithm can be scaled very well and that the scaling factor for extremely large networks would be roughly linear in log ⁡ n {\\displaystyle \\log n} , where n is the size of the network. As a result of Markov theory , it can be shown that the PageRank of a page is the probability of arriving at that page after a large number of clicks. This happens to equal t − 1 {\\displaystyle t^{-1}} where t {\\displaystyle t} is the expectation of the number of clicks (or random jumps) required to get from the page back to itself. One main disadvantage of PageRank is that it favors older pages. A new page, even a very good one, will not have many links unless it is part of an existing site (a site being a densely connected set of pages, such as Wikipedia ). Several strategies have been proposed to accelerate the computation of PageRank. [ 35 ] Various strategies to manipulate PageRank have been employed in concerted efforts to improve search results rankings and monetize advertising links. These strategies have severely impacted the reliability of the PageRank concept, [ citation needed ] which purports to determine which documents are actually highly valued by the Web community. Since December 2007, when it started actively penalizing sites selling paid text links, Google has combatted link farms and other schemes designed to artificially inflate PageRank. How Google identifies link farms and other PageRank manipulation tools is among Google's trade secrets . PageRank can be computed either iteratively or algebraically. The iterative method can be viewed as the power iteration method [ 36 ] [ 37 ] or the power method. The basic mathematical operations performed are identical. At t = 0 {\\displaystyle t=0} , an initial probability distribution is assumed, usually where N is the total number of pages, and p i ; 0 {\\displaystyle p_{i};0} is page i at time 0. At each time step, the computation, as detailed above, yields where d is the damping factor, or in matrix notation where R i ( t ) = P R ( p i ; t ) {\\displaystyle \\mathbf {R} _{i}(t)=PR(p_{i};t)} and 1 {\\displaystyle \\mathbf {1} } is the column vector of length N {\\displaystyle N} containing only ones. The matrix M {\\displaystyle {\\mathcal {M}}} is defined as i.e., where A {\\displaystyle A} denotes the adjacency matrix of the graph and K {\\displaystyle K} is the diagonal matrix with the outdegrees in the diagonal. The probability calculation is made for each page at a time point, then repeated for the next time point. The computation ends when for some small ϵ {\\displaystyle \\epsilon } i.e., when convergence is assumed. If the matrix M {\\displaystyle {\\mathcal {M}}} is a transition probability, i.e., column-stochastic and R {\\displaystyle \\mathbf {R} } is a probability distribution (i.e., | R | = 1 {\\displaystyle |\\mathbf {R} |=1} , E R = 1 {\\displaystyle \\mathbf {E} \\mathbf {R} =\\mathbf {1} } where E {\\displaystyle \\mathbf {E} } is matrix of all ones), then equation ( 2 ) is equivalent to Hence PageRank R {\\displaystyle \\mathbf {R} } is the principal eigenvector of M ^ {\\displaystyle {\\widehat {\\mathcal {M}}}} . A fast and easy way to compute this is using the power method : starting with an arbitrary vector x ( 0 ) {\\displaystyle x(0)} , the operator M ^ {\\displaystyle {\\widehat {\\mathcal {M}}}} is applied in succession, i.e., until Note that in equation ( 3 ) the matrix on the right-hand side in the parenthesis can be interpreted as where P {\\displaystyle \\mathbf {P} } is an initial probability distribution. n the current case Finally, if M {\\displaystyle {\\mathcal {M}}} has columns with only zero values, they should be replaced with the initial probability vector P {\\displaystyle \\mathbf {P} } . In other words, where the matrix D {\\displaystyle {\\mathcal {D}}} is defined as with In this case, the above two computations using M {\\displaystyle {\\mathcal {M}}} only give the same PageRank if their results are normalized: The PageRank of an undirected graph G {\\displaystyle G} is statistically close to the degree distribution of the graph G {\\displaystyle G} , [ 38 ] but they are generally not identical: If R {\\displaystyle R} is the PageRank vector defined above, and D {\\displaystyle D} is the degree distribution vector where deg ⁡ ( p i ) {\\displaystyle \\deg(p_{i})} denotes the degree of vertex p i {\\displaystyle p_{i}} , and E {\\displaystyle E} is the edge-set of the graph, then, with Y = 1 N 1 {\\displaystyle Y={1 \\over N}\\mathbf {1} } , [ 39 ] shows that: 1 − d 1 + d ‖ Y − D ‖ 1 ≤ ‖ R − D ‖ 1 ≤ ‖ Y − D ‖ 1 , {\\displaystyle {1-d \\over 1+d}\\|Y-D\\|_{1}\\leq \\|R-D\\|_{1}\\leq \\|Y-D\\|_{1},} that is, the PageRank of an undirected graph equals to the degree distribution vector if and only if the graph is regular, i.e., every vertex has the same degree. A generalization of PageRank for the case of ranking two interacting groups of objects was described by Daugulis. [ 40 ] In applications it may be necessary to model systems having objects of two kinds where a weighted relation is defined on object pairs. This leads to considering bipartite graphs . For such graphs two related positive or nonnegative irreducible matrices corresponding to vertex partition sets can be defined. One can compute rankings of objects in both groups as eigenvectors corresponding to the maximal positive eigenvalues of these matrices. Normed eigenvectors exist and are unique by the Perron or Perron–Frobenius theorem. Example: consumers and products. The relation weight is the product consumption rate. Sarma et al. describe two random walk -based distributed algorithms for computing PageRank of nodes in a network. [ 41 ] One algorithm takes O ( log ⁡ n / ϵ ) {\\displaystyle O(\\log n/\\epsilon )} rounds with high probability on any graph (directed or undirected), where n is the network size and ϵ {\\displaystyle \\epsilon } is the reset probability ( 1 − ϵ {\\displaystyle 1-\\epsilon } , which is called the damping factor) used in the PageRank computation. They also present a faster algorithm that takes O ( log ⁡ n / ϵ ) {\\displaystyle O({\\sqrt {\\log n}}/\\epsilon )} rounds in undirected graphs. In both algorithms, each node processes and sends a number of bits per round that are polylogarithmic in n, the network size. The Google Toolbar long had a PageRank feature which displayed a visited page's PageRank as a whole number between 0 (least popular) and 10 (most popular). Google had not disclosed the specific method for determining a Toolbar PageRank value, which was to be considered only a rough indication of the value of a website. The \"Toolbar Pagerank\" was available for verified site maintainers through the Google Webmaster Tools interface. However, on October 15, 2009, a Google employee confirmed that the company had removed PageRank from its Webmaster Tools section, saying that \"We've been telling people for a long time that they shouldn't focus on PageRank so much. Many site owners seem to think it's the most important metric for them to track, which is simply not true.\" [ 42 ] The \"Toolbar Pagerank\" was updated very infrequently. It was last updated in November 2013. In October 2014 Matt Cutts announced that another visible pagerank update would not be coming. [ 43 ] In March 2016 Google announced it would no longer support this feature, and the underlying API would soon cease to operate. [ 44 ] On April 15, 2016, Google turned off display of PageRank Data in Google Toolbar, [ 45 ] though the PageRank continued to be used internally to rank content in search results. [ 46 ] The search engine results page (SERP) is the actual result returned by a search engine in response to a keyword query. The SERP consists of a list of links to web pages with associated text snippets, paid ads, featured snippets, and Q&A. The SERP rank of a web page refers to the placement of the corresponding link on the SERP, where higher placement means higher SERP rank. The SERP rank of a web page is a function not only of its PageRank, but of a relatively large and continuously adjusted set of factors (over 200). [ 47 ] [ unreliable source? ] Search engine optimization (SEO) is aimed at influencing the SERP rank for a website or a set of web pages. Positioning of a webpage on Google SERPs for a keyword depends on relevance and reputation, also known as authority and popularity. PageRank is Google's indication of its assessment of the reputation of a webpage: It is non-keyword specific. Google uses a combination of webpage and website authority to determine the overall authority of a webpage competing for a keyword. [ 48 ] The PageRank of the HomePage of a website is the best indication Google offers for website authority. [ 49 ] After the introduction of Google Places into the mainstream organic SERP, numerous other factors in addition to PageRank affect ranking a business in Local Business Results. [ 50 ] When Google elaborated on the reasons for PageRank deprecation at Q&A #March 2016, they announced Links and Content as the Top Ranking Factors. RankBrain had earlier in October 2015 been announced as the #3 Ranking Factor, so the Top 3 Factors have been confirmed officially by Google. [ 51 ] The Google Directory PageRank was an 8-unit measurement. Unlike the Google Toolbar, which showed a numeric PageRank value upon mouseover of the green bar, the Google Directory only displayed the bar, never the numeric values. Google Directory was closed on July 20, 2011. [ 52 ] It was known that the PageRank shown in the Toolbar could easily be spoofed . Redirection from one page to another, either via a HTTP 302 response or a \"Refresh\" meta tag , caused the source page to acquire the PageRank of the destination page. Hence, a new page with PR 0 and no incoming links could have acquired PR 10 by redirecting to the Google home page. Spoofing can usually be detected by performing a Google search for a source URL; if the URL of an entirely different site is displayed in the results, the latter URL may represent the destination of a redirection. For search engine optimization purposes, some companies offer to sell high PageRank links to webmasters. [ 53 ] As links from higher-PR pages are believed to be more valuable, they tend to be more expensive. It can be an effective and viable marketing strategy to buy link advertisements on content pages of quality and relevant sites to drive traffic and increase a webmaster's link popularity. However, Google has publicly warned webmasters that if they are or were discovered to be selling links for the purpose of conferring PageRank and reputation, their links will be devalued (ignored in the calculation of other pages' PageRanks). The practice of buying and selling [ 54 ] is intensely debated across the Webmaster community. Google advised webmasters to use the nofollow HTML attribute value on paid links. According to Matt Cutts , Google is concerned about webmasters who try to game the system , and thereby reduce the quality and relevance of Google search results. [ 53 ] In 2019, Google announced two additional link attributes providing hints about which links to consider or exclude within Search: rel=\"ugc\" as a tag for user-generated content , such as comments; and rel=\"sponsored\" as a tag for advertisements or other types of sponsored content. Multiple rel values are also allowed, for example, rel=\"ugc sponsored\" can be used to hint that the link came from user-generated content and is sponsored. [ 55 ] Even though PageRank has become less important for SEO purposes, the existence of back-links from more popular websites continues to push a webpage higher up in search rankings. [ 56 ] A more intelligent surfer that probabilistically hops from page to page depending on the content of the pages and query terms the surfer is looking for. This model is based on a query-dependent PageRank score of a page which as the name suggests is also a function of query. When given a multiple-term query, Q = { q 1 , q 2 , ⋯ } {\\displaystyle Q=\\{q1,q2,\\cdots \\}} , the surfer selects a q {\\displaystyle q} according to some probability distribution, P ( q ) {\\displaystyle P(q)} , and uses that term to guide its behavior for a large number of steps. It then selects another term according to the distribution to determine its behavior, and so on. The resulting distribution over visited web pages is QD-PageRank. [ 57 ] The mathematics of PageRank are entirely general and apply to any graph or network in any domain. Thus, PageRank is now regularly used in bibliometrics, social and information network analysis, and for link prediction and recommendation. It is used for systems analysis of road networks, and in biology, chemistry, neuroscience, and physics. [ 58 ] PageRank has been used to quantify the scientific impact of researchers. The underlying citation and collaboration networks are used in conjunction with PageRank algorithm in order to come up with a ranking system for individual publications which propagates to individual authors. The new index known as pagerank-index (Pi) is demonstrated to be fairer compared to h-index in the context of many drawbacks exhibited by h-index. [ 59 ] For the analysis of protein networks in biology PageRank is also a useful tool. [ 60 ] [ 61 ] In any ecosystem, a modified version of PageRank may be used to determine species that are essential to the continuing health of the environment. [ 62 ] A similar newer use of PageRank is to rank academic doctoral programs based on their records of placing their graduates in faculty positions. In PageRank terms, academic departments link to each other by hiring their faculty from each other (and from themselves). [ 63 ] A version of PageRank has recently been proposed as a replacement for the traditional Institute for Scientific Information (ISI) impact factor , [ 64 ] and implemented at Eigenfactor as well as at SCImago . Instead of merely counting total citations to a journal, the \"importance\" of each citation is determined in a PageRank fashion. In neuroscience , the PageRank of a neuron in a neural network has been found to correlate with its relative firing rate. [ 65 ] Personalized PageRank is used by Twitter to present users with other accounts they may wish to follow. [ 66 ] Swiftype 's site search product builds a \"PageRank that's specific to individual websites\" by looking at each website's signals of importance and prioritizing content based on factors such as number of links from the home page. [ 67 ] A Web crawler may use PageRank as one of a number of importance metrics it uses to determine which URL to visit during a crawl of the web. One of the early working papers [ 68 ] that were used in the creation of Google is Efficient crawling through URL ordering , [ 69 ] which discusses the use of a number of different importance metrics to determine how deeply, and how much of a site Google will crawl. PageRank is presented as one of a number of these importance metrics, though there are others listed such as the number of inbound and outbound links for a URL, and the distance from the root directory on a site to the URL. The PageRank may also be used as a methodology to measure the apparent impact of a community like the Blogosphere on the overall Web itself. This approach uses therefore the PageRank to measure the distribution of attention in reflection of the Scale-free network paradigm. [ citation needed ] In 2005, in a pilot study in Pakistan, Structural Deep Democracy, SD2 [ 70 ] [ 71 ] was used for leadership selection in a sustainable agriculture group called Contact Youth. SD2 uses PageRank for the processing of the transitive proxy votes, with the additional constraints of mandating at least two initial proxies per voter, and all voters are proxy candidates. More complex variants can be built on top of SD2, such as adding specialist proxies and direct votes for specific issues, but SD2 as the underlying umbrella system, mandates that generalist proxies should always be used. In sport the PageRank algorithm has been used to rank the performance of: teams in the National Football League (NFL) in the USA; [ 72 ] individual soccer players; [ 73 ] and athletes in the Diamond League. [ 74 ] PageRank has been used to rank spaces or streets to predict how many people (pedestrians or vehicles) come to the individual spaces or streets. [ 75 ] [ 76 ] In lexical semantics it has been used to perform Word Sense Disambiguation , [ 77 ] Semantic similarity , [ 78 ] and also to automatically rank WordNet synsets according to how strongly they possess a given semantic property, such as positivity or negativity. [ 79 ] How a traffic system changes its operational mode can be described by transitions between quasi-stationary states in correlation structures of traffic flow. PageRank has been used to identify and explore the dominant states among these quasi-stationary states in traffic systems. [ 80 ] In early 2005, Google implemented a new value, \" nofollow \", [ 81 ] for the rel attribute of HTML link and anchor elements, so that website developers and bloggers can make links that Google will not consider for the purposes of PageRank—they are links that no longer constitute a \"vote\" in the PageRank system. The nofollow relationship was added in an attempt to help combat spamdexing . As an example, people could previously create many message-board posts with links to their website to artificially inflate their PageRank. With the nofollow value, message-board administrators can modify their code to automatically insert \"rel='nofollow'\" to all hyperlinks in posts, thus preventing PageRank from being affected by those particular posts. This method of avoidance, however, also has various drawbacks, such as reducing the link value of legitimate comments. (See: Spam in blogs#nofollow ) In an effort to manually control the flow of PageRank among pages within a website, many webmasters practice what is known as PageRank Sculpting [ 82 ] —which is the act of strategically placing the nofollow attribute on certain internal links of a website in order to funnel PageRank towards those pages the webmaster deemed most important. This tactic had been used since the inception of the nofollow attribute, but may no longer be effective since Google announced that blocking PageRank transfer with nofollow does not redirect that PageRank to other links. [ 83 ]",
    "links": [
      "Probability distribution",
      "TrustRank",
      "Eigenvector centrality",
      "Google Search",
      "Keyword research",
      "Doi (identifier)",
      "Web analytics",
      "Google Patents",
      "Search engine results page",
      "Google Toolbar",
      "Webgraph",
      "Behavioral targeting",
      "S2CID (identifier)",
      "Mobilegeddon",
      "SafeSearch",
      "Search engine marketing",
      "Trade secret",
      "Google Penguin",
      "Python (programming language)",
      "Pay-per-click",
      "Hdl (identifier)",
      "Google Get Your Business Online",
      "Uniform Resource Locator",
      "Sergey Brin",
      "Google SearchWiki",
      "Domain authority",
      "Incoming link",
      "Scientometrics",
      "Baidu",
      "Software patent",
      "Meta tag",
      "Power iteration",
      "RankBrain",
      "Blogosphere",
      "AI Overviews",
      "Google Dataset Search",
      "Bipartite graphs",
      "Markov process",
      "Rajeev Motwani",
      "Reciprocal link",
      "Native advertising",
      "User-generated content",
      "HITS algorithm",
      "Swiftype",
      "Graph (data structure)",
      "Local search engine optimisation",
      "Google Code Search",
      "Network theory",
      "Forbes",
      "Bibcode (identifier)",
      "Google Trends",
      "Knowledge Graph (Google)",
      "Katz centrality",
      "Google Search Console",
      "Google Shopping",
      "Expected value",
      "Spamdexing",
      "Google Books Ngram Viewer",
      "CiteSeerX (identifier)",
      "Ask.com",
      "Eigenfactor",
      "Website spoofing",
      "Attention inequality",
      "Googlebot",
      "EigenTrust",
      "Display advertising",
      "Amy Langville",
      "Google Finance",
      "Random walk",
      "Nofollow",
      "Teoma",
      "Web page",
      "SCImago",
      "Google Pigeon",
      "PMID (identifier)",
      "Twitter",
      "AI Mode",
      "Ranking",
      "Matt Cutts",
      "Cnn.com",
      "Eigenvalue",
      "Web crawler",
      "Google matrix",
      "Social media marketing",
      "Google Search Appliance",
      "Game the system",
      "Search analytics",
      "ArXiv (identifier)",
      "Content marketing",
      "Mobile advertising",
      "Word Sense Disambiguation",
      "Keyword density",
      "SimRank",
      "Eugene Garfield",
      "Ad blocking",
      "World Wide Web",
      "Wikipedia",
      "Google Scholar",
      "Adjacency matrix",
      "Stanford University",
      "Héctor García-Molina",
      "Hilltop algorithm",
      "Google Insights for Search",
      "Google Inc.",
      "Google bombing",
      "Link analysis",
      "Google News",
      "Lexical semantics",
      "FairSearch",
      "Weighting",
      "Link farm",
      "Scale-free network",
      "Cost per action",
      "Distributed algorithms",
      "Google News Archive",
      "WordPress",
      "Revenue sharing",
      "Stochastic matrix",
      "Link building",
      "Google Images",
      "WordNet",
      "Synsets",
      "Google Places",
      "Online advertising",
      "Affiliate marketing",
      "CLEVER project",
      "Timeline of Google Search",
      "Google Hummingbird",
      "Eigenvector",
      "Neuron",
      "Hyperlink",
      "The New York Times",
      "Google Blog Search",
      "Citation analysis",
      "Cognitive model",
      "Search Engine Optimization Metrics",
      "Barry Schwartz (technologist)",
      "Google Directory",
      "Google Personalized Search",
      "Google Programmable Search Engine",
      "Algorithm",
      "Search Engine Land",
      "Semantic similarity",
      "Massimo Marchiori",
      "VisualRank",
      "Analytic Hierarchy Process",
      "SALSA algorithm",
      "Recursion",
      "Hyper Search",
      "Lesley Ward",
      "Mayoclinic.org",
      "Search engine",
      "Email marketing",
      "Wayback Machine",
      "Dragonfly (search engine)",
      "Reunion (advertisement)",
      "Jon Kleinberg",
      "Google Voice Search",
      "ISSN (identifier)",
      "Edmund Landau",
      "Google Books",
      "Cost per impression",
      "Degree distribution",
      "Marketing strategy",
      "ISBN (identifier)",
      "Webpages",
      "Robin Li",
      "Contextual advertising",
      "HTML attribute",
      "Set (computer science)",
      "Markov chain",
      "RankDex",
      "List of Year in Search top searches",
      "University of Padua",
      "Referral marketing",
      "Google Panda",
      "CheiRank",
      "PMC (identifier)",
      "Institute for Scientific Information",
      "Thomas Saaty",
      "Neuroscience",
      "HTTP 302",
      "Impact factor",
      "Semantic link",
      "Blog",
      "Larry Page",
      "Terry Winograd",
      "Eigengap",
      "Search engine optimization"
    ]
  },
  "Search engine": {
    "url": "https://en.wikipedia.org/wiki/Search_engine",
    "title": "Search engine",
    "content": "A search engine is a software system that provides hyperlinks to web pages , and other relevant information on the Web in response to a user's query . The user enters a query in a web browser or a mobile app , and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images , videos , or news . For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers . This can include data mining the files and databases stored on web servers , although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s; however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo ! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %). [ 1 ] Notably, this marks the first time in over a decade that Google's share has fallen below the 90% threshold. The business of websites improving their visibility in search results , known as marketing and optimization , has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex . [ 2 ] He described this system in an article titled \" As We May Think \" in The Atlantic Monthly . [ 3 ] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks . [ 4 ] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank . [ 5 ] [ 6 ] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982, [ 7 ] and the Knowbot Information Service multi-network user search was first implemented in 1989. [ 8 ] The first well documented search engine that searched content files, namely FTP files, was Archie , which debuted on 10 September 1990. [ 9 ] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver . One snapshot of the list in 1992 remains, [ 10 ] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\". [ 11 ] The first tool used for searching content (as opposed to users) on the Internet was Archie . [ 12 ] The name stands for \"archive\" without the \"v\". [ 13 ] It was created by Alan Emtage , [ 13 ] [ 14 ] [ 15 ] [ 16 ] computer science student at McGill University in Montreal, Quebec , Canada. The program downloaded the directory listings of all the files located on public anonymous FTP ( File Transfer Protocol ) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota ) led to two new search programs, Veronica and Jughead . Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \" Archie Search Engine \" was not a reference to the Archie comic book series, \" Veronica \" and \" Jughead \" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog , the web's first primitive search engine, released on September 2, 1993. [ 17 ] In June 1993, Matthew Gray, then at MIT , produced what was probably the first web robot , the Perl -based World Wide Web Wanderer , and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot , but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993 [ 18 ] by Jonathon Fletcher ) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler , which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page , which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University ) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search . [ 19 ] The first product from Yahoo! , founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory . In 1995, a search function was added, allowing users to search Yahoo! Directory. [ 20 ] [ 21 ] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan , Excite , Infoseek , Inktomi , Northern Light , and AltaVista . Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking [ 22 ] [ 23 ] [ 24 ] and received a US patent for the technology. [ 25 ] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing, [ 26 ] predating the very similar algorithm patent filed by Google two years later in 1998. [ 27 ] Larry Page referenced Li's work in some of his U.S. patents for PageRank. [ 28 ] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite. [ 29 ] [ 30 ] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com . This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet. [ 31 ] [ 32 ] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s. [ 33 ] Several companies entered the market spectacularly, receiving record gains during their initial public offerings . Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble , a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence. [ 34 ] The company achieved better results for many searches with an algorithm called PageRank , as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page , the later founders of Google. [ 6 ] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li 's earlier RankDex patent as an influence. [ 28 ] [ 24 ] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal . In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker . By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart , blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot ). Microsoft's rebranded search engine, Bing , was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019, [update] active search engine crawlers include those of Baidu , Bing, Brave , [ 35 ] Google, DuckDuckGo , Gigablast , Mojeek , Sogou and Yandex . A search engine maintains the following processes in near real time: [ 36 ] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt , addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript , Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags . After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\". [ 38 ] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML -based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible. [ 37 ] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider , the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed. [ 37 ] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot . Typically, when a user enters a query into a search engine it is a few keywords . [ 39 ] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes. [ 37 ] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI - or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range. [ 40 ] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query . Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search , which allows users to define the distance between keywords. [ 37 ] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another. [ 37 ] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \" inverted index \" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads. [ 41 ] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches. [ 42 ] As of January 2022, [update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share. [ 43 ] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice. [ 44 ] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%. [ 45 ] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity. [ 46 ] [ 47 ] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints. [ 48 ] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023. [ 49 ] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share. [ 50 ] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind. [ 51 ] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide [ 52 ] [ 53 ] and the underlying assumptions about the technology. [ 54 ] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws). [ 55 ] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results. [ 56 ] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries. [ 53 ] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines, [ 57 ] and the representation of certain controversial topics in their results, such as terrorism in Ireland , [ 58 ] climate change denial , [ 59 ] and conspiracy theories . [ 60 ] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011. [ 61 ] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo . However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble. [ 62 ] [ 63 ] [ 64 ] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search, [ 64 ] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets. [ 65 ] [ 63 ] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent , to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches . More than usual safe search filters, these Islamic web portals categorizing websites into being either \" halal \" or \" haram \", based on interpretation of Sharia law . ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others). [ 66 ] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google, [ 67 ] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith. [ 68 ] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap , but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking , because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking. [ 69 ] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders . All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders. [ 70 ] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it. However, both types of ranking are vulnerable to fraud, (see Gaming the system ), and both need technical countermeasures to try to deal with this. The first web search engine was Archie , created in 1990 [ 71 ] by Alan Emtage , a student at McGill University in Montreal. The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database. [ 72 ] In 1993, the University of Nevada System Computing Services group developed Veronica . [ 71 ] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges. [ 72 ] The World Wide Web Wanderer , developed by Matthew Gray in 1993 [ 73 ] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos. [ 72 ] Excite , initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet. Their project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers. [ 72 ] Excite was the first serious commercial search engine which launched in 1995. [ 74 ] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite [ 75 ] [ 39 ] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang , created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks. [ 76 ] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers ; ants or spiders), those that are powered by human submissions, and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing . Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google ), database or structured data search engines (e.g. Dieselpoint ), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo! , utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The most prominent example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings. [ 77 ]",
    "links": [
      "Graphical user interface",
      "Audio search engine",
      "Wide area information server",
      "Info.com",
      "Sogou.com",
      "Search.com",
      "Search engine marketing",
      "Robots.txt",
      "As We May Think",
      "Web crawling",
      "Website",
      "Jonathon Fletcher",
      "Jerry Yang (entrepreneur)",
      "Iterative algorithm",
      "Hdl (identifier)",
      "Sergey Brin",
      "Social media optimization",
      "Goo.ne.jp",
      "Internet search engines and libraries",
      "Keyword (Internet search)",
      "Yahoo",
      "Dieselpoint",
      "Search engine technology",
      "Online search",
      "Eli Pariser",
      "DuckDuckGo",
      "Knowbot Information Service",
      "Ixquick",
      "Egerin",
      "Wikia Search",
      "Distributed computing",
      "Seznam.cz",
      "Boolean operator (computer programming)",
      "Swisscows",
      "Live Search",
      "Webmaster",
      "Forbes",
      "Bibcode (identifier)",
      "Bookmark (digital)",
      "Database",
      "Msnbot",
      "CiteSeerX (identifier)",
      "Ask.com",
      "Representational State Transfer",
      "Paid inclusion",
      "Organic search",
      "Vertical search",
      "ChaCha (search engine)",
      "Web page",
      "Natural language search engine",
      "Video",
      "Blackle",
      "W3Catalog",
      "Voice search",
      "KidzSearch",
      "Desktop search",
      "McGill University",
      "Figure of merit",
      "Itpints",
      "Climate change denial",
      "SearXNG",
      "Petal Search",
      "Yahoo! Search",
      "Home page",
      "Archie (search engine)",
      "OpenSearch (specification)",
      "Filter bubble",
      "Yahoo! Search Marketing",
      "Northern Light Group",
      "List of academic databases and search engines",
      "Hiroko Tabuchi",
      "WebCrawler",
      "Algorithm",
      "Spell checker",
      "Search aggregator",
      "HTML",
      "Tim Berners-Lee",
      "Veronica (search engine)",
      "Metadata",
      "Judit Bar-Ilan",
      "Hyper Search",
      "Goby Inc.",
      "Vannevar Bush",
      "News",
      "Content-control software",
      "Comparison of search engines",
      "ISSN (identifier)",
      "Google Bombing",
      "Collaborative search engine",
      "Montreal, Quebec",
      "Image retrieval",
      "ISBN (identifier)",
      "File Transfer Protocol",
      "HuffPost",
      "Inverted index",
      "Web query",
      "Infoseek",
      "Cross-language information retrieval",
      "Search engine optimization",
      "Google Search",
      "Helen Nissenbaum",
      "Search engine results page",
      "Metasearch engine",
      "S2CID (identifier)",
      "Request for Comments",
      "AlltheWeb",
      "InfoSpace",
      "Haram",
      "Cache (computing)",
      "Holocaust denial",
      "Search engine cache",
      "Semantic Web",
      "Dot-com bubble",
      "Scroogle",
      "TinEye",
      "Jughead (search engine)",
      "Kartoo",
      "ImHalal",
      "Arab",
      "Qwant",
      "Algorithmic bias",
      "Empas",
      "Spamdexing",
      "Data mining",
      "Forestle",
      "Mark McCahill",
      "Perplexity AI",
      "Looksmart",
      "Webserver",
      "YaCy",
      "Cross-language search",
      "Multimedia search",
      "Stack Exchange",
      "Microsoft",
      "Spider trap",
      "Muxlim",
      "Website mirroring software",
      "Ranking (information retrieval)",
      "Mobile app",
      "Search engine privacy",
      "National Center for Supercomputing Applications",
      "Quaero",
      "University of Geneva",
      "Yandex",
      "Parsijoo",
      "Internet search",
      "PageRank",
      "Brave Search",
      "Nate (web portal)",
      "Search Engine Watch",
      "Barry Schwartz (technologist)",
      "End-user",
      "Local search (Internet)",
      "University of Minnesota",
      "Neeva",
      "LeapFish",
      "Timeline of web search engines",
      "Halalgoogling",
      "Daum (web portal)",
      "Yahoo! Directory",
      "Semantic search",
      "Magellan (search engine)",
      "Archie search engine",
      "Web development tools",
      "Web proxy",
      "JavaScript",
      "Robin Li",
      "A9.com",
      "Kiddle (search engine)",
      "Archie Comics",
      "Web robot",
      "Jerry Yang",
      "Search.ch",
      "Pipilika",
      "Cascading Style Sheets",
      "Web directory",
      "Web archiving",
      "Search/Retrieve Web Service",
      "JumpStation",
      "Evaluation measures (information retrieval)",
      "Document retrieval",
      "Muslim",
      "JSTOR (identifier)",
      "Search by sound",
      "Distributed web crawling",
      "Terrorism in Ireland",
      "Information retrieval",
      "Presearch (search engine)",
      "World-Wide Web Worm",
      "Focused crawler",
      "Web form",
      "Software engine",
      "Mullvad Leta",
      "Cuil",
      "Gigablast",
      "Federated search",
      "Baidu",
      "Dogpile",
      "ITHAKA",
      "Wikiseek",
      "GenieKnows",
      "Google search",
      "Fireball (search engine)",
      "Content moderation",
      "Middle East",
      "Gopher (protocol)",
      "Concept search",
      "FTP",
      "Powerset (company)",
      "Sogou",
      "Sharia",
      "Selection-based search",
      "PMID (identifier)",
      "CERN",
      "Index (search engine)",
      "Microsoft Bing",
      "Relevance (information retrieval)",
      "OCLC (identifier)",
      "World Wide Web",
      "Carnegie Mellon University",
      "Go.com",
      "Cliqz",
      "Advertising",
      "Vivisimo",
      "Sitemap",
      "Internet",
      "Computer science",
      "123people",
      "Robots exclusion standard",
      "Google",
      "TechTarget",
      "Text mining",
      "Oscar Nierstrasz",
      "Search engine (computing)",
      "You.com",
      "Alan Emtage",
      "AOL",
      "Yippy",
      "Volunia",
      "Neo-Nazism",
      "Data center",
      "Bing (search engine)",
      "Kagi (search engine)",
      "HotBot",
      "Contextual advertising",
      "Inktomi",
      "NPR",
      "RankDex",
      "Lycos",
      "Perl",
      "Ahmia",
      "Inktomi (company)",
      "Doi (identifier)",
      "Conspiracy theory",
      "Web server",
      "Naver",
      "Mystery Seeker",
      "World Wide Web Wanderer",
      "Image",
      "Snippet (programming)",
      "Search Engine Roundtable",
      "Software system",
      "Enterprise search",
      "Search engine indexing",
      "Social search",
      "Rank order",
      "Sproose",
      "Search engine manipulation effect",
      "ALIWEB",
      "Search engine (disambiguation)",
      "Search/Retrieve via URL",
      "Netscape",
      "List of search engines",
      "Peer-to-peer",
      "Yahoo!",
      "Halal",
      "Multisearch",
      "Excite (web portal)",
      "Sputnik (search engine)",
      "Scientific American",
      "Startpage.com",
      "Yooz",
      "Google effect",
      "SearchMe",
      "Teoma",
      "Content-based image retrieval",
      "Web query classification",
      "Blekko",
      "Web browser",
      "Yandex Search",
      "Veronica Lodge",
      "Web portal",
      "Exalead",
      "Web crawler",
      "Chorki (search engine)",
      "PCMag",
      "Soso (search engine)",
      "Searx",
      "MetaCrawler",
      "Mojeek",
      "Massachusetts Institute of Technology",
      "Link analysis",
      "Weighting",
      "Gaming the system",
      "Proximity search (text)",
      "Comparison of web search engines",
      "Video search engine",
      "Viewzi",
      "Yahoo Search",
      "David Filo",
      "MSN Search",
      "KidRex",
      "Hyperlink",
      "AltaVista",
      "Z39.50",
      "Ecosia",
      "Indian subcontinent",
      "WHOIS",
      "Meta tags",
      "Nature (journal)",
      "Web indexing",
      "Picollator",
      "Blackle.com",
      "Memex",
      "Deep web",
      "Web search query",
      "The Atlantic",
      "SAPO (company)",
      "Question answering",
      "Jughead Jones",
      "Web search engine",
      "Computer file",
      "Initial public offering",
      "AOL Search",
      "Yebol",
      "Yahoo! Native",
      "MetaGer",
      "Linkrot",
      "Search results",
      "Youdao",
      "Larry Page",
      "Aliweb"
    ]
  },
  "Web search engine": {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine",
    "title": "Web search engine",
    "content": "A search engine is a software system that provides hyperlinks to web pages , and other relevant information on the Web in response to a user's query . The user enters a query in a web browser or a mobile app , and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images , videos , or news . For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers . This can include data mining the files and databases stored on web servers , although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s; however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo ! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %). [ 1 ] Notably, this marks the first time in over a decade that Google's share has fallen below the 90% threshold. The business of websites improving their visibility in search results , known as marketing and optimization , has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex . [ 2 ] He described this system in an article titled \" As We May Think \" in The Atlantic Monthly . [ 3 ] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks . [ 4 ] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank . [ 5 ] [ 6 ] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982, [ 7 ] and the Knowbot Information Service multi-network user search was first implemented in 1989. [ 8 ] The first well documented search engine that searched content files, namely FTP files, was Archie , which debuted on 10 September 1990. [ 9 ] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver . One snapshot of the list in 1992 remains, [ 10 ] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\". [ 11 ] The first tool used for searching content (as opposed to users) on the Internet was Archie . [ 12 ] The name stands for \"archive\" without the \"v\". [ 13 ] It was created by Alan Emtage , [ 13 ] [ 14 ] [ 15 ] [ 16 ] computer science student at McGill University in Montreal, Quebec , Canada. The program downloaded the directory listings of all the files located on public anonymous FTP ( File Transfer Protocol ) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota ) led to two new search programs, Veronica and Jughead . Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \" Archie Search Engine \" was not a reference to the Archie comic book series, \" Veronica \" and \" Jughead \" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog , the web's first primitive search engine, released on September 2, 1993. [ 17 ] In June 1993, Matthew Gray, then at MIT , produced what was probably the first web robot , the Perl -based World Wide Web Wanderer , and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot , but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993 [ 18 ] by Jonathon Fletcher ) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler , which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page , which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University ) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search . [ 19 ] The first product from Yahoo! , founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory . In 1995, a search function was added, allowing users to search Yahoo! Directory. [ 20 ] [ 21 ] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan , Excite , Infoseek , Inktomi , Northern Light , and AltaVista . Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking [ 22 ] [ 23 ] [ 24 ] and received a US patent for the technology. [ 25 ] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing, [ 26 ] predating the very similar algorithm patent filed by Google two years later in 1998. [ 27 ] Larry Page referenced Li's work in some of his U.S. patents for PageRank. [ 28 ] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite. [ 29 ] [ 30 ] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com . This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet. [ 31 ] [ 32 ] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s. [ 33 ] Several companies entered the market spectacularly, receiving record gains during their initial public offerings . Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble , a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence. [ 34 ] The company achieved better results for many searches with an algorithm called PageRank , as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page , the later founders of Google. [ 6 ] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li 's earlier RankDex patent as an influence. [ 28 ] [ 24 ] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal . In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker . By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart , blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot ). Microsoft's rebranded search engine, Bing , was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019, [update] active search engine crawlers include those of Baidu , Bing, Brave , [ 35 ] Google, DuckDuckGo , Gigablast , Mojeek , Sogou and Yandex . A search engine maintains the following processes in near real time: [ 36 ] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt , addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript , Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags . After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\". [ 38 ] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML -based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible. [ 37 ] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider , the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed. [ 37 ] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot . Typically, when a user enters a query into a search engine it is a few keywords . [ 39 ] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes. [ 37 ] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI - or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range. [ 40 ] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query . Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search , which allows users to define the distance between keywords. [ 37 ] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another. [ 37 ] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \" inverted index \" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads. [ 41 ] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches. [ 42 ] As of January 2022, [update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share. [ 43 ] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice. [ 44 ] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%. [ 45 ] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity. [ 46 ] [ 47 ] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints. [ 48 ] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023. [ 49 ] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share. [ 50 ] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind. [ 51 ] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide [ 52 ] [ 53 ] and the underlying assumptions about the technology. [ 54 ] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws). [ 55 ] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results. [ 56 ] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries. [ 53 ] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines, [ 57 ] and the representation of certain controversial topics in their results, such as terrorism in Ireland , [ 58 ] climate change denial , [ 59 ] and conspiracy theories . [ 60 ] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011. [ 61 ] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo . However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble. [ 62 ] [ 63 ] [ 64 ] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search, [ 64 ] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets. [ 65 ] [ 63 ] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent , to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches . More than usual safe search filters, these Islamic web portals categorizing websites into being either \" halal \" or \" haram \", based on interpretation of Sharia law . ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others). [ 66 ] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google, [ 67 ] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith. [ 68 ] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap , but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking , because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking. [ 69 ] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders . All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders. [ 70 ] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it. However, both types of ranking are vulnerable to fraud, (see Gaming the system ), and both need technical countermeasures to try to deal with this. The first web search engine was Archie , created in 1990 [ 71 ] by Alan Emtage , a student at McGill University in Montreal. The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database. [ 72 ] In 1993, the University of Nevada System Computing Services group developed Veronica . [ 71 ] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges. [ 72 ] The World Wide Web Wanderer , developed by Matthew Gray in 1993 [ 73 ] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos. [ 72 ] Excite , initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet. Their project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers. [ 72 ] Excite was the first serious commercial search engine which launched in 1995. [ 74 ] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite [ 75 ] [ 39 ] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang , created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks. [ 76 ] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers ; ants or spiders), those that are powered by human submissions, and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing . Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google ), database or structured data search engines (e.g. Dieselpoint ), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo! , utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The most prominent example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings. [ 77 ]",
    "links": [
      "Graphical user interface",
      "Audio search engine",
      "Wide area information server",
      "Info.com",
      "Sogou.com",
      "Search.com",
      "Search engine marketing",
      "Robots.txt",
      "As We May Think",
      "Web crawling",
      "Website",
      "Jonathon Fletcher",
      "Jerry Yang (entrepreneur)",
      "Iterative algorithm",
      "Hdl (identifier)",
      "Sergey Brin",
      "Social media optimization",
      "Goo.ne.jp",
      "Internet search engines and libraries",
      "Keyword (Internet search)",
      "Yahoo",
      "Dieselpoint",
      "Search engine technology",
      "Online search",
      "Eli Pariser",
      "DuckDuckGo",
      "Knowbot Information Service",
      "Ixquick",
      "Egerin",
      "Wikia Search",
      "Distributed computing",
      "Seznam.cz",
      "Boolean operator (computer programming)",
      "Swisscows",
      "Live Search",
      "Webmaster",
      "Forbes",
      "Bibcode (identifier)",
      "Bookmark (digital)",
      "Database",
      "Msnbot",
      "CiteSeerX (identifier)",
      "Ask.com",
      "Representational State Transfer",
      "Paid inclusion",
      "Organic search",
      "Vertical search",
      "ChaCha (search engine)",
      "Web page",
      "Natural language search engine",
      "Video",
      "Blackle",
      "W3Catalog",
      "Voice search",
      "KidzSearch",
      "Desktop search",
      "McGill University",
      "Figure of merit",
      "Itpints",
      "Climate change denial",
      "SearXNG",
      "Petal Search",
      "Yahoo! Search",
      "Home page",
      "Archie (search engine)",
      "OpenSearch (specification)",
      "Filter bubble",
      "Yahoo! Search Marketing",
      "Northern Light Group",
      "List of academic databases and search engines",
      "Hiroko Tabuchi",
      "WebCrawler",
      "Algorithm",
      "Spell checker",
      "Search aggregator",
      "HTML",
      "Tim Berners-Lee",
      "Veronica (search engine)",
      "Metadata",
      "Judit Bar-Ilan",
      "Hyper Search",
      "Goby Inc.",
      "Vannevar Bush",
      "News",
      "Content-control software",
      "Comparison of search engines",
      "ISSN (identifier)",
      "Google Bombing",
      "Collaborative search engine",
      "Montreal, Quebec",
      "Image retrieval",
      "ISBN (identifier)",
      "File Transfer Protocol",
      "HuffPost",
      "Inverted index",
      "Web query",
      "Infoseek",
      "Cross-language information retrieval",
      "Search engine optimization",
      "Google Search",
      "Helen Nissenbaum",
      "Search engine results page",
      "Metasearch engine",
      "S2CID (identifier)",
      "Request for Comments",
      "AlltheWeb",
      "InfoSpace",
      "Haram",
      "Cache (computing)",
      "Holocaust denial",
      "Search engine cache",
      "Semantic Web",
      "Dot-com bubble",
      "Scroogle",
      "TinEye",
      "Jughead (search engine)",
      "Kartoo",
      "ImHalal",
      "Arab",
      "Qwant",
      "Algorithmic bias",
      "Empas",
      "Spamdexing",
      "Data mining",
      "Forestle",
      "Mark McCahill",
      "Perplexity AI",
      "Looksmart",
      "Webserver",
      "YaCy",
      "Cross-language search",
      "Multimedia search",
      "Stack Exchange",
      "Microsoft",
      "Spider trap",
      "Muxlim",
      "Website mirroring software",
      "Ranking (information retrieval)",
      "Mobile app",
      "Search engine privacy",
      "National Center for Supercomputing Applications",
      "Quaero",
      "University of Geneva",
      "Yandex",
      "Parsijoo",
      "Internet search",
      "PageRank",
      "Brave Search",
      "Nate (web portal)",
      "Search Engine Watch",
      "Barry Schwartz (technologist)",
      "End-user",
      "Local search (Internet)",
      "University of Minnesota",
      "Neeva",
      "LeapFish",
      "Timeline of web search engines",
      "Halalgoogling",
      "Daum (web portal)",
      "Yahoo! Directory",
      "Semantic search",
      "Magellan (search engine)",
      "Archie search engine",
      "Web development tools",
      "Web proxy",
      "JavaScript",
      "Robin Li",
      "A9.com",
      "Kiddle (search engine)",
      "Archie Comics",
      "Web robot",
      "Jerry Yang",
      "Search.ch",
      "Pipilika",
      "Cascading Style Sheets",
      "Web directory",
      "Web archiving",
      "Search/Retrieve Web Service",
      "JumpStation",
      "Evaluation measures (information retrieval)",
      "Document retrieval",
      "Muslim",
      "JSTOR (identifier)",
      "Search by sound",
      "Distributed web crawling",
      "Terrorism in Ireland",
      "Information retrieval",
      "Presearch (search engine)",
      "World-Wide Web Worm",
      "Focused crawler",
      "Web form",
      "Software engine",
      "Mullvad Leta",
      "Cuil",
      "Gigablast",
      "Federated search",
      "Baidu",
      "Dogpile",
      "ITHAKA",
      "Wikiseek",
      "GenieKnows",
      "Google search",
      "Fireball (search engine)",
      "Content moderation",
      "Middle East",
      "Gopher (protocol)",
      "Concept search",
      "FTP",
      "Powerset (company)",
      "Sogou",
      "Sharia",
      "Selection-based search",
      "PMID (identifier)",
      "CERN",
      "Index (search engine)",
      "Microsoft Bing",
      "Relevance (information retrieval)",
      "OCLC (identifier)",
      "World Wide Web",
      "Carnegie Mellon University",
      "Go.com",
      "Cliqz",
      "Advertising",
      "Vivisimo",
      "Sitemap",
      "Internet",
      "Computer science",
      "123people",
      "Robots exclusion standard",
      "Google",
      "TechTarget",
      "Text mining",
      "Oscar Nierstrasz",
      "Search engine (computing)",
      "You.com",
      "Alan Emtage",
      "AOL",
      "Yippy",
      "Volunia",
      "Neo-Nazism",
      "Data center",
      "Bing (search engine)",
      "Kagi (search engine)",
      "HotBot",
      "Contextual advertising",
      "Inktomi",
      "NPR",
      "RankDex",
      "Lycos",
      "Perl",
      "Ahmia",
      "Inktomi (company)",
      "Doi (identifier)",
      "Conspiracy theory",
      "Web server",
      "Naver",
      "Mystery Seeker",
      "World Wide Web Wanderer",
      "Image",
      "Snippet (programming)",
      "Search Engine Roundtable",
      "Software system",
      "Enterprise search",
      "Search engine indexing",
      "Social search",
      "Rank order",
      "Sproose",
      "Search engine manipulation effect",
      "ALIWEB",
      "Search engine (disambiguation)",
      "Search/Retrieve via URL",
      "Netscape",
      "List of search engines",
      "Peer-to-peer",
      "Yahoo!",
      "Halal",
      "Multisearch",
      "Excite (web portal)",
      "Sputnik (search engine)",
      "Scientific American",
      "Startpage.com",
      "Yooz",
      "Google effect",
      "SearchMe",
      "Teoma",
      "Content-based image retrieval",
      "Web query classification",
      "Blekko",
      "Web browser",
      "Yandex Search",
      "Veronica Lodge",
      "Web portal",
      "Exalead",
      "Web crawler",
      "Chorki (search engine)",
      "PCMag",
      "Soso (search engine)",
      "Searx",
      "MetaCrawler",
      "Mojeek",
      "Massachusetts Institute of Technology",
      "Link analysis",
      "Weighting",
      "Gaming the system",
      "Proximity search (text)",
      "Comparison of web search engines",
      "Video search engine",
      "Viewzi",
      "Yahoo Search",
      "David Filo",
      "MSN Search",
      "KidRex",
      "Hyperlink",
      "AltaVista",
      "Z39.50",
      "Ecosia",
      "Indian subcontinent",
      "WHOIS",
      "Meta tags",
      "Nature (journal)",
      "Web indexing",
      "Picollator",
      "Blackle.com",
      "Memex",
      "Deep web",
      "Web search query",
      "The Atlantic",
      "SAPO (company)",
      "Question answering",
      "Jughead Jones",
      "Computer file",
      "Initial public offering",
      "AOL Search",
      "Yebol",
      "Yahoo! Native",
      "MetaGer",
      "Linkrot",
      "Search results",
      "Youdao",
      "Larry Page",
      "Aliweb"
    ]
  },
  "Inverted index": {
    "url": "https://en.wikipedia.org/wiki/Inverted_index",
    "title": "Inverted index",
    "content": "In computer science , an inverted index (also referred to as a postings list , postings file , or inverted file ) is a database index storing a mapping from content, such as words or numbers, to its locations in a table , or in a document or a set of documents (named in contrast to a forward index , which maps from documents to content). [ 1 ] The purpose of an inverted index is to allow fast full-text searches , at a cost of increased processing when a document is added to the database. [ 2 ] The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, [ 3 ] used on a large scale for example in search engines . Additionally, several significant general-purpose mainframe -based database management systems have used inverted list architectures, including ADABAS , DATACOM/DB , and Model 204 . There are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file ) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list ) additionally contains the positions of each word within a document. [ 4 ] The latter form offers more functionality (like phrase searches ), but needs more processing power and space to be created. The inverted index data structure is a central component of a typical search engine indexing algorithm . [ 5 ] A goal of a search engine implementation is to optimize the speed of the query: find the documents where word X occurs. [ 6 ] Once a forward index is developed, which stores lists of words per document, it is next inverted to develop an inverted index. Querying the forward index would require sequential iteration through each document and to each word to verify a matching document. The time, memory, and processing resources to perform such a query are not always technically realistic. Instead of listing the words per document in the forward index, the inverted index data structure is developed which lists the documents per word. With the inverted index created, the query can be resolved by jumping to the word ID (via random access ) in the inverted index. In pre-computer times, concordances to important books were manually assembled. These were effectively inverted indexes with a small amount of accompanying commentary that required a tremendous amount of effort to produce. In bioinformatics, inverted indexes are very important in the sequence assembly of short fragments of sequenced DNA. One way to find the source of a fragment is to search for it against a reference DNA sequence. A small number of mismatches (due to differences between the sequenced DNA and reference DNA, or errors) can be accounted for by dividing the fragment into smaller fragments—at least one subfragment is likely to match the reference DNA sequence. The matching requires constructing an inverted index of all substrings of a certain length from the reference DNA sequence. Since the human DNA contains more than 3 billion base pairs, and we need to store a DNA substring for every index and a 32-bit integer for index itself, the storage requirement for such an inverted index would probably be in the tens of gigabytes. For historical reasons, inverted list compression and bitmap compression were developed as separate lines of research, and only later were recognized as solving essentially the same problem. [ 7 ]",
    "links": [
      "Data structure",
      "Concordance (publishing)",
      "Index (search engine)",
      "Database index",
      "Addison-Wesley",
      "Doi (identifier)",
      "DATACOM/DB",
      "Reverse index",
      "Ricardo Baeza-Yates",
      "Search engine",
      "Document retrieval",
      "Forward index",
      "S2CID (identifier)",
      "Database management systems",
      "Random access",
      "Vector space model",
      "Phrase search",
      "Sequence assembly",
      "Hdl (identifier)",
      "ISBN (identifier)",
      "The Art of Computer Programming",
      "Model 204",
      "Reading, Massachusetts",
      "Association for Computing Machinery",
      "ADABAS",
      "Computer science",
      "Full-text search",
      "Donald Knuth",
      "Rosetta Code",
      "Mainframe computer",
      "Table (database)"
    ]
  },
  "Document retrieval": {
    "url": "https://en.wikipedia.org/wiki/Document_retrieval",
    "title": "Document retrieval",
    "content": "Document retrieval is defined as the matching of some stated user query against a set of free-text records. These records could be any type of mainly unstructured text , such as newspaper articles , real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words. Document retrieval is sometimes referred to as, or as a branch of, text retrieval . Text retrieval is a branch of information retrieval where the information is stored primarily in the form of text . Text databases became decentralized thanks to the personal computer . Text retrieval is a critical area of study today, since it is the fundamental basis of all internet search engines . Document retrieval systems find information to given criteria by matching text records ( documents ) against user queries, as opposed to expert systems that answer questions by inferring over a logical knowledge database . A document retrieval system consists of a database of documents, a classification algorithm to build a full text index, and a user interface to access the database. A document retrieval system has two main tasks: Internet search engines are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using statistical or natural language processing techniques. There are two main classes of indexing schemata for document retrieval systems: form based (or word based ), and content based indexing. The document classification scheme (or indexing algorithm ) in use determines the nature of the document retrieval system. Form based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A suffix tree algorithm is an example for form based indexing. The content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an inverted index algorithm. A signature file is a technique that creates a quick and dirty filter, for example a Bloom filter , that will keep all the documents that match to the query and hopefully a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to inverted indexes in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted indexes in certain environments. The PubMed [ 1 ] form interface features the \"related articles\" search which works through a comparison of words from the documents' title, abstract, and MeSH terms using a word-weighted algorithm. [ 2 ] [ 3 ]",
    "links": [
      "Natural language processing",
      "Latent semantic indexing",
      "Knowledge base",
      "Doi (identifier)",
      "Expert system",
      "Suffix tree",
      "Newspaper article",
      "Search engine",
      "S2CID (identifier)",
      "Search engines",
      "CiteSeerX (identifier)",
      "Compound term processing",
      "Information retrieval",
      "Inverted indexes",
      "Bloom filter",
      "PMID (identifier)",
      "PageRank",
      "Statistical",
      "Personal computer",
      "Document classification",
      "Internet",
      "PMC (identifier)",
      "Enterprise search",
      "Classification algorithm",
      "Search engine indexing",
      "Inverted index",
      "Free-text",
      "Inference",
      "Medical Subject Headings",
      "Full text search",
      "Evaluation measures (information retrieval)",
      "Natural language",
      "PubMed"
    ]
  },
  "Text mining": {
    "url": "https://en.wikipedia.org/wiki/Text_mining",
    "title": "Text mining",
    "content": "Text mining , text data mining ( TDM ) or text analytics is the process of deriving high-quality information from text . It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" [ 1 ] Written resources may include websites , books , emails , reviews , and articles. [ 2 ] High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning . According to Hotho et al. (2005), there are three perspectives of text mining: information extraction , data mining , and knowledge discovery in databases (KDD). [ 3 ] Text mining usually involves the process of structuring the input text (usually parsing , along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database ), deriving patterns within the structured data , and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance , novelty , and interest. Typical text mining tasks include text categorization , text clustering , concept/entity extraction, production of granular taxonomies, sentiment analysis , document summarization , and entity relation modeling ( i.e. , learning relations between named entities ). Text analysis involves information retrieval , lexical analysis to study word frequency distributions, pattern recognition , tagging / annotation , information extraction , data mining techniques including link and association analysis, visualization , and predictive analytics . The overarching goal is, essentially, to turn text into data for analysis, via the application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information. A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted. The document is the basic element when starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections. [ 4 ] Text analytics describes a set of linguistic , statistical , and machine learning techniques that model and structure the information content of textual sources for business intelligence , exploratory data analysis , research , or investigation. [ 5 ] The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of \"text mining\" [ 6 ] in 2004 to describe \"text analytics\". [ 7 ] The latter term is now used more frequently in business settings while \"text mining\" is used in some of the earliest application areas, dating to the 1980s, [ 8 ] notably life-sciences research and government intelligence. The term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80% of business-relevant information originates in unstructured form, primarily text. [ 9 ] These techniques and processes discover and present knowledge – facts, business rules , and relationships – that is otherwise locked in textual form, impenetrable to automated processing. Subtasks—components of a larger text-analytics effort—typically include: Text mining technology is now broadly applied to a wide variety of government, research, and business needs. All these groups may use text mining for records management and searching documents relevant to their daily activities. Legal professionals may use text mining for e-discovery , for example. Governments and military groups use text mining for national security and intelligence purposes. Scientific researchers incorporate text mining approaches into efforts to organize large sets of text data (i.e., addressing the problem of unstructured data ), to determine ideas communicated through text (e.g., sentiment analysis in social media [ 15 ] [ 16 ] [ 17 ] ) and to support scientific discovery in fields such as the life sciences and bioinformatics . In business, applications are used to support competitive intelligence and automated ad placement , among numerous other activities. Many text mining software packages are marketed for security applications , especially monitoring and analysis of online plain text sources such as Internet news , blogs , etc. for national security purposes. [ 18 ] It is also involved in the study of text encryption / decryption . A range of text mining applications in the biomedical literature has been described, [ 20 ] including computational approaches to assist with studies in protein docking , [ 21 ] protein interactions , [ 22 ] [ 23 ] and protein-disease associations. [ 24 ] In addition, with large patient textual datasets in the clinical field, datasets of demographic information in population studies and adverse event reports, text mining can facilitate clinical studies and precision medicine. Text mining algorithms can facilitate the stratification and indexing of specific clinical events in large patient textual datasets of symptoms, side effects, and comorbidities from electronic health records, event reports, and reports from specific diagnostic tests. [ 25 ] One online text mining application in the biomedical literature is PubGene , a publicly accessible search engine that combines biomedical text mining with network visualization. [ 26 ] [ 27 ] GoPubMed is a knowledge-based search engine for biomedical texts. Text mining techniques also enable us to extract unknown knowledge from unstructured documents in the clinical domain [ 28 ] Text mining methods and software is also being researched and developed by major firms, including IBM and Microsoft , to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results. Within the public sector, much effort has been concentrated on creating software for tracking and monitoring terrorist activities . [ 29 ] For study purposes, Weka software is one of the most popular options in the scientific world, acting as an excellent entry point for beginners. For Python programmers, there is an excellent toolkit called NLTK for more general purposes. For more advanced programmers, there's also the Gensim library, which focuses on word embedding-based text representations. Text mining is being used by large media companies, such as the Tribune Company , to clarify information and to provide readers with greater search experiences, which in turn increases site \"stickiness\" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content. Text analytics is being used in business, particularly, in marketing, such as in customer relationship management . [ 30 ] Coussement and Van den Poel (2008) [ 31 ] [ 32 ] apply it to improve predictive analytics models for customer churn ( customer attrition ). [ 31 ] Text mining is also being applied in stock returns prediction. [ 33 ] Sentiment analysis may involve analysis of products such as movies, books, or hotel reviews for estimating how favorable a review is for the product. [ 34 ] Such an analysis may need a labeled data set or labeling of the affectivity of words. Resources for affectivity of words and concepts have been made for WordNet [ 35 ] and ConceptNet , [ 36 ] respectively. Text has been used to detect emotions in the related area of affective computing. [ 37 ] Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories. The issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within the written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health 's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within the text without removing publisher barriers to public access. Academic institutions have also become involved in the text mining initiative: Computational methods have been developed to assist with information retrieval from scientific literature. Published approaches include methods for searching, [ 41 ] determining novelty, [ 42 ] and clarifying homonyms [ 43 ] among technical reports. The automatic analysis of vast textual corpora has created the possibility for scholars to analyze millions of documents in multiple languages with very limited manual intervention. Key enabling technologies have been parsing, machine translation , topic categorization , and machine learning. The automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data. The resulting networks, which can contain thousands of nodes, are then analyzed by using tools from network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes. [ 45 ] This automates the approach introduced by quantitative narrative analysis, [ 46 ] whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object. [ 44 ] Content analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a \" big data \" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias , readability , content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. [ 47 ] [ 48 ] [ 49 ] [ 50 ] [ 51 ] The analysis of readability, gender bias and topic bias was demonstrated in Flaounas et al. [ 52 ] showing how different topics have different gender biases and levels of readability; the possibility to detect mood patterns in a vast population by analyzing Twitter content was demonstrated as well. [ 53 ] [ 54 ] Text mining computer programs are available from many commercial and open source companies and sources. Under European copyright and database laws , the mining of in-copyright works (such as by web mining ) without the permission of the copyright owner is permitted under Articles 3 and 4 of the 2019 Directive on Copyright in the Digital Single Market . A specific TDM exception for scientific research is described in article 3, whereas a more general exception described in article 4 only applies if the copyright holder has not opted out. [ 55 ] The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licenses for Europe. [ 56 ] The fact that the focus on the solution to this legal issue was licenses, and not limitations and exceptions to copyright law, led representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013. [ 57 ] US copyright law , and in particular its fair use provisions, means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea, is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one such use being text and data mining. [ 58 ] There is no exception in copyright law of Australia for text or data mining within the Copyright Act 1968 . The Australian Law Reform Commission has noted that it is unlikely that the \"research and study\" fair dealing exception would extend to cover such a topic either, given it would be beyond the \"reasonable portion\" requirement. [ 59 ] In the UK in 2014, on the recommendation of the Hargreaves review , the government amended copyright law [ 60 ] to allow text mining as a limitation and exception . It was the second country in the world to do so, following Japan , which introduced a mining-specific exception in 2009. However, owing to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions. Until recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web , text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events. For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence . In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment .",
    "links": [
      "Joint Information Systems Committee",
      "Australian Law Reform Commission",
      "Latent Dirichlet allocation",
      "Stop word",
      "Open access",
      "Corpus manager",
      "Scientific discovery",
      "Text clustering",
      "Psychological profiling",
      "Competitive intelligence",
      "Categorization",
      "Doi (identifier)",
      "Part-of-speech tagging",
      "Automatic summarization",
      "Interactive fiction",
      "AI-complete",
      "Word-sense disambiguation",
      "S2CID (identifier)",
      "University of Alberta",
      "National Centre for Text Mining",
      "Syntactic parsing (computational linguistics)",
      "Entity–relationship model",
      "Corpus linguistics",
      "Latent semantic analysis",
      "Document Type Definition",
      "Sentence extraction",
      "Bank of English",
      "Website",
      "Topic model",
      "Optical character recognition",
      "Information retrieval",
      "Machine translation",
      "Hdl (identifier)",
      "Grammar checker",
      "Name resolution (semantics and text extraction)",
      "Book",
      "Sentiment analysis",
      "Document clustering",
      "Semantic analysis (machine learning)",
      "Database Directive",
      "File system",
      "Large language model",
      "Neural machine translation",
      "University of Tokyo",
      "Document summarization",
      "Fair use",
      "Universal Dependencies",
      "European Commission",
      "Google Ngram Viewer",
      "Document",
      "Big data",
      "Formal semantics (natural language)",
      "Plain text",
      "National security",
      "Counter-intelligence",
      "Natural-language user interface",
      "Record linkage",
      "Computer-assisted reviewing",
      "Named entity recognition",
      "Security appliance",
      "Affect (psychology)",
      "Pattern matching",
      "Business intelligence",
      "Commercial software",
      "UC Berkeley School of Information",
      "List of text mining software",
      "Bag-of-words model",
      "Parsing",
      "Part of speech tagging",
      "Predictive text",
      "Bibcode (identifier)",
      "GoPubMed",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "Web mining",
      "Named-entity recognition",
      "Database",
      "Association of European Research Libraries",
      "Parallel text",
      "Word embedding",
      "Argument mining",
      "SpaCy",
      "Ontology learning",
      "CiteSeerX (identifier)",
      "Data mining",
      "Decryption",
      "Protein docking",
      "Customer attrition",
      "Spam filter",
      "DBpedia",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Directive on Copyright in the Digital Single Market",
      "Open source",
      "Computational linguistics",
      "Information extraction",
      "Google Book Search Settlement Agreement",
      "Collocation extraction",
      "Word2vec",
      "Natural Language Toolkit",
      "Thesaurus (information retrieval)",
      "Email",
      "Pachinko allocation",
      "Lexical resource",
      "Word-sense induction",
      "Speech synthesis",
      "Biology",
      "Document processing",
      "PMID (identifier)",
      "Language model",
      "Wikidata",
      "Statistical",
      "Shallow parsing",
      "Text Analysis Portal for Research",
      "Annotation",
      "Life sciences",
      "Social sciences",
      "Microsoft",
      "Speech segmentation",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Copyright law of the United States",
      "Homonym",
      "National Institutes of Health",
      "Bioinformatics",
      "Limitations and exceptions to copyright",
      "Document-term matrix",
      "Natural language",
      "Market sentiment",
      "Structured data",
      "Copyright law of the European Union",
      "Small language model",
      "Speech corpus",
      "Natural language processing",
      "Internet news",
      "Pronunciation assessment",
      "Relevance (information retrieval)",
      "Rule-based machine translation",
      "Hallucination (artificial intelligence)",
      "BBSRC",
      "Machine learning",
      "W-shingling",
      "Subject-verb-object",
      "Copyright Act 1968",
      "Stemming",
      "UBY",
      "Compound-term processing",
      "Information Society Directive",
      "Text simplification",
      "Bigram",
      "Semantic parsing",
      "Text segmentation",
      "GloVe",
      "Pattern recognition",
      "Example-based machine translation",
      "Coreference",
      "Index (database)",
      "Distant reading",
      "Multi-document summarization",
      "Statistical machine translation",
      "Protein interactions",
      "ConceptNet",
      "University of Manchester",
      "Semantic web",
      "Trigram",
      "Text corpus",
      "WordNet",
      "Concordancer",
      "Natural language understanding",
      "FastText",
      "BabelNet",
      "Biomedical",
      "Research council (United Kingdom)",
      "EPSRC",
      "Tribune Company",
      "Treebank",
      "BERT (language model)",
      "Knowledge discovery in databases",
      "Algorithm",
      "Semantic decomposition (natural language processing)",
      "Copyright law of Japan",
      "Tag (metadata)",
      "Full text search",
      "Lillian Lee (computer scientist)",
      "Semantic similarity",
      "Spell checker",
      "Novelty (patent)",
      "Automatic identification and data capture",
      "Explicit semantic analysis",
      "Lexical analysis",
      "Concept mining",
      "Language resource",
      "Predictive classification",
      "Semantic network",
      "Linguistics",
      "Text categorization",
      "Linguistic Linked Open Data",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Transfer-based machine translation",
      "Search engine",
      "Nature (journal)",
      "Ad serving",
      "Wayback Machine",
      "Textual entailment",
      "Speech recognition",
      "University of California, Berkeley",
      "Terminology extraction",
      "Context (language use)",
      "Information Awareness Office",
      "Unstructured data",
      "Fair dealing",
      "ISSN (identifier)",
      "List of text mining methods",
      "Lemmatisation",
      "E-discovery",
      "Gender bias",
      "Text processing",
      "Noun phrase",
      "ISBN (identifier)",
      "Information visualization",
      "Review",
      "Encryption",
      "Biomedical text mining",
      "Question answering",
      "Customer relationship management",
      "Weka (machine learning)",
      "Intelligence analyst",
      "Chatbot",
      "Document classification",
      "Automated essay scoring",
      "Copyright law of Australia",
      "PMC (identifier)",
      "Business rule",
      "Sequential pattern mining",
      "Gensim",
      "Blog",
      "Truecasing",
      "Long short-term memory",
      "Information",
      "Research",
      "Social media",
      "Exploratory data analysis",
      "FrameNet",
      "IBM",
      "Predictive analytics",
      "Dimensionality reduction",
      "Transformer (deep learning architecture)",
      "Machine-readable dictionary",
      "Content analysis",
      "PubGene",
      "Readability"
    ]
  },
  "Natural language processing": {
    "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "title": "Natural language processing",
    "content": "Natural language processing ( NLP ) is the processing of natural language information by a computer . NLP is a subfield of computer science and is closely associated with artificial intelligence . NLP is also related to information retrieval , knowledge representation , computational linguistics , and linguistics more broadly. [ 1 ] Major processing tasks in an NLP system include: speech recognition , text classification , natural language understanding , and natural language generation . Natural language processing has its roots in the 1950s. [ 2 ] Already in 1950, Alan Turing published an article titled \" Computing Machinery and Intelligence \" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. The premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law ) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar ), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. [ 9 ] Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [ 19 ] [ 20 ] such as by writing grammars or devising heuristic rules for stemming . Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: Rule-based systems are commonly used: In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter , which was caused by the inefficiencies of the rule-based approaches. [ 21 ] [ 22 ] The earliest decision trees , producing systems of hard if–then rules , were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach. A major drawback of statistical methods is that they require elaborate feature engineering . Since 2015, [ 23 ] neural network –based methods have increasingly replaced traditional statistical approaches, using semantic networks [ 24 ] and word embeddings to capture semantic properties of words. Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. Neural machine translation , based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation . The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [ 48 ] Most higher-level NLP applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" [ 49 ] Cognitive science is the interdisciplinary, scientific study of the mind and its processes. [ 50 ] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [ 51 ] Especially during the age of symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [ 52 ] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [ 55 ] functional grammar, [ 56 ] construction grammar, [ 57 ] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [ 58 ] of the ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability , e.g., under the notion of \"cognitive AI\". [ 59 ] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) [ 60 ] and developments in artificial intelligence , specifically tools and technologies using large language model approaches [ 61 ] and new directions in artificial general intelligence based on the free energy principle [ 62 ] by British neuroscientist and theoretician at University College London Karl J. Friston .",
    "links": [
      "Artificial intelligence",
      "Feature engineering",
      "Word-sense disambiguation",
      "Corpus linguistics",
      "Referring expression",
      "Hdl (identifier)",
      "Grammar checker",
      "Atomic formula",
      "Sentiment analysis",
      "Large language model",
      "Semantic analysis (machine learning)",
      "Formation rule",
      "Universal Dependencies",
      "Grammar induction",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Transformational grammar",
      "Free energy principle",
      "Word n-gram language model",
      "Rogerian psychotherapy",
      "Punctuation mark",
      "Parsing",
      "Knowledge representation and reasoning",
      "Bibcode (identifier)",
      "Sentence boundary disambiguation",
      "Association for Computational Linguistics",
      "Seq2seq",
      "Named-entity recognition",
      "Formal verification",
      "Word embedding",
      "SpaCy",
      "Ontology learning",
      "CiteSeerX (identifier)",
      "Cain's Jawbone",
      "Abstract Meaning Representation",
      "Computational linguistics",
      "Collocation extraction",
      "Racter",
      "Pachinko allocation",
      "Unsupervised learning",
      "Syntax (logic)",
      "Speech synthesis",
      "Speech segmentation",
      "Training data",
      "Outline of natural language processing",
      "ArXiv (identifier)",
      "Pronunciation assessment",
      "Multi-layer perceptron",
      "Formal methods",
      "Stemming",
      "UBY",
      "Semantic parsing",
      "Artificial intelligence in healthcare",
      "Example-based machine translation",
      "Tokenization (lexical analysis)",
      "Corner case",
      "Dialogue system",
      "Semantic similarity",
      "Spell checker",
      "Tokenization (data security)",
      "Automatic identification and data capture",
      "1 the Road",
      "Natural speech",
      "Linguistic Linked Open Data",
      "Deep learning",
      "Knowledge extraction",
      "Probabilistic context-free grammar",
      "ISSN (identifier)",
      "Natural-language processing",
      "ISBN (identifier)",
      "Term frequency-inverse document frequency",
      "Automated essay scoring",
      "PMC (identifier)",
      "Neural network (machine learning)",
      "Grammar",
      "Machine-readable dictionary",
      "Arabic language",
      "Stochastic grammar",
      "Stop word",
      "Automatic summarization",
      "Thought experiment",
      "Interactive fiction",
      "IBM alignment models",
      "S2CID (identifier)",
      "Vocabulary",
      "Latent semantic analysis",
      "Bank of English",
      "Sentence extraction",
      "Computer",
      "Semantics of logic",
      "Alan Turing",
      "Programming language theory",
      "Language modeling",
      "GPT-2",
      "ELIZA",
      "Query understanding",
      "Text-to-image generation",
      "Named entity recognition",
      "Topic segmentation",
      "Time complexity",
      "Markov model",
      "Argument scheme",
      "Natural-language understanding",
      "Language and Communication Technologies",
      "Multi-agent system",
      "John Searle",
      "Automatic translation",
      "Information extraction",
      "Thesaurus (information retrieval)",
      "Abbreviation",
      "Lexical resource",
      "Language model",
      "Multimodal sentiment analysis",
      "Shallow parsing",
      "Language technology",
      "Mathematical notation",
      "Parse tree",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Document-term matrix",
      "Natural language",
      "Formal languages",
      "Speech processing",
      "Text-to-speech",
      "Ground expression",
      "Small language model",
      "Supervised learning",
      "Poverty of the stimulus",
      "Machine learning",
      "Controlled natural language",
      "GloVe",
      "Blocks world",
      "German language",
      "Compound term processing",
      "Alphabet (formal languages)",
      "Inflectional morphology",
      "Distant reading",
      "Multi-document summarization",
      "Logic translation",
      "Text corpus",
      "Syntax analysis",
      "Morpheme",
      "SHRDLU",
      "Analog signal",
      "Semantic decomposition (natural language processing)",
      "Georgetown-IBM experiment",
      "Explicit semantic analysis",
      "Turkish language",
      "Linguistics",
      "Deep linguistic processing",
      "PropBank",
      "Textual entailment",
      "Foreign language writing aid",
      "Text processing",
      "George Lakoff",
      "Propositional calculus",
      "Chatbot",
      "Decision tree",
      "Noun",
      "Transformer (deep learning architecture)",
      "DBpedia",
      "Discourse analysis",
      "Jabberwacky",
      "Relationship extraction",
      "AI-complete",
      "Cognition",
      "Syntactic parsing (computational linguistics)",
      "Morphology (linguistics)",
      "Information retrieval",
      "Kimmo Koskenniemi",
      "Machine translation",
      "Semantics (computer science)",
      "Neural machine translation",
      "Google Ngram Viewer",
      "Dictionary.com",
      "Apertium",
      "Document AI",
      "Formal system",
      "Generative grammar",
      "Parliament of Canada",
      "Multimodal interaction",
      "Text classification",
      "Chomskyan",
      "Yoshua Bengio",
      "Recurrent neural network",
      "Formal grammar",
      "Predictive text",
      "Civilization",
      "ALPAC",
      "Discourse representation theory",
      "Parallel text",
      "Full stop",
      "Argument mining",
      "Conceptual metaphor",
      "Text-to-video model",
      "PMID (identifier)",
      "Wikidata",
      "Spanish language",
      "Artificial neural network",
      "3D model",
      "Semi-supervised learning",
      "University of Helsinki",
      "Foreign language reading aid",
      "English language",
      "Rule-based machine translation",
      "World Wide Web",
      "Lesk algorithm",
      "Semantic networks",
      "Ambiguous",
      "Lexical semantics",
      "Joseph Weizenbaum",
      "Artificial general intelligence",
      "Statistical machine translation",
      "Concordancer",
      "Bag of words",
      "FastText",
      "BabelNet",
      "Reification (linguistics)",
      "Computer science",
      "Brno University of Technology",
      "Treebank",
      "PARRY",
      "Explainable artificial intelligence",
      "Lexical analysis",
      "Text mining",
      "Language resource",
      "Meitei language",
      "Japanese language",
      "Semantic network",
      "Verb",
      "Transfer-based machine translation",
      "Speech recognition",
      "Semantic roles",
      "Predicate logic",
      "Cognitive science",
      "Lemmatisation",
      "Anaphora resolution",
      "Tomáš Mikolov",
      "Named entity",
      "Text to speech",
      "Adjective",
      "Speech act",
      "Latent Dirichlet allocation",
      "Doi (identifier)",
      "Part-of-speech tagging",
      "Discourse",
      "Coarticulation",
      "Thai language",
      "Topic model",
      "ACT-R",
      "Optical character recognition",
      "Chinese language",
      "Word segmentation",
      "Context (linguistics)",
      "Natural-language programming",
      "Native-language identification",
      "Oxford University Press",
      "Sentence breaking",
      "Automated theorem proving",
      "Formal semantics (natural language)",
      "Entity linking",
      "Text-proofing",
      "Mathematical linguistics",
      "Yingli Tian",
      "History of natural language processing",
      "Bag-of-words model",
      "Agglutination",
      "Distributional semantics",
      "N-gram",
      "Intractable problem",
      "Electronic health record",
      "Moore's law",
      "Pathological (mathematics)",
      "Scientific American",
      "Artificial intelligence detection software",
      "Automata theory",
      "Semantic role labeling",
      "Computer-assisted translation",
      "Word2vec",
      "Natural Language Toolkit",
      "Word-sense induction",
      "French language",
      "Regular expression",
      "AI winter",
      "Production (computer science)",
      "Virtual assistant",
      "Meaning (linguistics)",
      "Speech corpus",
      "Hallucination (artificial intelligence)",
      "Ancient language",
      "European Union",
      "Frame semantics (linguistics)",
      "First-order logic",
      "Pro-drop language",
      "Compound-term processing",
      "Text simplification",
      "Bigram",
      "Text segmentation",
      "Karl J. Friston",
      "Coreference",
      "Trigram",
      "Spoken dialogue systems",
      "WordNet",
      "Capitalization",
      "Natural language understanding",
      "Pronoun",
      "Transformer (machine learning model)",
      "Rhetorical structure theory",
      "BERT (language model)",
      "Part of speech",
      "Cognitive linguistics",
      "Concept mining",
      "Latent semantic indexing",
      "Natural language generation",
      "Open-world assumption",
      "Chatterbots",
      "Terminology extraction",
      "Representation learning",
      "Computing Machinery and Intelligence",
      "Closed-world assumption",
      "Well-formed formula",
      "Question answering",
      "Biomedical text mining",
      "Head-driven phrase structure grammar",
      "Query expansion",
      "Document classification",
      "Turing test",
      "Ontology (information science)",
      "Truecasing",
      "Long short-term memory",
      "FrameNet"
    ]
  },
  "Latent Dirichlet allocation": {
    "url": "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation",
    "title": "Latent Dirichlet allocation",
    "content": "In natural language processing , latent Dirichlet allocation ( LDA ) is a generative statistical model that explains how a collection of text documents can be described by a set of unobserved \"topics.\" For example, given a set of news articles, LDA might discover that one topic is characterized by words like \"president\", \"government\", and \"election\", while another is characterized by \"team\", \"game\", and \"score\". It is one of the most common topic models . The LDA model was first presented as a graphical model for population genetics by J. K. Pritchard , M. Stephens and P. Donnelly in 2000. [ 1 ] The model was subsequently applied to machine learning by David Blei , Andrew Ng , and Michael I. Jordan in 2003. [ 2 ] Although its most frequent application is in modeling text corpora, it has also been used for other problems, such as in clinical psychology, social science, and computational musicology . The core assumption of LDA is that documents are represented as a random mixture of latent topics, and each topic is characterized by a probability distribution over words. The model is a generalization of probabilistic latent semantic analysis (pLSA), differing primarily in that LDA treats the topic mixture as a Dirichlet prior, leading to more reasonable mixtures and less susceptibility to overfitting . Learning the latent topics and their associated probabilities from a corpus is typically done using Bayesian inference , often with methods like Gibbs sampling or variational Bayes . In the context of population genetics , LDA was proposed by J. K. Pritchard , M. Stephens and P. Donnelly in 2000. [ 1 ] [ 3 ] LDA was applied in machine learning by David Blei , Andrew Ng and Michael I. Jordan in 2003. [ 2 ] In population genetics, the model is used to detect the presence of structured genetic variation in a group of individuals. The model assumes that alleles carried by individuals under study have origin in various extant or past populations. The model and various inference algorithms allow scientists to estimate the allele frequencies in those source populations and the origin of alleles carried by individuals under study. The source populations can be interpreted ex-post in terms of various evolutionary scenarios. In association studies , detecting the presence of genetic structure is considered a necessary preliminary step to avoid confounding . In clinical psychology research, LDA has been used to identify common themes of self-images experienced by young people in social situations. [ 4 ] Other social scientists have used LDA to examine large sets of topical data from discussions on social media (e.g., tweets about prescription drugs). [ 5 ] Additionally, supervised Latent Dirichlet Allocation with covariates (SLDAX) has been specifically developed to combine latent topics identified in texts with other manifest variables. This approach allows for the integration of text data as predictors in statistical regression analyses, improving the accuracy of mental health predictions. One of the main advantages of SLDAX over traditional two-stage approaches is its ability to avoid biased estimates and incorrect standard errors, allowing for a more accurate analysis of psychological texts. [ 6 ] [ 7 ] In the field of social sciences, LDA has proven to be useful for analyzing large datasets, such as social media discussions. For instance, researchers have used LDA to investigate tweets discussing socially relevant topics, like the use of prescription drugs and cultural differences in China. [ 8 ] By analyzing these large text corpora, it is possible to uncover patterns and themes that might otherwise go unnoticed, offering valuable insights into public discourse and perception in real time. [ 9 ] [ 10 ] In the context of computational musicology , LDA has been used to discover tonal structures in different corpora. [ 11 ] One application of LDA in machine learning – specifically, topic discovery , a subproblem in natural language processing – is to discover topics in a collection of documents, and then automatically classify any individual document within the collection in terms of how \"relevant\" it is to each of the discovered topics. A topic is considered to be a set of terms (i.e., individual words or phrases) that, taken together, suggest a shared theme. For example, in a document collection related to pet animals, the terms dog , spaniel , beagle , golden retriever , puppy , bark , and woof would suggest a DOG_related theme, while the terms cat , siamese , Maine coon , tabby , manx , meow , purr , and kitten would suggest a CAT_related theme. There may be many more topics in the collection – e.g., related to diet, grooming, healthcare, behavior, etc. that we do not discuss for simplicity's sake. (Very common, so called stop words in a language – e.g., \"the\", \"an\", \"that\", \"are\", \"is\", etc., – would not discriminate between topics and are usually filtered out by pre-processing before LDA is performed. Pre-processing also converts terms to their \"root\" lexical forms – e.g., \"barks\", \"barking\", and \"barked\" would be converted to \"bark\".) If the document collection is sufficiently large, LDA will discover such sets of terms (i.e., topics) based upon the co-occurrence of individual terms, though the task of assigning a meaningful label to an individual topic (i.e., that all the terms are DOG_related) is up to the user, and often requires specialized knowledge (e.g., for collection of technical documents). The LDA approach assumes that: When LDA machine learning is employed, both sets of probabilities are computed during the training phase, using Bayesian methods and an expectation–maximization algorithm . LDA is a generalization of older approach of probabilistic latent semantic analysis (pLSA), The pLSA model is equivalent to LDA under a uniform Dirichlet prior distribution. [ 12 ] pLSA relies on only the first two assumptions above and does not care about the remainder. While both methods are similar in principle and require the user to specify the number of topics to be discovered before the start of training (as with k -means clustering ) LDA has the following advantages over pLSA: With plate notation , which is often used to represent probabilistic graphical models (PGMs), the dependencies among the many variables can be captured concisely. The boxes are \"plates\" representing replicates, which are repeated entities. The outer plate represents documents, while the inner plate represents the repeated word positions in a given document; each position is associated with a choice of topic and word. The variable names are defined as follows: The fact that W is grayed out means that words w i j {\\displaystyle w_{ij}} are the only observable variables , and the other variables are latent variables . As proposed in the original paper, [ 2 ] a sparse Dirichlet prior can be used to model the topic-word distribution, following the intuition that the probability distribution over words in a topic is skewed, so that only a small set of words have high probability. The resulting model is the most widely applied variant of LDA today. The plate notation for this model is shown on the right, where K {\\displaystyle K} denotes the number of topics and φ 1 , … , φ K {\\displaystyle \\varphi _{1},\\dots ,\\varphi _{K}} are V {\\displaystyle V} -dimensional vectors storing the parameters of the Dirichlet-distributed topic-word distributions ( V {\\displaystyle V} is the number of words in the vocabulary). It is helpful to think of the entities represented by θ {\\displaystyle \\theta } and φ {\\displaystyle \\varphi } as matrices created by decomposing the original document-word matrix that represents the corpus of documents being modeled. In this view, θ {\\displaystyle \\theta } consists of rows defined by documents and columns defined by topics, while φ {\\displaystyle \\varphi } consists of rows defined by topics and columns defined by words. Thus, φ 1 , … , φ K {\\displaystyle \\varphi _{1},\\dots ,\\varphi _{K}} refers to a set of rows, or vectors, each of which is a distribution over words, and θ 1 , … , θ M {\\displaystyle \\theta _{1},\\dots ,\\theta _{M}} refers to a set of rows, each of which is a distribution over topics. To actually infer the topics in a corpus, we imagine a generative process whereby the documents are created, so that we may infer, or reverse engineer, it. We imagine the generative process as follows. Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over all the words. LDA assumes the following generative process for a corpus D {\\displaystyle D} consisting of M {\\displaystyle M} documents each of length N i {\\displaystyle N_{i}} : 1. Choose θ i ∼ Dir ⁡ ( α ) {\\displaystyle \\theta _{i}\\sim \\operatorname {Dir} (\\alpha )} , where i ∈ { 1 , … , M } {\\displaystyle i\\in \\{1,\\dots ,M\\}} and D i r ( α ) {\\displaystyle \\mathrm {Dir} (\\alpha )} is a Dirichlet distribution with a symmetric parameter α {\\displaystyle \\alpha } which typically is sparse ( α < 1 {\\displaystyle \\alpha <1} ) 2. Choose φ k ∼ Dir ⁡ ( β ) {\\displaystyle \\varphi _{k}\\sim \\operatorname {Dir} (\\beta )} , where k ∈ { 1 , … , K } {\\displaystyle k\\in \\{1,\\dots ,K\\}} and β {\\displaystyle \\beta } typically is sparse 3. For each of the word positions i , j {\\displaystyle i,j} , where i ∈ { 1 , … , M } {\\displaystyle i\\in \\{1,\\dots ,M\\}} , and j ∈ { 1 , … , N i } {\\displaystyle j\\in \\{1,\\dots ,N_{i}\\}} (Note that multinomial distribution here refers to the multinomial with only one trial, which is also known as the categorical distribution .) The lengths N i {\\displaystyle N_{i}} are treated as independent of all the other data generating variables ( w {\\displaystyle w} and z {\\displaystyle z} ). The subscript is often dropped, as in the plate diagrams shown here. A formal description of LDA is as follows: We can then mathematically describe the random variables as follows: Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of statistical inference . The original paper by Pritchard et al. [ 1 ] used approximation of the posterior distribution by Monte Carlo simulation. Alternative proposal of inference techniques include Gibbs sampling . [ 13 ] The original ML paper used a variational Bayes approximation of the posterior distribution . [ 2 ] A direct optimization of the likelihood with a block relaxation algorithm proves to be a fast alternative to MCMC. [ 14 ] In practice, the optimal number of populations or topics is not known beforehand. It can be estimated by approximation of the posterior distribution with reversible-jump Markov chain Monte Carlo . [ 15 ] Alternative approaches include expectation propagation . [ 16 ] Recent research has been focused on speeding up the inference of latent Dirichlet allocation to support the capture of a massive number of topics in a large number of documents. The update equation of the collapsed Gibbs sampler mentioned in the earlier section has a natural sparsity within it that can be taken advantage of. Intuitively, since each document only contains a subset of topics K d {\\displaystyle K_{d}} , and a word also only appears in a subset of topics K w {\\displaystyle K_{w}} , the above update equation could be rewritten to take advantage of this sparsity. [ 17 ] In this equation, we have three terms, out of which two are sparse, and the other is small. We call these terms a , b {\\displaystyle a,b} and c {\\displaystyle c} respectively. Now, if we normalize each term by summing over all the topics, we get: Here, we can see that B {\\displaystyle B} is a summation of the topics that appear in document d {\\displaystyle d} , and C {\\displaystyle C} is also a sparse summation of the topics that a word w {\\displaystyle w} is assigned to across the whole corpus. A {\\displaystyle A} on the other hand, is dense but because of the small values of α {\\displaystyle \\alpha } & β {\\displaystyle \\beta } , the value is very small compared to the two other terms. Now, while sampling a topic, if we sample a random variable uniformly from s ∼ U ( s | ∣ A + B + C ) {\\displaystyle s\\sim U(s|\\mid A+B+C)} , we can check which bucket our sample lands in. Since A {\\displaystyle A} is small, we are very unlikely to fall into this bucket; however, if we do fall into this bucket, sampling a topic takes O ( K ) {\\displaystyle O(K)} time (same as the original Collapsed Gibbs Sampler). However, if we fall into the other two buckets, we only need to check a subset of topics if we keep a record of the sparse topics. A topic can be sampled from the B {\\displaystyle B} bucket in O ( K d ) {\\displaystyle O(K_{d})} time, and a topic can be sampled from the C {\\displaystyle C} bucket in O ( K w ) {\\displaystyle O(K_{w})} time where K d {\\displaystyle K_{d}} and K w {\\displaystyle K_{w}} denotes the number of topics assigned to the current document and current word type respectively. Notice that after sampling each topic, updating these buckets is all basic O ( 1 ) {\\displaystyle O(1)} arithmetic operations. Following is the derivation of the equations for collapsed Gibbs sampling , which means φ {\\displaystyle \\varphi } s and θ {\\displaystyle \\theta } s will be integrated out. For simplicity, in this derivation the documents are all assumed to have the same length N {\\displaystyle N_{}} . The derivation is equally valid if the document lengths vary. According to the model, the total probability of the model is: where the bold-font variables denote the vector version of the variables. First, φ {\\displaystyle {\\boldsymbol {\\varphi }}} and θ {\\displaystyle {\\boldsymbol {\\theta }}} need to be integrated out. All the θ {\\displaystyle \\theta } s are independent to each other and the same to all the φ {\\displaystyle \\varphi } s. So we can treat each θ {\\displaystyle \\theta } and each φ {\\displaystyle \\varphi } separately. We now focus only on the θ {\\displaystyle \\theta } part. We can further focus on only one θ {\\displaystyle \\theta } as the following: Actually, it is the hidden part of the model for the j t h {\\displaystyle j^{th}} document. Now we replace the probabilities in the above equation by the true distribution expression to write out the explicit equation. Let n j , r i {\\displaystyle n_{j,r}^{i}} be the number of word tokens in the j t h {\\displaystyle j^{th}} document with the same word symbol (the r t h {\\displaystyle r^{th}} word in the vocabulary) assigned to the i t h {\\displaystyle i^{th}} topic. So, n j , r i {\\displaystyle n_{j,r}^{i}} is three dimensional. If any of the three dimensions is not limited to a specific value, we use a parenthesized point ( ⋅ ) {\\displaystyle (\\cdot )} to denote. For example, n j , ( ⋅ ) i {\\displaystyle n_{j,(\\cdot )}^{i}} denotes the number of word tokens in the j t h {\\displaystyle j^{th}} document assigned to the i t h {\\displaystyle i^{th}} topic. Thus, the right most part of the above equation can be rewritten as: So the θ j {\\displaystyle \\theta _{j}} integration formula can be changed to: The equation inside the integration has the same form as the Dirichlet distribution . According to the Dirichlet distribution , Thus, Now we turn our attention to the φ {\\displaystyle {\\boldsymbol {\\varphi }}} part. Actually, the derivation of the φ {\\displaystyle {\\boldsymbol {\\varphi }}} part is very similar to the θ {\\displaystyle {\\boldsymbol {\\theta }}} part. Here we only list the steps of the derivation: For clarity, here we write down the final equation with both ϕ {\\displaystyle {\\boldsymbol {\\phi }}} and θ {\\displaystyle {\\boldsymbol {\\theta }}} integrated out: The goal of Gibbs Sampling here is to approximate the distribution of P ( Z ∣ W ; α , β ) {\\displaystyle P({\\boldsymbol {Z}}\\mid {\\boldsymbol {W}};\\alpha ,\\beta )} . Since P ( W ; α , β ) {\\displaystyle P({\\boldsymbol {W}};\\alpha ,\\beta )} is invariable for any of Z, Gibbs Sampling equations can be derived from P ( Z , W ; α , β ) {\\displaystyle P({\\boldsymbol {Z}},{\\boldsymbol {W}};\\alpha ,\\beta )} directly. The key point is to derive the following conditional probability: where Z ( m , n ) {\\displaystyle Z_{(m,n)}} denotes the Z {\\displaystyle Z} hidden variable of the n t h {\\displaystyle n^{th}} word token in the m t h {\\displaystyle m^{th}} document. And further we assume that the word symbol of it is the v t h {\\displaystyle v^{th}} word in the vocabulary, i.e. W ( m , n ) = v {\\displaystyle W_{(m,n)}=v} . Z − ( m , n ) {\\displaystyle {\\boldsymbol {Z_{-(m,n)}}}} denotes all the Z {\\displaystyle Z} s but Z ( m , n ) {\\displaystyle Z_{(m,n)}} . Note that Gibbs Sampling needs only to sample a value for Z ( m , n ) {\\displaystyle Z_{(m,n)}} , according to the above probability, we do not need the exact value of but the ratios among the probabilities that Z ( m , n ) {\\displaystyle Z_{(m,n)}} can take value. So, the above equation can be simplified as: Finally, let n j , r i , − ( m , n ) {\\displaystyle n_{j,r}^{i,-(m,n)}} be the same meaning as n j , r i {\\displaystyle n_{j,r}^{i}} but with the Z ( m , n ) {\\displaystyle Z_{(m,n)}} excluded. The above equation can be further simplified leveraging the property of gamma function . We first split the summation and then merge it back to obtain a k {\\displaystyle k} -independent summation, which could be dropped: Note that the same formula is derived in the article on the Dirichlet-multinomial distribution , as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network . Topic modeling is a classic solution to the problem of information retrieval using linked data and semantic web technology. [ 18 ] Related models and techniques are, among others, latent semantic indexing , independent component analysis , probabilistic latent semantic indexing , non-negative matrix factorization , and Gamma-Poisson distribution . The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model [ 19 ] follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA), [ 20 ] where topics are joined together in a hierarchy by using the nested Chinese restaurant process , whose structure is learnt from data. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model . [ 21 ] Nonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data. As noted earlier, pLSA is similar to LDA. The LDA model is essentially the Bayesian version of pLSA model. The Bayesian formulation tends to perform better on small datasets because Bayesian methods can avoid overfitting the data. For very large datasets, the results of the two models tend to converge. One difference is that pLSA uses a variable d {\\displaystyle d} to represent a document in the training set. So in pLSA, when presented with a document the model has not seen before, we fix Pr ( w ∣ z ) {\\displaystyle \\Pr(w\\mid z)} —the probability of words under topics—to be that learned from the training set and use the same EM algorithm to infer Pr ( z ∣ d ) {\\displaystyle \\Pr(z\\mid d)} —the topic distribution under d {\\displaystyle d} . Blei argues that this step is cheating because you are essentially refitting the model to the new data. In evolutionary biology, it is often natural to assume that the geographic locations of the individuals observed bring some information about their ancestry. This is the rational of various models for geo-referenced genetic data. [ 15 ] [ 22 ] Variations on LDA have been used to automatically put natural images into categories, such as \"bedroom\" or \"forest\", by treating an image as a document, and small patches of the image as words; [ 23 ] one of the variations is called spatial latent Dirichlet allocation . [ 24 ]",
    "links": [
      "Stop word",
      "Latent variable",
      "Doi (identifier)",
      "Probabilistic graphical model",
      "Part-of-speech tagging",
      "Automatic summarization",
      "Interactive fiction",
      "AI-complete",
      "Word-sense disambiguation",
      "S2CID (identifier)",
      "Plate notation",
      "Syntactic parsing (computational linguistics)",
      "Corpus linguistics",
      "Latent semantic analysis",
      "Topic model",
      "Overfitting",
      "Sentence extraction",
      "Bank of English",
      "Optical character recognition",
      "Association studies",
      "Expectation propagation",
      "Information retrieval",
      "Machine translation",
      "Gamma-Poisson distribution",
      "Grammar checker",
      "Sentiment analysis",
      "Multinomial distribution",
      "Semantic analysis (machine learning)",
      "Large language model",
      "Independent component analysis",
      "Neural machine translation",
      "Expectation–maximization algorithm",
      "Universal Dependencies",
      "Google Ngram Viewer",
      "Variational Bayesian methods",
      "Collapsed Gibbs sampling",
      "Formal semantics (natural language)",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Statistical inference",
      "Gibbs sampling",
      "Bag-of-words model",
      "Hierarchical Dirichlet process",
      "Non-negative matrix factorization",
      "Parsing",
      "Predictive text",
      "Bibcode (identifier)",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "Named-entity recognition",
      "Generative model",
      "Parallel text",
      "Word embedding",
      "Argument mining",
      "SpaCy",
      "Ontology learning",
      "Matthew Stephens (statistician)",
      "Chinese restaurant process",
      "Logistic normal distribution",
      "DBpedia",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Jonathan K. Pritchard",
      "Word2vec",
      "Computational linguistics",
      "Thesaurus (information retrieval)",
      "Information extraction",
      "Pachinko allocation",
      "Collocation extraction",
      "Natural Language Toolkit",
      "Stop words",
      "Dirichlet-multinomial distribution",
      "Lexical resource",
      "Word-sense induction",
      "Probabilistic latent semantic indexing",
      "PMID (identifier)",
      "Language model",
      "Speech synthesis",
      "Wikidata",
      "Shallow parsing",
      "Bayesian inference",
      "Speech segmentation",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Document-term matrix",
      "Bayesian estimator",
      "ArXiv (identifier)",
      "Small language model",
      "David Blei",
      "Natural language processing",
      "Allele",
      "Speech corpus",
      "Pronunciation assessment",
      "Hallucination (artificial intelligence)",
      "Rule-based machine translation",
      "Machine learning",
      "Confounding",
      "Posterior distribution",
      "Journal of Machine Learning Research",
      "Stemming",
      "UBY",
      "Computational musicology",
      "Compound-term processing",
      "Text simplification",
      "Bigram",
      "Semantic parsing",
      "Text segmentation",
      "GloVe",
      "Example-based machine translation",
      "Observable variable",
      "Variational Bayes",
      "Distant reading",
      "Multi-document summarization",
      "Tf-idf",
      "Statistical machine translation",
      "Michael I. Jordan",
      "Trigram",
      "Concordancer",
      "Text corpus",
      "WordNet",
      "Natural language understanding",
      "FastText",
      "BabelNet",
      "Reversible-jump Markov chain Monte Carlo",
      "Treebank",
      "BERT (language model)",
      "Andrew Ng",
      "Semantic decomposition (natural language processing)",
      "Categorical distribution",
      "Semantic similarity",
      "Spell checker",
      "K-means clustering",
      "Automatic identification and data capture",
      "Explicit semantic analysis",
      "Lexical analysis",
      "Text mining",
      "Concept mining",
      "Language resource",
      "Latent semantic indexing",
      "Semantic network",
      "Linguistic Linked Open Data",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Transfer-based machine translation",
      "Peter Donnelly",
      "Textual entailment",
      "MapReduce",
      "Speech recognition",
      "Terminology extraction",
      "ISSN (identifier)",
      "Lemmatisation",
      "Gamma function",
      "Text processing",
      "Probabilistic latent semantic analysis",
      "ISBN (identifier)",
      "Question answering",
      "Chatbot",
      "Document classification",
      "Automated essay scoring",
      "PMC (identifier)",
      "Dirichlet distribution",
      "Bayesian network",
      "Population genetics",
      "Truecasing",
      "Long short-term memory",
      "FrameNet",
      "Transformer (deep learning architecture)",
      "Machine-readable dictionary",
      "Infer.NET"
    ]
  },
  "Information access": {
    "url": "https://en.wikipedia.org/wiki/Information_access",
    "title": "Information access",
    "content": "Information access is the freedom or ability to identify, obtain and make use of database or information effectively. [ 1 ] There are various research efforts in information access for which the objective is to simplify and make it more effective for human users to access and further process large and unwieldy amounts of data and information . Several technologies applicable to the general area are Information Retrieval , Text Mining , Machine Translation , and Text Categorisation . [ 2 ] During discussions on free access to information as well as on information policy , information access is understood as concerning the insurance of free and closed access to information . Information access covers many issues including copyright , open source , privacy , and security . Groups such as the American Library Association , the American Association of Law Libraries , Ralph Nader 's Taxpayers Assets Project have advocated for free access to legal information . The vendor neutral citation movement in the legal field is working to ensure that courts will accept citations from cases on the web which do not have the traditional (copyrighted) page numbers from the West Publishing company. There is a worldwide Free Access to Law Movement which advocates free access to legal information. The Wired article \" Who Owns The Law \" is an introduction to the access to legal information issue. Postsecondary organizations such as K-12 work to share information. They feel it is a legal and moral obligation to provide access (including to people with disabilities or impairments) to information through the services and programs they offer. [ 3 ] Some effects of charging for information access, such as literature searches for physicians, is studied in the article \"Fee or Free: The Effect of Charging on Information Demand\". In this study, a $5 charge resulted in a 77% decrease in searches. [ 4 ]",
    "links": [
      "Library and information science",
      "Text categorisation",
      "Text mining",
      "Intellectual property",
      "Cultural studies",
      "Ralph Nader",
      "Open access",
      "Information society",
      "Categorization",
      "Free Access to Law Movement",
      "Free culture",
      "Security",
      "Taxonomy",
      "Case citation",
      "Machine Translation",
      "West Publishing",
      "Information architecture",
      "Access to Knowledge movement",
      "Informatics",
      "Legal research",
      "Information Retrieval",
      "Databases",
      "Open content",
      "Censorship",
      "Outline of information science",
      "Access to Information",
      "Information behavior",
      "Information retrieval",
      "Knowledge organization",
      "Wired (magazine)",
      "Information seeking",
      "Copyright",
      "Open-source model",
      "Intellectual freedom",
      "Science and technology studies",
      "ERIC (identifier)",
      "Memory",
      "Bibliometrics",
      "American Library Association",
      "Privacy",
      "Data modeling",
      "Ontology (information science)",
      "Information policy",
      "Quantum information science",
      "Information",
      "Philosophy of information",
      "Legal",
      "Preservation (library and archival science)",
      "Institut de l'information scientifique et technique",
      "Information management",
      "Computer data storage",
      "Information science",
      "Information technology",
      "Library classification"
    ]
  },
  "Tf–idf": {
    "url": "https://en.wikipedia.org/wiki/Tf–idf",
    "title": "Tf–idf",
    "content": "In information retrieval , tf–idf ( term frequency–inverse document frequency , TF*IDF , TFIDF , TF–IDF , or Tf–idf ) is a measure of importance of a word to a document in a collection or corpus , adjusted for the fact that some words appear more frequently in general. [ 1 ] Like the bag-of-words model , it models a document as a multiset of words, without word order . It is a refinement over the simple bag-of-words model , by allowing the weight of words to depend on the rest of the corpus. It was often used as a weighting factor in searches of information retrieval, text mining , and user modeling . A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries used tf–idf. [ 2 ] Variations of the tf–idf weighting scheme were often used by search engines as a central tool in scoring and ranking a document's relevance given a user query . In applied search engine optimization practice, tf–idf is also described as a method for analysing term importance within web pages and supporting semantic SEO techniques. [ 3 ] One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. Karen Spärck Jones (1972) conceived a statistical interpretation of term-specificity called Inverse Document Frequency (idf), which became a cornerstone of term weighting: [ 4 ] The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs. For example, the df (document frequency) and idf for some words in Shakespeare's 37 plays might be represented as follows: We see that \" Romeo \", \" Falstaff \", and \"salad\" appears in very few plays, so seeing these words, one could get a good idea as to which play it might be. In contrast, \"good\" and \"sweet\" appears in every play and are completely uninformative as to which play it is. Term frequency, tf( t , d ) , is the relative frequency of term t within document d , where f t , d is the raw count of a term in a document, i.e., the number of times that term t occurs in document d . Note the denominator is simply the total number of terms in document d (counting each occurrence of the same term separately). There are various other ways to define term frequency: [ 5 ] : 128 The inverse document frequency is a measure of how much information the word provides, i.e., how common or rare it is across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient): with Then tf–idf is calculated as A high weight in tf–idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf–idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf–idf closer to 0. Idf was introduced as \"term specificity\" by Karen Spärck Jones in a 1972 paper. Although it has worked well as a heuristic , its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find information theoretic justifications for it. [ 7 ] Spärck Jones's own explanation did not propose much theory, aside from a connection to Zipf's law . [ 7 ] Attempts have been made to put idf on a probabilistic footing, [ 8 ] by estimating the probability that a given document d contains a term t as the relative document frequency, so that we can define idf as Namely, the inverse document frequency is the logarithm of \"inverse\" relative document frequency. This probabilistic interpretation in turn takes the same form as that of self-information . However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate event spaces for the required probability distributions : not only documents need to be taken into account, but also queries and terms. [ 7 ] Both term frequency and inverse document frequency can be formulated in terms of information theory ; it helps to understand why their product has a meaning in terms of joint informational content of a document. A characteristic assumption about the distribution p ( d , t ) {\\displaystyle p(d,t)} is that: This assumption and its implications, according to Aizawa: \"represent the heuristic that tf–idf employs.\" [ 9 ] The conditional entropy of a \"randomly chosen\" document in the corpus D {\\displaystyle D} , conditional to the fact it contains a specific term t {\\displaystyle t} (and assuming that all documents have equal probability to be chosen) is: In terms of notation, D {\\displaystyle {\\cal {D}}} and T {\\displaystyle {\\cal {T}}} are \"random variables\" corresponding to respectively draw a document or a term. The mutual information can be expressed as The last step is to expand p t {\\displaystyle p_{t}} , the unconditional probability to draw a term, with respect to the (random) choice of a document, to obtain: This expression shows that summing the Tf–idf of all possible terms and documents recovers the mutual information between documents and term taking into account all the specificities of their joint distribution. [ 9 ] Each Tf–idf hence carries the \"bit of information\" attached to a term x document pair. Tf–idf is closely related to the negative logarithmically transformed p -value from a one-tailed formulation of Fisher's exact test when the underlying corpus documents satisfy certain idealized assumptions. [ 10 ] Suppose that we have term count tables of a corpus consisting of only two documents: The calculation of tf–idf for the term \"this\" is performed as follows: In its raw frequency form, tf is just the frequency of the \"this\" for each document. In each document, the word \"this\" appears once; but as the document 2 has more words, its relative frequency is smaller. An idf is constant per corpus, and accounts for the ratio of documents that include the word \"this\". In this case, we have a corpus of two documents and all of them include the word \"this\". So tf–idf is zero for the word \"this\", which implies that the word is not very informative as it appears in all documents. The word \"example\" is more interesting - it occurs three times, but only in the second document: Finally, (using the base 10 logarithm ). The idea behind tf–idf also applies to entities other than terms. In 1998, the concept of idf was applied to citations. [ 11 ] The authors argued that \"if a very uncommon citation is shared by two documents, this should be weighted more highly than a citation made by a large number of documents\". In addition, tf–idf was applied to \"visual words\" with the purpose of conducting object matching in videos, [ 12 ] and entire sentences. [ 13 ] However, the concept of tf–idf did not prove to be more effective in all cases than a plain tf scheme (without idf). When tf–idf was applied to citations, researchers could find no improvement over a simple citation-count weight that had no idf component. [ 14 ] A number of term-weighting schemes have derived from tf–idf. One of them is TF–PDF (term frequency * proportional document frequency). [ 15 ] TF–PDF was introduced in 2001 in the context of identifying emerging topics in the media. The PDF component measures the difference of how often a term occurs in different domains. Another derivate is TF–IDuF. In TF–IDuF, [ 16 ] idf is not calculated based on the document corpus that is to be searched or recommended. Instead, idf is calculated on users' personal document collections. The authors report that TF–IDuF was equally effective as tf–idf but could also be applied in situations when, e.g., a user modeling system has no access to a global document corpus. The DELTA TF-IDF [ 17 ] derivative uses the difference in importance of a term across two specific classes, like positive and negative sentiment. For example, it can assign a high score to a word like \"excellent\" in positive reviews and a low score to the same word in negative reviews. This helps identify words that strongly indicate the sentiment of a document, potentially leading to improved accuracy in text classification tasks.",
    "links": [
      "ArXiv (identifier)",
      "Information theory",
      "Text mining",
      "Probability distribution",
      "Latent Dirichlet allocation",
      "Kullback–Leibler divergence",
      "Ranking (information retrieval)",
      "Bag-of-words model",
      "Word count",
      "Relevance (information retrieval)",
      "Doi (identifier)",
      "Lucene",
      "John Falstaff",
      "Logarithmic scale",
      "Search engine",
      "Wayback Machine",
      "S2CID (identifier)",
      "Frequency (statistics)",
      "Mutual information",
      "Fisher's exact test",
      "Word embedding",
      "Weighting factor",
      "CiteSeerX (identifier)",
      "Latent semantic analysis",
      "Vector space model",
      "ISSN (identifier)",
      "Boolean data type",
      "Karen Spärck Jones",
      "Information retrieval",
      "Scikit-learn",
      "Noun phrase",
      "Stephen Robertson (computer scientist)",
      "Base 10 logarithm",
      "Heuristic",
      "ISBN (identifier)",
      "McGraw-Hill",
      "Hdl (identifier)",
      "Word order",
      "Text corpus",
      "SMART Information Retrieval System",
      "Okapi BM25",
      "Multiset",
      "User modeling",
      "Probability theory",
      "Zipf's law",
      "P-value",
      "PageRank",
      "Gerard Salton",
      "Self-information",
      "Romeo",
      "Document",
      "Gensim",
      "Conditional entropy",
      "Event space",
      "Search engine optimization"
    ]
  },
  "Cross-modal retrieval": {
    "url": "https://en.wikipedia.org/wiki/Cross-modal_retrieval",
    "title": "Cross-modal retrieval",
    "content": "Cross-modal retrieval is a subfield of information retrieval that enables users to search for and retrieve information across different data modalities, such as text, images, audio, and video. [ 1 ] Unlike traditional information retrieval systems that match queries and documents within the same modality (e.g., text-to-text search), cross-modal retrieval bridges different types of media to facilitate more flexible information access. [ 2 ] [ 3 ] [ 4 ] Cross-modal retrieval addresses scenarios where the query and target documents are of different types. Common applications include: Cross-modal retrieval presents several challenges: Modern cross-modal retrieval systems employ various techniques: Cross-modal retrieval has numerous practical applications including:",
    "links": [
      "Library and information science",
      "ArXiv (identifier)",
      "Information access",
      "Cultural studies",
      "Intellectual property",
      "Natural language processing",
      "Information society",
      "Categorization",
      "Computer vision",
      "Doi (identifier)",
      "Taxonomy",
      "Information architecture",
      "Informatics",
      "Censorship",
      "Outline of information science",
      "Information behavior",
      "Information retrieval",
      "Knowledge organization",
      "Information seeking",
      "ISBN (identifier)",
      "Intellectual freedom",
      "Content-based image retrieval",
      "Science and technology studies",
      "Memory",
      "Bibliometrics",
      "Privacy",
      "Data modeling",
      "Multimodal learning",
      "Ontology (information science)",
      "Quantum information science",
      "Philosophy of information",
      "Preservation (library and archival science)",
      "Information management",
      "Computer data storage",
      "Information science",
      "Information technology",
      "Contrastive learning",
      "Library classification"
    ]
  },
  "Categorization": {
    "url": "https://en.wikipedia.org/wiki/Categorization",
    "title": "Categorization",
    "content": "Classification is the activity of assigning objects to some pre-existing classes or categories. This is distinct from the task of establishing the classes themselves (for example through cluster analysis ). [ 1 ] Examples include diagnostic tests, identifying spam emails and deciding whether to give someone a driving license. As well as 'category', synonyms or near-synonyms for 'class' include 'type', 'species', 'forms', 'order', 'concept', 'taxon', 'group', 'identification' and 'division'. The meaning of the word 'classification' (and its synonyms) may take on one of several related meanings. It may encompass both classification and the creation of classes, as for example in 'the task of categorizing pages in Wikipedia'; this overall activity is listed under taxonomy . It may refer exclusively to the underlying scheme of classes (which otherwise may be called a taxonomy). Or it may refer to the label given to an object by the classifier. Classification is a part of many different kinds of activities and is studied from many different points of view including medicine , philosophy , [ 2 ] law , anthropology , biology , taxonomy , cognition , communications , knowledge organization , psychology , statistics , machine learning , economics and mathematics . Methodological work aimed at improving the accuracy of a classifier is commonly divided between cases where there are exactly two classes ( binary classification ) and cases where there are three or more classes ( multiclass classification ). Unlike in decision theory , it is assumed that a classifier repeats the classification task over and over. And unlike a lottery , it is assumed that each classification can be either right or wrong; in the theory of measurement, classification is understood as measurement against a nominal scale. Thus it is possible to try to measure the accuracy of a classifier. Measuring the accuracy of a classifier allows a choice to be made between two alternative classifiers. This is important both when developing a classifier and in choosing which classifier to deploy. There are however many different methods for evaluating the accuracy of a classifier and no general method for determining which method should be used in which circumstances. Different fields have taken different approaches, even in binary classification (see Evaluation of binary classifiers ). In pattern recognition , error rate is popular. The Gini coefficient and KS statistic are widely used in the credit scoring industry. Sensitivity and specificity are widely used in epidemiology and medicine. Precision and recall are widely used in information retrieval. [ 3 ] Classifier accuracy depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems (a phenomenon that may be explained by the no-free-lunch theorem ).",
    "links": [
      "Medical classification",
      "International Statistical Review",
      "Folk taxonomy",
      "Lottery",
      "Doi (identifier)",
      "Machine learning",
      "Taxonomy",
      "Classification theorem",
      "Anthropology",
      "Class (disambiguation)",
      "Decision theory",
      "Psychology",
      "Economics",
      "No free lunch in search and optimization",
      "Fuzzy classification",
      "Classifier (disambiguation)",
      "Data classification (disambiguation)",
      "Cognition",
      "Law",
      "Pattern recognition",
      "Gini coefficient",
      "Knowledge organization",
      "Cluster analysis",
      "Cognitive categorization",
      "Communications",
      "Classified (disambiguation)",
      "Biology",
      "Philosophy",
      "Evaluation of binary classifiers",
      "Statistics",
      "Precision and recall",
      "Sensitivity and specificity",
      "Multiclass classification",
      "Mathematics",
      "Binary classification"
    ]
  },
  "Digital libraries": {
    "url": "https://en.wikipedia.org/wiki/Digital_libraries",
    "title": "Digital libraries",
    "content": "A digital library (also called an online library , an internet library , a digital repository , a library without walls , or a digital collection ) is an online database of digital resources that can include text, still images, audio, video, digital documents , or other digital media formats or a library accessible through the internet . Objects can consist of digitized content like print or photographs , as well as originally produced digital content like word processor files or social media posts. In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations. [ 1 ] The digital content may be stored locally, or accessed remotely via computer networks . These information retrieval systems are able to exchange information with each other through interoperability and sustainability . [ 2 ] The early history of digital libraries is not well documented, but several key thinkers are connected to the emergence of the concept. [ 3 ] Predecessors include Paul Otlet and Henri La Fontaine 's Mundaneum , an attempt begun in 1895 to gather and systematically catalogue the world's knowledge, with the hope of bringing about world peace. [ 4 ] The visions of the digital library were largely realized a century later during the great expansion of the Internet. [ 5 ] Vannevar Bush and J. C. R. Licklider are two contributors that advanced this idea into then current technology. Bush had supported research that led to the bomb that was dropped on Hiroshima . After seeing the city's destruction, he wanted to create a machine that would show how technology can lead to understanding instead of destruction. This machine would include a desk with two screens, switches and buttons, and a keyboard. [ 6 ] He named this the \" Memex \". This way individuals would be able to access stored books and files at a rapid speed. In 1956, Ford Foundation funded Licklider to analyze how libraries could be improved with technology. Almost a decade later, his book entitled \" Libraries of the Future \" included his vision. He wanted to create a system that would use computers and networks so human knowledge would be accessible for human needs and feedback would be automatic for machine purposes. This system contained three components, the corpus of knowledge, the question, and the answer. Licklider called it a procognitive system. In 1980, the role of the library in an electronic society was the focus of a clinic on library applications of data processing . Participants included Frederick Wilfrid Lancaster , Derek De Solla Price , Gerard Salton , and Michael Gorman) . [ 7 ] Early projects centered on the creation of an electronic card catalogue known as Online Public Access Catalog (OPAC). By the 1980s, the success of these endeavors resulted in OPAC replacing the traditional card catalog in many academic, public and special libraries. This permitted libraries to undertake additional rewarding co-operative efforts to support resource sharing and expand access to library materials beyond an individual library. An early example of a digital library is the Education Resources Information Center (ERIC), a database of education citations, abstracts and texts that was created in 1964 and made available online through DIALOG in 1969. [ 8 ] In 1994, digital libraries became widely visible in the research community due to a $24.4 million NSF managed program supported jointly by DARPA 's Intelligent Integration of Information (I3) program, NASA , and NSF itself. [ 9 ] Successful research proposals came from six U.S. universities. [ 10 ] The universities included Carnegie Mellon University , University of California-Berkeley , University of Michigan , University of Illinois , University of California-Santa Barbara , and Stanford University . Articles from the projects summarized their progress at their halfway point in May 1996. [ 11 ] Stanford research, by Sergey Brin and Larry Page , led to the founding of Google . Early attempts at creating a model for digital libraries included the DELOS Digital Library Reference Model [ 12 ] [ 13 ] and the 5S Framework. [ 14 ] [ 15 ] The term digital library was first popularized by the NSF / DARPA / NASA Digital Libraries Initiative in 1994. [ 16 ] With the availability of the computer networks the information resources are expected to stay distributed and accessed as needed, whereas in Vannevar Bush 's essay As We May Think (1945) they were to be collected and kept within the researcher's Memex . The term virtual library was initially used interchangeably with digital library, but is now primarily used for libraries that are virtual in other senses (such as libraries which aggregate distributed content). In the early days of digital libraries, there was discussion of the similarities and differences among the terms digital , virtual , and electronic . [ 17 ] A distinction is often made between content that was created in a digital format, known as born-digital , and information that has been converted from a physical medium, e.g. paper, through digitization . Not all electronic content is in digital data format. The term hybrid library is sometimes used for libraries that have both physical collections and electronic collections. For example, American Memory is a digital library within the Library of Congress . Some important digital libraries also serve as long term archives, such as arXiv and the Internet Archive . Others, such as the Digital Public Library of America , seek to make digital information from various institutions widely accessible online. [ 18 ] Many academic libraries are actively involved in building repositories of their institution's books, papers, theses, and other works that can be digitized or were 'born digital'. Many of these repositories are made available to the general public with few restrictions, in accordance with the goals of open access , in contrast to the publication of research in commercial journals, where the publishers usually limit access rights. Irrespective of access rights, institutional, truly free, and corporate repositories can be referred to as digital libraries. Institutional repository software is designed for archiving, organizing, and searching a library's content. Popular open-source solutions include DSpace , Greenstone Digital Library (GSDL) , EPrints , Digital Commons , and the Fedora Commons -based systems Islandora and Samvera . [ 19 ] Legal deposit is often covered by copyright legislation and sometimes by laws specific to legal deposit, and requires that one or more copies of all material published in a country should be submitted for preservation in an institution, typically the national library . Since the advent of electronic documents , legislation has had to be amended to cover the new formats, such as the 2016 amendment to the Copyright Act 1968 in Australia. [ 20 ] [ 21 ] [ 22 ] Since then various types of electronic depositories have been built. The British Library 's Publisher Submission Portal and the German model at the Deutsche Nationalbibliothek have one deposit point for a network of libraries, but public access is only available in the reading rooms in the libraries. The Australian National edeposit system has the same features, but also allows for remote access by the general public for most of the content. [ 23 ] Physical archives differ from physical libraries in several ways. Traditionally, archives are defined as: The technology used to create digital libraries is even more revolutionary for archives since it breaks down the second and third of these general rules. In other words, \"digital archives\" or \"online archives\" will still generally contain primary sources, but they are likely to be described individually rather than (or in addition to) in groups or collections. Further, because they are digital, their contents are easily reproducible and may indeed have been reproduced from elsewhere. The Oxford Text Archive is generally considered to be the oldest digital archive of academic physical primary source materials. Archives differ from libraries in the nature of the materials held. Libraries collect individual published books and serials, or bounded sets of individual items. The books and journals held by libraries are not unique, since multiple copies exist and any given copy will generally prove as satisfactory as any other copy. The material in archives and manuscript libraries are \"the unique records of corporate bodies and the papers of individuals and families\". [ 24 ] A fundamental characteristic of archives is that they have to keep the context in which their records have been created and the network of relationships between them in order to preserve their informative content and provide understandable and useful information over time. The fundamental characteristic of archives resides in their hierarchical organization expressing the context by means of the archival bond . Archival descriptions are the fundamental means to describe, understand, retrieve and access archival material. At the digital level, archival descriptions are usually encoded by means of the Encoded Archival Description XML format. The EAD is a standardized electronic representation of archival description which makes it possible to provide union access to detailed archival descriptions and resources in repositories distributed throughout the world. Given the importance of archives, a dedicated formal model, called NEsted SeTs for Object Hierarchies (NESTOR), [ 25 ] built around their peculiar constituents, has been defined. NESTOR is based on the idea of expressing the hierarchical relationships between objects through the inclusion property between sets, in contrast to the binary relation between nodes exploited by the tree. NESTOR has been used to formally extend the 5S model to define a digital archive as a specific case of digital library able to take into consideration the peculiar features of archives. A computer-aided design library or CAD library is a cloud based repository of 3D models or parts for computer-aided design (CAD), computer-aided engineering (CAE), computer-aided manufacturing (CAM), or Building information modeling (BIM). The models can be free and open source or proprietary and have to pay a subscription to have access to the CAD library 3D models. Generative AI CAD libraries are being developed using linked open data of schematics and diagrams . [ 26 ] CAD libraries can have assets such as 3D models , materials/ textures , bump maps , trees/plants, HDRIs , and different Computer graphics lighting sources to be rendered . [ 27 ] [ unreliable source? ] A 2D graphics repository/library are vector graphics or raster graphics images/ icons that can be free use or proprietary . [ 28 ] [ unreliable source? ] The advantages of digital libraries as a means of easily and rapidly accessing books, archives and images of various types are now widely recognized by commercial interests and public bodies alike. [ 29 ] Traditional libraries are limited by storage space; digital libraries have the potential to store much more information, simply because digital information requires very little physical space to contain it. [ 30 ] As such, the cost of maintaining a digital library can be much lower than that of a traditional library. A physical library must spend large sums of money paying for staff, book maintenance, rent, and additional books. Digital libraries may reduce or, in some instances, do away with these fees. Both types of library require cataloging input to allow users to locate and retrieve material. Digital libraries may be more willing to adopt innovations in technology providing users with improvements in electronic and audio book technology as well as presenting new forms of communication such as wikis and blogs; conventional libraries may consider that providing online access to their OP AC catalog is sufficient. An important advantage to digital conversion is increased accessibility to users. They also increase availability to individuals who may not be traditional patrons of a library, due to geographic location or organizational affiliation. Digital libraries offer a variety of software packages, including those tailored for kids' educational games . [ 32 ] Institutional repository software, which focuses primarily on ingest, preservation and access of locally produced documents, particularly locally produced academic outputs, can be found in Institutional repository software . This software may be proprietary, as is the case with the Library of Congress which uses Digiboard and CTS to manage digital content. [ 33 ] The design and implementation in digital libraries are constructed so computer systems and software can make use of the information when it is exchanged. These are referred to as semantic digital libraries. Semantic libraries are also used to socialize with different communities from a mass of social networks. [ 34 ] DjDL is a type of semantic digital library. Keywords-based and semantic search are the two main types of searches. A tool is provided in the semantic search that create a group for augmentation and refinement for keywords-based search. Conceptual knowledge used in DjDL is centered around two forms; the subject ontology and the set of concept search patterns based on the ontology. The three type of ontologies that are associated to this search are bibliographic ontologies , community-aware ontologies, and subject ontologies. In traditional libraries, the ability to find works of interest is directly related to how well they were cataloged. While cataloging electronic works digitized from a library's existing holding may be as simple as copying or moving a record from the print to the electronic form, complex and born-digital works require substantially more effort. To handle the growing volume of electronic publications, new tools and technologies have to be designed to allow effective automated semantic classification and searching. While full-text search can be used for some items, there are many common catalog searches which cannot be performed using full text, including: Most digital libraries provide a search interface which allows resources to be found. These resources are typically deep web (or invisible web) resources since they frequently cannot be located by search engine crawlers . Some digital libraries create special pages or sitemaps to allow search engines to find all their resources. Digital libraries frequently use the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) to expose their metadata to other digital libraries, and search engines like Google Scholar , Yahoo! and Scirus can also use OAI-PMH to find these deep web resources. [ 35 ] As with physical libraries, very relatively little is known about how users actually select books. [ 36 ] There are two general strategies for searching a federation of digital libraries: distributed searching and searching previously harvested metadata . Distributed searching typically involves a client sending multiple search requests in parallel to a number of servers in the federation. The results are gathered, duplicates are eliminated or clustered, and the remaining items are sorted and presented back to the client. Protocols like Z39.50 are frequently used in distributed searching. A benefit to this approach is that the resource-intensive tasks of indexing and storage are left to the respective servers in the federation. A drawback to this approach is that the search mechanism is limited by the different indexing and ranking capabilities of each database; therefore, making it difficult to assemble a combined result consisting of the most relevant found items. Searching over previously harvested metadata involves searching a locally stored index of information that has previously been collected from the libraries in the federation. When a search is performed, the search mechanism does not need to make connections with the digital libraries it is searching—it already has a local representation of the information. This approach requires the creation of an indexing and harvesting mechanism which operates regularly, connecting to all the digital libraries and querying the whole collection in order to discover new and updated resources. OAI-PMH is frequently used by digital libraries for allowing metadata to be harvested. A benefit to this approach is that the search mechanism has full control over indexing and ranking algorithms, possibly allowing more consistent results. A drawback is that harvesting and indexing systems are more resource-intensive and therefore expensive. Digital preservation aims to ensure that digital media and information systems are still interpretable into the indefinite future. [ 37 ] Each necessary component of this must be migrated, preserved or emulated . [ 38 ] Typically lower levels of systems ( floppy disks for example) are emulated, bit-streams (the actual files stored in the disks) are preserved and operating systems are emulated as a virtual machine . Only where the meaning and content of digital media and information systems are well understood is migration possible, as is the case for office documents. [ 38 ] [ 39 ] [ 40 ] However, at least one organization, the Wider Net Project, has created an offline digital library, the eGranary , by reproducing materials on a 6 TB hard drive . Instead of a bit-stream environment, the digital library contains a built-in proxy server and search engine so the digital materials can be accessed using a web browser . [ 41 ] Also, the materials are not preserved for the future. The eGranary is intended for use in places or situations where Internet connectivity is very slow, non-existent, unreliable, unsuitable or too expensive. In the past few years, procedures for digitizing books at high speed and comparatively low cost have improved considerably with the result that it is now possible to digitize millions of books per year. [ 42 ] The Google book-scanning project is also working with libraries to offer digitize books pushing forward on the digitize book realm. Digital libraries are hampered by copyright law because, unlike with traditional printed works, the laws of digital copyright are still being formed. The republication of material on the web by libraries may require permission from rights holders, and there is a conflict of interest between libraries and the publishers who may wish to create online versions of their acquired content for commercial purposes. In 2010, it was estimated that twenty-three percent of books in existence were created before 1923 and thus out of copyright. Of those printed after this date, only five percent were still in print as of 2010. [update] Thus, approximately seventy-two percent of books were not available to the public. [ 43 ] There is a dilution of responsibility that occurs as a result of the distributed nature of digital resources. Complex intellectual property matters may become involved since digital material is not always owned by a library. [ 44 ] The content is, in many cases, public domain or self-generated content only. Some digital libraries, such as Project Gutenberg , work to digitize out-of-copyright works and make them freely available to the public. An estimate of the number of distinct books still existent in library catalogues from 2000 BC to 1960, has been made. [ 45 ] [ 46 ] The Fair Use Provisions (17 USC § 107) under the Copyright Act of 1976 provide specific guidelines under which circumstances libraries are allowed to copy digital resources. Four factors that constitute fair use are \"Purpose of the use, Nature of the work, Amount or substantiality used and Market impact\". [ 47 ] Some digital libraries acquire a license to lend their resources. This may involve the restriction of lending out only one copy at a time for each license, and applying a system of digital rights management for this purpose. The Digital Millennium Copyright Act of 1998 was an act created in the United States to attempt to deal with the introduction of digital works. This Act incorporates two treaties from the year 1996. It criminalizes the attempt to circumvent measures which limit access to copyrighted materials. It also criminalizes the act of attempting to circumvent access control. [ 48 ] This act provides an exemption for nonprofit libraries and archives which allows up to three copies to be made, one of which may be digital. This may not be made public or distributed on the web, however. Further, it allows libraries and archives to copy a work if its format becomes obsolete. [ 48 ] Copyright issues persist. As such, proposals have been put forward suggesting that digital libraries be exempt from copyright law. Although this would be very beneficial to the public, it may have a negative economic effect and authors may be less inclined to create new works. [ 49 ] Another issue that complicates matters is the desire of some publishing houses to restrict the use of digit materials such as e-books purchased by libraries. Whereas with printed books, the library owns the book until it can no longer be circulated, publishers want to limit the number of times an e-book can be checked out before the library would need to repurchase that book. \"[HarperCollins] began licensing use of each e-book copy for a maximum of 26 loans. This affects only the most popular titles and has no practical effect on others. After the limit is reached, the library can repurchase access rights at a lower cost than the original price.\" [ 50 ] While from a publishing perspective, this sounds like a good balance of library lending and protecting themselves from a feared decrease in book sales, libraries are not set up to monitor their collections as such. They acknowledge the increased demand of digital materials available to patrons and the desire of a digital library to become expanded to include best sellers, but publisher licensing may hinder the process. Many digital libraries offer recommender systems to reduce information overload and help their users discovering relevant literature. Some examples of digital libraries offering recommender systems are IEEE Xplore , Europeana , and GESIS Sowiport . The recommender systems work mostly based on content-based filtering but also other approaches are used such as collaborative filtering and citation-based recommendations. [ 51 ] Beel et al. report that there are more than 90 different recommendation approaches for digital libraries, presented in more than 200 research articles . [ 51 ] Typically, digital libraries develop and maintain their own recommender systems based on existing search and recommendation frameworks such as Apache Lucene or Apache Mahout . Digital libraries, or at least their digital collections, also have brought their own problems and challenges in areas such as: There are many large scale digitisation projects that perpetuate these problems. Large scale digitization projects are underway at Google , the Million Book Project , and Internet Archive . With continued improvements in book handling and presentation technologies such as optical character recognition and development of alternative depositories and business models, digital libraries are rapidly growing in popularity. Just as libraries have ventured into audio and video collections, so have digital libraries such as the Internet Archive. In 2016, Google Books project received a court victory on proceeding with their book-scanning project that was halted by the Authors' Guild. [ 53 ] This helped open the road for libraries to work with Google to better reach patrons who are accustomed to computerized information. According to Larry Lannom, Director of Information Management Technology at the nonprofit Corporation for National Research Initiatives (CNRI), \"all the problems associated with digital libraries are wrapped up in archiving\". He goes on to state, \"If in 100 years people can still read your article, we'll have solved the problem.\" Daniel Akst , author of The Webster Chronicle , proposes that \"the future of libraries—and of information—is digital\". Peter Lyman and Hal Variant , information scientists at the University of California, Berkeley , estimate that \"the world's total yearly production of print, film, optical, and magnetic content would require roughly 1.5 billion gigabytes of storage\". Therefore, they believe that \"soon it will be technologically possible for an average person to access virtually all recorded information\". [ 54 ] Digital archives are an evolving medium and they develop under various circumstances. Alongside large scale repositories, other digital archiving projects have also evolved in response to needs in research and research communication on various institutional levels. For example, during the COVID-19 pandemic , libraries and higher education institutions have launched digital archiving projects to document life during the pandemic, thus creating a digital, cultural record of collective memories from the period. [ 55 ] Researchers have also utilized digital archiving to create specialized research databases . These databases compile digital records for use on international and interdisciplinary levels. COVID CORPUS, launched in October 2020, is an example of such a database, built in response to scientific communication needs in light of the pandemic. [ 56 ] Beyond academia, digital collections have also recently been developed to appeal to a more general audience, as is the case with the Selected General Audience Content of the Internet-First University Press developed by Cornell University. This general-audience database contains specialized research information but is digitally organized for accessibility. [ 57 ] The establishment of these archives has facilitated specialized forms of digital recordkeeping to fulfill various niches in online, research-based communication.",
    "links": [
      "Archaeological science",
      "Cydia",
      "Very-large-scale integration",
      "Internet Underground Music Archive",
      "Video game",
      "TouchVision",
      "Artificial intelligence",
      "SoundCloud",
      "Battle.net",
      "Bilibili",
      "Networking hardware",
      "Web engineering",
      "Kartridge",
      "Facebook Platform",
      "Daisuki (website)",
      "Interoperability",
      "List of digital library projects",
      "NESiCAxLive",
      "Special library",
      "Cultural property imaging",
      "Pluto TV",
      "Tencent Appstore",
      "Islandora",
      "Rendering (computer graphics)",
      "As We May Think",
      "Multiprocessing",
      "Pokki",
      "Cataloging (library science)",
      "Electronic resource management",
      "Conservation and restoration of lighthouses",
      "Microformat",
      "Pandora (streaming service)",
      "Hdl (identifier)",
      "Programming team",
      "Semantic service-oriented architecture",
      "3D renderer",
      "Interaction design",
      "Ovi (Nokia)",
      "Terabyte",
      "Computational geometry",
      "Sergey Brin",
      "Industrial process control",
      "HRecipe",
      "BuyMusic",
      "MyVideo",
      "Anastylosis",
      "Textile stabilization",
      "Paper splitting",
      "Software framework",
      "List of female librarians",
      "Fault tolerance",
      "System on a chip",
      "Digital marketing",
      "Printed circuit board",
      "MixRadio",
      "AcFun",
      "Tubi",
      "Document management system",
      "Digital Millennium Copyright Act",
      "WiMP",
      "List of libraries in the ancient world",
      "EA (service)",
      "Integrated development environment",
      "Wireless sensor network",
      "Icecast",
      "VK (service)",
      "PictureBox Films",
      "Distributed computing",
      "Software engineering",
      "Conservation and restoration of textiles",
      "Paramount+",
      "E-commerce",
      "Computational biology",
      "Peacock (streaming service)",
      "Bump mapping",
      "WeShow",
      "Conservation and restoration of papyrus",
      "Semantic matching",
      "Communication studies",
      "Computational philosophy",
      "Knowledge representation and reasoning",
      "Conservator-restorer",
      "Bibcode (identifier)",
      "Paintings conservator",
      "Hidive",
      "Amazon Music",
      "VBox7",
      "Academic library",
      "Hypertext",
      "Biodiversity Heritage Library",
      "Database",
      "Digital humanities",
      "Public library",
      "MusicStation",
      "Buzznet",
      "Traditional knowledge",
      "Nutaku",
      "Cloud computing",
      "CiteSeerX (identifier)",
      "Ubuntu App Store",
      "Law library",
      "Reinforcement learning",
      "Concurrency (computer science)",
      "Over-the-air update",
      "Chaos (company)",
      "MiKandi",
      "Gamesplanet",
      "ESPN+",
      "Warner Archive Collection",
      "OverDrive, Inc.",
      "Cultural resource management",
      "Hardware security",
      "Nokia Store",
      "Modern and Contemporary Art Research Initiative",
      "Library",
      "Tool library",
      "Cultural heritage",
      "Open source",
      "Collecting",
      "Heritage science",
      "Provenance",
      "Paleo-inspiration",
      "Library publishing",
      "Conservation and restoration of wooden artifacts",
      "Ethnochoreology",
      "Conservation and restoration of neon objects",
      "Augmented reality",
      "Internet Archive",
      "Unsupervised learning",
      "Geotagging",
      "Last.fm",
      "MAD Solutions",
      "Form factor (design)",
      "Spotify",
      "Streaming media",
      "Electronic design automation",
      "Computational archaeology",
      "TwitCasting",
      "Conservation-restoration of Thomas Eakins' The Gross Clinic",
      "Patari (service)",
      "Digital Collections Selection Criteria",
      "The Atlantic Monthly",
      "Network architecture",
      "MUZU.TV",
      "GhostTunes",
      "Cultural heritage management",
      "Conservation and restoration of bone, horn, and antler objects",
      "Language preservation",
      "Control theory",
      "The Film Detective",
      "Conservation and restoration of new media art",
      "Fotki",
      "Intel AppUp",
      "Objects conservator",
      "Library catalog",
      "Conservation and restoration of lacquerware",
      "Annika Hinze",
      "Map collection",
      "Bigo Live",
      "Cyber-physical system",
      "Conservation issues of Pompeii and Herculaneum",
      "Born-digital",
      "Shadow library",
      "Modeling language",
      "List of national and state libraries",
      "Satellaview",
      "Cradling (paintings)",
      "Schematic",
      "Hoopla (digital media service)",
      "Art dealer",
      "Oxford Text Archive",
      "Formal methods",
      "Google Scholar",
      "UltraViolet (website)",
      "Anna's Archive",
      "Computer security",
      "DIALOG",
      "Conservation scientist",
      "Network scheduler",
      "Games for Windows Marketplace",
      "Glossary of library and information science",
      "TurboSquid",
      "Information overload",
      "Ethnopoetics",
      "Public domain",
      "Juno Records",
      "Vector graphics",
      "Special collections",
      "Description logic",
      "Tencent Video",
      "Steam (service)",
      "Radical.fm",
      "Zune software",
      "Folklore studies",
      "RDF/XML",
      "Computational mathematics",
      "Topic map",
      "Live365",
      "Endeavor Streaming",
      "Medici.tv",
      "Computing",
      "BIBFRAME",
      "Encoded Archival Description",
      "Preservation (library and archive)",
      "Bioarchaeology",
      "SAWSDL",
      "Deutsche Nationalbibliothek",
      "Research Resource Identifier",
      "Line Music",
      "Fonds",
      "The New York Times",
      "Club Nokia",
      "Metadata Authority Description Schema",
      "Bleep.com",
      "Electronic document",
      "Djshop",
      "Schema.org",
      "Windows Marketplace",
      "FilmStruck",
      "Family folklore",
      "Algorithm",
      "Impulse (software)",
      "Conservation and restoration of time-based media art",
      "List of Android app stores",
      "HBO Max",
      "D-Lib",
      "Peter Lyman",
      "Display case",
      "Library and information science",
      "Binge (Bangladeshi streaming service)",
      "Virtual reality headset",
      "Digital classics",
      "Metadata",
      "Title 17 of the United States Code",
      "Knowledge management",
      "Digital rights management",
      "Electronic voting",
      "Mathematical software",
      "Conservation and restoration of leather objects",
      "Presto (streaming company)",
      "Google Video",
      "Algorithmic efficiency",
      "Jewel Quest",
      "Vannevar Bush",
      "Europeana",
      "Open-source software",
      "Knowledge extraction",
      "Conservation and restoration of flags and banners",
      "Zune Marketplace",
      "Wayback Machine",
      "Music Glue",
      "Conservation and restoration of books, manuscripts, documents, and ephemera",
      "Software",
      "NASA",
      "Computational complexity",
      "Conservation and restoration of movable cultural property",
      "ISSN (identifier)",
      "TuneCore",
      "Integrated circuit",
      "Computer-aided design",
      "Card catalog",
      "Snap Store",
      "YouNow",
      "Human–computer interaction",
      "Conservation and restoration of glass objects",
      "Quantum computing",
      "ISBN (identifier)",
      "Mobile computing",
      "Ditto Music",
      "Nintendo Video",
      "Education Resources Information Center",
      "Hard drive",
      "Mobile device",
      "Kick (service)",
      "Software development process",
      "Requirements analysis",
      "Reference (computer science)",
      "Mathematical optimization",
      "Web Science Trust",
      "Yandex Music",
      "Library science",
      "YouTube Music",
      "TalkTalk TV Store",
      "Free content",
      "Solid (web decentralization project)",
      "Intrusion detection system",
      "Open Archives Initiative Protocol for Metadata Harvesting",
      "Floppy disk",
      "Mac App Store",
      "Integrated pest management (cultural property)",
      "AOL Radio",
      "UnitedMasters",
      "Online Public Access Catalog",
      "Statistics",
      "Raster graphics",
      "Common Logic",
      "IHeartRadio",
      "Apple TV+",
      "Digital ontology",
      "CinemaNow",
      "WWE Classics on Demand",
      "Hyperdata",
      "Derek De Solla Price",
      "Information system",
      "Preservationist",
      "Conservation and restoration of insect specimens",
      "Vodafone live!",
      "Electronic literature",
      "Cultural property storage",
      "Internationalized Resource Identifier",
      "Amazon Appstore",
      "Library and information scientist",
      "PlayStation Video",
      "Conservation and restoration of musical instruments",
      "Access control",
      "Preservation of cultural venues",
      "Conservation and restoration of vinyl discs",
      "Stardock Central",
      "List of female archivists",
      "MacUpdate",
      "HDtracks",
      "Software distribution",
      "Collection catalog",
      "Conservation and restoration of film",
      "Playism",
      "Funshion",
      "Plex",
      "Crunchyroll",
      "LiveLeak",
      "S2CID (identifier)",
      "Documentary editing",
      "Traditional medicine",
      "Ubisoft Connect",
      "Conservation and restoration of photographic plates",
      "Full-text database",
      "Digital music store",
      "Megaupload",
      "Fearnet",
      "Oral history preservation",
      "Library technical services",
      "Bibliographic Ontology",
      "Dailymotion",
      "Library consortium",
      "Rockstar Games Social Club",
      "TeacherTube",
      "Detachment of wall paintings",
      "Information literacy",
      "Curiosity Stream",
      "Book",
      "Archivist",
      "Programming language theory",
      "Online book rental",
      "Le.com",
      "Galaxy Store",
      "Taazi",
      "Digital repository audit method based on risk assessment",
      "SHACL",
      "Music library",
      "Optical media preservation",
      "Toffee (streaming service)",
      "Periodicals librarian",
      "Software maintenance",
      "Computer animation",
      "DARPA",
      "Symphonic Distribution",
      "Digitization",
      "Wowloud",
      "Conservation and restoration of plastic objects",
      "SlideME",
      "Thermodynamic computing",
      "Collections management system",
      "Cryptography",
      "FilmOn",
      "Folk play",
      "Concurrent computing",
      "Semantic Web",
      "LANDR",
      "New media",
      "VisualAudio",
      "Conservation and restoration of rail vehicles",
      "Image compression",
      "Xigua Video",
      "Educational technology",
      "Streamworks International",
      "Digital Commons",
      "Midden",
      "ITunes Store",
      "Spinner (website)",
      "Registrar (cultural property)",
      "Direct2Drive",
      "Yahoo! Music Unlimited",
      "Distance education librarian",
      "Domain-specific language",
      "Private library",
      "RockMyRun",
      "Desmet method",
      "Traveling library",
      "List of computer size categories",
      "Software repository",
      "Software quality",
      "List of destroyed libraries",
      "Discovery system (bibliographic search)",
      "School library",
      "ROXi",
      "List of 3D modeling software",
      "Grooveshark",
      "List of medical libraries",
      "Google Play",
      "Nintendo Music",
      "Chorki",
      "Institutional repository",
      "LBRY",
      "Chrome Web Store",
      "Software configuration management",
      "Henri La Fontaine",
      "Collaborative filtering",
      "Dlive",
      "Data mining",
      "Electric Jukebox",
      "Mixcloud",
      "Operating system",
      "Repatriation (cultural property)",
      "Ontology",
      "Windows Phone Store",
      "Jamendo",
      "Xbox Live Arcade",
      "Amazon Digital Game Store",
      "Digital media",
      "Subscription business model",
      "Interpreter (computing)",
      "Smart TV",
      "Art handler",
      "Conservation and restoration of performance art",
      "Flixster",
      "Epic Games Store",
      "Conservation and restoration of outdoor bronze objects",
      "Software design",
      "Odnoklassniki",
      "Movies Anywhere",
      "Preservation of meaning",
      "SpiralFrog",
      "Vudu",
      "Social computing",
      "Middleware",
      "Emulated",
      "Inside Mac Games",
      "Visualization (graphics)",
      "Linked data",
      "Content delivery network",
      "Cultural property exhibition",
      "Systems theory",
      "Million Book Project",
      "Simple Knowledge Organization System",
      "Web Ontology Language",
      "ONErpm",
      "Transportation library",
      "Funny or Die",
      "Dataspaces",
      "Institutional repositories",
      "British Library",
      "BBC Store",
      "BluTV",
      "Teaching Channel",
      "PressPlay",
      "Mixamo",
      "Information theory",
      "Presidential library system",
      "Database preservation",
      "Control flow",
      "Supervised learning",
      "Viveport",
      "GameLine",
      "Information security",
      "Digital preservation",
      "Machine learning",
      "Text Encoding Initiative",
      "Numerical analysis",
      "Model of computation",
      "Preservation metadata",
      "Photograph conservator",
      "TapTap",
      "Legal deposit",
      "Information architecture",
      "Vessel (website)",
      "Conservation and restoration of clocks",
      "Heritage language",
      "Heritage railway",
      "Music librarianship",
      "Resource Description Framework",
      "Pandora TV",
      "Exxen",
      "Discord (software)",
      "PlayCable",
      "DI.FM",
      "Mark Twain",
      "Real-time computing",
      "KDE Gear",
      "Fedora Commons",
      "Recommender systems",
      "Guvera",
      "Librarian",
      "NoiseTrade",
      "Brightcove",
      "Microsoft Movies & TV",
      "Openfilm",
      "Online database",
      "Conservation and restoration of silver objects",
      "ArXiv",
      "Conservation and restoration of stained glass",
      "IQIYI",
      "Digital Library Federation",
      "GameAgent",
      "HTTP",
      "AllOfMP3",
      "HCard",
      "Research Article",
      "Enterprise information system",
      "Conservation and restoration of archaeological sites",
      "GameHouse",
      "Automated planning and scheduling",
      "Rakuten TV",
      "Dance notation",
      "Deaccessioning",
      "Online video platform",
      "Michael Gorman (librarian)",
      "Hardware acceleration",
      "Printing",
      "Multithreading (computer architecture)",
      "Content repository",
      "Blender (software)",
      "Teacher-librarian",
      "HReview",
      "Mora (music store)",
      "Folk instrument",
      "Package manager",
      "Fandango Media",
      "Frederick Wilfrid Lancaster",
      "Analysis of algorithms",
      "List of online educational resources",
      "National library",
      "ACM Computing Classification System",
      "Uniform Resource Identifier",
      "N-Triples",
      "Hulu",
      "Niconico",
      "Conservation of South Asian household shrines",
      "Sony Pictures Core",
      "MSN Games",
      "Archives",
      "Learning Resource Centre",
      "Vidme",
      "Software deployment",
      "Bandit.fm",
      "Break.com",
      "Conservation and restoration of taxidermy",
      "Folk music",
      "GetJar",
      "Disney+",
      "Computers and writing",
      "Daum (web portal)",
      "Conservation and restoration of illuminated manuscripts",
      "Humble Store",
      "Digital distribution",
      "Probability",
      "Semantic search",
      "Beatport",
      "Collective memory",
      "Dependability",
      "WeGame",
      "Quickflix",
      "Music download",
      "Ubuntu Software Center",
      "Bandcamp",
      "GamersGate",
      "Puhutv",
      "Theory of computation",
      "Mobile library",
      "Cultural property documentation",
      "App store",
      "LoveFilm",
      "Indigenous culture",
      "Samuel Clemens",
      "Amuse (music company)",
      "CHZZK",
      "Presidential library",
      "Digital River",
      "Reconstruction (architecture)",
      "WildTangent",
      "Language death",
      "Apache Mahout",
      "Cloud gaming",
      "Personal computer",
      "Outline of computer science",
      "Anghami",
      "Folk dance",
      "Digital reference",
      "Paul Otlet",
      "National edeposit",
      "Web archiving",
      "Full-text search",
      "Believe Music",
      "Twitch (service)",
      "Vine (service)",
      "List of mobile app distribution platforms",
      "Collection manager",
      "Language revitalization",
      "1966 flood of the Arno",
      "Moov",
      "Viddler",
      "Compiler construction",
      "Bongo BD",
      "Information professional",
      "V-Ray",
      "Global Wrestling Network",
      "Museum",
      "RealArcade",
      "Google Play Music",
      "AMC+",
      "Conservation and restoration of photographs",
      "Triton (content delivery)",
      "Spinlet",
      "Metacafe",
      "Digital document",
      "Virtual reality",
      "Computational social science",
      "Early music",
      "Netflix",
      "University of Illinois at Urbana–Champaign",
      "Fandango at Home",
      "Sony Entertainment Network",
      "Archaeology",
      "Five laws of library science",
      "YouTube Kids",
      "Conservation and restoration of Tibetan thangkas",
      "World Heritage Site",
      "Pono (digital music service)",
      "Scirus",
      "Youku",
      "Conservation and restoration of ivory objects",
      "Proxy server",
      "Information retrieval",
      "Knowledge organization",
      "Christian library",
      "Video game console",
      "IDAGIO",
      "Living history",
      "Digital divide",
      "Agents of deterioration",
      "N-Gage (service)",
      "Semantics (computer science)",
      "Flickr",
      "Open data",
      "COinS",
      "Toons.TV",
      "Found in collection",
      "List of library associations",
      "Discrete mathematics",
      "Toy library",
      "Disaster preparedness (cultural property)",
      "Qobuz",
      "Historic paint analysis",
      "Tank Top TV",
      "Inventory (library and archive)",
      "Word processor",
      "Intangible cultural heritage",
      "Corporation for National Research Initiatives",
      "Research database",
      "Starz",
      "Tidal (service)",
      "University of California, Santa Barbara",
      "Indigenous language",
      "Aha (streaming service)",
      "Reference desk",
      "56.com",
      "Semantic reasoner",
      "Appland",
      "Conservation and restoration of fur objects",
      "Green computing",
      "Inherent vice",
      "Logic in computer science",
      "Building information modeling",
      "Computer vision",
      "Ford Foundation",
      "Seamus Ross",
      "Stochastic computing",
      "Napster (streaming service)",
      "Semantic computing",
      "Streamwaves",
      "Folklore",
      "Audacy",
      "Cafe Bazaar",
      "PureOS",
      "Cultural property radiography",
      "Media preservation",
      "Library circulation",
      "Concept search",
      "Conservation and restoration of feathers",
      "Rule-based system",
      "Curator",
      "Algorithm design",
      "Weeding (library)",
      "TikTok",
      "New York Times Magazine",
      "Geographic information system",
      "COVID-19 pandemic",
      "Semantic analytics",
      "Game Jolt",
      "DSpace",
      "Ubuntu Touch",
      "Dublin Core",
      "Public bookcase",
      "Conservation-restoration of the Shroud of Turin",
      "PMID (identifier)",
      "Data processing",
      "Peripheral",
      "LiveOne",
      "Multimedia database",
      "Enterprise software",
      "Conservation and restoration of cultural property",
      "Randomized algorithm",
      "Blip.tv",
      "Bookmobile",
      "Nimbit",
      "Google TV (service)",
      "E-research",
      "Simfy",
      "Historic site",
      "Leafcasting",
      "Software construction",
      "Azubu",
      "Funimation",
      "Heritage asset",
      "Conservation and restoration of human remains",
      "Vongo (video on demand service)",
      "Web 2.0",
      "Conservation and restoration of totem poles",
      "Rumble (company)",
      "Electronic publishing",
      "Murfie",
      "SOOP",
      "Telly Inc.",
      "EGranary",
      "Digital history",
      "Boomplay",
      "Library branch",
      "BitChute",
      "Vimeo",
      "World Wide Web",
      "Mixcrate",
      "Meta Horizon Store",
      "Stanford University",
      "Carnegie Mellon University",
      "Digital Medievalist",
      "The Onion",
      "Aparat",
      "EmuBands",
      "Computer graphics lighting",
      "Imeem",
      "Adober Studios",
      "Hitbox (service)",
      "Sustainable preservation",
      "Collection (museum)",
      "Kintsugi",
      "Library of things",
      "Library acquisitions",
      "Operations research",
      "Bibliographic database",
      "Classical Archives",
      "Samvera",
      "RDFa",
      "Panjab Digital Library",
      "Collective collection",
      "Conservation and restoration of wooden furniture",
      "Computer-aided engineering",
      "Ruins",
      "Sony Connect",
      "Heritage language learning",
      "F-Droid",
      "IMesh",
      "Sitemap",
      "Internet",
      "Programming tool",
      "Computer science",
      "Noggin (brand)",
      "Film preservation",
      "Inpainting",
      "Cybertext",
      "Foodways",
      "Photograph manipulation",
      "Aptoide",
      "DMM Games",
      "Twango",
      "Amazon Freevee",
      "Virtual machine",
      "Flathub",
      "Handango",
      "Google",
      "Carnegie library",
      "Medical record librarian",
      "Parallel computing",
      "GoMusicNow",
      "Desura",
      "Pictogram",
      "Triller",
      "Network security",
      "TroopTube",
      "J. C. R. Licklider",
      "JSON-LD",
      "Yahoo! Music Radio",
      "Mathematical analysis",
      "Search engine (computing)",
      "Social software",
      "Conservation and restoration of ancient Greek pottery",
      "GameTap",
      "Sega Channel",
      "Multi-task learning",
      "Semantic network",
      "Arrested decay",
      "Content-based filtering",
      "Restoration of the Sistine Chapel frescoes",
      "Health informatics",
      "Communication protocol",
      "Interface design",
      "DOAP",
      "Conservation and restoration of metals",
      "Apple Music",
      "DLsite",
      "7digital",
      "Semantic triple",
      "OAI-PMH",
      "E-Science librarianship",
      "List of largest libraries",
      "Revver",
      "TriX (serialization format)",
      "Wii Shop Channel",
      "Computing platform",
      "Medical library",
      "FOAF",
      "Mass deacidification",
      "Semantic wiki",
      "Computer network",
      "Prison library",
      "Impact Wrestling",
      "CD Baby",
      "Collection development",
      "Apple TV app",
      "Mango TV",
      "Digital rhetoric",
      "Sua Música",
      "Readers' advisory",
      "Z-Library",
      "IEEE Xplore",
      "Computational problem",
      "Ubiquitous computing",
      "Conservation and restoration of ceramic objects",
      "Computer accessibility",
      "Endangered language",
      "Openload",
      "Windows Marketplace for Mobile",
      "Digital Public Library of America",
      "Conservation and restoration of painting frames",
      "Distributed artificial intelligence",
      "ThePlatform",
      "Social media",
      "Tudou",
      "Graphics processing unit",
      "Xunlei",
      "Conservation and restoration of road vehicles",
      "Copyright Act of 1976",
      "Education for librarianship",
      "History of libraries",
      "Proprietary software",
      "Applied folklore",
      "Rutube",
      "Smashcast",
      "Opera Mobile Store",
      "Disney Movies Anywhere",
      "Virtual school libraries in the United States",
      "Human-centered computing",
      "Pogo.com",
      "Itch.io",
      "Solid modeling",
      "Doi (identifier)",
      "American Memory",
      "List of libraries by country",
      "HCalendar",
      "Redbox",
      "Transliteracy",
      "TriG (syntax)",
      "Cultural analytics",
      "Semantic HTML",
      "In2TV",
      "Ancient music",
      "Showtime (TV network)",
      "Deezer",
      "Project Gutenberg",
      "3D scanning",
      "Audiomack",
      "Lending library",
      "Apache Lucene",
      "Libraries and librarians in fiction",
      "Security hacker",
      "Computational physics",
      "Xbox Games Store",
      "Programming paradigm",
      "HighBeam Research",
      "Semantic Web Rule Language",
      "Collections management",
      "Optical character recognition",
      "Digital physics",
      "Justin.tv",
      "Digital religion",
      "Copyright",
      "YouTube",
      "MOG (online music)",
      "PeerTube",
      "Subscription library",
      "University of Michigan",
      "Video on demand",
      "7plus",
      "Cross-validation (statistics)",
      "Conservation and restoration of woodblock prints",
      "GameShadow",
      "Sustainability",
      "National Science Foundation",
      "GRDDL",
      "Computability theory",
      "Windows Media Center",
      "Huawei AppGallery",
      "Search engine indexing",
      "Proprietary",
      "Online public access catalog",
      "Folk process",
      "Processor (computing)",
      "Yahoo! Games",
      "Hybrid library",
      "Computational engineering",
      "Cyberwarfare",
      "Showroom (streaming service)",
      "Prehistoric music",
      "Amie Street",
      "Programming language",
      "TuneTribe",
      "Digital scholarship",
      "SPARQL",
      "Ensemble librarianship",
      "Digital photograph restoration",
      "Conservation science (cultural property)",
      "Conservation and restoration of outdoor artworks",
      "Semantically Interlinked Online Communities",
      "Sega Meganet",
      "Music streaming service",
      "Puretracks",
      "Big Fish Games",
      "8tracks.com",
      "Yahoo!",
      "List of library science schools",
      "Finding aid",
      "Microsoft Store",
      "Software development",
      "Computer-aided manufacturing",
      "Historic preservation",
      "Conservation-restoration of the H.L. Hunley",
      "HAtom",
      "CGTrader",
      "Network service",
      "Trilulilu",
      "Porting",
      "Computer graphics",
      "CloudLibrary",
      "HProduct",
      "Melon (online music service)",
      "Kazaa",
      "Archival processing",
      "Automata theory",
      "RDF Schema",
      "Conservation and restoration of panel paintings",
      "Archive",
      "Sketchfab",
      "Diagram",
      "Roving reference",
      "Conservation and restoration of parchment",
      "EMusic",
      "Digital data",
      "Web browser",
      "Treasure",
      "Embedded system",
      "Conservation and restoration of books, manuscripts, documents and ephemera",
      "Computer hardware",
      "IXBRL",
      "Nintendo eShop",
      "Transfer of panel paintings",
      "Computer architecture",
      "Training and development",
      "Collections maintenance",
      "Computational chemistry",
      "Library (computing)",
      "GOG.com",
      "Computer data storage",
      "Library management",
      "Photography",
      "Web crawler",
      "Library history",
      "Conservation and restoration of immovable cultural property",
      "Application security",
      "Library classification",
      "Starlight Networks",
      "Conservation and restoration of frescos",
      "Notation3",
      "Natural language processing",
      "Aupeo",
      "Preservation survey",
      "Magnatune",
      "Rakuten.co.uk",
      "Bioscope (Live TV)",
      "Textile conservator",
      "Download!",
      "Collective intelligence",
      "GESIS – Leibniz Institute for the Social Sciences",
      "Philosophy of computer science",
      "Conservation-restoration of Leonardo da Vinci's The Last Supper",
      "Decision support system",
      "Copyright Act 1968",
      "Open access (publishing)",
      "Putlocker",
      "Conservation and restoration of paintings",
      "Formal language",
      "Metadata Object Description Schema",
      "Indigenous intellectual property",
      "Conservation and restoration of Pompeian frescoes",
      "Thingiverse",
      "Lining of paintings",
      "Conservation technician",
      "Humanistic informatics",
      "Joost",
      "Allmyapps",
      "DistroKid",
      "Semantic mapper",
      "Rara (service)",
      "List of archivists",
      "Semantic publishing",
      "Amazon Prime Video",
      "Fair Use",
      "Robot Cache",
      "Deep Web (search indexing)",
      "Archival science",
      "Library assessment",
      "Radionomy",
      "Overpainting",
      "Conservation and restoration of copper-based objects",
      "List of libraries",
      "Microdata (HTML)",
      "Odysee",
      "Primary source",
      "Digital distribution of video games",
      "EPrints",
      "Library of Congress",
      "Hiroshima",
      "Conservation and restoration of iron and steel objects",
      "List of librarians",
      "Z39.50",
      "XML",
      "MSN Music",
      "Conservation-restoration of the Statue of Liberty",
      "MP3.com",
      "Conservation and restoration of historic gardens",
      "Outline of library science",
      "Auction",
      "Library 2.0",
      "GrabCAD",
      "Mixer (service)",
      "Super Deluxe",
      "Ecce Homo (García Martínez and Giménez)",
      "UVC-based preservation",
      "Mount maker",
      "Texture mapping",
      "Computational theory of mind",
      "Search engine",
      "Rule Interchange Format",
      "Sohu",
      "Cultural property",
      "Conservation and restoration of shipwreck artifacts",
      "Folksonomy",
      "University of California, Berkeley",
      "Conservation and restoration of Judaica",
      "Greenstone (software)",
      "Stage6",
      "Mold control and prevention (library and archive)",
      "Scientific communication",
      "Google Books",
      "Exhibition designer",
      "Rissverklebung",
      "Memex",
      "SchoolTube",
      "Conservation and restoration of outdoor murals",
      "Inventory (museums)",
      "Philosophy of artificial intelligence",
      "Archival bond",
      "Style Jukebox",
      "Security service (telecommunication)",
      "IndieGala",
      "McMaster-Carr",
      "BlackBerry World",
      "Library instruction",
      "MeWATCH",
      "Mundaneum",
      "Digital art",
      "Aging (artwork)",
      "Taxonomy (general)",
      "Gerard Salton",
      "Zattoo",
      "Ethnomusicology",
      "Kanopy",
      "Informationist",
      "PlayStation Store",
      "Folk art",
      "Ontology (information science)",
      "Network performance",
      "App Store (Apple)",
      "Research library",
      "Turtle (syntax)",
      "Vdio",
      "Folk etymology",
      "Larry Page",
      "Digital theology",
      "Calendar (archives)",
      "Conservation and restoration of aircraft",
      "Conservation and restoration of herbaria",
      "Theoretical computer science",
      "Computational complexity theory",
      "PlayNow Arena"
    ]
  },
  "European Conference on Information Retrieval": {
    "url": "https://en.wikipedia.org/wiki/European_Conference_on_Information_Retrieval",
    "title": "European Conference on Information Retrieval",
    "content": "The European Conference on Information Retrieval ( ECIR ) is the main European research conference for the presentation of new results in the field of information retrieval (IR). It is organized by the Information Retrieval Specialist Group of the British Computer Society (BCS-IRSG). The event started its life as the Annual Colloquium on Information Retrieval Research in 1978 and was held in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has alternated between the United Kingdom and continental Europe. To mark the metamorphosis from a small informal colloquium to a major event in the IR research calendar, the BCS-IRSG later renamed the event to European Conference on Information Retrieval . In recent years, ECIR has continued to grow and has become the major European forum for the discussion of research in the field of Information Retrieval. Some of the topics dealt with include: The ECIR is generally held in Spring, near the Easter weekend. A list of locations and planned venues is presented below. *as the Annual Colloquium on Information Retrieval Research",
    "links": [
      "London",
      "Natural language processing",
      "Pisa",
      "Rome",
      "Cambridge",
      "Glasgow",
      "Dublin",
      "Darmstadt",
      "Leeds",
      "Dublin, Ireland",
      "British Computer Society",
      "Grenoble",
      "Manchester",
      "Crewe",
      "Birmingham",
      "Lucca",
      "Information retrieval",
      "COVID-19 pandemic",
      "Sheffield",
      "Huddersfield",
      "Bradford",
      "Sunderland, Tyne and Wear",
      "Lancaster, Lancashire",
      "Cologne",
      "Santiago de Compostela",
      "Aberdeen, Scotland",
      "Aberdeen",
      "Milton Keynes",
      "Stavanger",
      "Drymen",
      "Padova, Italy",
      "Amsterdam, Netherlands",
      "Toulouse",
      "Lisbon",
      "Vienna, Austria",
      "Moscow, Russia",
      "Barcelona, Spain"
    ]
  },
  "Doi (identifier)": {
    "url": "https://en.wikipedia.org/wiki/Doi_(identifier)",
    "title": "Doi (identifier)",
    "content": "A digital object identifier ( DOI ) is a persistent identifier or handle used to uniquely identify various objects, standardized by the International Organization for Standardization (ISO). [ 2 ] DOIs are an implementation of the Handle System ; [ 3 ] [ 4 ] they also fit within the URI system ( Uniform Resource Identifier ). They are widely used to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications . A DOI aims to resolve to its target, the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL where the object is located. Thus, by being actionable and interoperable , a DOI differs from ISBNs or ISRCs which are identifiers only. The DOI system uses the indecs Content Model to represent metadata . The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI should provide a more stable link than directly using its URL. But if its URL changes, the publisher must update the metadata for the DOI to maintain the link to the URL. [ 5 ] [ 6 ] [ 7 ] It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link , leaving the DOI useless. [ 8 ] The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000. [ 9 ] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs. [ 10 ] The DOI system is implemented through a federation of registration agencies coordinated by the IDF. [ 11 ] The cumulative number of DOIs has increased exponentially over time, from 50 million registrations in 2011 to 391 million in 2025. [ 12 ] The rate of registering organizations (\"members\") has also increased over time from 4,000 in 2011 to 9,500 in 2013, but the federated nature of the system means it is not immediately clear how many members there are in total today. [ 13 ] Fake registries have even appeared. [ 14 ] A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the identifier and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN , where NNNN is a number greater than or equal to 1000 , whose limit depends only on the total number of registrants. [ 15 ] [ 16 ] The prefix may be further subdivided with periods, like 10.NNNN.N . [ 17 ] For example, in the DOI name 10.1000/182 , the prefix is 10.1000 and the suffix is 182 . The \"10\" part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace, [ A ] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook ). DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances , and abstract works [ 18 ] such as licenses, parties to a transaction, etc. The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model . The official DOI Handbook explicitly states that DOIs should be displayed on screens and in print in the format doi:10.1000/182 . [ 19 ] Contrary to the DOI Handbook , Crossref , a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182 ) instead of the officially specified format. [ 20 ] [ 21 ] The DOI Foundation guarantees these URLs to be persistent [ 22 ] i.e. such URLs are PURLs — providing the location of a name resolver which will redirect HTTP requests to the correct online location of the linked item. [ 10 ] [ 23 ] The Crossref recommendation is primarily based on the assumption that the DOI is being displayed without being hyperlinked to its appropriate URL—the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents. [ 24 ] Major content of the DOI system currently includes: In the Organisation for Economic Co-operation and Development 's publication service OECD iLibrary , each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned. [ 26 ] Other registries include Crossref and the multilingual European DOI Registration Agency (mEDRA) . [ 27 ] Since 2015, RFCs can be referenced as doi:10.17487/rfc ... . [ 28 ] The IDF designed the DOI system to provide persistent identification . Each DOI name permanently and clearly identifies the object it belongs to (although when the publisher of a journal changes, sometimes all the DOIs will be changed, with the old DOIs no longer working). It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure. The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures , incorporates trust mechanisms , and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system. [ 29 ] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request. [ 30 ] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents, that would have been available for no additional fee from alternative locations. [ 31 ] The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL. The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN . A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier ( Uniform Resource Name ) concept and adds to it a data model and social infrastructure. [ 32 ] A DOI name also differs from standard identifier registries such as the ISBN , ISRC , etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections. [ 33 ] The DOI system offers persistent, semantically interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing both social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as \"identifiers\" does not mean that they can be compared easily. Other \"identifier systems\" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK ). A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name. In May, 2024, an Internet Draft was introduced to register the \"doi\" scheme,. [ 34 ] Many experts were not aware of this draft, [ 35 ] and the latest draft has currently expired. To resolve a DOI name, it may be input to a DOI resolver, such as one at the official website https://doi.org/ . DOI name resolution is provided through the Handle System , which is an infrastructure developed and operated by CNRI ( Corporation for National Research Initiatives ), and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues. Another approach, which avoids typing or copying and pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as https://doi.org/ (preferred) [ 36 ] or http://dx.doi.org/ , both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182 . This approach allows users to click on the DOI as a normal hyperlink . Indeed, as previously mentioned, this is how Crossref recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable. An interesting consequence of the fact that DOIs depend entirely on CNRI's Handle System infrastructure (whereby CNRI operates the global root servers and wrote the protocol) is that the proxy services DOI.org/<#> and hdl.handle.net/<#> are interoperable. For example, the following URIs resolve to the same publication: https://doi.org/10.1016/S0021-9258(19)52451-6 https://hdl.handle.net/10.1016/S0021-9258(19)52451-6 There are other DOI resolvers and HTTP Proxies apart from NCRI's Handle System . At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io/ (now discontinued [ 37 ] ). This service was unusual in that it tried to find a non-paywalled (often author archived ) version of a title and redirected the user to that instead of the publisher's version . [ 38 ] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016 [ 39 ] (rebranded in 2017 as https://unpaywall.org/ ). While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult multiple Open Access resources such as institutional libraries with the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH), or indexing services based in OAI-PMH, such as BASE (Bielefeld Academic Search Engine). [ 37 ] [ 39 ] An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers , thereby avoiding the conversion of the DOIs to URLs, [ 40 ] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader , or other software which does not have one of these plug-ins installed. The International DOI Foundation ( IDF ), a non-profit organization created in 1997, is the governance body of the DOI system. [ 41 ] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system. The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues. Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix. [ 42 ] Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis. The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9. [ 43 ] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot, [ 44 ] which was approved by 100% of those voting in a ballot closing on 15 November 2010. [ 45 ] The final standard was published on 23 April 2012. [ 2 ] DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. [ 46 ] info:doi/ is the infoURI Namespace of Digital Object Identifiers. [ 47 ] The DOI syntax is a NISO standard, first standardized in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier. [ 48 ] The maintainers of the DOI system have registered a DOI namespace for URNs . [ 49 ]",
    "links": [
      "ISO 2788",
      "ISO 233",
      "STEP-NC",
      "501(c)(6) organization",
      "URN",
      "Delivery Multimedia Integration Framework",
      "ISO/IEC 15693",
      "Cloud Infrastructure Management Interface",
      "Interoperability",
      "Kappa number",
      "BASE (search engine)",
      "ISO/IEC 8859-7",
      "Unicode",
      "Interested Parties Information",
      "ISO 2145",
      "ISO 13584",
      "Document Style Semantics and Specification Language",
      "Dynamic Adaptive Streaming over HTTP",
      "ISO/IEC 27000 family",
      "Doi (disambiguation)",
      "DataCite",
      "SQL",
      "MPEG-D",
      "Internet Architecture Board",
      "ISO 1629",
      "URL",
      "MP4 file format",
      "ISO/IEC 7811",
      "ISO 22300",
      "Open Data Protocol",
      "International Standard Audiovisual Number",
      "Romanization of Armenian",
      "JSON",
      "TIFF/EP",
      "ISO 13567",
      "ISO 7200",
      "ISO 2709",
      "ISO 898",
      "Versatile Video Coding",
      "ISO 14698",
      "DICOM",
      "SDMX",
      "ISO/IEC 20000",
      "ISO/IEC 19752",
      "ISO 20022",
      "ISO 45001",
      "Isofix",
      "Antimagnetic watch",
      "ISO 18245",
      "ISO 3864",
      "Internet Draft",
      "ISO 31000",
      "Multibus",
      "International Standard Identifier for Libraries and Related Organizations",
      "RELAX NG",
      "PMID",
      "Publication",
      "MPEG-7",
      "ISO 11783",
      "Lexical Markup Framework",
      "ISO 3166-1",
      "ISO/IEC 8859-6",
      "Pinyin",
      "Longitudinal redundancy check",
      "ISO/IEC 15288",
      "Ada Semantic Interface Specification",
      "ISO/IEC 15504",
      "ISO 6346",
      "Persistent Uniform Resource Locator",
      "Topic map",
      "SoftWare Hash IDentifier",
      "ISO/IEC 27005",
      "ISO 639-2",
      "ISO/IEC 8859-3",
      "Open Document Architecture",
      "OpenDocument",
      "Market Identifier Code",
      "JBIG",
      "HTML",
      "ISO/IEC 11801",
      "Metadata",
      "ISO 9564",
      "Flowchart",
      "ISO 8000",
      "Pascal (programming language)",
      "ISO 21001",
      "PDF/X",
      "ISO 31",
      "ISO/IEC 7812",
      "ISO 8601",
      "Trusted Platform Module",
      "ISO 19114",
      "ISSN",
      "ISO/IEC 17024",
      "ISSN (identifier)",
      "ISO 10007",
      "ISO 14224",
      "List of ISO romanizations",
      "PDF/A",
      "ISO 860",
      "ISBN (identifier)",
      "ISO 732",
      "ISO 31-6",
      "ISO 31-1",
      "ANSI C",
      "List of IEC standards",
      "Hot shoe",
      "ISO 19092-2",
      "ISO 965",
      "ISO 31-4",
      "ISO 21500",
      "Open Archives Initiative Protocol for Metadata Harvesting",
      "PMC (identifier)",
      "Object Constraint Language",
      "ISO 8691",
      "Common Logic",
      "ISO 14617",
      "ISO/IEC 8859-9",
      "ISO/IEC TR 12182",
      "Airiti",
      "ISO 11940",
      "ISO 20400",
      "John R. Levine",
      "ISO 26262",
      "XML Metadata Interchange",
      "ISO 216",
      "Globally Unique Identifier",
      "Open access",
      "International Standard Serial Number",
      "ISO 4157",
      "ISO/IEC 2022",
      "Publisher Item Identifier",
      "ISO 25964",
      "ORCID",
      "ISO 5427",
      "ISO/IEC 15897",
      "String (computer science)",
      "ISO 31-9",
      "ISO/IEC 21827",
      "ISO 15926",
      "Request for Comments",
      "Indecs Content Model",
      "ISO 31-3",
      "ISO 217",
      "A440 (pitch standard)",
      "JPEG XL",
      "ISO/IEC 8859-13",
      "IS-IS",
      "ISO 31-7",
      "ISO/IEC 8859-16",
      "ISO 4031",
      "Simple Features",
      "Guidelines for the Definition of Managed Objects",
      "Persistent identifier",
      "Software maintenance",
      "ISO 15919",
      "ISO/IEC 8859-14",
      "Requirements engineering",
      "ISO 7027",
      "ISO 639-1",
      "Language of Temporal Ordering Specification",
      "RFC (identifier)",
      "FTAM",
      "CAE number",
      "ISO/IEC 8859-11",
      "Hole punch",
      "ISO 22301",
      "Fiber Distributed Data Interface",
      "ISO 10628",
      "ISO/IEC 11179",
      "NISO",
      "MicroPDF417",
      "MPEG-H",
      "ISO 19011",
      "ISO 6438",
      "ISO/TR 11941",
      "ISO metric screw thread",
      "PDF417",
      "International Standard Name Identifier",
      "International Bank Account Number",
      "ISO/IEC 8859-15",
      "Graph Query Language",
      "Quality function deployment",
      "ISO/IEC 19770",
      "Web Content Accessibility Guidelines",
      "ISO 9",
      "ISO 11940-2",
      "ISO/IEC 18014",
      "MPEG-4",
      "Business Process Model and Notation",
      "ISO/IEC 80000",
      "Equal-loudness contour",
      "O-ring",
      "International Standard Atmosphere",
      "Film speed",
      "RM-ODP",
      "Ars Technica",
      "ISO 15398",
      "Essential Video Coding",
      "ISO/IEC 8859-10",
      "ISO 2033",
      "ISO/IEC 17025",
      "ArXiv",
      "PDF",
      "ISO 9897",
      "ISO/IEC 8859-1",
      "Open architecture",
      "ISO 2",
      "Full BASIC",
      "ISO 11784 and ISO 11785",
      "IATF 16949",
      "ISO/IEC 10116",
      "Uniform Resource Name",
      "ALGOL 60",
      "Publisher's version",
      "EIDR",
      "ECMAScript",
      "ISO/IEC 11404",
      "International Securities Identification Number",
      "ASMO 449",
      "GS1",
      "MPEG-4 Part 3",
      "ISO 31-11",
      "Uniform Resource Identifier",
      "POSIX",
      "110 film",
      "ISO 2848",
      "Paywall",
      "Archival Resource Key",
      "ISO/IEC 8859-8",
      "ISO 1000",
      "PDF/E",
      "ISO 10303-28",
      "Common Object Request Broker Architecture",
      "MPEG-A",
      "International Standard Musical Work Code",
      "ISO/IEC 8859-5",
      "ISO/IEC 12207",
      "ISO 6344",
      "ISO 9000 family",
      "Tsinghua University",
      "ISO 11170",
      "Romanization of Georgian",
      "ISO 668",
      "C++",
      "ECMAScript for XML",
      "ISO 31-13",
      "ISO 9362",
      "ISO/IEC 27001",
      "ISO 7637",
      "ISO 10303-21",
      "ISO 2711",
      "ISO 19092",
      "ISO/IEC 8859-2",
      "ISO 15686",
      "Common Language Infrastructure",
      "ISO 4217",
      "British Standard Pipe",
      "ISO 3166-3",
      "ISO 28000",
      "Rectangular Micro QR Code",
      "ISO/IEC 27006",
      "International Standard Text Code",
      "ISO 704",
      "Open Virtualization Format",
      "Process Specification Language",
      "Link rot",
      "ISO/IEC 19794-5",
      "Linux Standard Base",
      "ISO 31-5",
      "Horsepower",
      "Performance",
      "ISO 10303-22",
      "FDI World Dental Federation notation",
      "COLLADA",
      "Vicat softening point",
      "Corporation for National Research Initiatives",
      "ISO/IEC 8859",
      "ISO 10962",
      "Water Resistant mark",
      "ISO 14000 family",
      "Prontor-Compur",
      "ISO 25178",
      "Common Criteria",
      "ISO 4165",
      "ISO 639-5",
      "ISO 19439",
      "ISO 6943",
      "Office Open XML",
      "HTTP request",
      "PHIGS",
      "Accuracy and precision",
      "ISO 690",
      "Scientific literature",
      "ISO 13485",
      "ISO/IEC 38500",
      "MPEG-21",
      "ISO 2146",
      "ISO 8373",
      "ISO/IEC 8859-12",
      "ISO 1745",
      "ISO 7002",
      "OECD iLibrary",
      "Salt spray test",
      "PLOS Biology",
      "PMID (identifier)",
      "Power take-off",
      "Wikidata",
      "ISO",
      "ISO 31-12",
      "Academic journal",
      "ISO 5964",
      "On-board diagnostics",
      "ISO/IEC 7813",
      "MaxiCode",
      "ISO 2047",
      "ISO-8859-8-I",
      "ISO/IEC 4909",
      "MPEG-4 Part 2",
      "ISO 3166",
      "ISO 2852",
      "Copying and pasting",
      "ISO 5775",
      "ISO 9660",
      "Bibcode",
      "ISO/IEC 7810",
      "Universally unique identifier",
      "OCR-A",
      "Mail reader",
      "ISO/IEC 14651",
      "OpenURL",
      "Standard Generalized Markup Language",
      "135 film",
      "ISO/IEC 6523",
      "Unified Modeling Language",
      "Crossref",
      "ISO 6385",
      "Intellectual property",
      "Handle System",
      "ISO/IEC 27000",
      "Manufacturing Message Specification",
      "ISO 1",
      "Ruby (programming language)",
      "ISO 13399",
      "MPEG-1",
      "ISO 13490",
      "ISO 2015",
      "Not-for-profit",
      "Fuel oil",
      "Torx",
      "ISO 2014",
      "ISO 26000",
      "Handle (computing)",
      "CNKI",
      "International Organization for Standardization",
      "ISO 3307",
      "List of ISO standards",
      "ISO 31-0",
      "Motion JPEG 2000",
      "ISO 10006",
      "ISO/IEC 27002",
      "ISO 31-10",
      "CHILL",
      "ISO 3166-2",
      "Self-archiving",
      "EXPRESS (data modeling language)",
      "International Standard Music Number",
      "Meta-Object Facility",
      "Object identifier",
      "Semantic interoperability",
      "OCR-B",
      "PDF/UA",
      "ISO 10303",
      "Publications Office (European Union)",
      "ISO 6709",
      "ISO/IEC 10967",
      "MPEG-2",
      "PURL",
      "Uniform Resource Locator",
      "ISO 15189",
      "Advanced Video Coding",
      "ISO 259",
      "ISO/IEC 9995",
      "ISO 128",
      "Document",
      "ISO 16750",
      "ISO/IEC 42010",
      "ISO 657",
      "ISO 19092-1",
      "DOI Foundation",
      "ISO 14031",
      "Digital identity",
      "Universal Coded Character Set",
      "ISO 639-3",
      "Graphical Kernel System",
      "PDF/VT",
      "Photographic Activity Test",
      "Z notation",
      "JPEG XS",
      "ANSI escape code",
      "ISO 31-8",
      "ISO 639",
      "ISO/IEC 8652",
      "ISO 15924",
      "ISO 14971",
      "ISO 3977",
      "ISO 2921",
      "ISO 37001",
      "Whirlpool (hash function)",
      "ISO 3103",
      "ISO 17100",
      "ISO 13406-2",
      "COBOL",
      "Web browser",
      "ISBN",
      "ISO 5428",
      "ISO 15022",
      "ISO 22395",
      "ISO 9241",
      "ISO 20121",
      "ISO 999",
      "OSI model",
      "Magnetic ink character recognition",
      "ISO 7010",
      "ISO 15926 WIP",
      "Automotive fuse",
      "ISO/IEC 7816",
      "File Allocation Table",
      "MathML",
      "European Union",
      "ISO 10218",
      "ISLISP",
      "ISO 11992",
      "MPEG-4 Part 11",
      "ISO 12006",
      "X3D",
      "JPEG 2000",
      "ISO base media file format",
      "ISO 10160",
      "ISO 7001",
      "126 film",
      "ISO 5776",
      "ISO/IEC 9797-1",
      "ISO 22000",
      "ISO/IEC 9529",
      "Mixed raster content",
      "Info URI scheme",
      "Name server",
      "ISO 4",
      "ISO 843",
      "Hyperlink",
      "LCEVC",
      "ISO/IEC 646",
      "ISO 8583",
      "X.500",
      "ISO/IEC 9126",
      "ISO 8178",
      "Organisation for Economic Co-operation and Development",
      "ISO 5426",
      "ISO/IEC 5218",
      "ISO 14644",
      "Case-insensitive",
      "International Standard Recording Code",
      "ISO 7736",
      "Han Xin code",
      "ISO 19600",
      "Data dictionary",
      "ISO 50001",
      "Knowledge Discovery Metamodel",
      "ISO/IEC 8859-4",
      "Computer Graphics Metafile",
      "Permalink",
      "ArmSCII",
      "MPEG-G",
      "ISO-TimeML",
      "Shoe size",
      "Computational trust",
      "ISO/IEC 14443",
      "Metadata standard",
      "QR code",
      "Renard series",
      "Minimal BASIC",
      "ISO/IEC 7064",
      "Kunrei-shiki",
      "International Standard Link Identifier",
      "ISO 10161",
      "JPIP",
      "JPEG XR",
      "ISO 56000",
      "Legal Entity Identifier",
      "ISO/IEEE 11073",
      "ISO 55000",
      "C Sharp (programming language)",
      "Prolog",
      "ISO 639-6",
      "Virtual International Authority File"
    ]
  },
  "Allen Kent": {
    "url": "https://en.wikipedia.org/wiki/Allen_Kent",
    "title": "Allen Kent",
    "content": "Allen Kent (October 24, 1921 – May 1, 2014) was an American information scientist . He was born in New York City. [ 1 ] At City College of New York , he earned a degree in chemistry . [ 2 ] During World War II , he served in the United States Army Air Forces . [ 2 ] After the war, he worked on a classified project at MIT in mechanized document encoding and search. [ 1 ] In 1955, he helped found the Center for Documentation Communication Research at Western Reserve University . [ 3 ] This was \"the first academic program in the field of mechanized information retrieval, first using cards , then utilizing new reel-to-reel tape technology.\" [ 1 ] In the same year, he introduced the measures of precision and recall in Perry, Kent & Berry (1955) . In 1959, he wrote an article for Harper's magazine entitled, \"A Machine That Does Research\" which provided one of the first insights in mainstream media about how Americans' lives can change due to electronic information technology. [ 4 ] He joined the faculty of the University of Pittsburgh in 1963, where in 1970 he began the Department of Information Science. [ 5 ] He retired from the university in 1992. [ 5 ] At the time of his death, he was Distinguished Service Professor in the School of Information Sciences at the University of Pittsburgh [ 5 ] The school named a scholarship after him. [ 6 ]",
    "links": [
      "American Society for Information Science and Technology",
      "Wayback Machine",
      "Thomas J. Galvin",
      "Chemistry",
      "University of Pittsburgh",
      "United States Army Air Forces",
      "Precision and recall",
      "Western Reserve University",
      "Massachusetts Institute of Technology",
      "World War II",
      "Information scientist",
      "City College of New York",
      "Punched card",
      "Reel-to-reel",
      "Doi (identifier)",
      "Pittsburgh Post-Gazette"
    ]
  },
  "Dimension reduction": {
    "url": "https://en.wikipedia.org/wiki/Dimension_reduction",
    "title": "Dimension reduction",
    "content": "Dimensionality reduction , or dimension reduction , is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension . Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality , and analyzing the data is usually computationally intractable . Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing , speech recognition , neuroinformatics , and bioinformatics . [ 1 ] Methods are commonly divided into linear and nonlinear approaches. [ 1 ] Linear approaches can be further divided into feature selection and feature extraction . [ 2 ] Dimensionality reduction can be used for noise reduction , data visualization , cluster analysis , or as an intermediate step to facilitate other analyses. The process of feature selection aims to find a suitable subset of the input variables ( features , or attributes ) for the task at hand. The three strategies are: the filter strategy (e.g., information gain ), the wrapper strategy (e.g., accuracy-guided search), and the embedded strategy (features are added or removed while building the model based on prediction errors). Data analysis such as regression or classification can be done in the reduced space more accurately than in the original space. [ 3 ] Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. [ 4 ] [ 5 ] For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning . [ 6 ] The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation ) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proved on a case-by-case basis as not all systems exhibit this behavior. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors. [ citation needed ] NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist, [ 7 ] [ 8 ] such as astronomy. [ 9 ] [ 10 ] NMF is well known since the multiplicative update rule by Lee & Seung, [ 7 ] which has been continuously developed: the inclusion of uncertainties, [ 9 ] the consideration of missing data and parallel computation, [ 11 ] sequential construction [ 11 ] which leads to the stability and linearity of NMF, [ 10 ] as well as other updates including handling missing data in digital image processing . [ 12 ] With a stable component basis during construction, and a linear modeling process, sequential NMF [ 11 ] is able to preserve the flux in direct imaging of circumstellar structures in astronomy, [ 10 ] as one of the methods of detecting exoplanets , especially for the direct imaging of circumstellar discs . In comparison with PCA, NMF does not remove the mean of the matrices, which leads to physical non-negative fluxes; therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al. [ 10 ] Principal component analysis can be employed in a nonlinear way by means of the kernel trick . The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is called kernel PCA . Other prominent nonlinear techniques include manifold learning techniques such as Isomap , locally linear embedding (LLE), [ 13 ] Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis. [ 14 ] These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA. More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming . The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space) while maximizing the distances between points that are not nearest neighbors. An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling , which is identical to PCA; Isomap , which uses geodesic distances in the data space; diffusion maps , which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis. A different approach to nonlinear dimensionality reduction is through the use of autoencoders , a special kind of feedforward neural networks with a bottleneck hidden layer. [ 15 ] The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines ) that is followed by a finetuning stage based on backpropagation . Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support-vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. [ 16 ] [ 17 ] Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter. Autoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation. T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique useful for the visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well. [ 18 ] Uniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant. For high-dimensional datasets, dimension reduction is usually performed prior to applying a k -nearest neighbors ( k -NN) algorithm in order to mitigate the curse of dimensionality . [ 19 ] Feature extraction and dimension reduction can be combined in one step, using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques to pre-process the data, followed by clustering via k -NN on feature vectors in a reduced-dimension space. In machine learning , this process is also called low-dimensional embedding . [ 20 ] For high-dimensional datasets (e.g., when performing similarity search on live video streams, DNA data, or high-dimensional time series ), running a fast approximate k -NN search using locality-sensitive hashing , random projection , [ 21 ] \"sketches\", [ 22 ] or other high-dimensional similarity search techniques from the VLDB conference toolbox may be the only feasible option. A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions , [ 23 ] which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved.",
    "links": [
      "Doi (identifier)",
      "Statistical classification",
      "Digital image processing",
      "Neuroinformatics",
      "Principal component analysis",
      "Random projection",
      "S2CID (identifier)",
      "Feature selection",
      "Latent semantic analysis",
      "Data visualization",
      "Intrinsic dimension",
      "Weighted correlation network analysis",
      "T-distributed stochastic neighbor embedding",
      "Canonical correlation analysis",
      "Methods of detecting exoplanets",
      "Combinatorial optimization",
      "Correlation and dependence",
      "Diffusion map",
      "CUR matrix approximation",
      "Riemannian manifold",
      "Relevance",
      "Signal processing",
      "Matrix factorization (recommender systems)",
      "Kernel trick",
      "Long tail",
      "Locally linear embedding",
      "Recommender system",
      "Nonlinear dimensionality reduction",
      "K-nearest neighbors algorithm",
      "Implicit data collection",
      "Non-negative matrix factorization",
      "Multifactor dimensionality reduction",
      "Bibcode (identifier)",
      "Circumstellar disc",
      "Locally connected",
      "Feature (machine learning)",
      "Autoencoder",
      "Maximally informative dimensions",
      "Kernel principal component analysis",
      "Sammon mapping",
      "MovieLens",
      "Eigenvalues and eigenvectors",
      "Collaborative filtering",
      "CiteSeerX (identifier)",
      "Curse of dimensionality",
      "Similarity search",
      "Dimensional reduction",
      "Embedding",
      "Support-vector machine",
      "Sparse matrix",
      "Locality-sensitive hashing",
      "Product finder",
      "Local tangent space alignment",
      "High-dimensional space",
      "Linear discriminant analysis",
      "Riemannian metric",
      "PMID (identifier)",
      "Data analysis",
      "Covariance",
      "Sebastian Seung",
      "Item-item collaborative filtering",
      "Bioinformatics",
      "Cold start (recommender systems)",
      "ArXiv (identifier)",
      "Feedforward neural network",
      "Machine learning",
      "Collective intelligence",
      "Decision support system",
      "Content discovery platform",
      "Singular value decomposition",
      "Factor analysis",
      "Restricted Boltzmann machine",
      "Nearest neighbor search",
      "Cluster analysis",
      "Time series",
      "Semidefinite embedding",
      "Tensor representation",
      "Music Genome Project",
      "Johnson–Lindenstrauss lemma",
      "Information gain (decision tree)",
      "Backpropagation",
      "Multidimensional scaling",
      "Star (classification)",
      "Manifold learning",
      "Feature extraction",
      "Preference elicitation",
      "Hyperparameter optimization",
      "Data transformation (statistics)",
      "Nature (journal)",
      "Wayback Machine",
      "Mutual information",
      "ACM Conference on Recommender Systems",
      "Speech recognition",
      "Matrix (mathematics)",
      "ISSN (identifier)",
      "Collaborative search engine",
      "Multilinear subspace learning",
      "Netflix Prize",
      "Sufficient dimension reduction",
      "Information gain in decision trees",
      "Semidefinite programming",
      "ISBN (identifier)",
      "Semantic mapping (statistics)",
      "GroupLens Research",
      "Uniform manifold approximation and projection",
      "MinHash",
      "International Conference on Very Large Data Bases",
      "Neuroscience",
      "Maximum variance unfolding",
      "MIT Press",
      "Noise reduction",
      "Regression analysis",
      "Isomap",
      "Topological data analysis"
    ]
  },
  "Grateful Med": {
    "url": "https://en.wikipedia.org/wiki/Grateful_Med",
    "title": "Grateful Med",
    "content": "Grateful Med was a direct health professional interface to MEDLINE and other MEDLARS databases. Grateful Med, a pun on the rock band Grateful Dead , [ 1 ] [ 2 ] was adapted from Microsearch, an ELHILL user interface that assembled query language prior to connecting to the National Library of Medicine (NLM) mainframe. It was released to the public in 1986 after rapid development as a pet project of newly appointed NLM Director Donald A. B. Lindberg , who wanted to create a front-end software interface to make NLM's mainframe searches directly useful to physicians. Prior to its release, searches involved learning command language, asking librarians, or using CD-ROM -based MEDLINE subsets. [ 3 ] Grateful Med prepared the query prior to connecting to the mainframe, and disconnected immediately after retrieving the results. This significantly lowered connection time and search costs. Subsequent research demonstrated that when hospital-based physicians had to pay for Grateful Med search results, searches decreased by 30%. [ 4 ] In addition to hospital and library access, access to MEDLINE through GRATEFUL MED became possible for rural practitioners through procurement of NLM passwords. [ 5 ] Grateful Med 1.0, released in 1986, [ 6 ] was replaced by Grateful Med 2.0 in 1987 due to user input solicited and curated by Rosemary Woodsmall. [ 7 ] The database was later uploaded on the internet on July 1996, where it was branded as \"Internet Grateful Med\". [ 8 ] It was later made available for free on June 1997. [ 9 ] In total it remained in use from 1986 through 2001, when it was removed primarily due to its replacement by PubMed . Prior to the release of PubMed in 1996, Grateful Med was used in 80% of NLM database searches. [ 1 ]",
    "links": [
      "PMC (identifier)",
      "National Library of Medicine",
      "CD-ROM",
      "Donald A. B. Lindberg",
      "Doi (identifier)",
      "ISSN (identifier)",
      "PMID (identifier)",
      "Grateful Dead",
      "MEDLARS",
      "MEDLINE",
      "PubMed"
    ]
  },
  "Automatic summarization": {
    "url": "https://en.wikipedia.org/wiki/Automatic_summarization",
    "title": "Automatic summarization",
    "content": "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary ) that represents the most important or relevant information within the original content. Artificial intelligence (AI) algorithms are commonly developed and employed to achieve this, specialized for different types of data. Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document. [ 1 ] On the other hand, visual content can be summarized using computer vision algorithms. Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection. [ 2 ] [ 3 ] [ 4 ] Video summarization algorithms identify and extract from the original video content the most important frames ( key-frames ), and/or the most important video segments ( key-shots ), normally in a temporally ordered fashion. [ 5 ] [ 6 ] [ 7 ] [ 8 ] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content. In 2022 Google Docs released an automatic summarization feature. [ 9 ] There are two general approaches to automatic summarization: extraction and abstraction . Here, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail. [ 10 ] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome). [ 11 ] Abstractive summarization methods generate new text that did not exist in the original text. [ 12 ] This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to images and videos, which is why most summarization systems are extractive. Approaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate. There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization , which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization , sometimes called query-based summarization , which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs. An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization . A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary. Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images. [ 13 ] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured. At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the core-set . These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function , Determinantal point process , maximal marginal relevance (MMR) etc. The task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text. [ 14 ] In the case of research articles , many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below. Consider the example text from a news article: A keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep understanding of the text , which makes it difficult for a computer system. Keyphrases have many applications. They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search ), and be employed in generating index entries for a large text corpus. Depending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme. Beginning with the work of Turney, [ 15 ] many researchers have approached keyphrase extraction as a supervised machine learning problem. Given a document, we construct an example for each unigram , bigram , and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases. After training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases. Keyphrase extractors are generally evaluated using precision and recall . Precision measures how many of the proposed keyphrases are actually correct. Recall measures how many of the true keyphrases your system proposed. The two measures can be combined in an F-score, which is the harmonic mean of the two ( F = 2 PR /( P + R ) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization. Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision. We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various Boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper. In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number. Once examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes , and rule induction. In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases. Another keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data . Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate. Unsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm [ 16 ] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks . In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties. Thus the algorithm is easily portable to new domains and languages. TextRank is a general purpose graph -based ranking algorithm for NLP . Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph). The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step. Edges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2–10. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text cohesion \" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader. Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text. It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\". In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below. Like keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases. Supervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 evaluation only considers unigrams. During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a Naive Bayes classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain. A promising approach is adaptive document/text summarization. [ 17 ] It involves first recognizing the text genre and then applying summarization algorithms optimized for this genre. Such software has been created. [ 18 ] The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \" centroid \" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence. A more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank [ 19 ] is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task. In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document. The edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common ( normalized by the sentences' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous similarity scores as weights. In both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary. It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ( MEAD ) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary. Unlike TextRank, LexRank has been applied to multi-document summarization. Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload . Multi-document summarization may also be done in response to a question. [ 20 ] [ 11 ] Multi-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. [ dubious – discuss ] Multi-document extractive summarization faces a problem of redundancy. Ideally, we want to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). For example, in a set of news articles about some event, each article is likely to have many similar sentences. To address this issue, LexRank applies a heuristic post-processing step that adds sentences in rank order, but discards sentences that are too similar to ones already in the summary. This method is called Cross-Sentence Information Subsumption (CSIS). These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. Its importance also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to an arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain. A related method is Maximal Marginal Relevance (MMR), [ 21 ] which uses a general-purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks (a random walk where certain states end the walk). The algorithm is called GRASSHOPPER. [ 22 ] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization). The state of the art results for multi-document summarization are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07. [ 23 ] Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04. [ 24 ] A new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position. It does not use word frequency, training or preprocessing. It uses two user-supplied parameters: equivalence (when are two sentences to be considered equivalent?) and relevance (how long is the desired summary?). The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of coverage , information , representation and diversity . Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which cover a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a determinantal point process to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem. While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple greedy algorithm admits a constant factor guarantee. [ 25 ] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems. Submodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012 [ 26 ] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011, [ 27 ] shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems. [ citation needed ] Submodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show [ 28 ] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015 [ 29 ] show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets. [ 30 ] Specific applications of automatic summarization include: The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries. Evaluation can be intrinsic or extrinsic, [ 36 ] and inter-textual or intra-textual. [ 37 ] Intrinsic evaluation assesses the summaries directly, while extrinsic evaluation evaluates how the summarization system affects the completion of some other task. Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc. Intra-textual evaluation assess the output of a specific summarization system, while inter-textual evaluation focuses on contrastive analysis of outputs of several summarization systems. Human judgement often varies greatly in what it considers a \"good\" summary, so creating an automatic evaluation process is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive, as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage. The most common way to evaluate summaries is ROUGE (Recall-Oriented Understudy for Gisting Evaluation). It is very common for summarization and translation systems in NIST 's Document Understanding Conferences. [2] ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references. It calculates n-gram overlaps between automatically generated summaries and previously written human summaries. It is recall-based to encourage inclusion of all important topics in summaries. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is the fraction of unigrams that appear in both the reference summary and the automatic summary out of all unigrams in the reference summary. If there are multiple reference summaries, their scores are averaged. A high level of overlap should indicate a high degree of shared concepts between the two summaries. ROUGE cannot determine if the result is coherent, that is if sentences flow together in a sensibly. High-order n-gram ROUGE measures help to some degree. Another unsolved problem is Anaphor resolution . Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization. [ 38 ] Domain-independent summarization techniques apply sets of general features to identify information-rich text segments. Recent research focuses on domain-specific summarization using knowledge specific to the text's domain, such as medical knowledge and ontologies for summarizing medical texts. [ 39 ] The main drawback of the evaluation systems so far is that we need a reference summary (for some methods, more than one), to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be made to create corpora of texts and their corresponding summaries. Furthermore, some methods require manual annotation of the summaries (e.g. SCU in the Pyramid Method). Moreover, they all perform a quantitative evaluation with regard to different similarity metrics. The first publication in the area dates back to 1957 [ 40 ] ( Hans Peter Luhn ), starting with a statistical technique. Research increased significantly in 2015. Term frequency–inverse document frequency had been used by 2016. Pattern-based summarization was the most powerful option for multi-document summarization found by 2016. In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF). Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity. By 2020, the field was still very active and research is shifting towards abstractive summation and real-time summarization. [ 41 ] Recently the rise of transformer models replacing more traditional RNN ( LSTM ) have provided a flexibility in the mapping of text sequences to text sequences of a different type, which is well suited to automatic summarization. This includes models such as T5 [ 42 ] and Pegasus. [ 43 ]",
    "links": [
      "Lexical (semiotics)",
      "Latent Dirichlet allocation",
      "Stop word",
      "Google Docs",
      "Artificial intelligence",
      "Doi (identifier)",
      "Part-of-speech tagging",
      "Interactive fiction",
      "Automated paraphrasing",
      "NIST",
      "AI-complete",
      "Word-sense disambiguation",
      "S2CID (identifier)",
      "Unigram",
      "TL;DR",
      "Keyword extraction",
      "Syntactic parsing (computational linguistics)",
      "Similarity score",
      "Latent semantic analysis",
      "Corpus linguistics",
      "Sentence extraction",
      "Bank of English",
      "Topic model",
      "Optical character recognition",
      "Information retrieval",
      "Machine translation",
      "Hdl (identifier)",
      "Grammar checker",
      "Sentiment analysis",
      "Image",
      "Semantic analysis (machine learning)",
      "Large language model",
      "Optimal facility location",
      "LSTM",
      "Neural machine translation",
      "Combinatorial optimization",
      "Determinantal point process",
      "Graph (abstract data type)",
      "Universal Dependencies",
      "Google Ngram Viewer",
      "Coherence (linguistics)",
      "Formal semantics (natural language)",
      "Plain text",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Abstract (summary)",
      "Cosine similarity",
      "ProQuest",
      "Binary classification",
      "Stationary distribution",
      "Research article",
      "Bag-of-words model",
      "Parsing",
      "Computer vision",
      "Non-negative matrix factorization",
      "Predictive text",
      "Bibcode (identifier)",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "Named-entity recognition",
      "Parallel text",
      "Word embedding",
      "Argument mining",
      "SpaCy",
      "Ontology learning",
      "Natural-language understanding",
      "TF-IDF",
      "DBpedia",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Word2vec",
      "Random walk",
      "Computational linguistics",
      "Information extraction",
      "Collocation extraction",
      "Thesaurus (information retrieval)",
      "Pachinko allocation",
      "Natural Language Toolkit",
      "Term frequency–inverse document frequency",
      "Lexical resource",
      "Word-sense induction",
      "Speech synthesis",
      "PMID (identifier)",
      "Language model",
      "Wikidata",
      "Shallow parsing",
      "Precision and recall",
      "Speech segmentation",
      "Voice user interface",
      "Video synopsis",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Internet bot",
      "Eigenvalue",
      "Document-term matrix",
      "ArXiv (identifier)",
      "Small language model",
      "Speech corpus",
      "Natural language processing",
      "Pronunciation assessment",
      "Hallucination (artificial intelligence)",
      "Rule-based machine translation",
      "Cohesion (linguistics)",
      "Training set",
      "OCLC (identifier)",
      "Stemming",
      "ROUGE (metric)",
      "Co-occurrence",
      "Shixia Liu",
      "UBY",
      "Linear combination",
      "Social network",
      "Compound-term processing",
      "Text simplification",
      "GloVe",
      "Bigram",
      "Semantic parsing",
      "Text segmentation",
      "Centroid",
      "Information overload",
      "Example-based machine translation",
      "Multi-document summarization",
      "Distant reading",
      "Cluster analysis",
      "Naive Bayes classifier",
      "Statistical machine translation",
      "Trigram",
      "Submodular set function",
      "News aggregators",
      "Text corpus",
      "WordNet",
      "Concordancer",
      "Natural language understanding",
      "Anaphora (linguistics)",
      "FastText",
      "BabelNet",
      "Transformer (machine learning model)",
      "PageRank",
      "Maximum entropy classifier",
      "Treebank",
      "BERT (language model)",
      "Genetic algorithm",
      "Algorithm",
      "Semantic decomposition (natural language processing)",
      "Naive Bayes",
      "Semantic similarity",
      "Spell checker",
      "Netherlands Organisation for Applied Scientific Research",
      "Automatic identification and data capture",
      "Explicit semantic analysis",
      "Lexical analysis",
      "Text mining",
      "Concept mining",
      "Image collection exploration",
      "Language resource",
      "Hans Peter Luhn",
      "Semantic network",
      "Linguistic Linked Open Data",
      "Supervised machine learning",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Absorbing Markov chain",
      "Transfer-based machine translation",
      "Set cover problem",
      "Wayback Machine",
      "Megan Squire",
      "Textual entailment",
      "Speech recognition",
      "Terminology extraction",
      "ISSN (identifier)",
      "Lemmatisation",
      "Adversarial stylometry",
      "Text processing",
      "Quantile normalization",
      "Rnn (software)",
      "ISBN (identifier)",
      "Question answering",
      "Greedy algorithm",
      "Chatbot",
      "Document classification",
      "Automated essay scoring",
      "MEAD",
      "Reddit",
      "Truecasing",
      "Long short-term memory",
      "Full-text search",
      "FrameNet",
      "Transformer (deep learning architecture)",
      "Machine-readable dictionary",
      "Internet slang"
    ]
  },
  "Ricardo Baeza-Yates": {
    "url": "https://en.wikipedia.org/wiki/Ricardo_Baeza-Yates",
    "title": "Ricardo Baeza-Yates",
    "content": "Ricardo A. Baeza-Yates (born March 21, 1961) is a Chilean computer scientist specializing in algorithms, data structures, information retrieval, web search and responsible AI. Since 2025 he is a part-time Wallenberg AI, Autonomous Systems and Software Program professor at the KTH Royal Institute of Technology of Sweden in Stockholm. He is also part-time professor at Universitat Pompeu Fabra in Barcelona and Universidad de Chile in Santiago. He is an expert member of the Global Partnership on Artificial Intelligence , a member of the Association for Computing Machinery 's US Technology Policy Committee as well as IEEE 's AI Committee. He is member of the Chilean Academy of Sciences (2002), [ 1 ] founding member of the Chilean Academy of Engineering (2010), corresponding member of the Brazilian Academy of Sciences (2018), [ 2 ] and member of the Academia Europaea (2023). [ 3 ] He is an ACM Fellow (2009). [ 4 ] and an IEEE Fellow (2011). [ 5 ] He is a former member of Spain's Advisory Council on AI (2019–2023). From January 2021 to March 2025 he was the Director of Research at the Institute for Experiential AI at Northeastern University in the Silicon Valley campus. From June 2016 until June 2020 he was CTO of NTENT, a semantic search technology company. [ 6 ] Before, until February 2016, he was VP of Research for Yahoo! Labs , leading teams in United States, Europe, Asia and Latin America. [ 7 ] He obtained a PhD from the University of Waterloo with Efficient Text Searching , supervised by Gaston Gonnet and granted in 1989. [ 8 ] Dr. Baeza-Yates was awarded one of the Spanish national Computer Science awards in 2018 [ 15 ] as well as the J.W. Graham Medal in Computing and Innovation by the University of Waterloo, Canada, in 2007. In August 2008, Dr. Baeza-Yates was proposed for the first time to the Chilean National Prize in Applied Sciences (Premio Nacional de Ciencias Aplicadas) [ citation needed ] . He has been proposed again most of even years when this award is given. In 2024, he won the award. [ 16 ]",
    "links": [
      "Data science",
      "Academia Europaea",
      "Data structure",
      "Gonzalo Navarro",
      "Mathematics Genealogy Project",
      "Thesis",
      "Northeastern University",
      "Universidad de Chile",
      "Doi (identifier)",
      "Fuzzy string searching",
      "Google Scholar",
      "ACM Fellow",
      "LinkedIn",
      "J.W. Graham Medal",
      "S2CID (identifier)",
      "IEEE Fellow",
      "University of Chile",
      "Wallenberg AI, Autonomous Systems and Software Program",
      "Information retrieval",
      "Royal Institute of Technology",
      "Gaston Gonnet",
      "Hdl (identifier)",
      "ISBN (identifier)",
      "Global Partnership on Artificial Intelligence",
      "Responsible AI",
      "Yahoo! Labs",
      "Doctoral advisor",
      "Computer scientist",
      "Association for Computing Machinery",
      "Computer science",
      "IEEE",
      "Association for Information Science and Technology",
      "Shift Or Algorithm",
      "University of Waterloo",
      "Algorithm",
      "Universitat Pompeu Fabra",
      "Kurt Gödel",
      "List of University of Waterloo people",
      "Pompeu Fabra University",
      "NTENT",
      "Bitap algorithm"
    ]
  },
  "Pearl growing": {
    "url": "https://en.wikipedia.org/wiki/Pearl_growing",
    "title": "Pearl growing",
    "content": "Pearl growing is a metaphor taken from the process of small bits of sand growing to make a beautiful pearl, which is used in information literacy . This is also called \"snowballing\", [ 1 ] alluding to the process of how a snowball can grow into a big snow-man by accumulating snow. In this context this refers to the process of using one information item (like a subject term or citation ) to find content that provides more information items. This search strategy is most successfully employed at the beginning of the research process as the searcher uncovers new pearls about his or her topic. Citation pearl growing is the act of using one relevant source, or citation , to find more relevant sources on a topic. The searcher usually has a document that matches a topic or information need. From this document, the searcher is able to find other keywords, descriptors and themes to use in a subsequent search. [ 2 ] Citation Pearl Growing is a popular search and retrieval method used by librarians . [ 3 ] Subject pearl growing is a strategy used in an electronic database that has subject or keyword descriptors. By clicking on one subject , the searcher is able to find other related subjects and subdivisions that may or may not be useful to the search. Searchers use the pearl growing technique when surfing the Internet . Using the theory that websites that link to each other are similar, a searcher can move from site to site, collecting information. Ramer (2005) suggests pearl growing by using the pearl as a search term in search engines or even in the URL . In systematic literature reviews, pearl growing is a technique used to ensure all relevant articles are included. Pearl growing involves identifying a primary article that meets the inclusion criteria for the review. From this primary article, the researcher works backwards to find all the articles cited in the bibliography and checks them for eligibility for inclusion in the review. The researcher then works forwards to search for any articles that have cited the primary article. It is estimated that up to 51% of references in a systematic review are identified by pearl growing. [ 4 ] There is evidence that using pearl growing for systematic reviews is a more comprehensive approach and more likely to identify all relevant articles compared to online database searches. [ 5 ] Pearl growing, when applied to scientific literature, may also be referred to as citation mining or snowballing.",
    "links": [
      "Cultured pearl",
      "Metaphor",
      "Information literacy",
      "Internet",
      "URL",
      "ISBN (identifier)",
      "PMC (identifier)",
      "Wayback Machine",
      "Bibliographic database",
      "Web search engine",
      "Pearl",
      "Citation",
      "Subject term",
      "Doi (identifier)",
      "Librarian",
      "Keyword (computer programming)",
      "PMID (identifier)"
    ]
  },
  "Personal information management": {
    "url": "https://en.wikipedia.org/wiki/Personal_information_management",
    "title": "Personal information management",
    "content": "Personal information management ( PIM ) is the study and implementation of the activities that people perform to acquire or create, store, organize, maintain, retrieve, and use informational items such as documents (paper-based and digital), web pages , and email messages for everyday use to complete tasks (work-related or not) and fulfill a person's various roles (as parent, employee, friend, member of community, etc.); [ 1 ] [ 2 ] it is information management with intrapersonal scope. Personal knowledge management is by some definitions a subdomain. One ideal of PIM is that people should always have the right information in the right place, in the right form, and of sufficient completeness and quality to meet their current need. Technologies and tools can help so that people spend less time with time-consuming and error-prone clerical activities of PIM (such as looking for and organising information). But tools and technologies can also overwhelm people with too much information leading to information overload . A special focus of PIM concerns how people organize and maintain personal information collections, and methods that can help people in doing so. People may manage information in a variety of settings, for a variety of reasons, and with a variety of types of information. For example, a traditional office worker might manage physical documents in a filing cabinet by placing them in hanging folders organized alphabetically by project name. More recently, this office worker might organize digital documents into the virtual folders of a local, computer-based file system or into a cloud-based store using a file hosting service (e.g., Dropbox , Microsoft OneDrive , Google Drive ). People manage information in many more private, personal contexts as well. A parent may, for example, collect and organize photographs of their children into a photo album which might be paper-based or digital. PIM considers not only the methods used to store and organize information, but also is concerned with how people retrieve information from their collections for re-use. For example, the office worker might re-locate a physical document by remembering the name of the project and then finding the appropriate folder by an alphabetical search. On a computer system with a hierarchical file system , a person might need to remember the top-level folder in which a document is located, and then browse through the folder contents to navigate to the desired document. Email systems often support additional methods for re-finding such as fielded search (e.g., search by sender, subject, date). The characteristics of the document types, the data that can be used to describe them (meta-data), and features of the systems used to store and organize them (e.g. fielded search) are all components that may influence how users accomplish personal information management. The purview of PIM is broad. A person's perception of and ability to effect change in the world is determined, constrained, and sometimes greatly extended, by an ability to receive, send and otherwise manage information. Research in the field of personal information management has considered six senses in which information can be personal (to \"me\") and so an object of that person's PIM activities: [ 2 ] An encyclopaedic review of PIM literature suggests that all six senses of personal information listed above and the tools and technologies used to work with such information (from email applications and word processors to personal information managers and virtual assistants ) combine to form a personal space of information (PSI, pronounced as in the Greek letter , alternately referred to as a personal information space ) that is unique for each individual. [ 3 ] Within a person's PSI are personal information collections (PICs) or, simply, collections. Examples include: Activities of PIM – i.e., the actions people take to manage information that is personal to them in one or more of the ways listed above – can be seen as an effort to establish, use, and maintain a mapping between information and need. [ 2 ] Two activities of PIM occur repeatedly throughout a person's day and are often prompted by external events. Meta-level activities focus more broadly on aspects of the mapping itself. PIM activities overlap with one another. For example, the effort to keep an email attachment as a document in a personal file system may prompt an activity to organize the file system e.g., by creating a new folder for the document. Similarly, activities to organize may be initiated by a person's efforts to find a document as when, for example, a person discovers that two folders have overlapping content and should be consolidated. Meta-level activities overlap not only with finding and keeping activities but, even more so, with each other. For example, efforts to reorganize a personal file system can be motivated by the evaluation that the current file organization is too time-consuming to maintain and doesn't properly highlight the information most in need of attention. Information sent and received takes many different information forms in accordance with a growing list of communication modes, supporting tools, and people's customs, habits, and expectations. People still send paper-based letters, birthday cards, and thank you notes. But increasingly, people communicate using digital forms of information including emails, digital documents shared (as attachments or via a file hosting service such as Dropbox ), blog posts and social media updates (e.g., using a service such as Facebook ), text messages and links, text, photos, and videos shared via services such as Twitter , Snapchat , Reddit , and Instagram . People work with information items as packages of information with properties that vary depending upon the information form involved. Files, emails, \"tweets\", Facebook updates, blog posts, etc. are each examples of the information item. The ways in which an information item can be manipulated depend upon its underlying form. Items can be created but not always deleted (completely). Most items can be copied, sent and transformed as in, for example, when a digital photo is taken of a paper document (transforming from paper to digital) and then possibly further transformed as when optical character recognition is used to extract text from the digital photo, and then transformed yet again when this information is sent to others via a text message. Information fragmentation [ 4 ] [ 2 ] is a key problem of PIM often made worse by the many information forms a person must work with. Information is scattered widely across information forms on different devices, in different formats, in different organizations, with different supporting tools. Information fragmentation creates problems for each kind of PIM activity. Where to keep new information? Where to look for (re-find) information already kept? Meta-level activities, such as maintaining and organizing, are also more difficult and time-consuming when different stores on different devices must be separately maintained. Problems of information fragmentation are especially manifest when a person must look across multiple devices and applications to gather together the information needed to complete a project. [ 5 ] PIM is a new field with ancient roots. When the oral rather than the written word dominated, human memory was the primary means for information preservation. [ 6 ] As information was increasingly rendered in paper form, tools were developed over time to meet the growing challenges of management. For example, the vertical filing cabinet , now such a standard feature of home and workplace offices, was first commercially available in 1893. [ 7 ] With the increasing availability of computers in the 1950s came an interest in the computer as a source of metaphors and a test bed for efforts to understand the human ability to process information and to solve problems . Newell and Simon pioneered the computer's use as a tool to model human thought. [ 8 ] [ 9 ] They produced \"The Logic Theorist \", generally thought to be the first running artificial intelligence (AI) program. The computer of the 1950s was also an inspiration for the development of an information processing approach to human behavior and performance. [ 10 ] After the 1950s research showed that the computer, as a symbol processor, could \"think\" (to varying degrees of fidelity) like people do, the 1960s saw an increasing interest in the use of the computer to help people to think better and to process information more effectively. Working with Andries van Dam and others, Ted Nelson , who coined the word \" hypertext \", [ 11 ] developed one of the first hypertext systems, The Hypertext Editing System, in 1968. [ 12 ] That same year, Douglas Engelbart also completed work on a hypertext system called NLS (oN-Line System). [ 13 ] Engelbart advanced the notion that the computer could be used to augment the human intellect. [ 14 ] [ 15 ] As heralded by the publication of Ulric Neisser 's book Cognitive Psychology , [ 16 ] the 1960s also saw the emergence of cognitive psychology as a discipline that focused primarily on a better understanding of the human ability to think, learn, and remember. The computer as aid to the individual, rather than remote number cruncher in a refrigerated room, gained further validity from work in the late 1970s and through the 1980s to produce personal computers of increasing power and portability. These trends continue: computational power roughly equivalent to that of a desktop computer of a decade ago can now be found in devices that fit into the palm of a hand. The phrase \"Personal Information Management\" was itself apparently first used in the 1980s in the midst of general excitement over the potential of the personal computer to greatly enhance the human ability to process and manage information. [ 17 ] The 1980s also saw the advent of so-called \"PIM tools\" that provided limited support for the management of such things as appointments and scheduling, to-do lists, phone numbers, and addresses. A community dedicated to the study and improvement of human–computer interaction also emerged in the 1980s. [ 18 ] [ 19 ] Prior to the introduction of the term \" Personal digital assistant \" (\"PDA\") by Apple in 1992, handheld personal organizers such as the Psion Organiser and the Sharp Wizard were also referred to as \"PIMs\". [ 20 ] [ 21 ] The time management and communications functions of PIMs largely migrated from PDAs to smartphones, with Apple, RIM (Research In Motion, now BlackBerry ), and others all manufacturing smartphones that offer most of the functions of earlier PDAs. As befits the \"information\" focus of PIM, PIM-relevant research of the 1980s and 1990s extended beyond the study of a particular device or application towards larger ecosystems of information management to include, for example, the organization of the physical office and the management of paperwork. [ 22 ] [ 23 ] Malone characterized personal organization strategies as 'neat' or 'messy' and described 'filing' and 'piling' approaches to the organization of information. [ 24 ] Other studies showed that people vary their methods for keeping information according to anticipated uses of that information in the future. [ 25 ] Studies explored the practical implications that human memory research might carry in the design of, for example, personal filing systems, [ 26 ] [ 27 ] [ 28 ] and information retrieval systems. [ 29 ] Studies demonstrated a preference for navigation (browsing, \"location-based finding) in the return to personal files, [ 30 ] a preference that endures today notwithstanding significant improvements in search support. [ 31 ] [ 32 ] [ 33 ] [ 34 ] and an increasing use of search as the preferred method of return to e-mails. [ 35 ] [ 36 ] PIM, as a contemporary field of inquiry with a self-identified community of researchers, traces its origins to a Special Interest Group (SIG) session on PIM at the CHI 2004 conference and to a special National Science Foundation (NSF)-sponsored workshop held in Seattle in 2005. [ 37 ] [ 4 ] Much PIM research can be grouped according to the PIM activity that is the primary focus of the research. These activities are reflected in the two main models of PIM, i.e., that primary PIM activities are finding/re-finding, keeping and meta-level activities [ 38 ] [ 2 ] (see section Activities of PIM ) or, alternatively, keeping, managing, and exploiting. [ 39 ] [ 40 ] Important research is also being done under the special topics: Personality, mood, and emotion both as impacting and impacted by a person's practice of PIM, the management of personal health information and the management of personal information over the long run and for legacy. Throughout a typical day, people repeatedly experience the need for information in large amounts and small (e.g., \"When is my next meeting?\"; \"What's the status of the budget forecast?\" \"What's in the news today?\") prompting activities to find and re-find. A large body of research in information seeking , information behavior , and information retrieval relates and especially to efforts to find information in public spaces such as the Web or a traditional library. There is a strong personal component even in efforts to find new information, never before experienced, from a public store such as the Web. For example, efforts to find information may be directed by a personally created outline, self-addressed email reminder or a to-do list. In addition, information inside a person's PSI can be used to support a more targeted, personalized search of the web. [ 41 ] A person's efforts to find useful information are often a sequence of interactions rather than a single transaction. Under a \"berry picking\" model of finding, information is gathered in bits and pieces through a series of interactions, and during this time, a person's expression of need, as reflected in the current query, evolves. [ 42 ] People may favor stepwise approach to finding needed information to preserve a greater sense of control and context over the finding process and smaller steps may also reduce the cognitive burden associated with query formulation. [ 43 ] In some cases, there simply is not a \"direct\" way to access the information. For example, a person's remembrance for a needed Web site may only be through an email message sent by a colleague i.e., a person may not recall a Web address nor even keywords that might get be used in a Web search but the person does recall that the Web site was mentioned recently in an email from a colleague). People may find (rather than re-find) information even when this information is ostensibly under their control. For example, items may be \"pushed\" into the PSI (e.g., via the inbox, podcast subscriptions, downloads). If these items are discovered later, it is through an act of finding not re-finding (since the person has no remembrance for the information). Lansdale [ 17 ] characterized the retrieval of information as a two-step process involving interplay between actions to recall and recognize . The steps of recall and recognition can iterate to progressively narrow the efforts to find the desired information. This interplay happens, for example, when people move through a folder hierarchy to a desired file or e-mail message or navigate through a website to a desired page. But re-finding begins first with another step: Remember to look in the first place. People may take the trouble to create Web bookmarks or to file away documents and then forget about this information so that, in worst case, the original effort is wasted. [ 44 ] [ 45 ] [ 46 ] [ 47 ] Also, finding/re-finding often means not just assembling a single item of information but rather a set of information. The person may need to repeat the finding sequence several times. A challenge in tool support is to provide people with ways to group or interrelate information items so that their chances improve of retrieving a complete set of the information needed to complete a task. [ 3 ] Over the years, PIM studies have determined that people prefer to return to personal information, most notably the information kept in personal digital files, by navigating rather than searching. [ 30 ] [ 32 ] [ 34 ] Support for searching personal information has improved dramatically over the years most notably in the provision for full-text indexing to improve search speed. [ 48 ] With these improvements, preference may be shifting to search as a primary means for locating email messages (e.g., search on subject or sender, for messages not in view). [ 49 ] [ 50 ] However, a preference persists for navigation as the primary means of re-finding personal files (e.g., stepwise folder traversal; scanning a list of files within a folder for the desired file), notwithstanding ongoing improvements in search support. [ 32 ] The enduring preference for navigation as a primary means of return to files may have a neurological basis [ 51 ] i.e., navigation to files appears to use mental facilities similar to those people use to navigate in the physical world. Preference for navigation is also in line with a primacy effect repeatedly observed in psychological research such that preferred method of return aligns with initial exposure. Under a first impressions hypothesis, if a person's initial experience with a file included its placement in a folder, where the folder itself was reached by navigating through a hierarchy of containing folders, then the person will prefer a similar method – navigation – for return to the file later. [ 50 ] There have been some prototyping efforts to explore an in-context creation e.g., creation in the context of a project the person is working on, of not only files, but also other forms of information such as web references and email. [ 52 ] Prototyping efforts have also explored ways to improve support for navigation e.g., by highlighting and otherwise making it easier to follow, the paths people are more likely to take in their navigation back to a file. [ 53 ] Many events of daily life are roughly the converse of finding events: People encounter information and try to determine what, if anything, they should do with this information, i.e., people must match the information encountered to current or anticipated needs. Decisions and actions relating to encountered information are collectively referred to as keeping activities. The ability to effectively handle information that is encountered by happenstance is essential to a person's ability to discover new material and make new connections. [ 54 ] People also keep information that they have actively sought but do not have time to process currently. A search on the web, for example, often produces much more information than can be consumed in the current session. Both the decision to keep this information for later use and the steps to do so are keeping activities. Keeping activities are also triggered when people are interrupted during a current task and look for ways of preserving the current state so that work can be quickly resumed later. [ 55 ] People keep appointments by entering reminders into a calendar and keep good ideas or \"things to pick up at the grocery store\" by writing down a few cryptic lines on a loose piece of paper. People keep not only to ensure they have the information later, but also to build reminders to look for and use this information. Failure to remember to use information later is one kind of prospective memory failure. [ 56 ] In order to avoid such a failure, people may, for example, self-e-mail a web page reference in addition to or instead of making a bookmark because the e-mail message with the reference appears in the inbox where it is more likely to be noticed and used. [ 57 ] The keeping decision can be characterized as a signal detection task subject to errors of two kinds: 1) an incorrect rejection (\"miss\") when information is ignored that later is needed and should have been kept (e.g., proof of charitable donations needed now to file a tax return) and 2) a false positive when information kept as useful (incorrectly judged as \"signal\") turns out not to be used later. [ 58 ] Information kept and never used only adds to the clutter – digital and physical – in a person's life. [ 59 ] Keeping can be a difficult and error prone effort. Filing i.e., placing information items such as paper documents, digital documents and emails, into folders, can be especially so. [ 60 ] [ 61 ] To avoid, or delay filing information (e.g., until more is known concerning where the information might be used), people may opt to put information in \"piles\" instead. [ 24 ] (Digital counterparts to physical piling include leaving information in the email inbox or placing digital documents and web links into a holding folder such as \"stuff to look at later\".) But information kept in a pile, physical or virtual, is easily forgotten as the pile fades into a background of clutter and research indicates that a typical person's ability to keep track of different piles, by location alone, is limited. [ 62 ] Tagging provides another alternative to filing information items into folders. A strict folder hierarchy does not readily allow for the flexible classification of information even though, in a person's mind, an information item might fit in several different categories. [ 63 ] A number of tag-related prototypes for PIM have been developed over the years. [ 64 ] [ 65 ] A tagging approach has also been pursued in commercial systems, most notably Gmail (as \"labels\"), but the success of tags so far is mixed. Bergman et al. found that users, when provided with options to use folders or tags, preferred folders to tags and, even when using tags, they typically refrained from adding more than a single tag per information item. [ 66 ] [ 67 ] Civan et al., through an engagement of participants in critical, comparative observation of both tagging and the use of folders were able to elicit some limitations of tagging not previously discussed openly such as, for example, that once a person decides to use multiple tags, it is usually important to continue doing so (else the tag not applied consistently becomes ineffective as a means of retrieving a complete set of items). [ 68 ] Technologies may help to reduce the costs, in personal time and effort, of keeping and the likelihood of error. For example, the ability to take a digital photo of a sign, billboard announcement or the page of a paper document can obviate the task of otherwise transcribing (or photocopying) the information. A person's ongoing use of a smartphone through the day can create a time-stamped record of events as a kind of automated keeping and especially of information \"experienced by me\" (see section, \"The senses in which information is personal\") with potential use in a person's efforts to journal or to return to information previously experienced (\"I think I read the email while in the taxi on the way to the airport...\"). Activity tracking technology can further enrich the record of a person's daily activity with tremendous potential use for people to enrich their understanding of their daily lives and the healthiness of their diet and their activities. [ 69 ] Technologies to automate the keeping of personal information segue to personal informatics and the quantified self movement, life logging, in the extreme, a 'total capture\" of information. [ 70 ] Tracking technologies raise serious issues of privacy (see \" Managing privacy and the flow of information \"). Additional questions arise concerning the utility and even the practical accessibility of \"total capture\". [ 71 ] Activities of finding and, especially, keeping can segue into activities to maintain and organize as when, for example, efforts to keep a document in the file system prompt the creation of a new folder or efforts to re-find a document highlight the need to consolidate two folders with overlapping content and purpose. Differences between people are especially apparent in their approaches to the maintenance and organization of information. Malone [ 24 ] distinguished between \"neat\" and \"messy\" organizations of paper documents. \"Messy\" people had more piles in their offices and appeared to invest less effort than \"neat\" people in filing information. Comparable differences have been observed in the ways people organize digital documents, emails, and web references. [ 72 ] Activities of keeping correlate with activities of organizing so that, for example, people with more elaborate folder structures tend to file information more often and sooner. [ 72 ] However, people may be selective in the information forms for which they invest efforts to organize. The schoolteachers who participated in one study, for example, reported having regular \"spring cleaning\" habits for organization and maintenance of paper documents but no comparable habits for digital information. [ 73 ] Activities of organization (e.g., creating and naming folders) segue into activities of maintenance such as consolidating redundant folders, archiving information no longer in active use, and ensuring that information is properly backed up and otherwise secured . (See also section, \"Managing privacy and the flow of information\"). Studies of people's folder organizations for digital information indicate that these have uses going far beyond the organization of files for later retrieval. Folders are information in their own right – representing, for example, a person's evolving understanding of a project and its components. A folder hierarchy can sometimes represent an informal problem decomposition with a parent folder representing a project and subfolders representing major components of the project (e.g., \"wedding reception\" and \"church service\" for a \"wedding\" project). [ 74 ] However, people generally struggle to keep their information organized [ 75 ] and often do not have reliable backup routines. [ 76 ] People have trouble maintaining and organizing many distinct forms of information (e.g., digital documents, emails, and web references) [ 77 ] and are sometimes observed to make special efforts to consolidate different information forms into a single organization. [ 57 ] With ever increasing stores of personal digital information, people face challenges of digital curation for which they are not prepared. [ 78 ] [ 79 ] [ 80 ] At the same time, these stores offer their owners the opportunity, with the right training and tool support, for exploitation of their information in new, useful ways. [ 81 ] Empirical observations of PIM studies motivate prototyping efforts towards information tools to provide better support for the maintenance, organization and, going further, curation of personal information. For example, GrayArea [ 82 ] applies the demotion principle of the user-subjective approach to allow people to move less frequently used files in any given folder to a gray area at the bottom end of the listing of this folder. These files can still be accessed but are less visible and so less distracting of a person's attention. The Planz [ 52 ] prototype supports an in-context creation and integration of project-related files, emails, web references, informal notes and other forms of information into a simplified, document-like interface meant to represent the project with headings corresponding to folders in the personal file system and subheadings (for tasks, sub-projects, or other project components) corresponding to subfolders. The intention is that a single, useful organization should emerge incidentally as people focus on the planning and completion of their projects. People face a continual evaluation of tradeoffs in deciding what information \"flows\" into and out of their PSI. Each interaction poses some degree of risk to privacy and security. Letting out information to the wrong recipients can lead to identity theft . Letting in the wrong kind of information can mean that a person's devices are \"infected\" and the person's data is corrupted or \"locked\" for ransom . By some estimates, 30% or more of the computers in the United States are infected. [ 83 ] However, the exchange of information, incoming and outgoing, is an essential part of living in the modern world. To order goods and services online, people must be prepared to \"let out\" their credit card information. To try out a potentially useful, new information tool, people may need to \"let in\" a download that could potentially make unwelcome changes to the web browser or the desktop. Providing for adequate control over the information, coming into and out of a PSI, is a major challenge. Even more challenging is the user interface to make clear the implications for various privacy choices particularly regarding Internet privacy . What, for example, are the personal information privacy implications of clicking the \"Sign Up\" button for use of social media services such as Facebook. [ 84 ] People seek to understand how they might improve various aspects of their PIM practices with questions such as \"Do I really need to keep all this information?\"; \"Is this tool (application, applet, device) worth the troubles (time, frustration) of its use?\" and, perhaps most persistent, \"Where did the day go? Where has the time gone? What did I accomplish?\". These last questions may often be voiced in reflection, perhaps on the commute home from work at the end of the workday. But there is increasing reason to expect that answers will be based on more than remembrance and reflection. Increasingly data incidentally, automatically captured over the course of a person's day and the person's interactions with various information tools to work with various forms of information (files, emails, texts, pictures, etc.) can be brought to bear in evaluations of a person's PIM practice and the identification of possible ways to improve. [ 85 ] Efforts to make sense of information represent another set of meta-level activity that operate on personal information and the mapping between information and need. People must often assemble and analyze a larger collection of information to decide what to do next. \"Which job applicant is most likely to work best for us?\", \"Which retirement plan to choose?\", \"What should we pack for our trip?\". These and many other decisions are generally based not on a single information item but on a collection of information items – documents, emails (e.g., with advice or impressions from friends and colleagues), web references, etc. Making sense of information is \"meta\" not only for its broader focus on information collections but also because it permeates most PIM activity even when the primary purpose may ostensibly be something else. For example, as people organize information into folders, ostensibly to ensure its subsequent retrieval, people may also be making sense and coming to a deeper understanding of this information. Personality and mood can impact a person's practice of PIM and, in turn, a person's emotions can be impacted by the person's practice of PIM. In particular, personality traits (e.g., \"conscientiousness\" or \"neuroticism\") have, in certain circumstances, been shown to correlate with the extent to which a person keeps and organizes information into a personal archive such as a personal filing system. [ 86 ] However, another recent study found personality traits were not correlated with any aspects of personal filing systems, suggesting that PIM practices are influenced less by personality than by external factors such as the operating system used (i.e. Mac OS or Windows), which were seen to be much more predictive. [ 87 ] Aside from the correlation between practices of PIM and more enduring personality traits, there is evidence to indicate that a person's (more changeable) mood impacts activities of PIM so that, for example, a person experiencing negative moods, when organizing personal information, is more likely to create a structure with more folders where folders, on average, contain fewer files. [ 88 ] Conversely, the information a person keeps or routinely encounters (e.g., via social media), can profoundly impact a person's mood. Even as explorations continue into the potential for the automatic, incidental capture of information (see section Keeping ) there is growing awareness for the need to design for forgetting as well as for remembrance as, for example, when a person realizes the need to dispose of digital belongings in the aftermath of a romantic breakup or the death of a loved one. [ 89 ] Beyond the negative feelings induced by information associated with a failed relationship, people experience negative feelings about their PIM practices, per se. People are shown in general to experience anxiety and dissatisfaction with respect to their personal information archives including both concerns of possible loss of the information and also express concerns about their ability and effectiveness in managing and organizing their information. [ 90 ] [ 91 ] Traditional, personal health information resides in various information systems in healthcare institutions (e.g., clinics, hospitals, insurance providers), often in the form of medical records . People often have difficulty managing or even navigating a variety of paper or electronic medical records across multiple health services in different specializations and institutions. [ 92 ] Also referred to as personal health records , this type of personal health information usually requires people (i.e., patients) to engage in additional PIM finding activities to locate and gain access to health information and then to generate a comprehensible summary for their own use. With the rise of consumer-facing health products including activity trackers and health-related mobile apps , people are able to access new types of personal health data (e.g., physical activity, heart rate) outside healthcare institutions. PIM behavior also changes. Much of the effort to keep information is automated. But people may experience difficulties making sense of a using the information later, e.g., to plan future physical activities based on activity tracker data. People are also frequently engaged in other meta-level activities, such as maintaining and organizing (e.g., syncing data across different health-related mobile apps). [ 93 ] The purpose of PIM study is both descriptive and prescriptive. PIM research seeks to understand what people do now and the problems they encounter i.e., in the management of information and the use of information tools. This understanding is useful on its own but should also have application to understand what might be done in techniques, training and, especially, tool design to improve a person's practice of PIM. The nature of PIM makes its study challenging. [ 94 ] The techniques and preferred methods of a person's PIM practice can vary considerably with information form (e.g., files vs. emails) and over time. [ 72 ] [ 50 ] [ 95 ] The operating system and the default file manager are also shown to impact PIM practices especially in the management of files. [ 34 ] [ 96 ] A person's practice is also observed to vary in significant ways with gender, age and current life circumstances. [ 97 ] [ 98 ] [ 99 ] [ 100 ] Certainly, differences among people on different sides of the so-called \" digital divide \" will have profound impact on PIM practices. And, as noted in section \" Personality, mood, and emotion \", personality traits and even a person's current mood can impact PIM behavior. For research results to generalize, or else to be properly qualified, PIM research, at least in aggregate, should include the study of people, with a diversity of backgrounds and needs, over time as they work in many different situations, with different forms of information and different tools of information management. At the same time, PIM research, at least in initial exploratory phases, must often be done in situ (e.g., in a person's workplace or office or at least where people have access to their laptops, smartphones and other devices of information management) so that people can be observed as they manage information that is \"personal\" to them (see section \" The senses in which information is personal \"). Exploratory methods are demanding in the time of both observer and participant and can also be intrusive for the participants. Consequently, the number and nature of participants is likely to be limited i.e., participants may often be people \"close at hand\" to the observer as family, friends, colleagues or other members of the observer's community. For example, the guided tour , in which the participant is asked to give an interviewer a \"tour\" of the participant's various information collections (e.g., files, emails, Web bookmarks, digital photographs, paper documents, etc.), has proven a very useful, but expensive method of study with results bound by caveats reflecting the typically small number and narrow sampling of participants. The guided tour method is one of several methods that are excellent for exploratory work but expensive and impractical to do with a larger, more diverse sampling of people. Other exploratory methods include the use of think aloud protocols collected, for example, as a participant completes a keeping or finding task, [ 57 ] and the experience sampling method wherein participants report on their PIM actions and experiences over time possibly as prompted (e.g., by a beep or a text on a smartphone). A challenge is to combine, within or across studies, time-consuming (and often demographically biased) methods of exploratory observation with other methods that have broader, more economical reach. The exploratory methods bring out interesting patterns; the follow-on methods add in numbers and diversity of participants. Among these methods are: Another method using the Delphi technique for achieving consensus has been used to leverage the expertise and experience of PIM researchers as means of extending, indirectly, the number and diversity of PIM practices represented. [ 103 ] The purview of PIM tool design applies to virtually any tool people use to work with their information including \" sticky notes \" and hanging folders for paper-based information to a wide range of computer-based applications for the management of digital information, ranging from applications people use every day such as Web browsers , email applications and texting applications to personal information managers. With respect to methods for the evaluation of alternatives in PIM tools design, PIM researchers again face an \"in situ\" challenge. How to evaluate an alternative, as nearly as possible, in the working context of a person's PSI? One \"let it lie\" approach [ 104 ] would provide for interfaces between the tool under evaluation and a participant's PSI so that the tool can work with a participant's other tools and the participant's personal information (as opposed to working in a separate environment with \"test\" data). Dropbox and other file hosting services exemplify this approach: Users can continue to work with their files and folders locally on their computers through the file manager even as an installed applet works to seamlessly synchronize the users files and folders with a Web store for the added benefits of a backup and options to synchronize this information with other devices and share this information with other users. As what is better described as a methodology of tool design rather than a method, Bergman reports good success in the application of a user-subjective approach . The user-subjective approach advances three design principles. In brief, the design should allow the following: 1) all project-related items no matter their form (or format) are to be organized together (the subjective project classification principle); 2) the importance of information (to the user) should determine its visual salience and accessibility (the subjective importance principle); and 3) information should be retrieved and used by the user in the same context as it was previously used in (the subjective context principle). The approach may suggest design principles that serve not only in evaluating and improving existing systems but also in creating new implementations. For example, according to the demotion principle, information items of lower subjective importance should be demoted (i.e., by making them less visible) so as not to distract the user but be kept within their original context just in case they are needed. The principle has been applied in the creation of several interesting prototypes. [ 105 ] [ 82 ] Finally, a simple \"checklist\" methodology of tool design\", [ 3 ] follows from an assessment of a proposed tool design with respect to each of the six senses in which information can be personal (see section \" The senses in which information is personal \") and each of the six activities of PIM (finding, keeping and the four meta-level activities, see section \" Activities of PIM \"). A tool that is good with respect to one kind of personal information or one PIM activity, may be bad with respect to another. For example, a new smartphone app that promises to deliver information potentially \"relevant to me\" (the \"6th sense\" in which information is personal) may do so only at the cost of a distracting increase in the information \"directed to me\" and by keeping too much personal information \"about me\" in a place not under the person's control. A personal information manager (often referred to as a PIM tool or, more simply, a PIM ) is a type of application software that functions as a personal organizer. The acronym PIM is now, more commonly, used in reference to personal information management as a field of study. [ 106 ] As an information management tool, a PIM tool's purpose is to facilitate the recording, tracking, and management of certain types of \"personal information\". Some PIM/ PDM software products are capable of synchronizing data over a computer network , including mobile ad hoc networks (MANETs). This feature typically stores the personal data on cloud drives allowing for continuous concurrent data updates/access, on the user's computers, including desktop computers , laptop computers, and mobile devices, such a personal digital assistants or smartphones .) [ 107 ] Personal information can include any of the following: [ 108 ] PIM is a practical meeting ground for many disciplines including cognitive psychology , cognitive science , human-computer interaction (HCI), human information interaction (HII), library and information science (LIS), artificial intelligence (AI), information retrieval, information behavior, organizational information management , and information science . Cognitive psychology, as the study of how people learn and remember, problem solve, and make decisions, necessarily also includes the study of how people make smart use of available information. The related field of cognitive science, in its efforts to apply these questions more broadly to the study and simulation of intelligent behavior, is also related to PIM. (Cognitive science, in turn, has significant overlap with the field of artificial intelligence). There is great potential for a mutually beneficial interplay between cognitive science and PIM. Sub-areas of cognitive science of clear relevance to PIM include problem solving and decision making . For example, folders created to hold information for a big project such as \"plan my wedding\" may sometimes resemble a problem-decomposition . [ 109 ] To take another example, the signal detection task [ 110 ] has long been used to frame and explain human behavior and has recently been used as a basis for analyzing our choices concerning what information to keep and how – a key activity of PIM. [ 58 ] Similarly, there is interplay between the psychological study of categorization and concept formation and the PIM study of how people use tags and folders to describe and organize their information. Now large portions of a document may be the product of \"copy-and-paste\" operations (from our previous writings) rather than a product of original writing. Certainly, management of text pieces pasted for re-use is a PIM activity, and this raises several interesting questions. How do we go about deciding when to re-use and when to write from scratch? We may sometimes spend more time chasing down a paragraph we have previously written than it would have taken to simply write a new paragraph expressing the same thoughts. Beyond this, we can wonder at what point a reliance on an increasing (and increasingly available) supply of previously written material begins to impact our creativity. As people do PIM they work in an external environment that includes other people, available technology, and, often, an organizational setting. This means that situated cognition , distributed cognition , and social cognition all relate to the study of PIM. The study of PIM is also related to the field of human–computer interaction (HCI). Some of the more influential papers on PIM over the years have been published in HCI journals and conference proceedings. However, the \"I\" in PIM is for information – in various forms, paper-based and digital (e.g., books, digital documents, emails and, even, the letter magnets on a refrigerator in the kitchen). The \"I\" in HCI stands for \"interaction\" as this relates to the \"C\" – computers. (An argument has been advanced that HCI should be focused more on information rather than computers. [ 111 ] ) Group information management (GIM, usually pronounced with a soft \"G\") has been written about elsewhere in the context of PIM. [ 112 ] [ 113 ] The study of GIM, in turn, has clear relevance to the study of computer-supported cooperative work (CSCW). GIM is to CSCW as PIM is to HCI. Just as concerns of PIM substantially overlap with but are not fully subsumed by concerns of HCI (nor vice versa), concerns of GIM overlap with but are not subsumed by concerns of CSCW. Information in support of GIM activities can be in non-digital forms such as paper calendars and bulletin boards that do not involve computers. Group and social considerations frequently enter into a person's PIM strategy. [ 114 ] For example, one member of a household may agree to manage medical information for everyone in the household (e.g., shot records) while another member of the household manages financial information for the household. But the collaborative organization and sharing of information is often difficult because, for example, the people working together in a group may have many different perspectives on how best to organize information. [ 115 ] [ 116 ] In larger organizational settings, the GIM goals of the organization may conflict with the PIM goals of individuals working within the organization, where the goals of different individuals may also conflict. [ 117 ] Individuals may, for example, keep copies of secure documents on their private laptops for the sake of convenience even though doing so violates group (organizational) security. [ 118 ] Given drawbacks—real or perceived—in the use of web services that support a shared use of folders, [ 119 ] [ 120 ] people working in a group may opt to share information instead through the use of e-mail attachments. [ 121 ] Concerns of data management relate to PIM especially with respect to the safe, secure, long-term preservation of personal information in digital form. The study of information management and knowledge management in organizations also relates to the study of PIM and issues seen first at an organizational level often migrate to the PIM domain. [ 122 ] Concerns of knowledge management on a personal (vs. organizational) level have given rise to arguments for a field of personal knowledge management (PKM). However, knowledge is not a \"thing\" to be managed directly but rather indirectly e.g., through items of information such as Web pages, emails and paper documents. PKM is best regarded as a useful subset of PIM [ 122 ] with special focus on important issues that might otherwise be overlooked such as self-directed efforts of knowledge elicitation (\"What do I know? What have I learned?\") and knowledge instillation (\"how better to learn what it is I want to know?\") Both time management and task management on a personal level make heavy use of information tools and external forms of information such as to-do lists, calendars, timelines, and email exchange. These are another form of information to be managed. Over the years, email, in particular, has been used in an ad hoc manner in support of task management. [ 123 ] [ 124 ] Much of the useful information a person receives comes, often unprompted, through a person's network of family, friends and colleagues. People reciprocate and much of the information a person sends to others reflects an attempt to build relationships and influence the behavior of others. As such, personal network management (PNM) is a crucial aspect of PIM and can be understood as the practice of managing the links and connections to other people for social and professional benefits.",
    "links": [
      "Smartphone",
      "Address book",
      "Digital calendar",
      "Artificial intelligence",
      "Categorization",
      "Doi (identifier)",
      "Douglas Engelbart",
      "File manager",
      "Dropbox (service)",
      "S2CID (identifier)",
      "Cut, copy, and paste",
      "Feng Office Community Edition",
      "Activity tracker",
      "Google Drive",
      "Human information interaction",
      "List of personal information managers",
      "Ransom",
      "Voicemail",
      "Optical character recognition",
      "Survey methodology",
      "Information retrieval",
      "Distributed cognition",
      "Hdl (identifier)",
      "Information seeking",
      "Computational power",
      "Personal wiki",
      "EGroupWare",
      "Digital divide",
      "Situated cognition",
      "Personal network",
      "Personal data manager",
      "File system",
      "Anniversary",
      "Psion Organiser",
      "Personal knowledge management",
      "Filing cabinet",
      "Document",
      "Relevance",
      "Privacy",
      "National Science Foundation",
      "Social cognition",
      "Mobile ad hoc network",
      "Andries van Dam",
      "Personal health record",
      "Project management",
      "Zimbra",
      "Mozilla Thunderbird",
      "BlackBerry (company)",
      "Lightning (software)",
      "Task list",
      "Sharp Wizard",
      "Number cruncher",
      "Cognitive psychology",
      "Scalix",
      "Instagram",
      "Mobile apps",
      "Task management",
      "Bibcode (identifier)",
      "Attention management",
      "Horde (software)",
      "Group information management",
      "Personal data",
      "Information filtering system",
      "Hypertext",
      "Planz",
      "Mozilla Sunbird",
      "Comparison of note-taking software",
      "Evolution (software)",
      "Internet privacy",
      "Electronic health record",
      "Lifelog",
      "Cloud computing",
      "Medical record",
      "RSS (file format)",
      "Operating system",
      "Ted Nelson",
      "Activity trackers",
      "Personal archiving",
      "Digital curation",
      "Meeting",
      "Personality",
      "Fax",
      "Personality traits",
      "Information science",
      "Herbert A. Simon",
      "Web page",
      "Email",
      "Personal information manager",
      "Bongo (software)",
      "Recipes",
      "Web browser",
      "Twitter",
      "PMID (identifier)",
      "Think aloud protocol",
      "Greek letter",
      "Virtual assistant",
      "Password manager",
      "Oral tradition",
      "Simple Groupware",
      "Identity theft",
      "Hierarchical file system",
      "Atom (standard)",
      "ArXiv (identifier)",
      "Logic Theorist",
      "Information security",
      "Experience sampling method",
      "Digital preservation",
      "OCLC (identifier)",
      "Personal organizer",
      "Kolab",
      "Bynari",
      "Problem solving",
      "Delphi method",
      "Text messaging",
      "Collabora Online",
      "Mood (psychology)",
      "File hosting service",
      "Privacy settings",
      "Kopano (software)",
      "Recommender systems",
      "Information overload",
      "Concept formation",
      "Zarafa (software)",
      "Information behavior",
      "Post-it Note",
      "Laptop",
      "SOGo",
      "Fitbit",
      "Quantified self",
      "Desktop computer",
      "Desktop wiki",
      "DAViCal",
      "Personal digital assistant",
      "Computer-supported cooperative work",
      "Facebook",
      "Alerts",
      "Ulric Neisser",
      "Library and information science",
      "Nextcloud",
      "Group-Office",
      "Calendar date",
      "Decision making",
      "Apple Watch",
      "Personal websites",
      "Time management",
      "Prospective memory",
      "Tickler file",
      "Cognitive science",
      "ISSN (identifier)",
      "Decomposition (computer science)",
      "Computer network",
      "Instant message",
      "ISBN (identifier)",
      "Snapchat",
      "Information processing (psychology)",
      "API",
      "File folder",
      "Kontact",
      "Gmail",
      "Personal computer",
      "Citadel/UX",
      "Microsoft OneDrive",
      "PMC (identifier)",
      "Allen Newell",
      "Tine 2.0",
      "Personalized search",
      "Reddit",
      "Blog",
      "Information",
      "Social media",
      "Diary",
      "Detection theory",
      "Birthday",
      "OnlyOffice",
      "Human-computer interaction",
      "Information management",
      "Backup",
      "Information system",
      "Semantic desktop"
    ]
  },
  "Social information seeking": {
    "url": "https://en.wikipedia.org/wiki/Social_information_seeking",
    "title": "Social information seeking",
    "content": "Social information seeking is a field of research that involves studying situations, motivations, and methods for people seeking and sharing information in participatory online social sites, such as Yahoo! Answers , Answerbag, WikiAnswers and Twitter as well as building systems for supporting such activities. Highly related topics involve traditional and virtual reference services, information retrieval , information extraction , and knowledge representation . [ 1 ] Social information seeking is often materialized in online question-answering (QA) websites, which are driven by a community. Such QA sites have emerged in the past few years as an enormous market, so to speak, for the fulfillment of information needs. Estimates of the volume of questions answered are difficult to come by, but it is likely that the number of questions answered on social/community QA (cQA) sites far exceeds the number of questions answered by library reference services, [ 2 ] which until recently were one of the few institutional sources for such question answering . cQA sites make their content – questions and associated answers submitted on the site – available on the open web, and indexable by search engines, thus enabling web users to find answers provided for previously asked questions in response to new queries. The popularity of such sites have been increasing dramatically for the past several years. Major sites that provide a general platform for questions of all types include Yahoo! Answers , Answerbag and Quora . While other sites that focus on particular fields; for example, StackOverflow (computing). StackOverflow has 3.45 million questions, 1.3 million users and over 6.86 million answers since July 2008 while Quora has 437 thousand questions, 264 thousand users and 979 thousand answers. [ 3 ] Social Q&A or cQA, according to Shah et al., [ 4 ] consists of three components: a mechanism for users to submit questions in natural language, a venue for users to submit answers to questions, and a community built around this exchange. Viewed in that light, online communities have performed a question answering function perhaps since the advent of Usenet and Bulletin Board Systems, so in one sense cQA is nothing new. Websites dedicated to cQA, however, have emerged on the web only within the past few years: the first cQA site was the Korean Naver Knowledge iN, launched in 2002, while the first English-language CQA site was Answerbag, launched in April 2003. Despite this short history, however, cQA has already attracted a great deal of attention from researchers investigating information seeking behaviors, [ 5 ] selection of resources, [ 6 ] social annotations, [ 7 ] user motivations, [ 8 ] comparisons with other types of question answering services, [ 9 ] and a range of other information-related behaviors. Some of the interesting and important research questions in this area include: Shah et al. [ 10 ] provide a detailed research agenda for social Q&A. A new book by Shah [ 11 ] presents a more recent and comprehensive information pertaining to social information seeking. Friendsourcing is an important component of social question and answering, including how to route questions to friends or others who will most likely answer the question. [ 12 ] The important questions include what people's behaviors are in social networks, especially what kinds of questions people ask from their social networks and how different question types affect the frequency, speed and quality of answers they receive. Morris et al. (2010) [ 13 ] conducted a survey of question and answering within social networks with 624 people, and gathered detailed data about the behavior of Q&A, including frequency, types of questions and answers, and motivations. They found that half (50.6%) of respondents reported having used their status messages to ask a question, which indicated that Q&A on social networks is popular. Also, the types of questions people asked include recommendation, opinion, factual knowledge, rhetorical, etc. And motivations for asking include trust, asking subjective questions, etc. Their analysis also explored the relationships between answer speed and quality, questions’ property and participants’ property. Only a very small portion (6.5%) of the questions were answered, but the 89.3% of the respondents were satisfied with the response time they experienced even though there's a discrepancy between that and expectation. Also, the responses gathered via social networks appear to be very valuable. Their findings implied design for search tools that could combine the speed and breadth of traditional search engines with the trustworthiness, personalization, and the high engagement of social media Q&A. Paul et al. (2011) [ 14 ] did a study on question and answering on Twitter, and found that out of the 1152 questions they examined, the most popular question types asked on Twitter were rhetorical (42%) and factual (16%). Surprisingly, along with entertainment (29%) and technology (29%) questions, people asked personal and health-related questions (11%). Only 18.7% questions received response, while a handful of questions received a high number of responses. The larger the askers’ network, the more responses she received; however, posting more tweets or posting more frequently did not increase chances of receiving a response. Most often the “follow” relationship between asker and answerer was one-way. Paul et al. also examined what factors of the askers would increase the chance of getting a response and found that more relevant responses are received when there is a mutual relationship between askers and answerers. Intuitively, we would expect this, as mutual relationship would indicate stronger tie strength and hence, more number of relevant answers. Existing social Q&A services can be characterized from the three perspectives, by the definition of social Q&A as a service involving (1) a method for presenting information needs, (2) a place for responding to information need, and (3) participation as a community. These social networks support various friendsourcing behavior, provide information benefits that oftentimes traditional search tools cannot, and also may reinforce social bonds through the process. However, there are many questions and limitations that may prevent people from asking questions on their social networks. For example, they may feel uncomfortable asking questions that are too private, might not want to cost too much other people's time and effort, or might feel the burden of social debts. Rzeszotarski and Morris (2014) [ 15 ] took a novel approach to explore the perceived social costs of friendsourcing on Twitter via monetary choices. They modeled friendsourcing costs across users, and compared it with crowdsourcing on Amazon Mechanical Turk . Their findings suggested interesting design considerations for minimizing social cost by building a hybrid system combining friendsourcing and crowdsourcing with microtask markets. Sometimes, only asking question from people's own social networks or friends is not enough. If the question is obscure or time sensitive, no members of their social networks may know the answer. For example, this person's friends might not have expertise in providing evaluations for a specific model of digital camera. Also asking the current wait time for security at the local airport might not be possible if none of this person's friends are currently at the airport. Nichols and Kang (2012) [ 16 ] leveraged Twitter for question and answering with targeted strangers by taking advantage of its public accessibility. In their approach, they mined the public status updates posted on Twitter to find strangers with potentially useful information, and send questions to these strangers to collect responses. As a feasibility study, they collected information regarding response rate, and response time. 42% of users responded to questions from strangers, and 44% of the responses arrived within 30 minutes. Another important and unique component of social Q&A system is that it is a community which allows members to form relationships and bonds, so that their behavior in these social Q&A services will also add to their social capital. Gray et al. (2013) [ 17 ] explored how bridging social capital, question type and relational closeness influence the perceived usefulness and satisfaction of information obtained through questions asked on Facebook. Their results indicated that bridging social capital could positively predict the perceived utility of the acquired information, meaning that information exchanges on social networks is an effective way of social capital conversion. Also, useful answers are more likely to be received from weak ties than strong ties. In order to recommend the most appropriate users to provide answers in a social network, we need to find approaches to detect users' authority in a social network. In the field of information retrieval, there has been a trend of research investigating ways to detect users' authority effectively and accurately in a social network. Cha et al. [ 18 ] investigate possible metrics for determining authority users on popular social network Twitter. They propose the following three simple network-based metrics and discuss their usefulness in determining a user's influence. An initial analysis of the three aforementioned metrics showed that the users with the highest indegrees and the users with the highest retweet/mention counts were not the same. The top 1% of users by indegree are shown to have very low correlation with the same percentile of users by retweets and by mentions. This implies that follower count is not useful in determining whether a user's tweets get retweeted or whether the other users engage with them. Pal et al. [ 19 ] designed features to measure a user's authority on a certain topic. For example, retweet impact refers to how many times a certain user has been retweeted on a certain topic. The impact is dampened by a factor measuring how many times the user had been retweeted by a unique author to avoid the cases when a user has fans who retweet regardless of the content. They first used a clustering approach to find the target cluster which has the highest average score across all features, and used a ranking algorithm to find the most authoritative users within the cluster. With these authority detection methods, social Q&A could be more effective in providing accurate answers to askers. People associated with social information seeking include:",
    "links": [
      "Knowledge representation",
      "Jessica Vitak",
      "ISBN (identifier)",
      "S2CID (identifier)",
      "Question answering",
      "Naver",
      "Information extraction",
      "Yahoo! Answers",
      "WikiAnswers",
      "Amazon Mechanical Turk",
      "Doi (identifier)",
      "Virtual reference",
      "StackOverflow",
      "Twitter",
      "Quora",
      "Information retrieval",
      "Social cost"
    ]
  },
  "Alvin Weinberg": {
    "url": "https://en.wikipedia.org/wiki/Alvin_Weinberg",
    "title": "Alvin Weinberg",
    "content": "Alvin Martin Weinberg ( / ˈ w aɪ n b ɜːr ɡ / ; April 20, 1915 – October 18, 2006) was an American nuclear physicist who was the administrator of Oak Ridge National Laboratory (ORNL) during and after the Manhattan Project . He came to Oak Ridge, Tennessee , in 1945 and remained there until his death in 2006. He was the first to use the term \" Faustian bargain \" to describe nuclear energy. A graduate of the University of Chicago , which awarded him his doctorate in mathematical biophysics in 1939, Weinberg joined the Manhattan Project's Metallurgical Laboratory in September 1941. The following year he became part of Eugene Wigner 's Theoretical Group, whose task was to design the nuclear reactors that would convert uranium into plutonium. Weinberg replaced Wigner as director of research at ORNL in 1948, and became director of the laboratory in 1955. Under his direction it worked on the Aircraft Nuclear Propulsion program, and pioneered many innovative reactor designs, including the pressurized water reactors (PWRs) and boiling water reactors (BWRs) which have since become the dominant reactor types in commercial nuclear power plants , and Aqueous Homogeneous Reactor designs. In 1960, Weinberg was appointed to the President's Science Advisory Committee in the Eisenhower administration and later served on it in the Kennedy administration . After leaving the ORNL in 1973, he was named director of the Office of Energy Research and Development in Washington, D.C., in 1974. The following year he founded and became the first director of the Institute for Energy Analysis at Oak Ridge Associated Universities (ORAU). Alvin Martin Weinberg was born April 20, 1915, in Chicago, Illinois, [ 1 ] the son of Jacob Weinberg and Emma Levinson Weinberg, [ 2 ] two Russian Jewish emigrants who met in 1905 on board the boat carrying them to the United States. [ 1 ] He had an older sister, Fay Goleman, who was born on November 30, 1910. She later became a sociology professor at the University of the Pacific , [ 3 ] [ 4 ] and was the mother of Daniel Goleman . He attended Theodore Roosevelt High School in Chicago. [ 5 ] Weinberg entered the University of Chicago , from which he received his Bachelor of Science (B.S.) degree in physics in 1935, and his Master of Science (M.S.) in physics the following year. [ 6 ] He received his Ph.D. from the University of Chicago in mathematical biophysics in 1939, writing his thesis on Mathematical foundations for a theory of biophysical periodicity , [ 7 ] under the supervision of Carl Eckart . [ 8 ] Weinberg later lamented that, in restricting his thesis to linear systems , he had overlooked interesting nonlinear systems that Ilya Prigogine later received the Nobel Prize in Chemistry for studying. [ 9 ] While at Chicago, Weinberg was hired by the family of Margaret Despres, a student at the University of Chicago, to tutor her in mathematics. [ 8 ] They were married on June 14, 1940. [ 4 ] They had two sons, David Robert Weinberg and Richard J. Weinberg. [ 10 ] [ 11 ] Weinberg taught courses at Wright Junior College . He applied for and received a National Research Council fellowship to study under Kenneth S. Cole at Columbia University , but never took it up, as Cole came to Chicago to work on the Manhattan Project as a radiation biologist. Weinberg was recruited to work at its Metallurgical Laboratory at the University of Chicago in September 1941 by Eckart and Samuel Allison , who needed someone to work on the latter's neutron capture calculations. [ 12 ] In early 1942, Arthur Compton concentrated the Manhattan Project's various teams working on plutonium at the University of Chicago. This brought in many top scientists including Herbert Anderson , Bernard Feld , Enrico Fermi , Leó Szilárd and Walter Zinn from Columbia, and Edward Creutz , Gilbert Plass , Eugene Wigner and John Wheeler from Princeton University . Weinberg became a protégé of Wigner. [ 13 ] Wigner led the Theoretical Group at the Metallurgical Laboratory that included Alvin Weinberg, Katharine Way , Gale Young and Edward Creutz. The group's task was to design the production nuclear reactors that would convert uranium into plutonium. At the time, reactors existed only on paper, and no reactor had yet gone critical. In July 1942, Wigner chose a conservative 100 MW design, with a graphite neutron moderator and water cooling. [ 14 ] The choice of water as a coolant was controversial at the time. Water was known to absorb neutrons , thereby reducing the efficiency of the reactor, but Wigner was confident that his group's calculations were correct and that water would work, while the technical difficulties involved in using helium or liquid metal as coolants would delay the project. [ 15 ] After the United States Army Corps of Engineers took over the Manhattan Project, it gave responsibility for the detailed design and construction of the reactors to DuPont . There was friction between the company and Wigner and his team. Major differences between Wigner's reactor design and DuPont's included increasing the number of process tubes from 1,500 in a circular array to 2,004 in a square array, and cutting the power from 500 MW to 250 MW. As it turned out, the design decision by DuPont to give the reactor additional tubes came in handy when neutron poisoning became a problem for the B Reactor at the Hanford Site . The extra tubes allowed a greater fuel load to overcome the poisoning. Without them the reactor would have had to be run at low power until enough of the boron impurities in the graphite had been burned up to allow it to reach full power, which would have delayed full operation by up to a year. [ 16 ] [ 17 ] As the reactors at Hanford came online, the Metallurgical Laboratory turned its attention back to theoretical designs. The discovery of spontaneous fission in reactor-bred plutonium due to contamination by plutonium-240 led Wigner to propose switching to breeding uranium-233 from thorium , but the challenge was met by the Los Alamos Laboratory developing an implosion-type nuclear weapon design. [ 18 ] Wigner was also intrigued by the possibility of doing away with much of the complexities of a reactor by having the uranium in solution or a slurry in heavy water . The Metallurgical Laboratory attempted to find a way of doing this. [ 19 ] Amongst the competing designs, Weinberg proposed the pressurized water reactor , which ultimately became the most common design. [ 20 ] This was only one of the many possibilities discussed by Weinberg and his colleagues at Chicago and Oak Ridge. Later, he wrote: In these early days we explored all sorts of power reactors, comparing the advantages and disadvantages of each type. The number of possibilities was enormous, since there are many possibilities for each component of a reactor—fuel, coolant, moderator. The fissile material may be 233 U, 235 U, or 239 Pu; the coolant may be: water, heavy water, gas, or liquid metal; the moderator may be: water, heavy water, beryllium, graphite—or, in a fast- neutron reactor, no moderator. I have calculated that, if one counted all the combinations of fuel, coolant, and moderator, one could identify about a thousand distinct reactors. Thus, at the very beginning of nuclear power, we had to choose which possibilities to pursue, which to ignore. [ 21 ] The ultimate success of the pressurized water reactor, he wrote, was due less to any superior characteristics of water, but rather to the decision to power the prototype of the Mark I submarine thermal reactor with a pressurized version of the Materials Testing Reactor at Oak Ridge. Once pressurized water was established, other possibilities became too expensive to pursue, [ 22 ] but Weinberg remained interested in other possibilities. According to Freeman Dyson , he was the only nuclear pioneer who supported the wide universe of reactor designs. [ 23 ] In 1945, Wigner accepted a position as the director of research at the Clinton Laboratories in Oak Ridge, Tennessee , which then had a staff of about 800. He took with him his protégés Gale Young , Katherine Way and Weinberg. Weinberg, who was the first to arrive at Oak Ridge in May 1945, [ 24 ] became head of the Physics Division in 1946. [ 25 ] But after the Atomic Energy Commission took over responsibility for the laboratory's operations from the Manhattan Project at the start of 1947, Wigner, feeling unsuited to a managerial role in the new environment, left Oak Ridge at the end of summer in 1947 and returned to Princeton University. [ 26 ] The administration of the Clinton Laboratories passed from Monsanto to the University of Chicago in May 1947, and then to Union Carbide in December 1947. [ 27 ] The Atomic Energy Commission's influential General Advisory Committee, chaired by J. Robert Oppenheimer , recommended that all work on reactors be concentrated at the Argonne National Laboratory , the successor to the Metallurgical Laboratory, near Chicago. There was also competition for staff and resources from the newly established Brookhaven National Laboratory near New York. Morale was low, and no one could be found to take on the job of director of research at the laboratory, renamed the Oak Ridge National Laboratory (ORNL) in January 1948. At least six people turned down the job before Union Carbide's acting Director, Nelson (Bunny) Rucker, asked Weinberg to become Director of Research in March 1948. [ 28 ] [ 29 ] Weinberg was subsequently appointed director in 1955. He often sat in the front row at ORNL division information meetings and he would ask the first, often very penetrating, question after each scientific talk. For young scientists giving their first presentation, the experience could be frightening, but it was also exciting and stimulating. When asked how he found the time to attend every meeting, Weinberg replied jokingly, \"We didn't have a DOE in those days.\" [ 25 ] The Aircraft Nuclear Propulsion (ANP) project was ORNL's biggest program, using 25% of ORNL's budget. The ANP project's military goal was to produce a nuclear-powered aircraft (a bomber) to overcome the range limitations of jet-fueled aircraft at that time. That the project had little chance of success was not overlooked, but it provided employment and allowed ORNL to stay in the reactor development business. ORNL successfully built and operated a prototype of an aircraft reactor power plant by creating the world's first molten salt fueled and cooled reactor called the Aircraft Reactor Experiment (ARE) in 1954, which set a record high temperature of operation of 1,600 °F (870 °C). Due to the radiation hazard posed to aircrew, and people on the ground in the event of a crash, new developments in ballistic missile technology, aerial refueling and longer range jet bombers, President Kennedy canceled the program in June 1961. [ 30 ] [ 31 ] Weinberg had the Materials Testing Reactor converted into a mock-up of a real reactor called the Low Intensity Test Reactor (LITR) or \"Poor Man's Pile\". Experiments at the LITR led to the design of both pressurized water reactors (PWRs) and boiling water reactors (BWRs), which have since become the dominant reactor types in commercial nuclear power plants . [ 32 ] Weinberg was attracted to the simplicity and self-controlling features of nuclear reactors that used fluid fuels, such as Harold Urey and Eugene Wigner's proposed Aqueous Homogeneous Reactor . Therefore, to support the Nuclear Aircraft project in the late 1940s, Weinberg asked ORNL's reactor engineers to design a reactor using liquid instead of solid fuel. [ 33 ] This Homogeneous Reactor Experiment (HRE) was affectionately dubbed \"Alvin's 3P reactor\" because it required a pot, a pipe, and a pump. The HRE went into operation in 1950 and, at the criticality party, Weinberg brought the appropriate spirits: \"When piles go critical in Chicago, we celebrate with wine. When piles go critical in Tennessee, we celebrate with Jack Daniel's .\" [ 25 ] The HRE operated for 105 days before it was closed down. Despite its leaks and corrosion, valuable information was gained from its operation and it proved a simple and safe reactor to control. [ 34 ] During the time the HRE was online, Senators John F. Kennedy and Albert Gore, Sr. visited ORNL and were hosted by Weinberg. [ 25 ] Under Weinberg, ORNL shifted its focus to a civilian version of the meltdown-proof Molten Salt Reactor (MSR) away from the military's \"daft\" [ 35 ] idea of nuclear-powered aircraft. The Molten-Salt Reactor Experiment (MSRE) set a record for continuous operation and was the first to use Thorium irradiated to produce uranium-233 as fuel. It also used plutonium-239 and the standard, naturally occurring uranium-235 . The MSR was known as the \"chemist's reactor\" because it was proposed mainly by chemists (ORNL's Ray Briant and Ed Bettis (an engineer) and NEPA's Vince Calkins), [ 34 ] and because it used a chemical solution of melted salts containing the actinides (uranium, thorium, and/or plutonium) in a carrier salt, most often composed of beryllium (BeF 2 ) and lithium (LiF) (isotopically depleted in Lithium-6 to prevent excessive neutron capture or tritium production) – FLiBe . [ 36 ] The MSR also afforded the opportunity to change the chemistry of the molten salt while the reactor was operating to remove fission products and add new fuel or change the fuel, all of which is called \"online processing\". [ 37 ] Under Weinberg's tenure as director, ORNL's Biology Division grew to five times the size of the next largest division. This division was charged with understanding how ionizing radiation interacts with living things and to try to find ways to help them survive radiation damage, such as bone marrow transplants . In the 1960s Weinberg also pursued new missions for ORNL, such as using nuclear energy to desalinate seawater. He recruited Philip Hammond from the Los Alamos National Laboratory to further this mission and in 1970 started the first big ecology project in the United States: the National Science Foundation – Research Applied to National Needs Environmental Program. [ 38 ] In 1958, Weinberg coauthored the first nuclear reactor textbook, The Physical Theory of Neutron Chain Reactors , with Wigner. The following year, 1959, he was elected president of the American Nuclear Society and, in 1960, began service on the President's Science Advisory Committee under the Eisenhower and Kennedy administrations . [ 39 ] Starting in 1945 with Patent #2,736,696, Weinberg, usually with Wigner, filed numerous patents on the light water reactor (LWR) technology that has provided the United States' primary nuclear reactors. The main LWR types are Pressurized Water Reactors (PWRs) and Boiling Water Reactors (BWRs), that serve in Naval propulsion and commercial nuclear power. [ 40 ] In 1965 he was appointed vice president of Union Carbide's Nuclear Division. [ 41 ] In a 1971 paper, Weinberg first used the term \" Faustian bargain \" to describe nuclear energy: We nuclear people have made a Faustian bargain with society. On the one hand we offer—in the catalytic nuclear burner (i.e., the breeder)—an inexhaustible source of energy. Even in the short range, when we use ordinary reactors, we offer energy that is cheaper than energy from fossil fuel. Moreover, this source of energy when properly handled is almost nonpolluting. Whereas fossil-fuel burners emit oxides of carbon, nitrogen, and sulfur... there is no intrinsic reason why nuclear systems must emit any pollutant except heat and traces of radioactivity. But the price that we demand of society for this magical source is both a vigilance from and longevity of our social institutions that we are quite unaccustomed to. [ 42 ] Weinberg was fired by the Nixon administration from ORNL in 1973 after 18 years as the laboratory's director, because he continued to advocate increased nuclear safety and molten salt reactors (MSRs), instead of the Administration's chosen Liquid Metal Fast Breeder Reactor (LMFBR) that the AEC's Director of Reactor Division, Milton Shaw , was appointed to develop. Weinberg's firing effectively halted development of the MSR, as it was virtually unknown by other nuclear laboratories and specialists. [ 43 ] There was a brief revival of MSR research at ORNL as part of the Carter administration 's nonproliferation interests, culminating in ORNL-TM-7207, \"Conceptual Design Characteristics of a Denatured Molten-Salt Reactor with Once-Through Fueling\", by Engel, et al. , which is still considered by many to be the \"reference design\" for commercial molten salt reactors. [ 44 ] [ 45 ] Weinberg was named director of the Office of Energy Research and Development in Washington, D.C., in 1974. The following year he founded and became the first director of Institute for Energy Analysis at Oak Ridge Associated Universities (ORAU). This institute focused on evaluating alternatives for meeting future energy requirements. From 1976 to 1984, the Institute for Energy Analysis was a center for study of diverse issues related to carbon dioxide and global warming . [ 46 ] He worked at ORAU until retiring to become an ORAU distinguished fellow in 1985. [ 25 ] In 1972 Weinberg published a landmark article in Minerva entitled Science and Trans-science , in which he focused on the interface between science and policy matters, especially governmental policy decisions: Many of the issues which arise in the course of the interaction between science or technology and society—e.g., the deleterious side effects of technology, or the attempts to deal with social problems through the procedures of science—hang on the answers to questions which can be asked of science and yet which cannot be answered by science. I propose the term trans-scientific for these questions since, though they are, epistemologically speaking, questions of fact and can be stated in the language of science, they are unanswerable by science; they transcend science. In so far as public policy involves trans-scientific rather than scientific issues, the role of the scientist in contributing to the promulgation of such policy must be different from his role when the issues can be unambiguously answered by science. [ 47 ] In June, 1977, Weinberg testified at a congressional hearing of the House Subcommittee on the Environment and the Atmosphere concerning the impact of increasing carbon dioxide emissions on global average temperatures. He stated that a doubling of global carbon dioxide emissions by 2025, which some scientists predicted would occur, would lead to a two-degree Celsius increase in global average temperature. [ 48 ] Weinberg remained active in retirement. In 1992 he was named chairman of the International Friendship Bell Committee , which arranged for the installation of a Japanese bell in Oak Ridge. He also called for strengthening of the International Atomic Energy Agency and systems to defend against nuclear weapons . [ 49 ] His first wife, Margaret, died in 1969. He later married a stock broker, Genevieve DePersio, who died in 2004. [ 8 ] [ 10 ] His son David died in 2003. [ 11 ] Weinberg died at his home in Oak Ridge on October 18, 2006. He was survived by his other son, Richard, and sister Fay Goleman. [ 10 ] The Alvin Weinberg Foundation is named for him. [ 50 ]",
    "links": [
      "Nuclear weapon",
      "Thomas Farrell (United States Army officer)",
      "Andrew Szanton",
      "Henry DeWolf Smyth",
      "John F. Kennedy",
      "Bockscar",
      "Oak Ridge Associated Universities",
      "Deal with the Devil",
      "Linear system",
      "National Academy of Sciences",
      "Carl Eckart",
      "Doi (identifier)",
      "William Sterling Parsons",
      "Graphite",
      "Maria Goeppert Mayer",
      "Bernard T. Feld",
      "Presidency of Richard Nixon",
      "William R. Purnell",
      "Robert R. Wilson",
      "K-25",
      "Katherine Way",
      "B Reactor",
      "Enrico Fermi",
      "Los Alamos National Laboratory",
      "Katharine Way",
      "S2CID (identifier)",
      "S-1 Executive Committee",
      "Enrico Fermi Award",
      "Uranium",
      "Niels Bohr",
      "Bruno Rossi",
      "Bismuth phosphate process",
      "Ernest Orlando Lawrence Award",
      "S-50 (Manhattan Project)",
      "American Academy of Arts and Sciences",
      "Uranium-233",
      "John Lansdale Jr.",
      "Plutonium-239",
      "Nonlinear system",
      "Daniel Goleman",
      "Jack Daniel's",
      "509th Composite Group",
      "Nuclear power plant",
      "P-9 Project",
      "Actinide",
      "Milton Shaw",
      "American Philosophical Society",
      "Atomic Energy Act of 1946",
      "Roosevelt High School (Chicago, Illinois)",
      "Argonne National Laboratory",
      "The Alvin Weinberg Foundation",
      "American Nuclear Society",
      "Montreal Laboratory",
      "Leo Szilard",
      "Freeman Dyson",
      "Dorothy McKibbin",
      "National Science Foundation",
      "Heavy water",
      "President's Science Advisory Committee",
      "Alsos Mission",
      "Glenn T. Seaborg",
      "University of California Press",
      "Priscilla Duffield",
      "Fat Man",
      "Naomi Oreskes",
      "Biophysics",
      "Oak Ridge National Laboratory",
      "Nobel Prize in Chemistry",
      "Interim Committee",
      "George Koval",
      "Aage Bohr",
      "Edwin McMillan",
      "International Atomic Energy Agency",
      "Einstein–Szilard letter",
      "Wendover Air Force Base",
      "X-10 Graphite Reactor",
      "Emilio Segrè",
      "Operation Peppermint",
      "Ernest Lawrence",
      "Light water reactor",
      "Historical Studies in the Natural Sciences",
      "John von Neumann",
      "Mark Oliphant",
      "RaLa Experiment",
      "Hans Bethe",
      "Erik M. Conway",
      "George Kistiakowsky",
      "Columbia University",
      "Salt (chemistry)",
      "George B. Pegram",
      "Eugene Wigner",
      "James Franck",
      "Frederick Seitz",
      "Plutonium-240",
      "African-American scientists and technicians on the Manhattan Project",
      "Ames Project",
      "Smyth Report",
      "United States Atomic Energy Commission",
      "Biology",
      "Molten-Salt Reactor Experiment",
      "Beryllium",
      "PMID (identifier)",
      "University of the Pacific (United States)",
      "Ilya Prigogine",
      "Charles Critchfield",
      "Calutron",
      "Site A",
      "Clinton Engineer Works",
      "Aircraft Nuclear Propulsion",
      "Pumpkin bomb",
      "Thin Man (nuclear bomb)",
      "Leslie Groves",
      "Willard Libby",
      "Neutron poison",
      "James C. Marshall",
      "Gale Young",
      "John Archibald Wheeler",
      "Edward Teller",
      "Critical mass",
      "Liquid metal",
      "American Academy of Achievement",
      "Aerial refueling",
      "Neutron capture",
      "Nuclear reactor",
      "Hematopoietic stem cell transplantation",
      "Union Carbide",
      "James Chadwick",
      "Neutron moderator",
      "Herbert L. Anderson",
      "Leona Woods",
      "Presidency of Dwight D. Eisenhower",
      "Academy of Achievement",
      "OCLC (identifier)",
      "Silverplate",
      "Calutron Girls",
      "John R. Dunning",
      "Neutron",
      "Special Engineer Detachment",
      "Project Camel",
      "Presidency of Jimmy Carter",
      "United States Department of Energy",
      "Master of Science",
      "Norman Ramsey Jr.",
      "National Research Council (United States)",
      "Edward Creutz",
      "Kenneth Nichols",
      "Stafford L. Warren",
      "Val Logsdon Fitch",
      "James Rainwater",
      "Global warming",
      "Albert Gore, Sr.",
      "Illinois",
      "John Cockcroft",
      "Oppenheimer security hearing",
      "Robert Bacher",
      "Gilbert Plass",
      "Arthur Compton",
      "Chicago Pile-1",
      "Manhattan Project",
      "FLiBe",
      "Luis Walter Alvarez",
      "Doctoral advisor",
      "Pressurized water reactor",
      "Materials testing reactor",
      "Quebec Agreement",
      "The New York Times",
      "Salt Wells Pilot Plant",
      "Norris Bradbury",
      "Monsanto",
      "Ed Westcott",
      "Helium",
      "Boris Pash",
      "Project Alberta",
      "Erich Vogt",
      "Aqueous homogeneous reactor",
      "DuPont",
      "Thesis",
      "Demon core",
      "Project Y",
      "Boiling water reactor",
      "Paul Tibbets",
      "Carbon dioxide",
      "Richard Feynman",
      "Harry Daghlian",
      "University of Chicago",
      "Bachelor of Science",
      "Dayton Project",
      "Metallurgical Laboratory",
      "Isidor Isaac Rabi",
      "Nuclear physics",
      "Atomic bombings of Hiroshima and Nagasaki",
      "List of Nobel laureates who worked on the Manhattan Project",
      "Walter Zinn",
      "Uranium-235",
      "Vannevar Bush",
      "Lithium",
      "Enola Gay",
      "The Great Artiste",
      "William L. Uanna",
      "Hanford Engineer Works",
      "United States Army Corps of Engineers",
      "Thorium",
      "Stanisław Ulam",
      "Manhattan Project feed materials program",
      "James B. Conant",
      "Molten salt reactor",
      "ISSN (identifier)",
      "Ionizing radiation",
      "Klaus Fuchs",
      "Kenneth Stewart Cole",
      "British contribution to the Manhattan Project",
      "Princeton University",
      "Brookhaven National Laboratory",
      "Desalination",
      "Hanford Site",
      "Ballistic missile",
      "ISBN (identifier)",
      "Lyman Briggs",
      "Operation Crossroads",
      "Little Boy",
      "Atoms for Peace Award",
      "J. Robert Oppenheimer",
      "Louis Slotin",
      "The Guardian",
      "Walter R. Tschinkel",
      "Franklin Matthias",
      "Charles Allen Thomas",
      "Plutonium",
      "Samuel King Allison",
      "Chien-Shiung Wu",
      "Roscoe Charles Wilson",
      "Los Alamos Primer",
      "Wilbur Wright College",
      "Chicago Pile-3",
      "Sociology",
      "Boron",
      "Oak Ridge, Tennessee",
      "Timeline of the Manhattan Project",
      "United States Senate",
      "Franck Report",
      "Trinity (nuclear test)",
      "Chicago",
      "Frank Spedding",
      "Harold Urey"
    ]
  },
  "International World Wide Web Conference": {
    "url": "https://en.wikipedia.org/wiki/International_World_Wide_Web_Conference",
    "title": "International World Wide Web Conference",
    "content": "The ACM Web Conference (formerly known as International World Wide Web Conference , abbreviated as WWW ) is a yearly international academic conference on the topic of the future direction of the World Wide Web . The first conference of many was held and organized by Robert Cailliau in 1994 at CERN in Geneva , Switzerland . The conference has been organized by the International World Wide Web Conference Committee (IW3C2), also founded by Robert Cailliau and colleague Joseph Hardin, every year since. [ 1 ] In 2020, the Web Conference series became affiliated with the Association for Computing Machinery (ACM), [ 2 ] where it is supported by ACM SIGWEB . The conference's location rotates among North America, Europe, and Asia and its events usually span a period of five days. The conference aims to provide a forum in which \"key influencers, decision makers, technologists, businesses and standards bodies \" can both present their ongoing work, research, and opinions as well as receive feedback from some of the most knowledgeable people in the field. [ 1 ] The web conference series is aimed at providing a global forum for discussion and debate in regard to the standardization of its associated technologies and the impact of said technologies on society and culture. Developers, researchers, internet users as well as commercial ventures and organizations come together at the conference to discuss the latest advancements of the Web and its evolving uses and trends, such as the development and popularization of the eTV and eBusiness. [ 3 ] The conferences usually include a variety of events, such as tutorials and workshops, as well as the main conference and special dedications of space in memory of the history of the Web and specific notable events. [ 4 ] The conferences are organized by the IW3C2 in collaboration with the World Wide Web Consortium (W3C), Local Organizing Committees, and Technical Program Committees. [ 5 ] Robert Cailliau , a founder of the World Wide Web himself, lobbied inside CERN and at conferences like the Hypertext 1991 in San Antonio , Texas , and Hypertext 1993 in Seattle , Washington . [ 6 ] As he came back from the conference 1993 he announced a new conference called World Wide Web Conference 1 and was actually 23 hours faster than the NCSA announced Mosaic and the Web . [ 6 ] After founding the IW3C2 with Joseph Hardin from the NCSA they decided the next Conferences in Geneva. [ 6 ] Though the way in which its content is organized varies from year to year, the World-Wide Web Conference continues to call itself the \"premiere venue for researchers, academics, businesses, and standard bodies to come together and discuss [the] latest updates and the state and evolutionary path of the Web\". [ 7 ] People from all across the world come together and submit their own new research to be peer-reviewed by some of the World Wide Web community's most knowledgeable members. At the 2014 conference, WWW's largest program, peer-reviewed research paper presentations, fell into one of eleven categories: Those papers accepted were to be presented at the conference itself, and appear in the online conference proceedings published by the ACM Digital Library as well as the conference's website. Furthermore, many of these papers are submitted to other peer-reviewed journals after the conference. [ 7 ] In addition to presenting breakthrough research on the Web and its associated technologies, the Conference acts as a stage for developers to demonstrate and receive feedback on their ongoing work in a dedicated session. The Demo Track allows researchers and practitioners to demonstrate new systems in a dedicated session. The Developer Track is a track dedicated solely to web development, a stage upon which web developers can present \"new trends and interesting ideas [as well as the] code and APIs of emerging applications, platforms, and standards.\" [ 8 ] Though peer-reviewed research paper presentations, demo, and developer tracks are a large portion of the conference's program, it is not merely a launch pad for individuals who have completed cutting-edge research in the field. Students studying the Web and its associated technologies can submit unfinished work for review. Beginner as well as senior PhD students are encouraged to present their ideas to the PhD Symposium for review. This is a unique opportunity to receive feedback on their work from experienced researchers as well as other senior PhD students working in related research areas. All applications and submissions are looked over by the Symposium Program Committee. This committee includes other experienced researches. These people are able to help the applicants and guide them in their work. [ 9 ] Researchers and practitioners are also encouraged to submit their new and innovative work-in-progress. Providing them with a unique opportunity to gain feedback from their peers in an informal setting, the Poster Track provides its presenters invaluable feedback from knowledgeable sources as well as other conference attendees with an opportunity to learn about novel ongoing research projects whose results already appear promising, despite their incompletion. [ 10 ] Lastly, the Conference allows for a series of co-located workshops to its attendees dedicated to emergent Web topics. These workshops work to not only create an open dialogue amongst all researchers and practitioners of Web technologies but also a potential means of collaboration in present and future endeavors. [ 10 ] Past and future conferences include: [ 11 ]",
    "links": [
      "Australia",
      "ICML",
      "Conference proceedings",
      "Italy",
      "World Wide Web Conference 1",
      "Darmstadt",
      "Boston",
      "Sydney",
      "Washington (state)",
      "Lyon",
      "San Antonio",
      "Honolulu",
      "Chiba, Chiba",
      "Netherlands",
      "Japan",
      "Spain",
      "Robert Cailliau",
      "San Francisco",
      "Geneva",
      "Taipei",
      "Taiwan",
      "World Wide Web Consortium",
      "Edinburgh",
      "ProQuest",
      "Montreal",
      "United Kingdom",
      "Hawaii",
      "Institute of Electrical and Electronics Engineers",
      "Human factors",
      "Seattle",
      "China",
      "Academic conference",
      "India",
      "New York City",
      "ACM Digital Library",
      "ACM Conference on Hypertext and Social Media",
      "COVID-19 pandemic",
      "Perth",
      "Santa Clara, California",
      "List of academic disciplines and sub-disciplines",
      "ACM SIGIR Conference",
      "Academic journal",
      "History of the World Wide Web",
      "Banff, Alberta",
      "International World Wide Web Conference Committee",
      "CERN",
      "Texas",
      "Madrid",
      "Conference on Neural Information Processing Systems",
      "Budapest",
      "Hyderabad, India",
      "Slovenia",
      "World Wide Web",
      "National Center for Supercomputing Applications",
      "Brisbane",
      "Seoul",
      "Semantic web",
      "Paris",
      "Hong Kong",
      "Association for Computing Machinery",
      "Florence",
      "Toronto",
      "Canada",
      "Amsterdam",
      "United States",
      "Rio de Janeiro",
      "South Korea",
      "Singapore",
      "Brazil",
      "Wayback Machine",
      "Germany",
      "SIGKDD",
      "Standards bodies",
      "Raleigh, North Carolina",
      "Beijing",
      "Switzerland",
      "ACM SIGWEB",
      "Ljubljana",
      "Austin, Texas",
      "Hungary",
      "Chicago",
      "France"
    ]
  },
  "Text Retrieval Conference": {
    "url": "https://en.wikipedia.org/wiki/Text_Retrieval_Conference",
    "title": "Text Retrieval Conference",
    "content": "The Text REtrieval Conference ( TREC ) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks. It is co-sponsored by the National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (part of the office of the Director of National Intelligence ), and began in 1992 as part of the TIPSTER Text program . Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology . TREC's evaluation protocols have improved many search technologies. A 2010 study estimated that \"without TREC, U.S. Internet users would have spent up to 3.15 billion additional hours using web search engines between 1999 and 2009.\" [ 1 ] Hal Varian the Chief Economist at Google wrote that \"The TREC data revitalized research on information retrieval. Having a standard, widely available, and carefully constructed set of data laid the groundwork for further innovation in this field.\" [ 2 ] Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable features . Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.Text Retrieval Conference started in 1992, funded by DARPA (US Defense Advanced Research Project) and run by NIST. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. TREC is overseen by a program committee consisting of representatives from government, industry, and academia. For each TREC, NIST provide a set of documents and questions. Participants run their own retrieval system on the data and return to NIST a list of retrieved top-ranked documents. NIST pools the individual result judges the retrieved documents for correctness and evaluates the results. The TREC cycle ends with a workshop that is a forum for participants to share their experiences. TREC defines relevance as: \"If you were writing a report on the subject of the topic and would use the information contained in the document in the report, then the document is relevant.\" [ 3 ] Most TREC retrieval tasks use binary relevance: a document is either relevant or not relevant. Some TREC tasks use graded relevance, capturing multiple degrees of relevance. Most TREC collections are too large to perform complete relevance assessment; for these collections it is impossible to calculate the absolute recall for each query. To decide which documents to assess, TREC usually uses a method call pooling. In this method, the top-ranked n documents from each contributing run are aggregated, and the resulting document set is judged completely. In 1992 TREC-1 was held at NIST. The first conference attracted 28 groups of researchers from academia and industry. It demonstrated a wide range of different approaches to the retrieval of text from large document collections .Finally TREC1 revealed the facts that automatic construction of queries from natural language query statements seems to work. Techniques based on natural language processing were no better no worse than those based on vector or probabilistic approach. TREC2 Took place in August 1993. 31 group of researchers participated in this. Two types of retrieval were examined. Retrieval using an ‘ad hoc’ query and retrieval using a ‘routing' query In TREC-3 a small group experiments worked with Spanish language collection and others dealt with interactive query formulation in multiple databases TREC-4 they made even shorter to investigate the problems with very short user statements TREC-5 includes both short and long versions of the topics with the goal of carrying out deeper investigation into which types of techniques work well on various lengths of topics In TREC-6 Three new tracks speech, cross language, high precision information retrieval were introduced. The goal of cross language information retrieval is to facilitate research on system that are able to retrieve relevant document regardless of language of the source document TREC-7 contained seven tracks out of which two were new Query track and very large corpus track. The goal of the query track was to create a large query collection TREC-8 contain seven tracks out of which two –question answering and web tracks were new. The objective of QA query is to explore the possibilities of providing answers to specific natural language queries TREC-9 Includes seven tracks In TREC-10 Video tracks introduced Video tracks design to promote research in content based retrieval from digital video In TREC-11 Novelty tracks introduced. The goal of novelty track is to investigate systems abilities to locate relevant and new information within the ranked set of documents returned by a traditional document retrieval system TREC-12 held in 2003 added three new tracks; Genome track, robust retrieval track, HARD (Highly Accurate Retrieval from Documents) [ 4 ] New tracks are added as new research needs are identified, this list is current for TREC 2018. [ 5 ] In 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called NTCIR ( NII Test Collection for IR Systems), and in 2000, CLEF , a European counterpart, emphasizing cross-language information retrieval, was launched. The Forum for Information Retrieval Evaluation (FIRE) started in 2008 with the aim of building a South Asian counterpart for TREC, CLEF, and NTCIR. NIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled. [ 7 ] The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of publications . Technology first developed in TREC is now included in many of the world's commercial search engines . An independent report by RTII found that \"about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia.\" [ 8 ] [ 9 ] While one study suggests that the state of the art for ad hoc search did not advance substantially in the decade preceding 2009, [ 10 ] it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes. There have been advances in other types of ad hoc search. For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections. In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections. The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests. In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and other retrieval domains. [ when? ] [ citation needed ] TREC systems often provide a baseline for further research. Examples include: The conference is made up of a varied, international group of researchers and developers. [ 15 ] [ 16 ] [ 17 ] In 2003, there were 93 groups from both academia and industry from 22 countries participating.",
    "links": [
      "Spam (electronic)",
      "Patent search",
      "DARPA TIPSTER Program",
      "List of computer science awards",
      "Index (search engine)",
      "Natural language processing",
      "Features (pattern recognition)",
      "Microblog",
      "RTI International",
      "TREC (disambiguation)",
      "Digital video",
      "CLEF",
      "Genomics",
      "IBM Watson",
      "Search engine",
      "Information need",
      "Document retrieval",
      "Workshop",
      "Discovery (law)",
      "Stephen Tomlinson",
      "The Washington Post",
      "Diffeo (company)",
      "Crowdsourcing",
      "Information retrieval",
      "Spam filter",
      "Video search engine",
      "Relevance feedback",
      "National Institute of Informatics",
      "ISBN (identifier)",
      "Chemistry",
      "Question answering",
      "Terabyte",
      "Jeopardy!",
      "Intelligence Advanced Research Projects Activity",
      "Text retrieval",
      "List of academic disciplines and sub-disciplines",
      "National Institute of Standards and Technology",
      "Technology transfer",
      "Hal Varian",
      "Entity linking",
      "Federated search",
      "Electronic document",
      "Enterprise search",
      "Director of National Intelligence",
      "Blogspot",
      "Association for the Advancement of Artificial Intelligence",
      "TREC Genomics",
      "Cross-language information retrieval",
      "Human-computer interaction",
      "IBM",
      "DeepQA",
      "Google"
    ]
  },
  "Retrievability": {
    "url": "https://en.wikipedia.org/wiki/Retrievability",
    "title": "Retrievability",
    "content": "Retrievability is a term associated with the ease with which information can be found or retrieved using an information system, specifically a search engine or information retrieval system. A document (or information object) has high retrievability if there are many queries which retrieve the document via the search engine, and the document is ranked sufficiently high that a user would encounter the document. Conversely, if there are few queries that retrieve the document, or when the document is retrieved the documents are not high enough in the ranked list, then the document has low retrievability. Retrievability can be considered as one aspect of findability . Applications of retrievability include detecting search engine bias , measuring algorithmic bias, evaluating the influence of search technology, tuning information retrieval systems and evaluating the quality of documents in a collection . Retrievability is also key to the process of Retrieval-Augmented Generation (RAG) - a technique that enables large language models (LLMs) to retrieve and incorporate new information. RAG is used whenever AI needs to select information from a private corpus or any source it hasn't been trained on. Agentic AI web browsing applications also depend heavily on the retrievability of website content. [ 1 ]",
    "links": [
      "Search engine optimization",
      "ISBN (identifier)",
      "S2CID (identifier)",
      "Knowledge mining",
      "Text corpus",
      "Agentic AI",
      "Findability",
      "Doi (identifier)",
      "Retrieval-Augmented Generation",
      "Information retrieval",
      "Search engine"
    ]
  },
  "S2CID (identifier)": {
    "url": "https://en.wikipedia.org/wiki/S2CID_(identifier)",
    "title": "S2CID (identifier)",
    "content": "Semantic Scholar is a research tool for scientific literature. It is developed at the Allen Institute for AI and was publicly released in November 2015. [ 2 ] Semantic Scholar uses modern techniques in natural language processing to support the research process, for example by providing automatically generated summaries of scholarly papers. [ 3 ] The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing , machine learning , human–computer interaction , and information retrieval . [ 4 ] Semantic Scholar began as a database for the topics of computer science , geoscience , and neuroscience . [ 5 ] In 2017, the system began including biomedical literature in its corpus. [ 5 ] As of September 2022 [update] , it includes over 200 million publications from all fields of science. [ 6 ] Semantic Scholar provides a one-sentence summary of scientific literature . One of its aims was to address the challenge of reading numerous titles and lengthy abstracts on mobile devices. [ 7 ] It also seeks to ensure that the three million scientific papers published yearly reach readers, since it is estimated that only half of this literature is ever read. [ 8 ] Artificial intelligence is used to capture the essence of a paper, generating it through an \"abstractive\" technique. [ 3 ] The project uses a combination of machine learning , natural language processing , and machine vision to add a layer of semantic analysis to the traditional methods of citation analysis , and to extract relevant figures, tables , entities, and venues from papers. [ 9 ] [ 10 ] Another key AI-powered feature is Research Feeds, an adaptive research recommender that uses AI to quickly learn what papers users care about reading and recommends the latest research to help scholars stay up to date. It uses a paper embedding model trained using contrastive learning to find papers similar to those in each Library folder. [ 11 ] Semantic Scholar also offers Semantic Reader, an augmented reader with the potential to revolutionize scientific reading by making it more accessible and richly contextual. [ 12 ] Semantic Reader provides in-line citation cards that allow users to see citations with TLDR (short for Too Long, Didn't Read) automatically generated short summaries as they read and skimming highlights that capture key points of a paper so users can digest faster. In contrast with Google Scholar and PubMed , Semantic Scholar is designed to highlight the most important and influential elements of a paper. [ 13 ] The AI technology is designed to identify hidden connections and links between research topics. [ 14 ] Like the previously cited search engines, Semantic Scholar also exploits graph structures, which include the Microsoft Academic Knowledge Graph , Springer Nature's SciGraph , and the Semantic Scholar Corpus (originally a 45 million papers corpus in computer science, neuroscience and biomedicine). [ 15 ] [ 16 ] Each paper hosted by Semantic Scholar is assigned a unique identifier called the Semantic Scholar Corpus ID (abbreviated S2CID). The following entry is an example: Liu, Ying; Gayle, Albert A; Wilder-Smith, Annelies; Rocklöv, Joacim (March 2020). \"The reproductive number of COVID-19 is higher compared to SARS coronavirus\". Journal of Travel Medicine . 27 (2). doi : 10.1093/jtm/taaa021 . PMID 32052846 . S2CID 211099356 . Semantic Scholar is free to use and unlike similar search engines (i.e. Google Scholar ) does not search for material that is behind a paywall . [ 5 ] [ better source needed ] One study compared the index scope of Semantic Scholar to Google Scholar, and found that for the papers cited by secondary studies in computer science, the two indices had comparable coverage, each only missing a handful of the papers. [ 17 ] As of January 2018, following a 2017 project that added biomedical papers and topic summaries, the Semantic Scholar corpus included more than 40 million papers from computer science and biomedicine . [ 18 ] In March 2018, Doug Raymond, who developed machine learning initiatives for the Amazon Alexa platform, was hired to lead the Semantic Scholar project. [ 19 ] As of August 2019 [update] , the number of included papers metadata (not the actual PDFs) had grown to more than 173 million [ 20 ] after the addition of the Microsoft Academic Graph records. [ 21 ] In 2020, a partnership between Semantic Scholar and the University of Chicago Press Journals made all articles published under the University of Chicago Press available in the Semantic Scholar corpus. [ 22 ] At the end of 2020, Semantic Scholar had indexed 190 million papers. [ 23 ] In 2020, Semantic Scholar reached seven million users per month. [ 7 ]",
    "links": [
      "Scientific writing",
      "Poster session",
      "Public health journal",
      "ERIH PLUS",
      "Peer review",
      "Working paper",
      "Open access",
      "Conference proceedings",
      "Doi (identifier)",
      "Imprint (trade name)",
      "Monograph",
      "WDQ (identifier)",
      "Open access citation advantage",
      "ORCID",
      "Annual report",
      "Version of record",
      "Documentary editing",
      "Citation cartel",
      "TL;DR",
      "Rankings of academic publishers",
      "ICanHazPDF",
      "BASE (search engine)",
      "Information retrieval",
      "SCImago Journal Rank",
      "Biological patent",
      "Table extraction",
      "Book",
      "Least publishable unit",
      "Scientometrics",
      "H-index",
      "Acknowledgment index",
      "Copyright policies of academic publishers",
      "OpenAlex",
      "Sherpa Romeo",
      "Abstract (summary)",
      "Grey literature",
      "Semantic analysis (linguistics)",
      "Postprint",
      "Open-notebook science",
      "Microsoft Academic",
      "Chapter (books)",
      "Academic journal publishing reform",
      "Composite index (metrics)",
      "Sci-Hub",
      "Open scientific data",
      "Web of Science",
      "Article-level metrics",
      "Index Copernicus",
      "Technical report",
      "List of scientific journals",
      "Position paper",
      "G-index",
      "List of academic publishers by preprint policy",
      "Scientific literature",
      "Literature review",
      "Eigenfactor",
      "Microsoft Academic Graph",
      "OpenAIRE",
      "Journal of the Medical Library Association",
      "Review article",
      "PMID (identifier)",
      "Pamphlet",
      "Wikidata",
      "Identifier",
      "White paper",
      "Citation index",
      "Academic journal",
      "Retraction in academic publishing",
      "Science (journal)",
      "Science-wide author databases of standardized citation indicators",
      "Electronic publishing",
      "Geoscience",
      "Festschrift",
      "Treatise",
      "List of university presses",
      "Natural language processing",
      "Machine learning",
      "OCLC (identifier)",
      "Google Scholar",
      "Machine vision",
      "Allen Institute for Artificial Intelligence",
      "Edited volume",
      "Open research",
      "Publish or perish",
      "List of academic databases and search engines",
      "Altmetrics",
      "Chemical patent",
      "Computer science",
      "Preprint",
      "AMiner (database)",
      "Citation analysis",
      "Text publication society",
      "Paywall",
      "Amazon Alexa",
      "Essay",
      "Thesis",
      "SciGraph",
      "Erratum",
      "Citation impact",
      "Biomedicine",
      "Patent",
      "Lists of academic journals",
      "Search engine",
      "Knowledge extraction",
      "Nature (journal)",
      "List of open-access journals",
      "ISSN (identifier)",
      "Serials crisis",
      "Scholarly communication",
      "Ingelfinger rule",
      "Author-level metrics",
      "Human–computer interaction",
      "ISBN (identifier)",
      "Collection of articles",
      "Allen Institute for AI",
      "Academic publishing",
      "Highly Cited Researchers",
      "Biomedical literature",
      "Bibliometrics",
      "CORE (research service)",
      "PMC (identifier)",
      "Neuroscience",
      "University of Chicago Press",
      "Impact factor",
      "Journal ranking",
      "Scopus",
      "Paperity",
      "Learned society",
      "PubMed"
    ]
  },
  "JSTOR (identifier)": {
    "url": "https://en.wikipedia.org/wiki/JSTOR_(identifier)",
    "title": "JSTOR (identifier)",
    "content": "JSTOR ( / ˈ dʒ eɪ s t ɔːr / JAY -stor ; short for Journal Storage ) [ 2 ] is a digital library of academic journals, books, and primary sources founded in 1994. Originally containing digitized back issues of academic journals, it now encompasses books and other primary sources as well as current issues of journals in the humanities and social sciences. [ 3 ] It provides full-text searches of almost 2,000 journals. Most access is by subscription but some of the site is public domain , and open access content is available free of charge. [ 4 ] JSTOR is part of the non-profit US academic digital library and learning platform provider, Ithaka Harbors, Inc. [ 5 ] William G. Bowen , president of Princeton University from 1972 to 1988, [ 6 ] founded JSTOR in 1994. JSTOR was originally conceived as a solution to one of the problems faced by libraries, especially research and university libraries , due to the increasing number of academic journals in existence. Most libraries found it prohibitively expensive in terms of cost and space to maintain a comprehensive collection of journals. By digitizing many journal titles, JSTOR allowed libraries to outsource the storage of journals with the confidence that they would remain available long-term. Online access and full-text searchability improved access dramatically. [ 7 ] Bowen initially considered using CD-ROMs for distribution. [ 8 ] However, Ira Fuchs , Princeton University's vice president for Computing and Information Technology, convinced Bowen that CD-ROM was becoming an increasingly outdated technology and that network distribution could eliminate redundancy and increase accessibility (for example, all Princeton's administrative and academic buildings were networked by 1989; the student dormitory network was completed in 1994; and campus networks like the one at Princeton were, in turn, linked to larger networks such as BITNET and the Internet ). JSTOR was initiated in 1995 at seven different library sites, and originally encompassed ten economics and history journals. JSTOR access improved based on feedback from its initial sites, and it became a fully searchable index accessible from any ordinary web browser . Special software was put in place to make pictures and graphs clear and readable. [ 9 ] With the success of this limited project, Bowen, Fuchs, and Kevin Guthrie, the then-president of JSTOR, wanted to expand the number of participating journals. They met with representatives of the Royal Society of London and an agreement was made to digitize the Philosophical Transactions of the Royal Society dating from its beginning in 1665. The work of adding these volumes to JSTOR was completed by December 2000. [ 9 ] In 1999 JSTOR started a partnership with Joint Information Systems Committee and created a mirror website at the University of Manchester to make the JSTOR database available to over 20 higher education institutions in England, Scotland, Wales and Northern Ireland. [ 10 ] The Andrew W. Mellon Foundation funded JSTOR initially. Until January 2009, JSTOR operated as an independent, self-sustaining nonprofit organization with offices in New York City and in Ann Arbor, Michigan . Then JSTOR merged with the nonprofit Ithaka Harbors, Inc. [ 5 ] —a nonprofit organization founded in 2003 and \"dedicated to helping the academic community take full advantage of rapidly advancing information and networking technologies\". [ 1 ] In 2019, JSTOR's revenue was $79 million. [ 11 ] JSTOR content is provided by more than 900 publishers. [ 12 ] The database contains more than 12 million journal articles, in more than 75 disciplines. [ 5 ] Each object is uniquely identified by an integer value, starting at 1 , which is used to create a stable URL . [ 13 ] In addition to the main site, the JSTOR labs group operates an open service that allows access to the contents of the archives for the purposes of corpus analysis at its Data for Research service. [ 14 ] This site offers a search facility with graphical indication of the article coverage and loose integration into the main JSTOR site. Users may create focused sets of articles and then request a dataset containing word and n -gram frequencies and basic metadata. They are notified when the dataset is ready and may download it in either XML or CSV formats. The service does not offer full-text, although academics may request that from JSTOR, subject to a non-disclosure agreement. [ citation needed ] JSTOR Plant Science [ 15 ] is available in addition to the main site. JSTOR Plant Science provides access to content such as plant type specimens, taxonomic structures, scientific literature, and related materials and aimed at those researching, teaching, or studying botany, biology, ecology, environmental, and conservation studies. The materials on JSTOR Plant Science are contributed through the Global Plants Initiative (GPI) [ 16 ] and are accessible only to JSTOR and GPI members. Two partner networks are contributing to this: the African Plants Initiative, which focuses on plants from Africa, and the Latin American Plants Initiative, which contributes plants from Latin America. [ 17 ] JSTOR launched its Books at JSTOR program in November 2012, adding 15,000 current and backlist books to its site. The books are linked with reviews and from citations in journal articles. [ 18 ] In September 2014, JSTOR launched JSTOR Daily , an online magazine meant to bring academic research to a broader audience. Posted articles are generally based on JSTOR entries, and some entries provide the backstory to current events. [ 19 ] Reveal Digital is a JSTOR-hosted collection of documents produced by or about underground, marginalized and dissenting 20th century communities. [ 20 ] Reveal Digital's open access content includes zines, prison newspapers, AIDS art, student-movement documents, black civil rights materials, and a white supremacy archive. [ 20 ] JSTOR is licensed mainly to academic institutions, public libraries, research institutions, museums, and schools. More than 7,000 institutions in more than 150 countries have access. [ 3 ] JSTOR has been running a pilot program of allowing subscribing institutions to provide access to their alumni, in addition to current students and staff. The Alumni Access Program officially launched in January 2013. [ 21 ] Individual subscriptions also are available to certain journal titles through the journal publisher. [ 22 ] Every year, JSTOR blocks 150 million attempts by non-subscribers to read articles. [ 23 ] Inquiries have been made about the possibility of making JSTOR open access . According to Harvard Law professor Lawrence Lessig , JSTOR had been asked, \"How much would it cost to make this available to the whole world? What would we have to pay you?\", reportedly responding with a figure of $250 million dollars. [ 24 ] In late 2010 and early 2011, Aaron Swartz , an American computer programmer, writer, political organizer and Internet activist, used MIT 's data network to bulk-download a substantial portion of JSTOR's collection of academic journal articles. [ 25 ] [ 26 ] When the bulk-download was discovered, a video camera was placed in the room to film Swartz while the relevant computer was left untouched. Once video was captured of him, the download was stopped and he was identified. Rather than pursue a civil lawsuit against him, in June 2011 JSTOR reached a settlement wherein Swartz surrendered the downloaded data. [ 25 ] [ 26 ] The following month, federal authorities charged Swartz with several data theft –related crimes, including wire fraud , computer fraud, unlawfully obtaining information from a protected computer , and recklessly damaging a protected computer. [ 27 ] [ 28 ] Prosecutors in the case claimed that Swartz acted with the intention of making the papers available on P2P file-sharing sites . [ 26 ] [ 29 ] Swartz surrendered to authorities, pleaded not guilty to all counts, and was released on $100,000 bail. In September 2012, U.S. attorneys increased the number of charges against Swartz from four to thirteen, with a possible penalty of 35 years in prison and $1 million in fines. [ 30 ] [ 31 ] The case still was pending when Swartz died by suicide in January 2013. [ 32 ] The availability of most journals on JSTOR is controlled by a \" moving wall \", which is an agreed-upon delay between the current volume of the journal and the latest volume available on JSTOR. This time period is specified by agreement between JSTOR and the publisher of the journal, which usually is three to five years. Publishers may request that the period of a \"moving wall\" be changed or request discontinuation of coverage. Formerly, publishers also could request that the \"moving wall\" be changed to a \"fixed wall\"—a specified date after which JSTOR would not add new volumes to its database. As of November 2010 [update] , \"fixed wall\" agreements were still in effect with three publishers of 29 journals made available [ needs update ] online through sites controlled by the publishers. [ 33 ] In 2010, JSTOR started adding current issues of certain journals through its Current Scholarship Program. [ 34 ] Beginning September 6, 2011, JSTOR made public domain content available at no charge to the public. [ 35 ] [ 36 ] This \"Early Journal Content\" program constitutes about 6% of JSTOR's total content, and includes over 500,000 documents from more than 200 journals that were published before 1923 in the United States, and before 1870 in other countries. [ 35 ] [ 36 ] [ 37 ] JSTOR stated that it had been working on making this material free for some time. The Swartz controversy and Greg Maxwell's protest torrent of the same content led JSTOR to \"press ahead\" with the initiative. [ 35 ] [ 36 ] As of 2017 [update] , JSTOR does not have plans to extend it to other public domain content, stating that \"We do not believe that just because something is in the public domain, it can always be provided for free\". [ 38 ] In January 2012, JSTOR started a pilot program, \"Register & Read\", offering limited no-cost access (not open access ) to archived articles for individuals who register for the service. At the conclusion of the pilot, in January 2013, JSTOR expanded Register & Read from an initial 76 publishers to include about 1,200 journals from over 700 publishers. [ 39 ] Registered readers may read up to six articles online every calendar month, but may not print or download PDFs. [ 40 ] In 2013, more than 8,000 institutions in more than 160 countries had access to JSTOR. [ 12 ] As of 2014, JSTOR is conducting a pilot program with Wikipedia , whereby established editors are given reading privileges through the Wikipedia Library , as with a university library. [ 41 ] [ 42 ] In 2012, JSTOR users performed nearly 152 million searches, with more than 113 million article views and 73.5 million article downloads. [ 12 ] JSTOR has been used as a resource for linguistics research to investigate trends in language use over time and also to analyze gender differences and inequities in scholarly publishing, revealing that in certain fields, men predominate in the prestigious first and last author positions and that women are significantly underrepresented as authors of single-authored papers. [ 43 ] [ 44 ] [ 45 ] JSTOR metadata is available through CrossRef and the Unpaywall dump, [ 46 ] which as of 2020 identifies nearly 3 million works hosted by JSTOR as toll access , as opposed to over 200,000 available in open access (mainly through third party open access repositories ). [ citation needed ]",
    "links": [
      "Open access",
      "Doi (identifier)",
      "S2CID (identifier)",
      "Wikipedia Library",
      "Nieman Lab",
      "JHOVE",
      "Ira Fuchs",
      "Artstor",
      "United States v. Aaron Swartz",
      "URL",
      "Unpaywall",
      "OCLC",
      "Bibcode (identifier)",
      "N-gram",
      "Project Muse",
      "Aluka",
      "Data theft",
      "Aaron Swartz",
      "Open access repositories",
      "New York City",
      "Ann Arbor, Michigan",
      "Subscription business model",
      "Internet Archive",
      "PMID (identifier)",
      "Web browser",
      "Academic community",
      "Library Journal",
      "ArXiv (identifier)",
      "Journal of Library Administration",
      "Digital preservation",
      "Anna's Archive",
      "Computer security",
      "BBC News",
      "Massachusetts Institute of Technology",
      "Public domain",
      "ArXiv",
      "OurResearch",
      "BITNET",
      "Wire fraud",
      "Embargo (academic publishing)",
      "William G. Bowen",
      "University of Manchester",
      "Jisc",
      "Bibliographic database",
      "Text corpus",
      "List of academic databases and search engines",
      "Internet",
      "Peer-to-peer file sharing",
      "Toll access",
      "Royal Society",
      "XML",
      "Andrew W. Mellon Foundation",
      "Organizational founder",
      "ProPublica",
      "Lawrence Lessig",
      "Philosophical Transactions of the Royal Society",
      "Wayback Machine",
      "HAL (open archive)",
      "Learning management system",
      "ISSN (identifier)",
      "Digital library",
      "Princeton University",
      "ISBN (identifier)",
      "Academic publishing",
      "Nonprofit organization",
      "Ithaka Harbors, Inc.",
      "Japanese Historical Text Initiative",
      "Torrent file",
      "CrossRef",
      "PMC (identifier)",
      "Ithaka Harbors",
      "Research library",
      "Full-text search",
      "Inside Higher Ed",
      "Comma-separated values",
      "The Chronicle of Higher Education",
      "Digitized",
      "J-STAGE"
    ]
  },
  "Search engines": {
    "url": "https://en.wikipedia.org/wiki/Search_engines",
    "title": "Search engines",
    "content": "Search engines , including web search engines , selection-based search engines, metasearch engines , desktop search tools, and web portals and vertical market websites have a search facility for online databases . General: Academic materials only: Search engines dedicated to a specific kind of information These search engines work across the BitTorrent protocol . Desktop search engines listed on a light purple background are no longer in active development.",
    "links": [
      "Standard Tibetan",
      "Zabasearch.com",
      "OpenText",
      "Fabasoft",
      "Info.com",
      "Sepia Search",
      "The Pirate Bay",
      "Ht-//Dig",
      "WiseNut",
      "Technorati",
      "Xapian",
      "PriceRunner",
      "BASE (search engine)",
      "Mocavo.com",
      "Windows Server 2003",
      "Israel",
      "Yandex Maps",
      "Zoopla",
      "Portugal",
      "Lookeen",
      "DuckDuckGo",
      "Enterprise bookmarking",
      "Ixquick",
      "Egerin",
      "Wikia Search",
      "OpenStreetMap",
      "Trexy",
      "Swiftype",
      "Zettair",
      "Seznam.cz",
      "Terrier Search Engine",
      "Swisscows",
      "Searchdaimon",
      "HP Autonomy",
      "World Wide Web Worm",
      "Strigi",
      "Google Desktop",
      "China",
      "Jumper 2.0",
      "Google Shopping",
      "Ibibo",
      "Ask.com",
      "MySpace",
      "Dice.com",
      "ZoomInfo",
      "Otalo.com",
      "Daily Stocks",
      "Vertical search",
      "BSD License",
      "Yahoo! HotJobs",
      "Yummly",
      "Overture.com",
      "Natural language search engine",
      "Mozilla Public License",
      "Apple Inc.",
      "Blackle",
      "W3Catalog",
      "TickX",
      "BRS/Search",
      "Indeed",
      "Redfin",
      "MacOS",
      "Desktop search",
      "KidzSearch",
      "Krugle",
      "SearXNG",
      "Petal Search",
      "Google Scholar",
      "Deja News",
      "Yahoo! Search",
      "FindFace",
      "Elasticsearch",
      "Slovenian language",
      "Google News",
      "X11/MIT License",
      "Tencent Video",
      "Everything (software)",
      "Yahoo! Search Marketing",
      "Northern Light Group",
      "Delver (Social Search)",
      "Yahoo! Video",
      "List of academic databases and search engines",
      "WebCrawler",
      "Sphere (Website)",
      "Wikiloc",
      "Hakia",
      "Marketwire",
      "OpenSearch (software)",
      "Search aggregator",
      "KDE Applications",
      "BTDigg",
      "Sesam (search engine)",
      "Leit.is",
      "Direct Hit Technologies",
      "Google Video",
      "Lotus Magellan",
      "Business.com",
      "Germany",
      "Comparison of search engines",
      "RecipeBridge",
      "Windows Vista",
      "Czech Republic",
      "Bengali language",
      "Ubuntu Desktop",
      "Veveo",
      "Tropes Zoom",
      "Norway",
      "MapQuest",
      "Grams (search)",
      "Infoseek",
      "Nextbio",
      "Search engine optimization",
      "Apache License",
      "PubMed",
      "Windows Search",
      "Google Maps",
      "Google Search",
      "Tickex",
      "Google Patents",
      "MetaLib",
      "Metasearch engine",
      "AlltheWeb",
      "Job search engine",
      "Japan",
      "Goo (search engine)",
      "Ciao (website)",
      "Searchmedica",
      "Koders",
      "Mamma.com",
      "Microsoft Windows",
      "EB-eye",
      "Grokker",
      "Mobissimo",
      "Shopzilla",
      "Petal Maps",
      "Scroogle",
      "TinEye",
      "Recipe",
      "WebMD",
      "Kartoo",
      "Google Code Search",
      "Thomasnet",
      "Alexa Internet",
      "Qwant",
      "GoPubMed",
      "IBM Watson",
      "Kelkoo",
      "Empas",
      "FindSounds",
      "Mac OS X v10.4",
      "Unity Dash",
      "Apache Solr",
      "Perplexity.ai",
      "Forestle",
      "Singingfish",
      "Yahoo! Maps",
      "IBM STAIRS",
      "Openverse",
      "Perplexity AI",
      "Healthline",
      "Windows XP",
      "YaCy",
      "Multimedia search",
      "Microsoft",
      "Sweden",
      "Beagle (software)",
      "Entrez",
      "Search engine privacy",
      "Kayak.com",
      "Speechbot",
      "Shopping.com",
      "Online database",
      "German language",
      "Trulia",
      "Evi (software)",
      "Amatomu",
      "Parsijoo",
      "Brave Search",
      "Eclipse Public License",
      "Nate (web portal)",
      "Iceland",
      "HotPads.com",
      "CiteAb",
      "RetrievalWare",
      "Facebook",
      "Rightmove",
      "IWon",
      "Recoll",
      "Neeva",
      "Apache License 2.0",
      "LeapFish",
      "Daum (web portal)",
      "Semantic search",
      "NEPOMUK (software)",
      "Archive.today",
      "Yahoo! News",
      "A9.com",
      "Ms. Dewey",
      "LexisNexis",
      "Kiddle (search engine)",
      "Podscope",
      "Spokeo",
      "Chegg",
      "Ecocho",
      "Internet Archive Scholar",
      "Pipilika",
      "Search.ch",
      "Bustripping",
      "JumpStation",
      "Getit Infoservices Private Limited",
      "MySimon",
      "SearchGPT",
      "Korean language",
      "Czech language",
      "Géoportail",
      "Munax",
      "Oracle Corporation",
      "Baidu Maps",
      "Presearch (search engine)",
      "Unix",
      "Mullvad Leta",
      "Cuil",
      "Outline of search engines",
      "Gigablast",
      "Baidu",
      "Dogpile",
      "Rozee.pk",
      "Monster.com",
      "Sphinx (search engine)",
      "Whitepages (company)",
      "Wikiseek",
      "Google Dataset Search",
      "GenieKnows",
      "ISYS Search Software",
      "United Kingdom",
      "Dark web",
      "Fireball (search engine)",
      "Najdi.si",
      "List of search engines by popularity",
      "Q-Sensei",
      "Adzuna",
      "Bing Health",
      "Powerset (company)",
      "Sogou",
      "Russian language",
      "Selection-based search",
      "Rollyo",
      "Torrentz",
      "Fast Search & Transfer",
      "Pronto.com",
      "Songza",
      "Bing Shopping",
      "France",
      "X1 Technologies",
      "Tracker (search software)",
      "Swoogle",
      "Google Search Appliance",
      "English language",
      "Microsoft Bing",
      "Skyscanner",
      "DeepPeep",
      "Slovenia",
      "Snowflake Inc.",
      "Go.com",
      "Southeast Asia",
      "Blinkx",
      "TorrentSpy",
      "Wazap",
      "Google Answers",
      "Semantic Scholar",
      "Shodan (website)",
      "Portuguese language",
      "Seeks",
      "IFACnet",
      "Google Images",
      "Bangladesh",
      "Realtor.com",
      "Vivisimo",
      "BitTorrent (protocol)",
      "ChaCha Search",
      "Myriad Search",
      "WestLaw",
      "123people",
      "Naukri.com",
      "Canada",
      "Ask Jeeves",
      "Google",
      "Iran",
      "United States",
      "Startpage",
      "You.com",
      "Japanese language",
      "JobStreet.com",
      "DocFetcher",
      "The Walt Disney Company",
      "BIGLOBE",
      "AOL",
      "Yippy",
      "Volunia",
      "Taganode Local Search Engine",
      "Seznam",
      "Kagi (search engine)",
      "Bing (search engine)",
      "Mac OS X",
      "HotBot",
      "Korea",
      "MozDex",
      "Inktomi",
      "Craigslist",
      "Switzerland",
      "ZipLocal",
      "SWISH-E",
      "Lycos",
      "Google Groups",
      "IBM",
      "Linux",
      "Ahmia",
      "PubGene",
      "Yongzin",
      "Accoona",
      "Tencent Maps",
      "Lexxe",
      "Naver",
      "Apache Lucene",
      "Tafiti",
      "Mystery Seeker",
      "Chinese language",
      "Priyo.com",
      "IceRocket",
      "Windows",
      "PeekYou",
      "Nutch",
      "Wego.com",
      "Btjunkie",
      "Jubii",
      "Enterprise search",
      "Social search",
      "Sproose",
      "Bing News",
      "Grub (search engine)",
      "Regator",
      "Search appliance",
      "Netscape",
      "Visual search engine",
      "Groovle",
      "Quicklaw",
      "Yahoo!",
      "GNOME Storage",
      "PubSub Concepts",
      "Aliweb",
      "Isohunt",
      "MnoGoSearch",
      "Eurekster",
      "India",
      "Excite (web portal)",
      "Sputnik (search engine)",
      "Startpage.com",
      "GlobalSpec",
      "Relevance feedback",
      "Yooz",
      "SearchMe",
      "Russia",
      "Teoma",
      "Ripple (charitable organisation)",
      "GPL-2.0-or-later",
      "Blekko",
      "Bioinformatic Harvester",
      "Yandex Search",
      "Windows Desktop Search",
      "PriceGrabber",
      "List of web directories",
      "Locate32",
      "Web portal",
      "Nokia Maps",
      "Rediff",
      "Exalead",
      "Chorki (search engine)",
      "Soso (search engine)",
      "Bixee.com",
      "Searx",
      "MetaCrawler",
      "Yahoo Japan",
      "Icelandic language",
      "Lucene",
      "CareerBuilder",
      "Thunderstone Software LLC.",
      "Mahalo.com",
      "KRunner",
      "Mojeek",
      "WikiMapia",
      "Mininova",
      "Picsearch",
      "Taptu",
      "SeeqPod",
      "Viewzi",
      "Copernic Desktop Search",
      "Yahoo Search",
      "AT&T",
      "Macroglossa Visual Search",
      "KidRex",
      "Lemur Toolkit & Indri Search Engine",
      "Library of Congress",
      "AltaVista",
      "Persian language",
      "Ecosia",
      "TeraText",
      "Search engine",
      "Rambler (portal)",
      "BlogScope",
      "Bing Maps",
      "Solaris (operating system)",
      "StuRents.com",
      "Zillow",
      "Watson (computer)",
      "SAPO (company)",
      "Web search engine",
      "Kurdish language",
      "Spotlight (software)",
      "Bloglines",
      "Yebol",
      "Glassdoor",
      "TV Genius",
      "Walla!",
      "MetaGer",
      "Vertical market",
      "Youdao",
      "Isearch",
      "Bing Videos",
      "Healia",
      "Human search engine",
      "DtSearch"
    ]
  },
  "Ground truth": {
    "url": "https://en.wikipedia.org/wiki/Ground_truth",
    "title": "Ground truth",
    "content": "Ground truth is information that is known to be real or true, provided by direct observation and measurement (i.e. empirical evidence ) as opposed to information provided by inference . [ 1 ] The term ground truth appeared in remote sensing literature as early as 1972, when NASA described it as essential \"data about...materials on the earth’s surface” used to calibrate measurements. [ 2 ] It was later adopted by the statistical modeling and machine learning communities. [ 3 ] [ 4 ] The Oxford English Dictionary (s.v. ground truth ) records the use of the word Groundtruth in the sense of 'fundamental truth' from Henry Ellison's poem \"The Siberian Exile's Tale\", published in 1833. [ 5 ] The term \"ground truth\" can be used as a noun, adjective, and verb. In statistics and machine learning, ground truth is the ideal expected result, [ 6 ] used in statistical models to prove or disprove research hypotheses . \"Ground truthing\" is the process of gathering the good data for this test. Ground truth is typically included in labeled data . In machine learning, \"ground truth\" is not necessarily objectively correct or true. For example, in training AI models or relevance rankers , it may be a set of judgments made by people or inferred from user behavior, which may depend on context. [ 7 ] For example, in Bayesian spam filtering , a supervised learning system is typically trained by examples labeled as spam and non-spam. Although these labels may be subjective or inaccurate, they are considered ground truth. True ground truth in machine learning is objective data. For example, suppose we are testing a stereo vision system to see how well it can estimate 3D positions. A calibrated laser rangefinder may provide accurate distances as ground truth. In remote sensing , \"ground truth\" refers to information collected at the imaged location. Ground truth allows image data to be related to real features and materials on the ground. The collection of ground truth data enables calibration of remote-sensing data, and aids in the interpretation and analysis of what is being sensed. Examples include cartography , meteorology , analysis of aerial photographs , satellite imagery and other techniques in which data are gathered at a distance. More specifically, ground truth may refer to a process in which \" pixels \" [ 8 ] on a satellite image are compared to what is imaged (at the time of capture) in order to verify the contents of the \"pixels\" in the image (noting that the concept of \"pixel\" is imaging-system-dependent). In the case of a classified image, supervised classification can help to determine the accuracy of the classification by the remote sensing system which can minimize error in the classification. Ground truth is usually done on site, correlating what is known with surface observations and measurements of various properties of the features of the ground resolution cells under study in the remotely sensed digital image. The process also involves taking geographic coordinates of the ground resolution cell with GPS technology and comparing those with the coordinates of the \"pixel\" being studied provided by the remote sensing software to understand and analyze the location errors and how it may affect a particular study. Ground truth is important in the initial supervised classification of an image. When the identity and location of land cover types are known through a combination of field work, maps, and personal experience these areas are known as training sites. The spectral characteristics of these areas are used to train the remote sensing software using decision rules for classifying the rest of the image. These decision rules such as Maximum Likelihood Classification, Parallelopiped Classification, and Minimum Distance Classification offer different techniques to classify an image. Additional ground truth sites allow the remote sensor to establish an error matrix that validates the accuracy of the classification method used. Different classification methods may have different percentages of error for a given classification project. It is important that the remote sensor chooses a classification method that works best with the number of classifications used while providing the least amount of error. Ground truth also helps with atmospheric correction . Since images from satellites have to pass through the atmosphere, they can get distorted because of absorption in the atmosphere. So ground truth can help fully identify objects in satellite photos. An example of an error of commission is when a pixel reports the presence of a feature (such a tree) that, in reality, is absent (no tree is actually present). Ground truthing ensures that the error matrices have a higher accuracy percentage than would be the case if no pixels were ground-truthed. This value is the inverse of the user's accuracy, i.e. Commission Error = 1 - user's accuracy. An example of an error of omission is when pixels of a certain type, for example, maple trees, are not classified as maple trees. The process of ground-truthing helps to ensure that the pixel is classified correctly and the error matrices are more accurate. This value is the inverse of the producer's accuracy, i.e. Omission Error = 1 - producer's accuracy In GIS the spatial data is modeled as field (like in remote sensing raster images ) or as object (like in vectorial map representation). [ 9 ] They are modeled from the real world (also named geographical reality ), typically by a cartographic process (illustrated). Geographic information systems such as GIS, GPS, and GNSS, have become so widespread that the term \"ground truth\" has taken on special meaning in that context. If the location coordinates returned by a location method such as GPS are an estimate of a location, then the \"ground truth\" is the actual location on Earth. A smart phone might return a set of estimated location coordinates such as 43.87870, −103.45901. The ground truth being estimated by those coordinates is the tip of George Washington's nose on Mount Rushmore . The accuracy of the estimate is the maximum distance between the location coordinates and the ground truth. We could say in this case that the estimate accuracy is 10 meters, meaning that the point on Earth represented by the location coordinates is thought to be within 10 meters of George's nose—the ground truth. In slang, the coordinates indicate where we think George Washington's nose is located, and the ground truth is where it really is. In practice a smart phone or hand-held GPS unit is routinely able to estimate the ground truth within 6–10 meters. Specialized instruments can reduce GPS measurement error to under a centimeter. [ 10 ] US military slang uses \"ground truth\" to refer to the facts comprising a tactical situation—as opposed to intelligence reports, mission plans, and other descriptions reflecting the conative or policy-based projections of the industrial·military complex. The term appears in the title of the Iraq War documentary film The Ground Truth (2006), and also in military publications, for example Stars and Stripes saying: \"Stripes decided to figure out what the ground truth was in Iraq.\" [ citation needed ]",
    "links": [
      "Satellite imagery",
      "Mount Rushmore",
      "Calibration",
      "Baseline (medicine)",
      "Aerial photography",
      "Supervised learning",
      "Iraq War",
      "Compound verb",
      "Doi (identifier)",
      "Machine learning",
      "Bibcode (identifier)",
      "Statistical model",
      "Bayesian spam filtering",
      "Meteorology",
      "Remote sensing",
      "Cartography",
      "Judgment",
      "Military slang",
      "NASA",
      "Pixel",
      "Satellite",
      "Foundationalism",
      "Geographic information system",
      "Relevance feedback",
      "Compound adjective",
      "Empirical evidence",
      "The Ground Truth",
      "Computer stereo vision",
      "Vector Map",
      "Stars and Stripes (newspaper)",
      "Statistical models",
      "Research",
      "Hypothesis",
      "Labeled data",
      "Inference",
      "Oxford English Dictionary",
      "Atmospheric correction"
    ]
  },
  "Apache Lucene": {
    "url": "https://en.wikipedia.org/wiki/Apache_Lucene",
    "title": "Apache Lucene",
    "content": "Apache Lucene is a free and open-source search engine software library , originally written in Java by Doug Cutting . It is supported by the Apache Software Foundation and is released under the Apache Software License . Lucene is widely used as a standard foundation for production search applications. [ 2 ] [ 3 ] [ 4 ] Lucene has been ported to other programming languages including Object Pascal , Perl , C# , C++ , Python , Ruby and PHP . [ 5 ] Doug Cutting originally wrote Lucene in 1999. [ 6 ] Lucene was his fifth search engine. He had previously written two while at Xerox PARC , one at Apple , and a fourth at Excite . [ 7 ] It was initially available for download from its home at the SourceForge web site. It joined the Apache Software Foundation's Jakarta family of open-source Java products in September 2001 and became its own top-level Apache project in February 2005. The name Lucene is Doug Cutting's wife's middle name and her maternal grandmother's first name. [ 8 ] Lucene formerly included a number of sub-projects, such as Lucene.NET, Mahout , Tika and Nutch . These three are now independent top-level projects. In March 2010, the Apache Solr search server joined as a Lucene sub-project, merging the developer communities. Version 4.0 was released on October 12, 2012. [ 9 ] In March 2021, Lucene changed its logo, and Apache Solr became a top level Apache project again, independent from Lucene. While suitable for any application that requires full text indexing and searching capability, Lucene is recognized for its utility in the implementation of Internet search engines and local, single-site searching. [ 10 ] [ 11 ] Lucene includes a feature to perform a fuzzy search based on edit distance . [ 12 ] Lucene has also been used to implement recommendation systems. [ 13 ] For example, Lucene's 'MoreLikeThis' Class can generate recommendations for similar documents. In a comparison of the term vector-based similarity approach of 'MoreLikeThis' with citation-based document similarity measures, such as co-citation and co-citation proximity analysis, Lucene's approach excelled at recommending documents with very similar structural characteristics and more narrow relatedness. [ 14 ] In contrast, citation-based document similarity measures tended to be more suitable for recommending more broadly related documents, [ 14 ] meaning citation-based approaches may be more suitable for generating serendipitous recommendations, as long as documents to be recommended contain in-text citations. Lucene itself is just an indexing and search library and does not contain crawling and HTML parsing functionality. However, several projects extend Lucene's capability:",
    "links": [
      "Socialtext",
      "The Apache Software Foundation",
      "Apache Flume",
      "Apache Cocoon",
      "Apache HBase",
      "Software release life cycle",
      "Volker Markl",
      "Apache Tika",
      "Doi (identifier)",
      "AxKit",
      "CrateDB",
      "Apache Beam",
      "Apache MINA",
      "Apache Ignite",
      "Apache Ivy",
      "S2CID (identifier)",
      "Apache Portable Runtime",
      "Apache Superset",
      "Deltacloud",
      "Apache Allura",
      "Apache Software Foundation",
      "Web crawling",
      "Apache CouchDB",
      "Apache Apex",
      "Apache ActiveMQ",
      "Python (programming language)",
      "Apache Felix",
      "Information retrieval",
      "Apache CloudStack",
      "Apache Groovy",
      "Apache Spark",
      "Apache OFBiz",
      "Apache Axis",
      "Toxin and Toxin-Target Database",
      "Object Pascal",
      "Apache Commons Logging",
      "Apache Helix",
      "Nutch",
      "C (programming language)",
      "Apache Marmotta",
      "Apache Wave",
      "Enterprise search",
      "Apache MXNet",
      "Apache Directory",
      "Search engine indexing",
      "Apache Batik",
      "Apache Stanbol",
      "Free and open-source software",
      "Apache Mynewt",
      "Apache RocketMQ",
      "Apache Shale",
      "Apache Empire-db",
      "Apache FOP (Formatting Objects Processor)",
      "Apache Struts 1",
      "Swiftype",
      "SourceForge",
      "Apache cTAKES",
      "Apache Aries",
      "Apache Roller",
      "Apache Tomcat",
      "MongoDB",
      "Apache Software License",
      "PHP",
      "Apache OpenOffice",
      "Apache Parquet",
      "Doug Cutting",
      "Apache Drill",
      "Apache JMeter",
      "Apache Tapestry",
      "Apache Accumulo",
      "Apache Solr",
      "Repository (version control)",
      "Apache Nutch",
      "Porting",
      "Apache ZooKeeper",
      "Excite (web portal)",
      "Operating system",
      "Levenshtein distance",
      "Apress",
      "Apache Airflow",
      "Apache Ant",
      "Apache Iceberg",
      "MojoMojo",
      "Information extraction",
      "Apache Avro",
      "UIMA",
      "Bean Scripting Framework",
      "Multiplatform",
      "Human Metabolome Database",
      "Apache Tuscany",
      "PMID (identifier)",
      "Apple Inc.",
      "Apache CXF",
      "Apache HTTP Server",
      "Apache SystemDS",
      "Library (computing)",
      "Apache Oozie",
      "Apache Kudu",
      "Apache XML",
      "Commons Daemon",
      "Web spider",
      "Apache Taverna",
      "Sqoop",
      "Apache MyFaces",
      "Apache Pivot",
      "Index (search engine)",
      "Apache OpenNLP",
      "Apache Qpid",
      "Apache Beehive",
      "Apache PDFBox",
      "Manning Publications",
      "Log4j",
      "Apache CarbonData",
      "Apache Guacamole",
      "Java (programming language)",
      "Elasticsearch",
      "Apache Cassandra",
      "Apache SINGA",
      "Apache Hadoop",
      "Apache Ambari",
      "Nucleic Acids Res.",
      "Apache Jena",
      "Apache Maven",
      "Apache OpenJPA",
      "Apache Derby",
      "Apache Brooklyn",
      "Apache Subversion",
      "Apache Commons",
      "Apache Calcite",
      "Apache Impala",
      "Search algorithm",
      "Apache iBATIS",
      "Apache Xerces",
      "Apache Thrift",
      "NetBeans",
      "Apache Camel",
      "Apache SpamAssassin",
      "Apache Samza",
      "Apache Phoenix",
      "Parsers",
      "Apache Cordova",
      "Apache Jelly",
      "Co-citation",
      "OpenSearch (software)",
      "Text mining",
      "Search engine (computing)",
      "Xerox PARC",
      "Apache Kylin",
      "Apache Flex",
      "DocFetcher",
      "Apache Xalan",
      "Apache Airavata",
      "Ruby (programming language)",
      "Apache License 2.0",
      "Apache Hama",
      "Apache Sling",
      "Programmer",
      "Jakarta Project",
      "Apache Shiro",
      "Software license",
      "Apache ORC",
      "Apache OpenEJB",
      "NuttX",
      "Apache Struts",
      "Apache Arrow",
      "Apache Flink",
      "Apache Pinot",
      "ISSN (identifier)",
      "Apache Giraph",
      "Apache Pig",
      "Apache Harmony",
      "Apache Storm",
      "Apache Continuum",
      "Apache TinkerPop",
      "Apache Geronimo",
      "Internet search engine",
      "David S. Wishart",
      "Apache Trafodion",
      "C++",
      "ISBN (identifier)",
      "Apache Velocity",
      "Etch (protocol)",
      "Cross-platform",
      "Apache Cayenne",
      "Apache Hive",
      "Apache James",
      "Apache Wicket",
      "Apache Click",
      "Apache XMLBeans",
      "Apache ODE",
      "Mod perl",
      "Jakarta Slide",
      "Apache Mahout",
      "Apache NiFi",
      "PMC (identifier)",
      "C Sharp (programming language)",
      "Apache Kafka",
      "Apache Bloodhound",
      "Apache Jackrabbit",
      "Apache POI",
      "Jini",
      "Apache Traffic Server",
      "FreeMarker",
      "Apache Axis2",
      "Perl",
      "Apache Druid",
      "Apache License"
    ]
  },
  "Video retrieval": {
    "url": "https://en.wikipedia.org/wiki/Video_retrieval",
    "title": "Video retrieval",
    "content": "A video search engine is a web-based search engine which crawls the web for video content. Some video search engines parse externally hosted content while others allow content to be uploaded and hosted on their own servers. Some engines also allow users to search by video format type and by length of the clip. The video search results are usually accompanied by a thumbnail view of the video. Video search engines are computer programs designed to find videos stored on digital devices, either through Internet servers or in storage units from the same computer. These searches can be made through audiovisual indexing , which can extract information from audiovisual material and record it as metadata, which will be tracked by search engines. The main use of these search engines is the increasing creation of audiovisual content and the need to manage it properly. The digitization of audiovisual archives and the establishment of the Internet, has led to large quantities of video files stored in big databases, whose recovery can be very difficult because of the huge volumes of data and the existence of a semantic gap. The search criterion used by each search engine depends on its nature and purpose of the searches. Metadata is information about facts. It could be information about who is the author of the video, creation date, duration, and all the information that could be extracted and included in the same files. Internet is often used in a language called XML to encode metadata, which works very well through the web and is readable by people. Thus, through this information contained in these files is the easiest way to find data of interest to us. In the videos there are two types of metadata, that we can integrate in the video code itself and external metadata from the page where the video is. In both cases we optimize them to make them ideal when indexed. All video formats incorporate their own metadata. The title, description, coding quality or transcription of the content are possible. To review these data exist programs like FLV MetaData Injector, Sorenson Squeeze or Castfire. Each one has some utilities and special specifications. Converting from one format to another can lose much of this data, so check that the new format information is correct. It is therefore advisable to have the video in multiple formats, so all search robots will be able to find and index it. In most cases the same mechanisms must be applied as in the positioning of an image or text content. They are the most important factors when positioning a video, because they contain most of the necessary information. The titles have to be clearly descriptive and should remove every word or phrase that is not useful. It should be descriptive, including keywords that describe the video with no need to see their title or description. Ideally, separate the words by dashes \"-\". On the page where the video is, it should be a list of keywords linked to the microformat \"rel-tag\". These words will be used by search engines as a basis for organizing information. Although not completely standard, there are two formats that store information in a temporal component that is specified, one for subtitles and another for transcripts, which can also be used for subtitles. The formats are SRT or SUB for subtitles and TTXT for transcripts. Speech recognition consists of a transcript of the speech of the audio track of the videos, creating a text file. In this way and with the help of a phrase extractor can easily search if the video content is of interest. Some search engines apart from using speech recognition to search for videos, also use it to find the specific point of a multimedia file in which a specific word or phrase is located and so go directly to this point. Gaudi (Google Audio Indexing), a project developed by Google Labs , uses voice recognition technology to locate the exact moment that one or more words have been spoken within an audio, allowing the user to go directly to exact moment that the words were spoken. If the search query matches some videos from YouTube, the positions are indicated by yellow markers, and must pass the mouse over to read the transcribed text. In addition to transcription, analysis can detect different speakers and sometime attribute the speech to an identified name for the speaker. The text recognition can be very useful to recognize characters in the videos through \"chyrons\". As with speech recognizers, there are search engines that allow (through character recognition) to play a video from a particular point. TalkMiner, an example of search of specific fragments from videos by text recognition, analyzes each video once per second looking for identifier signs of a slide, such as its shape and static nature, captures the image of the slide and uses Optical Character Recognition (OCR) to detect the words on the slides. Then, these words are indexed in the search engine of TalkMiner, which currently offers to users more than 20,000 videos from institutions such as Stanford University, the University of California at Berkeley, and TED. Through the visual descriptors we can analyze the frames of a video and extract information that can be scored as metadata. Descriptions are generated automatically and can describe different aspects of the frames, such as color, texture, shape, motion, and the situation. The video analysis can lead to automatic chaptering, using technics such as change of camera angle, identification of audio jingles. By knowing the typical structure of a video document, it is possible to identify starting and ending credits, content parts and beginning and ending of advertising breaks. The usefulness of a search engine depends on the relevance of the result set returned. While there may be millions of videos that include a particular word or phrase, some videos may be more relevant, popular or have more authority than others. This arrangement has a lot to do with search engine optimization. Most search engines use different methods to classify the results and provide the best video in the first results. However, most programs allow sorting the results by several criteria. This criterion is more ambiguous and less objective, but sometimes it is the closest to what we want; depends entirely on the searcher and the algorithm that the owner has chosen. That's why it has always been discussed and now that search results are so ingrained into our society it has been discussed even more. This type of management often depends on the number of times that the searched word comes out, the number of viewings of this, the number of pages that link to this content and ratings given by users who have seen it. [ 1 ] This is a criterion based totally on timeline. Results can be sorted according to their seniority in the repository. It can give us an idea of the popularity of each video. This is the length of the video and can give a taste of which video it is. It is common practice in repositories let the users rate the videos, so that a content of quality and relevance will have a high rank on the list of results gaining visibility. This practice is closely related to virtual communities. We can distinguish two basic types of interfaces, some are web pages hosted on servers which are accessed by Internet and searched through the network, and the others are computer programs that search within a private network. Within Internet interfaces we can find repositories that host video files which incorporate a search engine that searches only their own databases, and video searchers without repository that search in sources of external software. Provides accommodation in video files stored on its servers and usually has an integrated search engine that searches through videos uploaded by its users. One of the first web repositories, or at least the most famous are the portals Vimeo, Dailymotion and YouTube. Their searches are often based on reading the metadata tags, titles and descriptions that users assign to their videos. The disposal and order criterion of the results of these searches are usually selectable between the file upload date, the number of viewings or what they call the relevance. Still, sorting criteria are nowadays the main weapon of these websites, because the positioning of videos is important in terms of promotion. [ citation needed ] They are websites specialized in searching videos across the network or certain pre-selected repositories. They work by web spiders that inspect the network in an automated way to create copies of the visited websites, which will then be indexed by search engines, so they can provide faster searches. Sometimes a search engine only searches in audiovisual files stored within a computer or, as it happens in televisions, on a private server where users access through a local area network. These searchers are usually software or rich Internet applications with a very specific search options for maximum speed and efficiency when presenting the results. They are typically used for large databases and are therefore highly focused to satisfy the needs of television companies. An example of this type of software would be the Digition Suite, which apart from being a benchmark in this kind of interfaces is very close to us as for the storage and retrieval files system from the Corporació Catalana de Mitjans Audiovisuals . [ 2 ] This particular suite and perhaps in its strongest point is that it integrates the entire process of creating, indexing, storing, searching, editing, and a recovery. Once we have a digitized audiovisual content is indexed with different techniques of different level depending on the importance of content and it's stored. The user, when he wants to retrieve a particular file, has to fill a search fields such as program title, issue date, characters who act or the name of the producer, and the robot starts the search. Once the results appear and they arranged according to preferences, the user can play the low quality videos to work as quickly as possible. When he finds the desired content, it is downloaded with good definition, it's edited and reproduced. [ 3 ] Video search has evolved slowly through several basic search formats which exist today and all use keywords . The keywords for each search can be found in the title of the media, any text attached to the media and content linked web pages, also defined by authors and users of video hosted resources. Some video search is performed using human powered search, others create technological systems that work automatically to detect what is in the video and match the searchers needs. Many efforts to improve video search including both human powered search as well as writing algorithm that recognize what's inside the video have meant complete redevelopment of search efforts. It is generally acknowledged that speech to text is possible, though recently Thomas Wilde, the new CEO of Everyzing, acknowledged that Everyzing works 70% of the time when there is music, ambient noise or more than one person speaking. If newscast style speaking (one person, speaking clearly, no ambient noise) is available, that can rise to 93%. (From the Web Video Summit, San Jose, CA, June 27, 2007). Around 40 phonemes exist in every language with about 400 in all spoken languages. Rather than applying a text search algorithm after speech-to-text processing is completed, some engines use a phonetic search algorithm to find results within the spoken word. Others work by literally listening to the entire podcast and creating a text transcription using a sophisticated speech-to-text process. Once the text file is created, the file can be searched for any number of search words and phrases. It is generally acknowledged that visual search into video does not work well and that no company is using it publicly. Researchers at UC San Diego and Carnegie Mellon University have been working on the visual search problem for more than 15 years, and admitted at a \"Future of Search\" conference at UC Berkeley in spring 2007 that it was years away from being viable even in simple search. Search that is not affected by the hosting of video, where results are agnostic no matter where the video is located: Search results are modified, or suspect, due to the large hosted video being given preferential treatment in search results: Despite technological advances, video search faces several significant challenges:",
    "links": [
      "Audiovisual archives",
      "Phonemes",
      "Audio search engine",
      "Draper Fisher Jurvetson",
      "Wide area information server",
      "Tencent",
      "Metasearch engine",
      "Document retrieval",
      "Search by sound",
      "Munax",
      "Search engine marketing",
      "Google Videos",
      "Truveo",
      "Distributed web crawling",
      "Optical character recognition",
      "Focused crawler",
      "Marc Andreessen",
      "Content ID",
      "Federated search",
      "Enterprise search",
      "Online search",
      "Search engine indexing",
      "Social search",
      "Search/Retrieve via URL",
      "List of search engines",
      "Yahoo!",
      "Multisearch",
      "Representational State Transfer",
      "Selection-based search",
      "Vertical search",
      "Content-based image retrieval",
      "Video browsing",
      "Web query classification",
      "Natural language search engine",
      "Video",
      "Cross-language search",
      "Multimedia search",
      "Voice search",
      "Google Labs",
      "Desktop search",
      "Web crawler",
      "Spider trap",
      "Website mirroring software",
      "Index (search engine)",
      "Relevance (information retrieval)",
      "Blinkx",
      "OpenSearch (specification)",
      "Picsearch",
      "Tencent Video",
      "Internet search",
      "Robots exclusion standard",
      "Z39.50",
      "Google",
      "Corporació Catalana de Mitjans Audiovisuals",
      "Search aggregator",
      "Text mining",
      "Metadata",
      "Search engine (computing)",
      "Local search (Internet)",
      "AOL",
      "Search engine",
      "Wayback Machine",
      "Index term",
      "Semantic search",
      "Web indexing",
      "Ron Conway",
      "Speech recognition",
      "Collaborative search engine",
      "Bing (search engine)",
      "Image retrieval",
      "Web search engine",
      "Thumbnail",
      "CastTV",
      "Video content analysis",
      "Visual descriptors",
      "Web archiving",
      "Web query",
      "Search/Retrieve Web Service",
      "Cross-language information retrieval",
      "Evaluation measures (information retrieval)",
      "Search engine optimization"
    ]
  },
  "Latent semantic analysis": {
    "url": "https://en.wikipedia.org/wiki/Latent_semantic_analysis",
    "title": "Latent semantic analysis",
    "content": "Latent semantic analysis ( LSA ) is a technique in natural language processing , in particular distributional semantics , of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis ). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by cosine similarity between any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents. [ 1 ] An information retrieval technique using latent semantic structure was patented in 1988 [ 2 ] by Scott Deerwester , Susan Dumais , George Furnas , Richard Harshman , Thomas Landauer , Karen Lochbaum and Lynn Streeter . In the context of its application to information retrieval , it is sometimes called latent semantic indexing ( LSI ). [ 3 ] LSA can use a document-term matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance. This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used. After the construction of the occurrence matrix, LSA finds a low-rank approximation [ 5 ] to the term-document matrix . There could be various reasons for these approximations: The consequence of the rank lowering is that some dimensions are combined and depend on more than one term: This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also partially mitigates the problem with polysemy , since components of polysemous words that point in the \"right\" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense. Let X {\\displaystyle X} be a matrix where element ( i , j ) {\\displaystyle (i,j)} describes the occurrence of term i {\\displaystyle i} in document j {\\displaystyle j} (this can be, for example, the frequency). X {\\displaystyle X} will look like this: Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document: Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term: Now the dot product t i T t p {\\displaystyle {\\textbf {t}}_{i}^{T}{\\textbf {t}}_{p}} between two term vectors gives the correlation between the terms over the set of documents. The matrix product X X T {\\displaystyle XX^{T}} contains all these dot products. Element ( i , p ) {\\displaystyle (i,p)} (which is equal to element ( p , i ) {\\displaystyle (p,i)} ) contains the dot product t i T t p {\\displaystyle {\\textbf {t}}_{i}^{T}{\\textbf {t}}_{p}} ( = t p T t i {\\displaystyle ={\\textbf {t}}_{p}^{T}{\\textbf {t}}_{i}} ). Likewise, the matrix X T X {\\displaystyle X^{T}X} contains the dot products between all the document vectors, giving their correlation over the terms: d j T d q = d q T d j {\\displaystyle {\\textbf {d}}_{j}^{T}{\\textbf {d}}_{q}={\\textbf {d}}_{q}^{T}{\\textbf {d}}_{j}} . Now, from the theory of linear algebra, there exists a decomposition of X {\\displaystyle X} such that U {\\displaystyle U} and V {\\displaystyle V} are orthogonal matrices and Σ {\\displaystyle \\Sigma } is a diagonal matrix . This is called a singular value decomposition (SVD): The matrix products giving us the term and document correlations then become Since Σ Σ T {\\displaystyle \\Sigma \\Sigma ^{T}} and Σ T Σ {\\displaystyle \\Sigma ^{T}\\Sigma } are diagonal we see that U {\\displaystyle U} must contain the eigenvectors of X X T {\\displaystyle XX^{T}} , while V {\\displaystyle V} must be the eigenvectors of X T X {\\displaystyle X^{T}X} . Both products have the same non-zero eigenvalues, given by the non-zero entries of Σ Σ T {\\displaystyle \\Sigma \\Sigma ^{T}} , or equally, by the non-zero entries of Σ T Σ {\\displaystyle \\Sigma ^{T}\\Sigma } . Now the decomposition looks like this: The values σ 1 , … , σ l {\\displaystyle \\sigma _{1},\\dots ,\\sigma _{l}} are called the singular values, and u 1 , … , u l {\\displaystyle u_{1},\\dots ,u_{l}} and v 1 , … , v l {\\displaystyle v_{1},\\dots ,v_{l}} the left and right singular vectors. Notice the only part of U {\\displaystyle U} that contributes to t i {\\displaystyle {\\textbf {t}}_{i}} is the i 'th {\\displaystyle i{\\textrm {'th}}} row. Let this row vector be called t ^ i T {\\displaystyle {\\hat {\\textrm {t}}}_{i}^{T}} . Likewise, the only part of V T {\\displaystyle V^{T}} that contributes to d j {\\displaystyle {\\textbf {d}}_{j}} is the j 'th {\\displaystyle j{\\textrm {'th}}} column, d ^ j {\\displaystyle {\\hat {\\textrm {d}}}_{j}} . These are not the eigenvectors, but depend on all the eigenvectors. It turns out that when you select the k {\\displaystyle k} largest singular values, and their corresponding singular vectors from U {\\displaystyle U} and V {\\displaystyle V} , you get the rank k {\\displaystyle k} approximation to X {\\displaystyle X} with the smallest error ( Frobenius norm ). This approximation has a minimal error. But more importantly we can now treat the term and document vectors as a \"semantic space\". The row \"term\" vector t ^ i T {\\displaystyle {\\hat {\\textbf {t}}}_{i}^{T}} then has k {\\displaystyle k} entries mapping it to a lower-dimensional space. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the \"document\" vector d ^ j {\\displaystyle {\\hat {\\textbf {d}}}_{j}} is an approximation in this lower-dimensional space. We write this approximation as You can now do the following: To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents: Note here that the inverse of the diagonal matrix Σ k {\\displaystyle \\Sigma _{k}} may be found by inverting each nonzero value within the matrix. This means that if you have a query vector q {\\displaystyle q} , you must do the translation q ^ = Σ k − 1 U k T q {\\displaystyle {\\hat {\\textbf {q}}}=\\Sigma _{k}^{-1}U_{k}^{T}{\\textbf {q}}} before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors: The new low-dimensional space typically can be used to: Synonymy and polysemy are fundamental problems in natural language processing : LSA has been used to assist in performing prior art searches for patents . [ 9 ] The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words. These findings are referred to as the Semantic Proximity Effect . [ 10 ] When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list. These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall. [ 11 ] Another model, termed Word Association Spaces (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs. [ 12 ] The SVD is typically computed using large matrix methods (for example, Lanczos methods ) but may also be computed incrementally and with greatly reduced resources via a neural network -like approach, which does not require the large, full-rank matrix to be held in memory. [ 13 ] A fast, incremental, low-memory, large-matrix SVD algorithm has been developed. [ 14 ] MATLAB [ 15 ] and Python [ 16 ] implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution. In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality. [ 17 ] Some of LSA's drawbacks include: In semantic hashing [ 21 ] documents are mapped to memory addresses by means of a neural network in such a way that semantically similar documents are located at nearby addresses. Deep neural network essentially builds a graphical model of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing , which is the fastest current method. [ clarification needed ] Latent semantic indexing ( LSI ) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts . [ 22 ] LSI is also an application of correspondence analysis , a multivariate statistical technique developed by Jean-Paul Benzécri [ 23 ] in the early 1970s, to a contingency table built from word counts in documents. Called \" latent semantic indexing\" because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s. The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches. Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria. LSI helps overcome synonymy by increasing recall , one of the most problematic constraints of Boolean keyword queries and vector space models. [ 18 ] Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of information retrieval systems. [ 24 ] As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant. LSI is also used to perform automated document categorization . In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text. [ 25 ] Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories. [ 26 ] LSI uses example documents to establish the conceptual basis for each category. During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents. Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI. Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster. This is very useful when dealing with an unknown collection of unstructured text. Because it uses a strictly mathematical approach, LSI is inherently independent of language. This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri. LSI can also perform cross-linguistic concept searching and example-based categorization. For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages. [ citation needed ] LSI is not restricted to working only with words. It can also process arbitrary character strings. Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations. [ 27 ] LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.). [ 28 ] This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion. LSI also deals effectively with sparse, ambiguous, and contradictory data. Text does not need to be in sentence form for LSI to be effective. It can work with lists, free-form notes, email, Web-based content, etc. As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text. LSI has proven to be a useful solution to a number of conceptual matching problems. [ 29 ] [ 30 ] The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information. [ 31 ] LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text. In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text. LSI begins by constructing a term-document matrix, A {\\displaystyle A} , to identify the occurrences of the m {\\displaystyle m} unique terms within a collection of n {\\displaystyle n} documents. In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, a i j {\\displaystyle a_{ij}} , initially representing the number of times the associated term appears in the indicated document, t f i j {\\displaystyle \\mathrm {tf_{ij}} } . This matrix is usually very large and very sparse. Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data. The weighting functions transform each cell, a i j {\\displaystyle a_{ij}} of A {\\displaystyle A} , to be the product of a local term weight, l i j {\\displaystyle l_{ij}} , which describes the relative frequency of a term in a document, and a global weight, g i {\\displaystyle g_{i}} , which describes the relative frequency of the term within the entire collection of documents. Some common local weighting functions [ 33 ] are defined in the following table. Some common global weighting functions are defined in the following table. Empirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets. [ 34 ] In other words, each entry a i j {\\displaystyle a_{ij}} of A {\\displaystyle A} is computed as: A rank-reduced, singular value decomposition is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text. The SVD forms the foundation for LSI. [ 35 ] It computes the term and document vector spaces by approximating the single term-frequency matrix, A {\\displaystyle A} , into three other matrices— an m by r term-concept vector matrix T {\\displaystyle T} , an r by r singular values matrix S {\\displaystyle S} , and a n by r concept-document vector matrix, D {\\displaystyle D} , which satisfy the following relations: A ≈ T S D T {\\displaystyle A\\approx TSD^{T}} T T T = I r D T D = I r {\\displaystyle T^{T}T=I_{r}\\quad D^{T}D=I_{r}} S 1 , 1 ≥ S 2 , 2 ≥ … ≥ S r , r > 0 S i , j = 0 where i ≠ j {\\displaystyle S_{1,1}\\geq S_{2,2}\\geq \\ldots \\geq S_{r,r}>0\\quad S_{i,j}=0\\;{\\text{where}}\\;i\\neq j} In the formula, A is the supplied m by n weighted matrix of term frequencies in a collection of text where m is the number of unique terms, and n is the number of documents. T is a computed m by r matrix of term vectors where r is the rank of A —a measure of its unique dimensions ≤ min( m,n ) . S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors. The SVD is then truncated to reduce the rank by keeping only the largest k « r diagonal entries in the singular value matrix S , where k is typically on the order 100 to 300 dimensions. This effectively reduces the term and document vector matrix sizes to m by k and n by k respectively. The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of A . This reduced set of matrices is often denoted with a modified formula such as: Efficient LSI algorithms only compute the first k singular values and term and document vectors as opposed to computing a full SVD and then truncating it. Note that this rank reduction is essentially the same as doing Principal Component Analysis (PCA) on the matrix A , except that PCA subtracts off the means. PCA loses the sparseness of the A matrix, which can make it infeasible for large lexicons. The computed T k and D k matrices define the term and document vector spaces, which with the computed singular values, S k , embody the conceptual information derived from the document collection. The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors. The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index. By a simple transformation of the A = T S D T equation into the equivalent D = A T T S −1 equation, a new vector, d , for a query or for a new document can be created by computing a new column in A and then multiplying the new column by T S −1 . The new column in A is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document. A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored. These terms will have no impact on the global weights and learned correlations derived from the original collection of text. However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors. The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called folding in . Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added. When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in [ 14 ] ) is needed. It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems. As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome. LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization. [ 36 ] Below are some other ways in which LSI is being used: LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation. In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential. Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003. [ 51 ] Early challenges to LSI focused on scalability and performance. LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques. [ 52 ] However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome. Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source gensim software package. [ 53 ] Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD. As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts. The actual number of dimensions that can be used is limited by the number of documents in the collection. Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents). [ 54 ] However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection. [ 55 ] Checking the proportion of variance retained, similar to PCA or factor analysis , to determine the optimal dimensionality is not suitable for LSI. Using a synonym test or prediction of missing words are two possible methods to find the correct dimensionality. [ 56 ] When LSI topics are used as features in supervised learning methods, one can use prediction error measurements to find the ideal dimensionality. Due to its cross-domain applications in Information Retrieval , Natural Language Processing (NLP), Cognitive Science and Computational Linguistics , LSA has been implemented to support many different kinds of applications.",
    "links": [
      "Multiple choice question",
      "Concept searching",
      "Latent Dirichlet allocation",
      "Bellcore",
      "Stop word",
      "Doi (identifier)",
      "Terminology",
      "Automatic summarization",
      "Diagonal matrix",
      "Part-of-speech tagging",
      "Interactive fiction",
      "Patents",
      "Principal component analysis",
      "AI-complete",
      "Word-sense disambiguation",
      "S2CID (identifier)",
      "Coh-Metrix",
      "Higher-order statistics",
      "Richard Harshman",
      "Syntactic parsing (computational linguistics)",
      "Ergodic hypothesis",
      "Information Retrieval",
      "Topic model",
      "Sentence extraction",
      "Corpus linguistics",
      "Bank of English",
      "Optical character recognition",
      "Concept",
      "Susan Dumais",
      "Information retrieval",
      "Machine translation",
      "Hdl (identifier)",
      "Grammar checker",
      "Formal semantics (logic)",
      "Sentiment analysis",
      "Multinomial distribution",
      "Semantic analysis (machine learning)",
      "Low-rank approximation",
      "Semantics (computer science)",
      "Large language model",
      "Neural machine translation",
      "Semantics",
      "Polysemy",
      "Universal Dependencies",
      "Google Ngram Viewer",
      "Action semantics",
      "Concordancer",
      "Automated document classification",
      "Formal semantics (natural language)",
      "Semantic query",
      "Natural-language user interface",
      "Semantic Web",
      "Computer-assisted reviewing",
      "Free recall",
      "Cosine similarity",
      "Semantic analysis (linguistics)",
      "George Furnas",
      "Semantic matching",
      "Bag-of-words model",
      "Parsing",
      "Computational Linguistics",
      "Force dynamics",
      "Predictive text",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "Cognitive Science",
      "Named-entity recognition",
      "Parallel text",
      "Word embedding",
      "Argument mining",
      "SpaCy",
      "Spamdexing",
      "Ontology learning",
      "CiteSeerX (identifier)",
      "Concurrency semantics",
      "Dot product",
      "Data clustering",
      "DBpedia",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Word2vec",
      "Computational linguistics",
      "Thesaurus (information retrieval)",
      "Information extraction",
      "Collocation extraction",
      "Sparse matrix",
      "Pachinko allocation",
      "Natural Language Toolkit",
      "Scott Deerwester",
      "Jean-Paul Benzécri",
      "Formal semantics (linguistics)",
      "Lexical resource",
      "Word-sense induction",
      "Speech synthesis",
      "Lexicology",
      "PMID (identifier)",
      "Language model",
      "Wikidata",
      "Shallow parsing",
      "Prior art",
      "Synonymy",
      "Speech segmentation",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Document-term matrix",
      "Spam (electronic)",
      "Distributional hypothesis",
      "ArXiv (identifier)",
      "Lanczos method",
      "Small language model",
      "Speech corpus",
      "Natural language processing",
      "Pronunciation assessment",
      "Term-document matrix",
      "Hallucination (artificial intelligence)",
      "Rule-based machine translation",
      "Language",
      "Lucene",
      "Probabilistic model",
      "Stemming",
      "Document categorization",
      "Correlation",
      "Co-occurrence",
      "UBY",
      "Statistical semantics",
      "Theory of descriptions",
      "Singular value decomposition",
      "Factor analysis",
      "Compound-term processing",
      "Text simplification",
      "Contingency table",
      "Bigram",
      "Semantic parsing",
      "Orthogonal matrix",
      "Natural Language Processing",
      "Vector space model",
      "Lexical semantics",
      "Text segmentation",
      "Example-based machine translation",
      "Semantic file system",
      "Literature-based discovery",
      "Compound term processing",
      "Distant reading",
      "Multi-document summarization",
      "Lexis (linguistics)",
      "Tf-idf",
      "Semantic analysis (computational)",
      "Predicate transformer semantics",
      "Semantic feature",
      "Statistical machine translation",
      "Trigram",
      "Thomas Landauer",
      "Structural semantics",
      "Physicians",
      "Text corpus",
      "Correspondence analysis",
      "Natural language understanding",
      "FastText",
      "BabelNet",
      "WordNet",
      "Graphical model",
      "Algebraic semantics (computer science)",
      "Bag of words model",
      "Eigenvector",
      "Treebank",
      "BERT (language model)",
      "Matrix product",
      "Locality sensitive hashing",
      "Semantic decomposition (natural language processing)",
      "Neural network",
      "Semantic similarity",
      "Principal components analysis",
      "Spell checker",
      "Operational semantics",
      "Abstract semantic graph",
      "Automatic identification and data capture",
      "Prototype theory",
      "Explicit semantic analysis",
      "Lexical analysis",
      "Text mining",
      "Concept mining",
      "Language resource",
      "Axiomatic semantics",
      "Denotational semantics",
      "Semantic network",
      "Linguistics",
      "Linguistic Linked Open Data",
      "Word sense disambiguation",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Transfer-based machine translation",
      "Deep learning",
      "Semantic gap",
      "Word vector",
      "Wayback Machine",
      "Computational semantics",
      "Textual entailment",
      "Archive.today",
      "Latent semantic mapping",
      "Speech recognition",
      "Terminology extraction",
      "Context (language use)",
      "GloVe",
      "Lemmatisation",
      "Semantic wiki",
      "Poisson distribution",
      "Singular Value Decomposition",
      "Sara Solla",
      "Science Applications International Corporation",
      "Text processing",
      "Probabilistic latent semantic analysis",
      "Transformer (deep learning architecture)",
      "Normal distribution",
      "ISBN (identifier)",
      "Question answering",
      "Frobenius norm",
      "Categorical logic",
      "Abstract interpretation",
      "Chatbot",
      "Document classification",
      "Boolean search",
      "Automated essay scoring",
      "PMC (identifier)",
      "Latent semantic structure indexing",
      "Gensim",
      "Electronic Discovery",
      "Truecasing",
      "Long short-term memory",
      "FrameNet",
      "Cross-language information retrieval",
      "Principal Component Analysis",
      "Semantic desktop",
      "Machine-readable dictionary",
      "Context",
      "Compositionality",
      "Game semantics"
    ]
  },
  "As We May Think": {
    "url": "https://en.wikipedia.org/wiki/As_We_May_Think",
    "title": "As We May Think",
    "content": "\" As We May Think \" is a 1945 essay by Vannevar Bush which has been described as visionary and influential, anticipating many aspects of information society . It was first published in The Atlantic in July 1945 and republished in an abridged version in September 1945—before and after the atomic bombings of Hiroshima and Nagasaki . Bush expresses his concern for the direction of scientific efforts toward destruction, rather than understanding, and explicates a desire for a sort of collective memory machine with his concept of the memex that would make knowledge more accessible, believing that it would help fix these problems. Through this machine, Bush hoped to transform an information explosion into a knowledge explosion. [ 1 ] The article was a reworked and expanded version of Bush's essay \"Mechanization and the Record\" (1939). Here, he described a machine that would combine lower level technologies to achieve a higher level of organized knowledge (like human memory processes). Shortly after the publication of this essay, Bush coined the term \" memex \" in a letter written to the editor of Fortune magazine. [ 2 ] That letter became the body of \"As We May Think\", which added only an introduction and conclusion. As described, Bush's memex was based on what was thought, at the time, to be advanced technology of the future: ultra high resolution microfilm reels, coupled to multiple screen viewers and cameras, by electromechanical controls. The memex, in essence, reflects a library of collective knowledge stored in a piece of machinery described in his essay as \"a piece of furniture\". [ 3 ] The Atlantic publication of Bush's article was followed by an abridged version in the September 10, 1945, issue of Life magazine, accompanied by fanciful illustrations of the proposed memex desk and other devices Bush projected. Bush also discussed other technologies such as dry photography and microphotography where he elaborates on the potentialities of their future use. For example, Bush states in his essay that: The combination of optical projection and photographic reduction is already producing some results in microfilm for scholarly purposes, and the potentialities are highly suggestive. — Vannevar Bush [ 3 ] \"As We May Think\" predicted (to some extent) many kinds of technology invented after its publication, including hypertext , personal computers , the Internet , the World Wide Web , speech recognition , and online encyclopedias such as Wikipedia : \"Wholly new forms of encyclopedias will appear, ready-made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified.\" [ 3 ] Bush envisioned the ability to retrieve several articles or pictures on one screen, with the possibility of writing comments that could be stored and recalled together. He believed people would create links between related articles, thus mapping the thought process and path of each user and saving it for others to experience. Bush's article also laid the foundation for new media. Doug Engelbart came across the essay shortly after its publication, and keeping the memex in mind, he began work that would eventually result in the invention of the mouse , the word processor , the hyperlink and concepts of new media for which these groundbreaking inventions were merely enabling technologies. [ 1 ] Since then, storage has greatly surpassed the level imagined by Vannevar Bush, The Encyclopædia Britannica could be reduced to the volume of a matchbox. A library of a million volumes could be compressed into one end of a desk. — Vannevar Bush [ 3 ] On the other hand, it still uses methods of indexing of information which Bush described as artificial: When data of any sort are placed in storage, they are filed alphabetically or numerically, and information is found (when it is) by tracing it down from subclass to subclass. It can be in only one place, unless duplicates are used. — Vannevar Bush [ 3 ] This description resembles popular file systems of modern computer operating systems ( FAT , NTFS , ext3 when used without hard links and symlinks, etc.), which do not easily enable associative indexing as imagined by Bush. Bush urges that scientists should turn to the massive task of creating more efficient accessibility to our fluctuating store of knowledge . For years inventions have extended people's physical powers rather than the powers of their mind. He argues that the instruments are at hand which, if properly developed, will give society access to and command over the inherited knowledge of the ages. The perfection of these pacific instruments, he suggests, should be the first objective of our scientists. [ 3 ] Through this process, society would be able to focus and evolve past the existing knowledge rather than looping through infinite calculations. We should be able to pass the tedious work of numbers to machines and work on the intricate theory which puts them best to use. If humanity were able to obtain the \"privilege of forgetting the manifold things he does not need to have immediately at hand, with some assurance that he can find them again if proven important\" only then \"will mathematics be practically effective in bringing the growing knowledge of atomistic to the useful solution of the advanced problems of chemistry, metallurgy, and biology\". [ 1 ] To exemplify the importance of this concept, consider the process involved in 'simple' shopping: \"Every time a charge sale is made, there are a number of things to be done. The inventory needs to be revised, the salesman needs to be given credit for the sale, the general accounts need an entry, and most important, the customer needs to be charged.\" [ 1 ] Due to the convenience of the store's central device, which rapidly manages thousands of these transactions, the employees may focus on the essential aspects of the department such as sales and advertising. Indeed, \"science has provided the swiftest communication between individuals; it has provided a record of ideas and has enabled man to manipulate and to make extracts from that record so that knowledge evolves and endures throughout the life of a race rather than of an individual\". [ 1 ] Improved technology has become an extension of our capabilities, much as how external hard drives function for computers so they may reserve memory for more practical tasks. Another significant role of practicality in technology is the method of association and selection. \"There may be millions of fine thoughts, and the account of the experience on which they are based, all encased within stone walls of acceptable architectural form; but if the scholar can get at only one a week by diligent search, his synthesis are not likely to keep up with the current scene.\" [ 1 ] Bush believes that the tools available in his time lacked this feature, but noted the emergence and development of ideas such as the Memex, a cross referencing system. Bush concludes his essay by stating that: The applications of science have built man a well-supplied house, and are teaching him to live healthily therein. They have enabled him to throw masses of people against one another with cruel weapons. They may yet allow him truly to encompass the great record and to grow in the wisdom of race experience. He may perish in conflict before he learns to wield that record for his true good. Yet, in the application of science to the needs and desires of man, it would seem to be a singularly unfortunate stage at which to terminate the process, or to lose hope as to the outcome. — Vannevar Bush [ 3 ] Editor's note: Technologies like trip hammers exist that can do physical labor better and faster. Soon, technologies will exist that can help people do intellectual labor better and faster. Introduction: Many scientists, especially physicists, obtained new duties during World War II . Now, after the war, they need new peaceful duties. Section 1: Scientific knowledge has grown considerably, but the way we manage knowledge has remained the same for centuries. We are no longer able to keep up and find relevant information in the flood of information. Leibniz 's computer and Charles Babbage 's computer were both failures because technologies of their times could not produce them cheaply and precisely, but now we have enough technology. Section 2: Science should not only be a vast store of knowledge, but also be frequently consulted and enhanced. Two kinds of technologies can help: analog information on microfilms, and digital information encoded by electric signals. While they are different, both kinds would be vastly cheaper than traditional printed media. With instant photography and microfilm , it will be cheap to copy and transmit analog information. Microfilm could shrink books and other paper-publications by a linear factor of 100x, or an area factor of 10000x. A library of 1 million books would occupy the volume of 100 books, which can fit on a bookshelf. All the world's books can fit inside a moving van . [ note 1 ] [ note 2 ] Production and transmission would cost pennies. [ note 3 ] A possible future device would be a walnut-sized camera strapped to the head of the wearer that can take a photo at the squeeze of a hand, and develop it. The photos can be taken out at the end of a day for further processing. (Illustrated in the header image.) Bush goes into some technical details about instant photography and electric fax machines. In his days, wet photography was the most common, yet it takes a long time and is hard to shrink into a small camera. However, whiteprint technology [ note 4 ] might be miniaturized, leading to miniature dry photography. [ note 5 ] Printed material could be transmitted cheaply by digital signals, as demonstrated by electric fax machines . The sending side uses photocells to convert images to electric signals, and on the receiving side, electric printers convert the electric signal into electric sparks hitting iodine-impregnated paper, turning it black. [ note 6 ] Section 3: Not only will it be cheap to transmit and copy digital material, it will also be cheap to convert printed material into digital form. Language is interconvertible with digital signals, as shown by three technologies: While currently Vocoders need human operators, a future Vocoder could transcribe speech automatically . A future researcher could walk around, take photos with the head-mounted camera, and record sound and speech. The photos and the sounds would have timing information. At the end of the day, this timed record of the day can be processed and reviewed. To study cosmic rays , physicists built vacuum tubes [ note 7 ] that could count at 0.1 MHz. Future electronic computers could operate at least 100 times faster, at 10 MHz. Herman Hollerith 's tabulating machine showed that simple machines programmed by punched cards could be commercially valuable. Future computers could perform complex programs according to punched cards or microfilms. Section 4: Most of the existing computing machines are tabulating machines , arithmetic machines . Some are more advanced, like tide-predicting machines , and machines for solving differential and integration equations . Future scientists will delegate even more advanced routine mathematics to machines, just as one would delegate the operation of a car to its engine. By delegating away more routines, scientists can perform creative, intuitive work. [ note 8 ] Section 5: Scientists and other knowledge workers manipulate data and perform logical inferences. Any routine logical process that a worker performs repeatedly could be programmed into a machine. Normal or even mathematical language is too vague for programming. A \"positional\" [ note 9 ] logical language would be needed for entering information the machines. Not only will they be for entering information, machines will also help people find information. For example, punched card sorters and telephone exchanges are both search machines: the sorter can quickly produce a stack of cards listing, for example, all employees who live in Trenton, New Jersey and know the Spanish language , and a telephone exchange can quickly connect to the line specified by a number sequence. Bush proceeds to describe in detail a management system for a department store , where a salesperson enters customer and product information, which a central machine uses to update inventory, credit sales, adjust accounts, and charge customers, using analog devices such as punched cards, dry photography, microfilms, Valdemar Poulsen 's magnetic wire recorder , and so on. Section 6: Traditional information systems, such as the library classification system, are tree-like. At the top are the biggest classes, and each class can have subclasses, and so on. Each item belongs uniquely to a leaf on the tree of information. This is cumbersome, and the human mind does not operate that way, but operates by association . In human thinking, one traces out a \"trail\" of information. This process can be augmented by the memex . Like human memory, it retrieves information by association, not by going down a tree of classification. The memex is a machine for individual use, where they could store all their books, records, and communications. The memex looks like a desk. It contains a storage unit for microfilms, sufficient for an individual's lifetime. [ note 10 ] Microfilms can be bought like books and magazines. Letters, documents, and hand-drawn manuscripts can be placed on a transparent plate that is then photographed and converted to microfilm. One can also manually type onto them with a keyboard. Typing on the keyboard, the user can find any microfilm by associative search. Pushing on levers allows users to flip through a microfilmed book, moving forward or backward at variable speeds. The user can open up several microfilms at once, then draw lines and commentaries between them using dry photography or by a telautograph -like pen. Section 7: The essence of memex is associative indexing : the user can make any item associate with any other, so that pulling up the first item automatically pulls up the second. Associations can be chained, building a \"trail\". A trail can be named and later retrieved by typing on the keyboard. Any item can be a part of many trails. Associative indexing can be implemented by coded dots printed on the bottoms of microfilms, and an optical reader can read the printed code and electrically signal the memex to pull up the next item. [ note 11 ] Bush describes a use scenario , where the user is studying why the short Turkish bow was apparently superior to the English longbow in the Crusades . He searches through encyclopedias and textbooks, building a trail of connections. He also branches off another trail through textbooks and handbooks on elasticity. Later, in conversation with a friend about how people resist innovation, he brings up the trail again, and then copies the whole trail out to be installed into his friend's memex. There, the trail is joined into a more general trail about how people resist innovation. Section 8: Bush envisions a future where memex machines are everywhere. There will be microfilmed encyclopedias with trails already installed. Lawyers, patent attorneys, and other knowledge workers will use the memex to store their associative trails accumulated over their professional life. There will be a new kind of job: \"trail blazers\", who find new and useful trails. Bush expect future technology to be superior than those described in the essay, but he keeps to only known technologies, instead of the possible unknown, [ note 12 ] to keep the idea of memex practical. More speculatively, since the human nervous system is electrical, future human-machine interfaces could be purely electrical . \"As We May Think\" has turned out to be a visionary and influential essay. In their introduction to a paper discussing information literacy as a discipline, Johnston and Webber write Bush's paper might be regarded as describing a microcosm of the information society, with the boundaries tightly drawn by the interests and experiences of a major scientist of the time, rather than the more open knowledge spaces of the 21st century. Bush provides a core vision of the importance of information to industrial/scientific society, using the image of an \"information explosion\" arising from the unprecedented demands on scientific production and technological application of World War II. He outlines a version of information science as a key discipline within the practice of scientific and technical knowledge domains. His view encompasses the problems of information overload and the need to devise efficient mechanisms to control and channel information for use. — Johnston and Webber [ 4 ] Indeed, Bush was very concerned with information overload inhibiting the research efforts of scientists. His scientist, operating under conditions of \"information explosion\" and requiring respite from the tide of scientific documents could be construed as a nascent image of the \"Information Literate Person\" in an information saturated society. There is a growing mountain of research. But there is increased evidence that we are being bogged down today as specialization extends. The investigator is staggered by the findings and conclusions of thousands of other workers. — Vannevar Bush [ 3 ] Schools, colleges, health care, government, etc., are all implicated in the distribution and use of information, under similar conditions of \"information explosion\" as Bush's post-war scientists. All these people arguably need some sort of personal \"information control\" in order to function. — Bill Johnston, Sheila Webber [ 4 ]",
    "links": [
      "Telephone exchange",
      "Transistor",
      "Facsimile",
      "Doi (identifier)",
      "Voder",
      "Douglas Engelbart",
      "Instant film",
      "Tabulating machine",
      "Stenotype",
      "Photographic processing",
      "Telecommunication",
      "Memory",
      "Trenton, New Jersey",
      "Fultograph",
      "Analytical engine",
      "Dry-plate photography",
      "Word processor",
      "Tide-predicting machine",
      "World War II",
      "English longbow",
      "Moving company",
      "Life magazine",
      "Information society",
      "Punched card",
      "Scenario (computing)",
      "Punched card sorter",
      "Cosmic ray visual phenomena",
      "Mechanical computer",
      "Hypertext",
      "Microfilm",
      "Knowledge",
      "Timeline of hypertext technology",
      "Vacuum tube",
      "Spanish language",
      "The Atlantic Monthly",
      "Gottfried Wilhelm Leibniz",
      "Mouse (computing)",
      "Association (psychology)",
      "Library classification",
      "File Allocation Table",
      "Valdemar Poulsen",
      "Herman Hollerith",
      "World Wide Web",
      "Wikipedia",
      "Printer (computing)",
      "Information overload",
      "Electromechanical",
      "Vocoder",
      "Stepped reckoner",
      "Microform",
      "Telautograph",
      "Microphotography",
      "Association for Computing Machinery",
      "Internet",
      "Charles Babbage",
      "Department store",
      "Hyperlink",
      "The Crusades",
      "Number crunching",
      "Personal computers",
      "Online encyclopedia",
      "Doug Engelbart",
      "Knowledge base",
      "Atomic bombings of Hiroshima and Nagasaki",
      "Fortune (magazine)",
      "Vannevar Bush",
      "NTFS",
      "Silver halide",
      "Differential analyser",
      "Collective memory",
      "Speech recognition",
      "Image scanner",
      "Google Books",
      "Memex",
      "Man-Computer Symbiosis",
      "The Atlantic",
      "Ext3",
      "Photodetector",
      "Whiteprint",
      "ISBN (identifier)",
      "Encyclopædia Britannica",
      "Brain–computer interface",
      "File systems",
      "Instant camera",
      "Wire recording",
      "Trip hammer"
    ]
  },
  "Spam filtering": {
    "url": "https://en.wikipedia.org/wiki/Spam_filtering",
    "title": "Spam filtering",
    "content": "Various anti-spam techniques are used to prevent email spam (unsolicited bulk email). No technique is a complete solution to the spam problem, and each has trade-offs between incorrectly rejecting legitimate email ( false positives ) as opposed to not rejecting all spam email ( false negatives ) – and the associated costs in time, effort, and cost of wrongfully obstructing good mail. [ 1 ] This leads to combinations of the many techniques in order to achieve the best protection against spam and the potential harms that may come with it, while keeping the emails that should be seen intact. Anti-spam techniques can be broken into four broad categories: those that require actions by individuals (end-user techniques) , those that can be automated by email administrators , those that can be automated by email senders and those employed by researchers and law enforcement officials . They are often used in conjunction with one another. There are a number of techniques that individuals can use to restrict the availability of their email addresses, with the goal of reducing their chance of receiving spam. Sharing an email address only among a limited group of correspondents is one way to limit the chance that the address will be \"harvested\" and targeted to receive spam. Similarly, when forwarding messages to a number of recipients who do not know one another, recipient addresses can be put in the bcc: field so that each recipient does not get a list of the other recipients' email addresses. When identifying spam, the email of the sender might be slightly off than that of an official company. Winning competitions and rewards, job offers, and anything revolving around the banking world are among the top spam subjects. [ 2 ] Writing might lack professionalism and grammar. Artificial intelligence can be used to create the messages and may have an automated or robotic style of language. [ 3 ] It has been found in the modern day that over half of the spam emails sent involve artificial intelligence in some form. Besides creating the spam message entirely, AI may also be used to revise writings of errors, making them appear more authentic. [ 4 ] As time goes on, it is very possible that the AI can become harder to detect and employ other methods that makes spam easier to fall victim to. As it stands currently, out of the most popular email service providers, Yahoo has best been able to prevent AI generated spam from penetrating through their integrated security systems. [ 5 ] In contrast, Gmail and Outlook had allowed more from a set of the same emails to go through their spam detectors. Email addresses posted on webpages , Usenet or chat rooms are vulnerable to e-mail address harvesting . [ 6 ] Address munging is the practice of disguising an e-mail address to prevent it from being automatically collected in this way, but still allow a human reader to reconstruct the original: an email address such as, \"no-one@example.com\", might be written as \"no-one at example dot com\", for instance. A related technique is to display all or part of the email address as an image, or as jumbled text with the order of characters restored using CSS . A common piece of advice is not to reply to spam messages [ 7 ] as spammers may simply regard responses as confirmation that an email address is valid. Disabling read receipts can help too, as even opening spam could signal activity. [ 8 ] [ 9 ] Similarly, many spam messages contain web links or addresses which the user is directed to follow to be removed from the spammer's mailing list – and these should be treated as dangerous. Even deleting a spam email can confirm validity and activity of the account. [ 10 ] In any case, sender addresses are often forged in spam messages, so that responding to spam may result in failed deliveries – or may reach completely innocent third parties. Some phishing campaigns use professional networking platforms such as LinkedIn to gather personal and employment details, enabling attackers to craft convincing messages that appear to come from coworkers, recruiters, or HR departments. These impostors acting as job recruiters can lead to scams, extorting money or personal information. [ 11 ] Interacting with such phishing attempts – including clicking links to \"unsubscribe\" or \"verify details\" – can confirm address validity to attackers and expose users to credential theft or malware. It should also be noted that successful removal of subscriptions still have only meager results at best, [ 12 ] and it is overall more likely to cause further issues rather than resolving any. These highly targeted, social engineering-style phishing messages are often based on publicly visible LinkedIn information and can bypass traditional spam filters, making user vigilance especially critical. [ 13 ] [ 14 ] Calling the customer service of the supposed sender trying to gather this information and illuminate if it is real, however, it should be an email found on the official cite or somewhere else that is verifiable, as any number attached to the email could be a part of their network. [ 15 ] Businesses and individuals sometimes avoid publicizing an email address by asking for contact to come via a \"contact form\" on a webpage – which then typically forwards the information via email. Such forms, however, are sometimes inconvenient to users, as they are not able to use their preferred email client, risk entering a faulty reply address, and are typically not notified about delivery problems. Further, contact forms have the drawback that they require a website with the appropriate technology. In some cases contact forms also send the message to the email address given by the user. This allows the contact form to be used for sending spam, which may incur email deliverability problems from the site once the spam is reported and the sending IP is blacklisted. Many modern mail programs incorporate web browser functionality, such as the display of HTML , URLs, and images. Avoiding or disabling this feature does not help avoid spam. It may, however, be useful to avoid some problems if a user opens a spam message: offensive images, obfuscated hyperlinks, being tracked by web bugs , being targeted by JavaScript or attacks upon security vulnerabilities in the HTML renderer. Mail clients which do not automatically download and display HTML, images or attachments have fewer risks, as do clients who have been configured to not display these by default. An email user may sometimes need to give an address to a site without complete assurance that the site owner will not use it for sending spam. One way to mitigate the risk is to provide a disposable email address — an address which the user can disable or abandon which forwards email to a real account. A number of services provide disposable address forwarding. Addresses can be manually disabled, can expire after a given time interval, or can expire after a certain number of messages have been forwarded. Disposable email addresses can be used by users to track whether a site owner has disclosed an address, or had a security breach . [ 16 ] Systems that use \"ham passwords\" ask unrecognized senders to include in their email a password that demonstrates that the email message is a \"ham\" (not spam) message. Typically the email address and ham password would be described on a web page, and the ham password would be included in the subject line of an email message (or appended to the \"username\" part of the email address using the \" plus addressing \" technique). Ham passwords are often combined with filtering systems which let through only those messages that have identified themselves as \"ham\". [ 17 ] Certain sites may have a financial incentive to spread email addresses to third parties, who then can send spam. To avoid this, a user can read the privacy policy when using a site for the first time; the site owner must explain what can and cannot be done with a user's email address. [ 18 ] A social media platform may grant other companies licenses to use personal information of the platform's users, such as email addresses. Platforms of this nature typically have privacy policies. [ 19 ] Timely updating software provides better protection against cybercriminal activity, including viruses and malware. [ 20 ] This can prevent spammers from getting an email to begin with, along with safeguarding devices from the malicious files that may accidentally be installed from spam mail. Tracking down a spammer's ISP and reporting the offense can lead to the spammer's service being terminated [ 21 ] and criminal prosecution. [ 22 ] Some online tools such as SpamCop and Network Abuse Clearinghouse are potentially helpful but not always accurate. Historically, such reports have not played a large part in abating spam, since the spammers generally move their operation to another URL, ISP or network of IP addresses. In many countries consumers may also report unwanted and deceptive commercial email to government agencies. In the US, the Federal Trade Commission (FTC), an agency of the Department of Commerce , has taken action against spammers. [ 23 ] Similar agencies exist in other countries. [ 24 ] There are now a large number of applications, appliances, services, and software systems that email administrators can use to reduce the load of spam on their systems and mailboxes. In general, these attempt to reject (or \"block\") the majority of spam email outright at the SMTP connection stage. If they do accept a message, they will typically then analyze the content further and may decide to \"quarantine\" any categorized as spam. A number of systems have been developed that allow domain name owners to identify email as authorized. Many of these systems use the DNS to list sites authorized to send email on their behalf. After many other proposals, SPF , DKIM and DMARC are all now widely supported with growing adoption. [ 25 ] [ 26 ] [ 27 ] While not directly attacking spam, these systems make it much harder to spoof addresses , a common technique of spammers also used in phishing and other types of fraud via email. Using any combination of these will help prevent emails from being mislabeled as spam or junk. [ 28 ] A method which may be used by internet service providers, by specialized services or enterprises to combat spam is to require unknown senders to pass various tests before their messages are delivered. These strategies are termed \"challenge/response systems\". Checksum-based filter exploits the fact that the messages are sent in bulk, that is that they will be identical with small variations. Checksum-based filters strip out everything that might vary between messages, reduce what remains to a checksum , and look that checksum up in a database such as the Distributed Checksum Clearinghouse which collects the checksums of messages that email recipients consider to be spam (some people have a button on their email client which they can click to nominate a message as being spam); if the checksum is in the database, the message is likely to be spam. To avoid being detected in this way, spammers will sometimes insert unique invisible gibberish known as hashbusters into the middle of each of their messages, to make each message have a unique checksum. Some email servers expect to never communicate with particular countries from which they receive a great deal of spam. Therefore, they use country-based filtering – a technique that blocks email from certain countries. This technique is based on country of origin determined by the sender's IP address rather than any trait of the sender. This can of course be bypassed by services that can displace a sender's IP, such as a VPN . There are large number of free and commercial DNS-based Blacklists, or DNSBLs which allow a mail server to quickly look up the IP of an incoming mail connection - and reject it if it is listed there. Administrators can choose from scores of DNSBLs, each of which reflects different policies: some list sites known to emit spam; others list open mail relays or proxies; others list ISPs known to support spam. This method can unfortunately result in false positives, potentially blocking real mail if a blacklisted IP is linked with spammers is also shared by authentic users. Virtual private networks and other methods of fronting with a false IP address can allow spammers to get around these established blacklists. [ 29 ] Essentially a DNS-based blacklist that is set up and kept by a third-party. These lists tend to be updated frequently, and can be comparable in efficiency to in-house blacklists. Naturally, this comes with the same downsides of possible false positives and being relatively easy to get around by spammers. [ 29 ] The polar opposite of a blacklist, permits mail from chosen users and sources only. This is incredibly restrictive in who is able to send messages, but is very effective. There are what are known as automatic whitelists that will mark senders as clear if they do not have any history of distributing spam mail, this can be much more reasonable to use rather than a standard whitelist. [ 29 ] Greylists work in a way that is very similar to that of whitelists. It will deny any email that is being sent from an unapproved account, and will then display a sign to the sender that this occurred. If another attempt is made to send an email it will go through and the sender will be added to the list (at this point functioning exactly like a whitelist, as they can now send mail whenever). While most real users will attempt to send out the email again, many spam systems only send out messages once. This results in spam mail not being received. [ 29 ] Most spam/phishing messages contain an URL that they entice victims into clicking on. Thus, a popular technique since the early 2000s consists of extracting URLs from messages and looking them up in databases such as Spamhaus ' Domain Block List (DBL), SURBL , and URIBL. [ 30 ] Many spammers use poorly written software or are unable to comply with the standards because they do not have legitimate control of the computer they are using to send spam ( zombie computer ). By setting tighter limits on the deviation from RFC standards that the MTA will accept, a mail administrator can reduce spam significantly - but this also runs the risk of rejecting mail from older or poorly written or configured servers. Greeting delay – A sending server is required to wait until it has received the SMTP greeting banner before it sends any data. A deliberate pause can be introduced by receiving servers to allow them to detect and deny any spam-sending applications that do not wait to receive this banner. Temporary rejection – The greylisting technique is built on the fact that the SMTP protocol allows for temporary rejection of incoming messages. Greylisting temporarily rejects all messages from unknown senders or mail servers – using the standard 4xx error codes. [ 31 ] All compliant MTAs will proceed to retry delivery later, but many spammers and spambots will not. The downside is that all legitimate messages from first-time senders will experience a delay in delivery. HELO/EHLO checking – RFC 5321 says that an SMTP server \"MAY verify that the domain name argument in the EHLO command actually corresponds to the IP address of the client. However, if the verification fails, the server MUST NOT refuse to accept a message on that basis.\" Systems can, however, be configured to Invalid pipelining – Several SMTP commands are allowed to be placed in one network packet and \"pipelined\". For example, if an email is sent with a CC: header, several SMTP \"RCPT TO\" commands might be placed in a single packet instead of one packet per \"RCPT TO\" command. The SMTP protocol, however, requires that errors be checked and everything is synchronized at certain points. Many spammers will send everything in a single packet since they do not care about errors and it is more efficient. Some MTAs will detect this invalid pipelining and reject email sent this way. Nolisting – The email servers for any given domain are specified in a prioritized list, via the MX records . The nolisting technique is simply the adding of an MX record pointing to a non-existent server as the \"primary\" (i.e. that with the lowest preference value) – which means that an initial mail contact will always fail. Many spam sources do not retry on failure, so the spammer will move on to the next victim; legitimate email servers should retry the next higher numbered MX, and normal email will be delivered with only a brief delay. Quit detection – An SMTP connection should always be closed with a QUIT command. Many spammers skip this step because their spam has already been sent and taking the time to properly close the connection takes time and bandwidth. Some MTAs are capable of detecting whether or not the connection is closed correctly and use this as a measure of how trustworthy the other system is. Another approach is simply creating an imitation MTA that gives the appearance of being an open mail relay, or an imitation TCP/IP proxy server that gives the appearance of being an open proxy. Spammers who probe systems for open relays and proxies will find such a host and attempt to send mail through it, wasting their time and resources, and potentially, revealing information about themselves and the origin of the spam they are sending to the entity that operates the honeypot. Such a system may simply discard the spam attempts, submit them to DNSBLs , or store them for analysis by the entity operating the honeypot that may enable identification of the spammer for blocking. SpamAssassin , Rspamd , Policyd-weight and others use some or all of the various tests for spam, and assign a numerical score to each test. Each message is scanned for these patterns, and the applicable scores tallied up. If the total is above a fixed value, the message is rejected or flagged as spam. By ensuring that no single spam test by itself can flag a message as spam, the false positive rate can be greatly reduced. Outbound spam protection involves scanning email traffic as it exits a network, identifying spam messages and then taking an action such as blocking the message or shutting off the source of the traffic. While the primary impact of spam is on spam recipients, sending networks also experience financial costs, such as wasted bandwidth, and the risk of having their IP addresses blocked by receiving networks. Outbound spam protection not only stops spam, but also lets system administrators track down spam sources on their network and remediate them – for example, clearing malware from machines which have become infected with a virus or are participating in a botnet . The PTR DNS records in the reverse DNS can be used for a number of things, including: Content filtering techniques rely on the specification of lists of words or regular expressions disallowed in mail messages. Thus, if a site receives spam advertising \"herbal Viagra\", the administrator might place this phrase in the filter configuration. The mail server would then reject any message containing the phrase. This could lead to real accounts being blocked from emailing mistakenly if the words or phrases that are restricted are fairly common. Also, it is important to note that many spammers may purposefully misspell words to get around this, along with some having different native languages that could result in spelling errors. [ 29 ] This leads to alternative spellings of blocked words to also be added to the list in order to better defend against spam. Heuristic filters can take this further and attribute points to certain words or phrases much like the standard rule-based filtering. These phrases can be worth more points than others, and when added will determine whether or not it is spam based on a threshold that is put into place. Depending on how low it is, this can lead to false positives. Header filtering looks at the header of the email which contains information about the origin, destination and content of the message. Although spammers will often spoof fields in the header in order to hide their identity, or to try to make the email look more legitimate than it is, many of these spoofing methods can be detected, and any violation of, e.g., RFC 5322 , 7208 , standards on how the header is to be formed can also serve as a basis for rejecting the message. Since a large percentage of spam has forged and invalid sender (\"from\") addresses, some spam can be detected by checking that this \"from\" address is valid. A mail server can try to verify the sender address by making an SMTP connection back to the mail exchanger for the address, as if it were creating a bounce, but stopping just before any email is sent. Callback verification has various drawbacks: (1) Since nearly all spam has forged return addresses , nearly all callbacks are to innocent third party mail servers that are unrelated to the spam; (2) When the spammer uses a trap address as his sender's address. If the receiving MTA tries to make the callback using the trap address in a MAIL FROM command, the receiving MTA's IP address will be blacklisted; (3) Finally, the standard VRFY and EXPN commands [ 33 ] used to verify an address have been so exploited by spammers that few mail administrators enable them, leaving the receiving SMTP server no effective way to validate the sender's email address. [ 34 ] SMTP proxies allow combating spam in real time, combining sender's behavior controls, providing legitimate users immediate feedback, eliminating a need for quarantine. Spamtrapping is the seeding of an email address so that spammers can find it, but normal users can not. If the email address is used then the sender must be a spammer and they are black listed. As an example, if the email address \"spamtrap@example.org\" is placed in the source HTML of a web site in a way that it isn't displayed on the web page, human visitors to the website would not see it. Spammers, on the other hand, use web page scrapers and bots to harvest email addresses from HTML source code - so they would find this address. When the spammer later sends to the address the spamtrap knows this is highly likely to be a spammer and can take appropriate action. Statistical, or Bayesian, filtering once set up requires no administrative maintenance per se: instead, users mark messages as spam or nonspam and the filtering software learns from these judgements. Thus, it is matched to the end user's needs, and as long as users consistently mark/tag the emails, can respond quickly to changes in spam content. Statistical filters typically also look at message headers, considering not just the content but also peculiarities of the transport mechanism of the email. In more recent times with the use of artificial intelligence and machine learning, these forms of filters have been able to go more in depth and overall improve upon their performance in combating against spam. Software programs that implement statistical filtering include Bogofilter , DSPAM , SpamBayes , ASSP , CRM114 , the email programs Mozilla and Mozilla Thunderbird , Mailwasher , and later revisions of SpamAssassin . A tarpit is any server software which intentionally responds extremely slowly to client commands. By running a tarpit which treats acceptable mail normally and known spam slowly or which appears to be an open mail relay, a site can slow down the rate at which spammers can inject messages into the mail facility. Depending on the server and internet speed, a tarpit can slow an attack by a factor of around 500. [ 35 ] Many systems will simply disconnect if the server doesn't respond quickly, which will eliminate the spam. However, a few legitimate email systems will also not deal correctly with these delays. The fundamental idea is to slow the attack so that the perpetrator has to waste time without any significant success. [ 36 ] An organization can successfully deploy a tarpit if it is able to define the range of addresses, protocols, and ports for deception. [ 37 ] The process involves a router passing the supported traffic to the appropriate server while those sent by other contacts are sent to the tarpit. [ 37 ] Examples of tarpits include the Labrea tarpit, Honeyd, [ 38 ] SMTP tarpits, and IP-level tarpits. Measures to protect against spam can cause collateral damage. This includes: There are a variety of techniques that email senders use to try to make sure that they do not send spam. Failure to control the amount of spam sent, as judged by email receivers, can often cause even legitimate email to be blocked and for the sender to be put on DNSBLs . Since spammer's accounts are frequently disabled due to violations of abuse policies, they are constantly trying to create new accounts. Due to the damage done to an ISP's reputation when it is the source of spam, many ISPs and web email providers use CAPTCHAs on new accounts to verify that it is a real human registering the account, and not an automated spamming system. They can also verify that credit cards are not stolen before accepting new customers, check the Spamhaus Project ROKSO list, and do other background checks. A malicious person can easily attempt to subscribe another user to a mailing list — to harass them, or to make the company or organisation appear to be spamming. To prevent this, all modern mailing list management programs (such as GNU Mailman , LISTSERV , Majordomo , and qmail 's ezmlm) support \"confirmed opt-in\" by default. Whenever an email address is presented for subscription to the list, the software will send a confirmation message to that address. The confirmation message contains no advertising content, so it is not construed to be spam itself, and the address is not added to the live mail list unless the recipient responds to the confirmation message. Email senders typically now do the same type of anti-spam checks on email coming from their users and customers as for inward email coming from the rest of the Internet. This protects their reputation, which could otherwise be harmed in the case of infection by spam-sending malware. If a receiving server initially fully accepts an email, and only later determines that the message is spam or to a non-existent recipient, it will generate a bounce message back to the supposed sender. However, if (as is often the case with spam), the sender information on the incoming email was forged to be that of an unrelated third party then this bounce message is backscatter spam . For this reason it is generally preferable for most rejection of incoming email to happen during the SMTP connection stage, with a 5xx error code, while the sending server is still connected. In this case then the sending server will report the problem to the real sender cleanly. Firewalls and routers can be programmed to not allow SMTP traffic (TCP port 25) from machines on the network that are not supposed to run message transfer agents or send email. [ 39 ] This practice is somewhat controversial when ISPs block home users, especially if the ISPs do not allow the blocking to be turned off upon request. Email can still be sent from these computers to designated smart hosts via port 25 and to other smart hosts via the email submission port 587. Network address translation can be used to intercept all port 25 (SMTP) traffic and direct it to a mail server that enforces rate limiting and egress spam filtering. This is commonly done in hotels, [ 40 ] but it can cause email privacy problems, as well making it impossible to use STARTTLS and SMTP-AUTH if the port 587 submission port isn't used. Machines that suddenly start sending unusual quantities of email may have become zombie computers . By limiting the rate that email can be sent around what is typical for the computer in question, legitimate email can still be sent, but large spam runs can be slowed down until manual investigation can be done. [ 41 ] By monitoring spam reports from sources such as SpamCop , AOL 's feedback loop, Network Abuse Clearinghouse, the domain's abuse@ mailbox, and others, ISPs can often learn of problems before they seriously damage the ISP's reputation and the ISP's mail servers are blacklisted. Both malicious software and human spam senders often use forged FROM addresses when sending spam messages. Control may be enforced on SMTP servers to ensure senders can only use their correct email address in the FROM field of outgoing messages. In an email users database each user has a record with an email address. The SMTP server must check if the email address in the FROM field of an outgoing message is the same address that belongs to the user's credentials, supplied for SMTP authentication. If the FROM field is forged, an SMTP error will be returned to the email client (e.g. \"You do not own the email address you are trying to send from\"). Most ISPs and webmail providers have either an Acceptable Use Policy (AUP) or a Terms of Service (TOS) agreement that discourages spammers from using their system and allows the spammer to be terminated quickly for violations. From 2000 onwards, many countries enacted specific legislation to criminalize spamming, and appropriate legislation and enforcement can have a significant impact on spamming activity. [ 42 ] Where legislation provides specific text that bulk emailers must include, this also makes \"legitimate\" bulk email easier to identify. Increasingly, anti-spam efforts have led to co-ordination between law enforcement, researchers, major consumer financial service companies and Internet service providers in monitoring and tracking email spam, identity theft and phishing activities and gathering evidence for criminal cases. [ 43 ] Analysis of the sites being spamvertised by a given piece of spam can often be followed up with domain registrars with good results. [ 44 ] Several approaches have been proposed to improve the email system. Since spamming is facilitated by the fact that large volumes of email are very inexpensive to send, one proposed set of solutions would require that senders pay some cost in order to send email, making it prohibitively expensive for spammers. Anti-spam activist Daniel Balsam attempts to make spamming less profitable by bringing lawsuits against spammers. [ 45 ] One group of researchers have been looking at model that hones in on establishing a stark defense, this would increase the cost needed to make the spam effective. They are doing this through the lens of game methodology and strategy, by deploying the strict defensive measures at times of high volume, the spammer would have to spend more money. [ 46 ] This would in turn decrease the amount of spam and possibly eliminate it in its entirety. In their experiments testing out this model it was shown to operate as intended, limiting spam overall. Artificial intelligence techniques can be deployed for filtering spam emails, such as artificial neural network algorithms and Bayesian filters. These methods use probabilistic methods to train the networks, such as examination of the concentration or frequency of words seen in spam versus legitimate email contents. [ 47 ] By combining these filters, large language models, and natural language processing models, advanced systems can be developed to create new screens of defense against spam. [ 48 ] This can be improved upon consistently and constantly, as this really serves as the foundation, counteracting the ever-growing nature of artificial intelligence on the other side of the spectrum that is being used for spam assault purposes. This can lead to artificial intelligence generation pop-ups to users that can serve as a warning however, which can reduce the amount that these emails are interacted with. The automated messages can also be fed into the anti-spam machine-learning systems that are in use or being researched, which can allow better results in the future. [ 49 ] On a personal scale one titled PhiShield is in development, an AI system that can detect phishing attempts and provide information about the email to a user without opening it. It has an incredible track record so far, nearly perfect in determining phishing spam from ordinary mail in over hundreds of thousands of research cases. [ 50 ] Text preprocessing constitutes altering text to make patterns pop out more, which could be paired with anti-spam methods to make them more productive. [ 51 ] This act simplifies words to their most basic form, or \"stem\". This allows words to be grouped together to make analysis easier. Sink, sunk, and sank for example would all be classified as sink. Tokenization basically takes out unnecessary bits from an email for preprocessing, like punctuation, and turns needed substance such as words into tokens. This can help with summing up the message. This, like tokenization, takes out unneeded parts, in this case stopwords or filler words. Things like prepositions or anything else that is not required to get the fundamental meaning of an email. The subtraction of these can make spam easier to detect as there is less to focus on. The normalization process makes text more conventional by way of righting capitalization, spelling, expanding contractions, and other ways to make it more uniform. This can be useful when encountering spam from non-native speakers of the language or from other dialects. Lexical analysis is used to find origin words similarly to stemming. This works along the lines of normalization to have one established tone and form of language. This helps with the grouping of words that are important in making a determination of spam. Keras makes use of a Python user interface to operate with neural networks. A group of researchers have been working on creating a framework that works with long-short term memory and convolutional neural networks to counter spam methods through email. It would meld these two ideas together with the use of Keras to create one of the most functional live spam filters to date. In testing and researching, it was shown to massively exceed current methods in terms of success rate. [ 52 ] Channel email is a new proposal for sending email that attempts to distribute anti-spam activities by forcing verification (probably using bounce messages so back-scatter does not occur) when the first email is sent for new contacts. Spam is the subject of several research conferences, including TREC .",
    "links": [
      "Outlook.com",
      "FQDN",
      "Doi (identifier)",
      "Address munging",
      "Firewall (networking)",
      "Text Retrieval Conference",
      "Social spam",
      "Spamvertising",
      "Rspamd",
      "Open mail relay",
      "Cold calling",
      "Voice phishing",
      "Daniel Balsam",
      "Python (programming language)",
      "Neil Schwartzman",
      "Lottery scam",
      "Context filtering",
      "Email spam",
      "Flyposting",
      "Mobile phone spam",
      "Reverse DNS lookup",
      "Smart host",
      "Yahoo",
      "Computer virus",
      "Associated Press",
      "Qmail",
      "Internet service provider",
      "Forward-confirmed reverse DNS",
      "Backscatter (email)",
      "Email-address harvesting",
      "Mozilla Thunderbird",
      "RFC (identifier)",
      "E-mail address harvesting",
      "Phishing",
      "U.S. Department of Commerce",
      "Whitelist",
      "Webmail",
      "Email authentication",
      "Bibcode (identifier)",
      "Bayesian spam filtering",
      "Newsgroup spam",
      "Callback verification",
      "GNU Mailman",
      "Checksum",
      "Amavis",
      "Internet fraud",
      "Spamdexing",
      "Convolutional neural network",
      "Bounce message",
      "E-mail address",
      "Terms of Service",
      "MX record",
      "Spam filter",
      "Referrer spam",
      "Distributed Checksum Clearinghouse",
      "Opt-in email",
      "Network address translation",
      "Email spoofing",
      "Challenge–response spam filtering",
      "Message transfer agent",
      "PMID (identifier)",
      "Web browser",
      "Directory harvest attack",
      "Scraper site",
      "Regular expression",
      "SpamAssassin",
      "Keyword stuffing",
      "SpamCop",
      "Tarpit (networking)",
      "Junk fax",
      "Robocall",
      "Cloaking",
      "Advance-fee scam",
      "VPN",
      "Bulk email software",
      "Spam reporting",
      "Domain Name System blocklist",
      "Akismet",
      "Spam in blogs",
      "Scunthorpe problem",
      "Spam (electronic)",
      "Identity theft",
      "MyWOT",
      "European Union",
      "Mozilla",
      "Email filtering",
      "Router (computing)",
      "DMARC",
      "Joe job",
      "Email spam legislation by country",
      "HTML email",
      "Disposable email address",
      "Google bombing",
      "Botnet",
      "SMTP",
      "Spoofing attack",
      "Spam and Open Relay Blocking System",
      "Link farm",
      "Make Money Fast",
      "SpamBayes",
      "Web bug",
      "Feedback loop (email)",
      "Federal Trade Commission",
      "Pink contract",
      "SMTP proxy",
      "Egress filtering",
      "Computerworld",
      "Spamming",
      "KnujOn",
      "Messaging spam",
      "Sender Policy Framework",
      "False positive",
      "Trade-off",
      "Greylisting (email)",
      "Spamtrap",
      "Bounce address",
      "SURBL",
      "Forum spam",
      "Acceptable Use Policy",
      "HTML",
      "Keras",
      "CSS",
      "Telemarketing",
      "United States",
      "Blind carbon copy",
      "Naive Bayes spam filtering",
      "Data Security",
      "Webpage",
      "CRM114 (program)",
      "AOL",
      "LISTSERV",
      "Nolisting",
      "Communication protocol",
      "List poisoning",
      "Mail transfer agent",
      "Wayback Machine",
      "Image spam",
      "DNSWL",
      "Challenge-response spam filtering",
      "ISSN (identifier)",
      "Sping",
      "DKIM",
      "DNSBL",
      "Hashbusters",
      "Email privacy",
      "Anti-spam techniques (users)",
      "Mailing list",
      "Usenet",
      "JavaScript",
      "John Levine",
      "ISBN (identifier)",
      "Honeypot (computing)",
      "Doorway page",
      "Chat rooms",
      "Anti-Spam SMTP Proxy",
      "CAPTCHA",
      "SMTP-AUTH",
      "MIMEDefang",
      "URL redirection",
      "Gmail",
      "Spam blog",
      "Zombie computer",
      "Spambot",
      "Anti-Spam Research Group",
      "PMC (identifier)",
      "VoIP spam",
      "The Spamhaus Project",
      "Majordomo (software)",
      "Long short-term memory",
      "Auto dialer",
      "Mailwasher",
      "Bogofilter",
      "Cost-based anti-spam systems"
    ]
  },
  "Xapian": {
    "url": "https://en.wikipedia.org/wiki/Xapian",
    "title": "Xapian",
    "content": "Xapian is a free and open-source probabilistic information retrieval library, released under the GNU General Public License (GPL). [ 2 ] It is a full-text search engine library for programmers. It is written in C++ , with bindings to allow use from Perl , Python (2 and 3), PHP (5 and 7), Java , Tcl , C# , Ruby , Lua , Erlang , Node.js and R . [ 1 ] [ 3 ] Xapian is highly portable and runs on Linux , OS X , FreeBSD , NetBSD , OpenBSD , Solaris , HP-UX , AIX , Windows , OS/2 [ 4 ] [ 2 ] and Hurd , [ 5 ] [ 6 ] as well as Tru64 . [ citation needed ] Xapian grew out of the Muscat search engine, written by Dr. Martin F. Porter at the University of Cambridge. [ 7 ] The first official release of Xapian was version 0.5.0 on September 20, 2002. [ 8 ] Xapian allows developers to add advanced indexing and search facilities to their own applications. Organisations and projects using Xapian include the Library of the University of Cologne, Debian , Die Zeit , MoinMoin , and One Laptop per Child . [ 9 ]",
    "links": [
      "Application programming interface",
      "GNU General Public License",
      "UTF-8",
      "Index (search engine)",
      "Search engine (computing)",
      "Software release life cycle",
      "OpenBSD",
      "Recoll",
      "Ruby programming language",
      "Erlang (programming language)",
      "Tru64",
      "AIX",
      "OS X",
      "Programmer",
      "PHP",
      "HP-UX",
      "Debian",
      "Java (Sun)",
      "List of information retrieval libraries",
      "Software license",
      "Tcl",
      "Unicode 9.0",
      "Hurd",
      "Repository (version control)",
      "Solaris (operating system)",
      "Proximity search (text)",
      "Python (programming language)",
      "Operating system",
      "Information retrieval",
      "NetBSD",
      "Martin Porter",
      "Relevance feedback",
      "C++",
      "Free and open-source",
      "R (programming language)",
      "OS/2",
      "Qt (software)",
      "Cross-platform",
      "One Laptop per Child",
      "MoinMoin",
      "FreeBSD",
      "Node.js",
      "Die Zeit",
      "Search algorithm",
      "C Sharp (programming language)",
      "Lua (programming language)",
      "Windows (operating system)",
      "Linux",
      "Perl",
      "Basic Multilingual Plane"
    ]
  },
  "OSTI (identifier)": {
    "url": "https://en.wikipedia.org/wiki/OSTI_(identifier)",
    "title": "OSTI (identifier)",
    "content": "The Office of Scientific and Technical Information ( OSTI ) is a component of the Office of Science within the U.S. Department of Energy (DOE). The Energy Policy Act PL 109–58, Section 982, called out the responsibility of OSTI: \"The Secretary, through the Office of Scientific and Technical Information, shall maintain with the Department publicly available collections of scientific and technical information resulting from research, development, demonstration, and commercial applications activities supported by the Department.\" OSTI provides access to energy, science, and technology information through publicly available web-based systems, with supporting tools and technologies to enable information search, retrieval and re-use. [ 1 ] [ 2 ] OSTI's entire line of electronic products may be accessed through its home page, where users may search multiple databases with one query.",
    "links": [
      "Stanford Synchrotron Radiation Lightsource",
      "National Center for Computational Sciences",
      "New Brunswick Laboratory",
      "Waste Isolation Pilot Plant",
      "Office of Electricity",
      "Rocky Flats Plant",
      "K-25",
      "Los Alamos National Laboratory",
      "Assistant Secretary of Energy for Fossil Energy",
      "Under Secretary of Energy for Infrastructure",
      "High Flux Isotope Reactor",
      "Energy Research and Development Administration",
      "United States Atomic Energy Act of 1946",
      "OSTI (disambiguation)",
      "Southeastern Power Administration",
      "Princeton Plasma Physics Laboratory",
      "Advanced Light Source",
      "Argonne National Laboratory",
      "Dual-Axis Radiographic Hydrodynamic Test Facility",
      "Yucca Mountain nuclear waste repository",
      "Relativistic Heavy Ion Collider",
      "United States Department of Energy national laboratories",
      "Federated search",
      "Spallation Neutron Source",
      "Santa Susana Field Laboratory",
      "Tokamak Fusion Test Reactor",
      "Federal Energy Regulatory Commission",
      "Office of Nuclear Energy",
      "National Synchrotron Light Source II",
      "Under Secretary of Energy for Nuclear Security",
      "Fernald Feed Materials Production Center",
      "National Renewable Energy Laboratory",
      "Oak Ridge National Laboratory",
      "National Center for Electron Microscopy",
      "Bonneville Power Administration",
      "Western Area Power Administration",
      "Kansas City Plant",
      "Z Pulsed Power Facility",
      "Molecular Foundry",
      "Savannah River National Laboratory",
      "Center for Nanophase Materials Sciences",
      "Office of Intelligence and Counterintelligence",
      "SLAC National Accelerator Laboratory",
      "National Ignition Facility",
      "Chris Wright",
      "National Synchrotron Light Source",
      "Lawrence Berkeley National Laboratory",
      "Web harvesting",
      "Electron Microscopy Center",
      "United States Atomic Energy Commission",
      "Wikidata",
      "Tevatron",
      "Power Marketing Administration",
      "National Nuclear Security Administration",
      "Atomic Energy Authority Act 1954",
      "James V. Forrestal Building",
      "Energy Policy Act of 2005",
      "Nuclear Regulatory Commission",
      "National Energy Research Scientific Computing Center",
      "Science.gov",
      "America COMPETES Act",
      "Idaho National Laboratory",
      "CENDI",
      "United States Secretary of Energy",
      "Under Secretary of Energy for Science and Innovation",
      "United States Department of Energy",
      "Radiological and Environmental Sciences Laboratory",
      "Energy Sciences Network",
      "Albany Research Center",
      "Lawrence Livermore National Laboratory",
      "Nevada Test Site",
      "Southwestern Power Administration",
      "Center for Functional Nanomaterials",
      "Energy Reorganization Act of 1974",
      "Strategic Petroleum Reserve (United States)",
      "Ames Laboratory",
      "National Spherical Torus Experiment",
      "Argonne Tandem Linear Accelerator System",
      "United States Government",
      "Pantex Plant",
      "Savannah River Site",
      "Joint Genome Institute",
      "Office of Energy Efficiency and Renewable Energy",
      "Sodium Reactor Experiment",
      "Thomas Jefferson National Accelerator Facility",
      "Wayback Machine",
      "Center for Nanoscale Materials",
      "Office of Science",
      "National Energy Technology Laboratory",
      "Assistant Secretary of Energy for Nuclear Energy",
      "Brookhaven National Laboratory",
      "Hanford Site",
      "James Danly",
      "United States Deputy Secretary of Energy",
      "Northeast Home Heating Oil Reserve",
      "ARPA-E",
      "Alternating Gradient Synchrotron",
      "National Atmospheric Release Advisory Center",
      "Pacific Northwest National Laboratory",
      "Energy Technology Engineering Center",
      "Energy Information Administration",
      "Y-12 National Security Complex",
      "Fermilab",
      "Advanced Photon Source",
      "Environmental Molecular Sciences Laboratory",
      "Sandia National Laboratories"
    ]
  },
  "Tabulating machine": {
    "url": "https://en.wikipedia.org/wiki/Tabulating_machine",
    "title": "Tabulating machine",
    "content": "The tabulating machine was an electromechanical machine designed to assist in summarizing information stored on punched cards . Invented by Herman Hollerith , the machine was developed to help process data for the 1890 U.S. Census . Later models were widely used for business applications such as accounting and inventory control . It spawned a class of machines, known as unit record equipment , and the data processing industry. The term \" Super Computing \" was used by the New York World newspaper in 1931 to refer to a large custom-built tabulator that IBM made for Columbia University . [ 1 ] The 1880 census had taken eight years to process. [ 2 ] Since the U.S. Constitution mandates a census every ten years to apportion both congressional representatives and direct taxes among the states , a combination of larger staff and faster-recording systems was required. In the late 1880s Herman Hollerith , inspired by conductors using holes punched in different positions on a railway ticket to record traveler details such as gender and approximate age, invented the recording of data on a machine-readable medium. Prior uses of machine-readable media had been for lists of instructions (not data) to drive programmed machines such as Jacquard looms . \"After some initial trials with paper tape, he settled on punched cards ...\" [ 3 ] Hollerith used punched cards with round holes, 12 rows, and 24 columns. The cards measured 3 + 1 ⁄ 4 by 6 + 5 ⁄ 8 inches (83 by 168 mm). [ 4 ] His tabulator used electromechanical solenoids to increment mechanical counters. A set of spring-loaded wires were suspended over the card reader. The card sat over pools of mercury , pools corresponding to the possible hole positions in the card. When the wires were pressed onto the card, punched holes allowed wires to dip into the mercury pools, making an electrical contact [ 5 ] [ 6 ] that could be used for counting, sorting, and setting off a bell to let the operator know the card had been read. The tabulator had 40 counters, each with a dial divided into 100 divisions, with two indicator hands; one which stepped one unit with each counting pulse, the other which advanced one unit every time the other dial made a complete revolution. This arrangement allowed a count of up to 9,999. During a given tabulating run, counters could be assigned to a specific hole or, by using relay logic , to a combination of holes, e.g. to count married couples. [ 7 ] If the card was to be sorted, a compartment lid of the sorting box would open for storage of the card, the choice of compartment depending on the data in the card. [ 8 ] Hollerith's method was used for the 1890 census. Clerks used keypunches to punch holes in the cards entering age, state of residence, gender, and other information from the returns. Some 100 million cards were generated and \"the cards were only passed through the machines four times during the whole of the operations.\" [ 4 ] According to the U.S. Census Bureau, the census results were \"... finished months ahead of schedule and far under budget.\" [ 9 ] The advantages of the technology were immediately apparent for accounting and tracking inventory . Hollerith started his own business as The Hollerith Electric Tabulating System , specializing in punched card data processing equipment . [ 10 ] In 1896, he incorporated the Tabulating Machine Company. In that year he introduced the Hollerith Integrating Tabulator, which could add numbers coded on punched cards, not just count the number of holes. Punched cards were still read manually using the pins and mercury pool reader. 1900 saw the Hollerith Automatic Feed Tabulator used in that year's U.S. census. A control panel was incorporated in the 1906 Type 1. [ 11 ] In 1911, four corporations, including Hollerith's firm, were amalgamated (via stock acquisition) to form a fifth company, the Computing-Tabulating-Recording Company (CTR). The Powers Accounting Machine Company was formed that same year and, like Hollerith, with machines first developed at the Census Bureau. In 1919, the first Bull tabulator prototype was developed. Tabulators that could print, and with removable control panels, appeared in the 1920s. In 1924, CTR was renamed International Business Machines (IBM). In 1927, Remington Rand acquired the Powers Accounting Machine Company. In 1933, The Tabulating Machine Company was subsumed into IBM. These companies continued to develop faster and more sophisticated tabulators, culminating in tabulators such as 1949 IBM 407 and 1952 Remington Rand 409 . Tabulating machines continued to be used well after the introduction of commercial electronic computers in the 1950s. Many applications using unit record tabulators were migrated to computers such as the IBM 1401 . Two programming languages, FARGO and RPG , were created to aid this migration. Since tabulator control panels were based on the machine cycle, both FARGO and RPG emulated the notion of the machine cycle and training material showed the control panel vs. programming language coding sheet relationships. In its basic form, a tabulating machine would read one card at a time, print portions (fields) of the card on fan-fold paper , possibly rearranged, and add one or more numbers punched on the card to one or more counters, called accumulators . On early models, the accumulator register dials would be read manually after a card run to get totals. Later models could print totals directly. Cards with a particular punch could be treated as master cards causing different behavior. For example, customer master cards could be merged with sorted cards recording individual items purchased. When read by the tabulating machine to create invoices, the billing address and customer number would be printed from the master card, and then individual items purchased and their price would be printed. When the next master card was detected, the total price would be printed from the accumulator and the page ejected to the top of the next page, typically using a carriage control tape . With successive stages or cycles of punched-card processing, fairly complex calculations could be made if one had a sufficient set of equipment. (In modern data processing terms, one can think of each stage as an SQL clause: SELECT (filter columns), then WHERE (filter cards, or \"rows\"), then maybe a GROUP BY for totals and counts, then a SORT BY; and then perhaps feed those back to another set of SELECT and WHERE cycles again if needed.) A human operator had to retrieve, load, and store the various card decks at each stage. Hollerith's first tabulators were used to compile mortality statistics for Baltimore, Jersey City and New York City in 1886. [ 13 ] The first Tabulating Machine Company (TMC) automatic feed tabulator, operating at 150 cards/minute, was developed in 1906. [ 14 ] The first TMC printing tabulator was developed in 1920. [ 15 ] TMC Type IV Accounting Machine (later renamed the IBM 301), from the IBM Archives : The 301 (better known as the Type IV ) Accounting Machine was the first card-controlled machine to incorporate class selection, automatic subtraction, and printing of a net positive or negative balance. Dating to 1928, this machine exemplifies the transition from tabulating to accounting machines. The Type IV could list 100 cards per minute. H.W.Egli - BULL Tabulator model T30 , 1931 IBM 401: The 401, introduced in 1933, was an early entry in a long series of IBM alphabetic tabulators and accounting machines. It was developed by a team headed by J. R. Peirce and incorporated significant functions and features invented by A. W. Mills , F. J. Furman and E. J. Rabenda . The 401 added at a speed of 150 cards per minute and listed alphanumerical data at 80 cards per minute. [ 16 ] IBM 405: Introduced in 1934, the 405 Alphabetical Accounting Machine was the basic bookkeeping and accounting machine marketed by IBM for many years. Important features were expanded adding capacity, greater flexibility of counter grouping, [ b ] direct printing of the entire alphabet, direct subtraction [ c ] and printing of either debit or credit balance from any counter. Commonly called the 405 \"tabulator,\" this machine remained the flagship of IBM's product line until after World War II. [ 17 ] [ 18 ] The British at Hut 8 used Hollerith machinery to gain some knowledge of Known-plaintext attack cribs used by encrypted German messages. [ 19 ] IBM 402 and 403, from 1948, were modernized successors to the 405. The 1952 Bull Gamma 3 could be attached to this tabulator or to a card read/punch. [ 20 ] [ 21 ] IBM 407 Introduced in 1949, the 407 was the mainstay of the IBM unit record product line for almost three decades. It was later adapted to serve as an input/output peripheral for several early electronic calculators and computers. Its printing mechanism was used in the IBM 716 line printer for the IBM 700/7000 series and later with the IBM 1130 through the mid-1970s. The IBM 407 Accounting Machine was withdrawn from marketing in 1976, signaling the end of the unit record era. [ 22 ] IBM 421 For early use of tabulators for scientific computations see",
    "links": [
      "Powers Accounting Machine Company",
      "U.S. Constitution",
      "IBM 407",
      "Remington Rand 409",
      "Punched card",
      "Direct tax",
      "Unit record equipment",
      "U.S. states",
      "Herman Hollerith",
      "Jacquard loom",
      "Plugboard",
      "Powers Accounting Machine",
      "IBM 700/7000 series",
      "British Tabulating Machine Company",
      "Doi (identifier)",
      "Computing-Tabulating-Recording Company",
      "U.S. Census, 1890",
      "International Business Machines",
      "Program (machine)",
      "JSTOR (identifier)",
      "Relay logic",
      "Inventory control",
      "New York World",
      "IBM 716",
      "Consolidation (business)",
      "Fredrik Rosing Bull",
      "Known-plaintext attack",
      "Electromechanical",
      "Hut 8",
      "Journal of the Royal Statistical Society",
      "IBM 1401",
      "Leslie Comrie",
      "IBM 421",
      "Mercury (element)",
      "Columbia University",
      "Accounting",
      "Computer",
      "ISBN (identifier)",
      "Keypunch",
      "RPG programming language",
      "Inventory",
      "Powers-Samas",
      "IBM and the Holocaust",
      "FARGO (programming language)",
      "Supercomputer",
      "IBM 1130",
      "Accumulator (computing)",
      "Leon E. Truesdell",
      "IBM 402",
      "Wallace John Eckert",
      "Ticket (admission)",
      "Carriage control tape",
      "A. W. Mills",
      "United States congressional apportionment",
      "Conductor (transportation)",
      "SQL",
      "Solenoid",
      "IBM",
      "Fan-fold paper",
      "U.S. Census, 1880"
    ]
  },
  "Knowledge organization": {
    "url": "https://en.wikipedia.org/wiki/Knowledge_organization",
    "title": "Knowledge organization",
    "content": "Knowledge organization ( KO ), organization of knowledge , organization of information , or information organization is an intellectual discipline concerned with activities such as document description , indexing , and classification that serve to provide systems of representation and order for knowledge and information objects. According to The Organization of Information by Joudrey and Taylor, information organization: examines the activities carried out and tools used by people who work in places that accumulate information resources (e.g., books, maps, documents, datasets, images) for the use of humankind, both immediately and for posterity. It discusses the processes that are in place to make resources findable, whether someone is searching for a single known item or is browsing through hundreds of resources just hoping to discover something useful. Information organization supports a myriad of information-seeking scenarios. [ 1 ] Issues related to knowledge sharing can be said to have been an important part of knowledge management for a long time. Knowledge sharing has received a lot of attention in research and business practice both within and outside organizations and its different levels. [ 2 ] Sharing knowledge is not only about giving it to others, but it also includes searching, locating, and absorbing knowledge. Unawareness of the employees' work and duties tends to provoke the repetition of mistakes, the waste of resources, and duplication of the same projects. Motivating co-workers to share their knowledge is called knowledge enabling. It leads to trust among individuals and encourages a more open and proactive relationship that grants the exchange of information easily. [ 3 ] Knowledge sharing is part of the three-phase knowledge management process which is a continuous process model. The three parts are knowledge creation, knowledge implementation, and knowledge sharing. The process is continuous, which is why the parts cannot be fully separated. Knowledge creation is the consequence of individuals' minds, interactions, and activities. Developing new ideas and arrangements alludes to the process of knowledge creation. Using the knowledge which is present at the company in the most effective manner stands for the implementation of knowledge. Knowledge sharing, the most essential part of the process for our topic, takes place when two or more people benefit by learning from each other. [ 4 ] Traditional human-based approaches performed by librarians, archivists, and subject specialists are increasingly challenged by computational ( big data ) algorithmic techniques. KO as a field of study is concerned with the nature and quality of such knowledge-organizing processes (KOP) (such as taxonomy and ontology ) as well as the resulting knowledge organizing systems (KOS). Among the major figures in the history of KO are Melvil Dewey (1851–1931) and Henry Bliss (1870–1955). Dewey's goal was an efficient way to manage library collections; not an optimal system to support users of libraries. His system was meant to be used in many libraries as a standardized way to manage collections. The first version of this system was created in 1876. [ 5 ] An important characteristic in Henry Bliss' (and many contemporary thinkers of KO) was that the sciences tend to reflect the order of Nature and that library classification should reflect the order of knowledge as uncovered by science: The implication is that librarians, in order to classify books, should know about scientific developments. This should also be reflected in their education: Again from the standpoint of the higher education of librarians, the teaching of systems of classification ... would be perhaps better conducted by including courses in the systematic encyclopedia and methodology of all the sciences, that is to say, outlines which try to summarize the most recent results in the relation to one another in which they are now studied together. ... ( Ernest Cushing Richardson , quoted from Bliss, 1935, p. 2) Among the other principles, which may be attributed to the traditional approach to KO are: Today, after more than 100 years of research and development in LIS, the \"traditional\" approach still has a strong position in KO and in many ways its principles still dominate. The date of the foundation of this approach may be chosen as the publication of S. R. Ranganathan 's colon classification in 1933. The approach has been further developed by, in particular, the British Classification Research Group . The best way to explain this approach is probably to explain its analytico-synthetic methodology. The meaning of the term \"analysis\" is: breaking down each subject into its basic concepts. The meaning of the term synthesis is: combining the relevant units and concepts to describe the subject matter of the information package in hand. Given subjects (as they appear in, for example, book titles) are first analyzed into a few common categories, which are termed \"facets\". Ranganathan proposed his PMEST formula: Personality, Matter, Energy, Space and Time: Important in the IR-tradition have been, among others, the Cranfield experiments , which were founded in the 1950s, and the TREC experiments ( Text Retrieval Conferences ) starting in 1992. It was the Cranfield experiments, which introduced the measures \"recall\" and \"precision\" as evaluation criteria for systems efficiency. The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (\"UNITERM\"). The Cranfield I test found, according to Ellis (1996, 3–6) the following results: Although these results have been criticized and questioned, the IR-tradition became much more influential while library classification research lost influence. The dominant trend has been to regard only statistical averages . What has largely been neglected is to ask: Are there certain kinds of questions in relation to which other kinds of representation, for example, controlled vocabularies, may improve recall and precision? The best way to define this approach is probably by method: Systems based upon user-oriented approaches must specify how the design of a system is made on the basis of empirical studies of users. User studies demonstrated very early that users prefer verbal search systems as opposed to systems based on classification notations. This is one example of a principle derived from empirical studies of users. Adherents of classification notations may, of course, still have an argument: That notations are well-defined and that users may miss important information by not considering them. Folksonomies is a recent kind of KO based on users' rather than on librarians' or subject specialists' indexing. These approaches are primarily based on using bibliographical references to organize networks of papers, mainly by bibliographic coupling (introduced by Kessler 1963) or co-citation analysis ( independently suggested by Marshakova 1973 [ 8 ] and Small 1973). In recent years it has become a popular activity to construe bibliometric maps as structures of research fields. Two considerations are important in considering bibliometric approaches to KO: Domain analysis is a sociological - epistemological standpoint that advocates that the indexing of a given document should reflect the needs of a given group of users or a given ideal purpose. In other words, any description or representation of a given document is more or less suited to the fulfillment of certain tasks. A description is never objective or neutral, and the goal is not to standardize descriptions or make one description once and for all for different target groups. The development of the Danish library \" KVINFO \" may serve as an example that explains the domain-analytic point of view. KVINFO was founded by the librarian and writer Nynne Koch and its history goes back to 1965. Nynne Koch was employed at the Royal Library in Copenhagen in a position without influence on book selection. She was interested in women's studies and began personally to collect printed catalog cards of books in the Royal Library, which were considered relevant for women's studies. She developed a classification system for this subject. Later she became the head of KVINFO and got a budget for buying books and journals, and still later, KVINFO became an independent library. The important theoretical point of view is that the Royal Library had an official systematic catalog of a high standard. Normally it is assumed that such a catalog is able to identify relevant books for users whatever their theoretical orientation. This example demonstrates, however, that for a specific user group (feminist scholars), an alternative way of organizing catalog cards was important. In other words: Different points of view need different systems of organization. Domain analysis has examined epistemological issues in the field, i.e. comparing the assumptions made in different approaches to KO and examining the questions regarding subjectivity and objectivity in KO. Subjectivity is not just about individual differences. Such differences are of minor interest because they cannot be used as guidelines for KO. What seems important are collective views shared by many users. A kind of subjectivity about many users is related to philosophical positions. In any field of knowledge different views are always at play. In arts, for example, different views of art are always present. Such views determine views on art works, writing on art works, how art works are organized in exhibitions and how writings on art are organized in libraries. In general it can be stated that different philosophical positions on any issue have implications for relevance criteria, information needs and for criteria of organizing knowledge. One widely used analysis of information-organizational principles, attributed to Richard Saul Wurman , summarizes them as Location, Alphabet, Time, Category, Hierarchy (LATCH). [ 9 ] [ 10 ]",
    "links": [
      "Information access",
      "Nippon Decimal Classification",
      "Categorization",
      "Doi (identifier)",
      "Brian Deer Classification System",
      "Personal information management",
      "Text Retrieval Conference",
      "Information retrieval",
      "Dewey Decimal Classification",
      "Information seeking",
      "Superintendent of Documents Classification",
      "Science and technology studies",
      "Memory",
      "Growth of knowledge",
      "Big data",
      "Privacy",
      "Libraries Unlimited",
      "Data modeling",
      "New Classification Scheme for Chinese Libraries",
      "Preservation (library and archival science)",
      "Information technology",
      "Classification Research Group",
      "Nynne Koch",
      "Statistical average",
      "Information society",
      "Knowledge representation and reasoning",
      "Taxonomy",
      "Classification (general theory)",
      "Censorship",
      "Outline of information science",
      "Knowledge organization system",
      "Korean decimal classification",
      "Henry E. Bliss",
      "Information science",
      "Controlled vocabulary",
      "Natural order (philosophy)",
      "Knowledge organization systems",
      "National Library of Medicine classification",
      "Outline of academic disciplines",
      "Precision and recall",
      "S. R. Ranganathan",
      "Swedish library classification system",
      "Computer data storage",
      "Richard Saul Wurman",
      "Library classification",
      "Sociological",
      "Knowledge Organization (journal)",
      "OCLC (identifier)",
      "Information architecture",
      "Melvil Dewey",
      "Scientific classification",
      "Information behavior",
      "Bibliographic index",
      "Automatic document classification",
      "Knowledge organization (management)",
      "Cutter Expansive Classification",
      "Documentation science",
      "Dewey decimal classification",
      "Quantum information science",
      "Philosophy of information",
      "Ernest Cushing Richardson",
      "Colon classification",
      "Library and information science",
      "Intellectual property",
      "Cultural studies",
      "Body of knowledge",
      "Informatics",
      "Knowledge sharing",
      "Folksonomy",
      "Bliss bibliographic classification",
      "Harvard–Yenching Classification",
      "ISBN (identifier)",
      "Cranfield experiments",
      "Chinese Library Classification",
      "Information ecology",
      "Intellectual freedom",
      "Universal Decimal Classification",
      "Library of Congress Classification",
      "Uniterm",
      "Bibliometrics",
      "CODOC",
      "Taxonomy (general)",
      "Epistemological",
      "KVINFO",
      "Document classification",
      "Ontology (information science)",
      "Discipline (academia)",
      "Faceted classification",
      "Information management",
      "Subject heading"
    ]
  },
  "Collaborative information seeking": {
    "url": "https://en.wikipedia.org/wiki/Collaborative_information_seeking",
    "title": "Collaborative information seeking",
    "content": "Collaborative information seeking ( CIS ) is a field of research that involves studying situations, motivations, and methods for people working in collaborative groups for information seeking projects, as well as building systems for supporting such activities. Such projects often involve information searching or information retrieval (IR), information gathering, and information sharing . Beyond that, CIS can extend to collaborative information synthesis and collaborative sense-making . Seeking for information is often considered a solo activity, but there are many situations that call for people working together for information seeking . Such situations are typically complex in nature, and involve working through several sessions exploring, evaluating, and gathering relevant information. Take for example, a couple going on a trip. They have the same goal, and in order to accomplish their goal, they need to seek out several kinds of information, including flights, hotels, and sightseeing. This may involve them working together over multiple sessions, exploring and collecting useful information, and collectively making decisions that help them move toward their common goal. It is a common knowledge that collaboration is either necessary or highly desired in many activities that are complex or difficult to deal with for an individual. Despite its natural appeal and situational necessity, collaboration in information seeking is an understudied domain. The nature of the available information and its role in our lives have changed significantly, but the methods and tools that are used to access and share that information in collaboration have remained largely unaltered. People still use general-purpose systems such as email and IM for doing CIS projects, and there is a lack of specialized tools and techniques to support CIS explicitly. There are also several models to explain information seeking and information behavior , [ 1 ] but the areas of collaborative information seeking and collaborative information behavior remain understudied. On the theory side, Shah has presented C5 Model [ 2 ] [ 3 ] for studying collaborative situations, including information seeking . On the practical side, a few specialized systems for supporting CIS have emerged in the recent past, but their usage and evaluations have underwhelmed. Despite such limitations, the field of CIS has been getting a lot of attention lately, and several promising theories and tools have come forth. Multiple reviews of CIS related literature are written by Shah. [ 4 ] Shah's book [ 5 ] provides a comprehensive review of this field, including theories, models, systems, evaluation, and future research directions. Other books in this area include one by Morris and Teevan , [ 6 ] as well as Foster's book on collaborative information behavior. [ 7 ] and Hansen, Shah, and Klas's edited book on CIS. [ 8 ] Depending upon what one includes or excludes while talking about CIS, we have many or hardly any theories. If we consider the past work on the groupware systems, many interesting insights can be obtained about people working on collaborative projects, the issues they face, and the guidelines for system designers. One of the notable works is by Grudin, [ 9 ] who laid out eight design principles for developers of groupware systems. The discussion below is primarily based on some of the recent works in the field of computer supported cooperative work CSCW , collaborative IR, and CIS. The literature is filled with works that use terms such as collaborative information retrieval , [ 10 ] [ 11 ] social searching , [ 12 ] concurrent search , [ 13 ] collaborative exploratory search , [ 14 ] co-browsing , [ 15 ] collaborative information behavior , [ 16 ] [ 17 ] collaborative information synthesis , [ 18 ] and collaborative information seeking , [ 19 ] [ 20 ] which are often used interchangeably. There are several definitions of such related or similar terms in the literature. For instance, Foster [ 21 ] defined collaborative IR as \"the study of the systems and practices that enable individuals to collaborate during the seeking, searching, and retrieval of information.\" Shah [ 22 ] defined CIS as a process of collaboratively seeking information that is \"defined explicitly among the participants, interactive, and mutually beneficial.\" While there is still a lack of a definition or a terminology that is universally accepted, but most agree that CIS is an active process, as opposed to collaborative filtering , where a system connects the users based on their passive involvement (e.g., buying similar products on Amazon). Foley and Smeaton [ 23 ] defined two key aspects of collaborative information seeking as division of labor and the sharing of knowledge . Division of labor allows collaborating searchers to tackle larger problems by reducing the duplication of effort (e.g., finding documents that one's collaborator has already discovered). The sharing of knowledge allows searchers to influence each other's activities as they interact with the retrieval system in pursuit of their (often evolving) information need. This influence can occur in real time if the collaborative search system supports it, or it can occur in a turn-taking, asynchronous manner if that is how interaction is structured. Teevan et al. [ 24 ] characterized two classes of collaboration, task-based vs. trait-based. Task-based collaboration corresponds to intentional collaboration; trait-based collaboration facilitates the sharing of knowledge through inferred similarity of information need. One of the important issues to study in CIS is the instance, reason, and the methods behind a collaboration. For instance, Morris, [ 25 ] using a survey with 204 knowledge workers at a large technology company found that people often like and want to collaborate, but they do not find specialized tools to help them in such endeavors. Some of the situations for doing collaborative information seeking in this survey were travel planning, shopping, and literature search. Shah, [ 26 ] similarly, using personal interviews, identified three main reasons why people collaborate. As far as the tools and/or methods for CIS are concerned, both Morris and Shah found that email is still the most used tool. Other popular methods are face-to-face meetings, IM, and phone or conference calls. In general, the choice of the method or tool for our respondents depended on their situation (co-located or remote), and objective (brainstorming or working on independent parts). The classical way of organizing collaborative activities is based on two factors: location and time. [ 27 ] Recently Hansen & Jarvelin [ 28 ] and Golovchinsky, Pickens, & Back [ 29 ] also classified approaches to collaborative IR using these two dimensions of space and time. See \"Browsing is a Collaborative Process\", [ 30 ] where the authors depict various library activities on these two dimensions. [ 31 ] As we can see from this figure, the majority of collaborative activities in conventional libraries are co-located and synchronous, whereas collaborative activities relating to digital libraries are more remote and synchronous. Social information filtering, or collaborative filtering, as we saw earlier, is a process benefitting from other users' actions in the past; thus, it falls under asynchronous and mostly remote domain. These days email also serves as a tool for doing asynchronous collaboration among users who are not co-located. Chat or IM (represented as 'internet' in the figure) helps to carry out synchronous and remote collaboration. Rodden, [ 27 ] similarly, presented a classification of CSCW systems using the form of interaction and the geographical nature of cooperative systems. Further, Rodden & Blair [ 32 ] presented an important characteristic to all CSCW systems – control. According to the authors, two predominant control mechanisms have emerged within CSCW systems: speech act theory systems, and procedure based systems. These mechanisms are tightly coupled with the kind of control the system can support in a collaborative environment (discussed later). Often researchers also talk about other dimensions, such as intentionality and depth of mediation (system mediated or user mediated), [ 29 ] while classifying various CIS systems. Three components specific to group-work or collaboration that are highly predominant in the CIS or CSCW literature are control, communication, and awareness. In this section key definitions and related works for these components will be highlighted. Understanding their roles can also help us address various design issues with CIS systems. Rodden identified the value of control in CSCW systems and listed a number of projects with their corresponding schemes for implementing for control. For instance, the COSMOS project [ 33 ] had a formal structure to represent control in the system. They used roles to represent people or automatons, and rules to represent the flow and processes. The roles of the people could be a supervisor, processor, or analyst. Rules could be a condition that a process needs to satisfy in order to start or finish. Due to such a structure seen in projects like COSMOS, Rodden classified these control systems as procedural based systems. The control penal was every effort to seeking people and control others in this method used for highly responsible people take control of another network system was supply chine managements or transformation into out connection processor information This is one of the most critical components of any collaboration. In fact, Rodden (1991) identified message or communication systems as the class of systems in CSCW that is most mature and most widely used. Since the focus here is on CIS systems that allow its participants to engage in an intentional and interactive collaboration, there must be a way for the participants to communicate with each other. What is interesting to note is that often, collaboration could begin by letting a group of users communicate with each other. For instance, Donath & Robertson [ 34 ] presented a system that allows a user to know that others were currently viewing the same webpage and communicate with those people to initiate a possible collaboration or at least a co-browsing experience. Providing communication capabilities even in an environment that was not originally designed for carrying out collaboration is an interesting way of encouraging collaboration. Awareness, in the context of CSCW, has been defined as \"an understanding of the activities of others, which provides a context for your own activity\" . [ 35 ] The following four kinds of awareness are often discussed and addressed in the CSCW literature: [ 36 ] Shah and Marchionini [ 37 ] studied awareness as provided by interface in collaborative information seeking. They found that one needs to provide \"right\" (not too little, not too much, and appropriate for the task at hand) kind of awareness to reduce the cost of coordination and maximize the benefits of collaboration. A number of specialized systems have been developed back from the days of the groupware systems to today's Web 2.0 interfaces. A few such examples, in chronological order, are given below. Twidale et al. [ 38 ] developed Ariadne to support the collaborative learning of database browsing skills. In addition to enhancing the opportunities and effectiveness of the collaborative learning that already occurred, Ariadne was designed to provide the facilities that would allow collaborations to persist as people increasingly searched information remotely and had less opportunity for spontaneous face-to-face collaboration. Ariadne was developed in the days when Telnet-based access to library catalogs was a common practice. Building on top of this command-line interface , Ariadne could capture the users’ input and the database’s output, and form them into a search history that consisted of a series of command-output pairs. Such a separation of capture and display allowed Ariadne to work with various forms of data capture methods. To support complex browsing processes in collaboration, Ariadne presented a visualization of the search process. [ 39 ] This visualization consisted of thumbnails of screens, looking like playing cards, which represented command-output pairs. Any such card can be expanded to reveal its details. The horizontal axis on Ariadne’s display represented time, and the vertical axis showed information on the semantics of the action it represented: the top row for the top level menus, the middle row for specifying a search, and the bottom row for looking at particular book details. This visualization of the search process in Ariadne makes it possible to annotate, discuss with colleagues around the screen, and distribute to remote collaborators for asynchronous commenting easily and effectively. As we saw in the previous section, having access to one’s history as well as the history of one’s collaborators are very crucial to effective collaboration. Ariadne implements these requirements with the features that let one visualize, save, and share a search process. In fact, the authors found one of the advantages of search visualization was the ability to recap previous searching sessions easily in a multi-session exploratory searching. More recently, one of the collaborative information seeking tools that have caught a lot of attention is SearchTogether, developed by Morris and Horvitz . [ 40 ] The design of this tool was motivated by a survey that the researchers did with 204 knowledge workers, [ 25 ] in which they discovered the following. Based on the survey responses, and the current and desired practices for collaborative search, the authors of SearchTogether identified three key features for supporting people’s collaborative information behavior while searching on the Web: awareness, division of labor, and persistence. Let us look at how these three features are implemented. SearchTogether instantiates awareness in several ways, one of which is per-user query histories. This is done by showing each group member’s screen name, his/her photo and queries in the “Query Awareness” region. The access to the query histories is immediate and interactive, as clicking on a query brings back the results of that query from when it was executed. The authors identified query awareness as a very important feature in collaborative searching, which allows group members to not only share their query terms, but also learn better query formulation techniques from one another. Another component of SearchTogether that facilitates awareness is the display of page-specific metadata. This region includes several pieces of information about the displayed page, including group members who viewed the given page, and their comments and ratings. The authors claim that such visitation information can help one either choose to avoid a page already visited by someone in the group to reduce the duplication of efforts, or perhaps choose to visit such pages, as they provide a sign of promising leads as indicated by the presence of comments and/or ratings. Division of labor in SearchTogether is implemented in three ways: (1) “Split Search” allows one to split the search results among all online group members in a round-robin fashion, (2) “Multi-Engine Search” takes a query and runs it on n different search engines, where n is the number of online group members, (3) manual division of labor can be facilitated using integrated IM. Finally, the persistence feature in SearchTogether is instantiated by storing all the objects and actions, including IM conversations, query histories, recommendation queues, and page-specific metadata. Such data about all the group members are available to each member when he/she logs in. This allows one to easily carry a multi-session collaborative project. Cerchiamo [ 41 ] [ 42 ] is a collaborative information seeking tool that explores issues related to algorithmic mediation of information seeking activities and how collaborators' roles can be used to structure the user interface. Cerchiamo introduced the notion of algorithmic mediation, that is, the ability of the system to collect input asynchronously from multiple collaborating searchers, and to use these multiple streams of input to affect the information that is being retrieved and displayed to the searchers. Cerchiamo collected judgments of relevance from multiple collaborating searchers and used those judgments to create a ranked list of items that were potentially relevant to the information need. This algorithm prioritized items that were retrieved by multiple queries and that were retrieved by queries that also retrieved many other relevant documents. This rank fusion is just one way in which a search system that manages activities of multiple collaborating searchers can combine their inputs to generate results that are better than those produced by individuals working independently. Cerchiamo implemented two roles—Prospector and Miner—that searchers could assume. Each role had an associated interface. The Prospector role/interface focused on running many queries and making a few judgments of relevance for each query to explore the information space. The Miner role/interface focused on making relevance judgments on a ranked list of items selected from items retrieved by all queries in the current session. This combination of roles allowed searchers to explore and exploit the information space, and led teams to discover more unique relevant documents than pairs of individuals working separately. [ 41 ] Coagmento (Latin for \"working together\") is a new and unique system that allows a group of people work together for their information seeking tasks without leaving their browsers. Coagmento has been developed with a client-server architecture, where the client is implemented as a Firefox plug-in that helps multiple people working in collaboration to communicate, and search, share and organize information. The server component stores and provides all the objects and actions collected from the client. Due to this decoupling, Coagmento provides a flexible architecture that allows its users to be co-located or remote, working synchronously or asynchronously, and use different platforms. Coagmento includes a toolbar and a sidebar. The toolbar has several buttons that helps one collect information and be aware of the progress in a given collaboration. The toolbar has three major parts: The sidebar features a chat window, under which there are three tabs with the history of search engine queries, saved pages and snippets. With each of these objects, the user who created or collected that object is shown. Anyone in the group can access an object by clicking on it. For instance, one can click on a query issued by anyone in the group to re-run that query and bring up the results in the main browser window. An Android (operating system) app for Coagmento can be found in the Android Market . Fernandez-Luna et al. [ 43 ] introduce Cosme (COde Search MEeting) as a NetBeans IDE plug-in that enables remote team of software developers to collaborate in real time during source-code search sessions. The COSME design was motivated by early studies of C. Foley, M. R. Morris, C. Shah, among others researchers, and by habits of software developers identified in a survey of 117 universities students and professors related with projects of software development, as well as to computer programmers of some companies. The five more commons collaborative search habits (or related to it) of the interviewees was: COSME is designed to enable either synchronous or asynchronous, but explicit remote collaboration among team developers with shared technical information needs. Its client user interface include a search panel that lets developers to specify queries, division of labor principle (possible combination include the use of different search engines, ranking fusion, and split algorithms), searching field (comments, source-code, class or methods declaration), and the collection type (source-code files or digital documentation). The sessions panel wraps the principal options to management the collaborative search sessions, which consists in a team of developers working together to satisfy their shared technical information needs. For example, a developer can use the embedded chat room to negotiate the creation of a collaborative search session, and show comments of the current and historical search results. The implementation of Cosme was based on CIRLab (Collaborative Information Retrieval Laboratory) instantiation, a groupware framework for CIS research and experimentation, Java as programming language, NetBeans IDE Platform as plug-in base, and Amenities (A MEthodology for aNalysis and desIgn of cooperaTIve systEmS) as software engineering methodology. CIS systems development is a complex task, which involves software technologies and Know-how in different areas such as distributed programming, information search and retrieval, collaboration among people, task coordination and many others according to the context. This situation is not ideal because it requires great programming efforts. Fortunately, some CIS application frameworks and toolkits are increasing their popularity since they have a high reusability impact for both developers and researchers, like Coagmento Collaboratory and DrakkarKeel. Many interesting and important questions remain to be addressed in the field of CIS, including",
    "links": [
      "Information sharing",
      "Collaborative working environment",
      "Doi (identifier)",
      "Human–computer information retrieval",
      "Advances in Librarianship",
      "S2CID (identifier)",
      "Knowledge worker",
      "Collaborative learning",
      "Collaborative filtering",
      "CiteSeerX (identifier)",
      "CSCW",
      "Collaborative software",
      "Collaborative innovation network",
      "Information behavior",
      "Information retrieval",
      "Information seeking",
      "ISBN (identifier)",
      "Groupware",
      "Computer-supported collaboration",
      "Sense-making",
      "Integrated collaboration environment",
      "Collaboration",
      "Android (operating system)",
      "Jaime Teevan",
      "Collaborative working system",
      "Intentionality",
      "Command-line interface",
      "Web 2.0",
      "Eric Horvitz",
      "Computer-supported collaborative learning"
    ]
  },
  "Hdl (identifier)": {
    "url": "https://en.wikipedia.org/wiki/Hdl_(identifier)",
    "title": "Hdl (identifier)",
    "content": "The Handle System is a proprietary registry assigning persistent identifiers , or handles , to information resources, and for resolving \"those handles into the information necessary to locate, access, and otherwise make use of the resources\". [ 1 ] As with handles used elsewhere in computing, Handle System handles are opaque, and encode no information about the underlying resource, being bound only to metadata regarding the resource. Consequently, the handles are not rendered invalid by changes to the metadata. The system was developed by Bob Kahn at the Corporation for National Research Initiatives (CNRI) as a part of the Digital Object Architecture (DOA). The original work was funded by the Defense Advanced Research Projects Agency (DARPA) between 1992 and 1996, as part of a wider framework for distributed digital object services, [ 2 ] and was thus contemporaneous with the early deployment of the World Wide Web , with similar goals. The Handle System was first implemented in autumn 1994, and was administered and operated by CNRI until December 2015, when a new \"multi-primary administrator\" (MPA) mode of operation was introduced. The DONA Foundation [ 3 ] now administers the system's Global Handle Registry and accredits MPAs, including CNRI and the International DOI Foundation . [ 4 ] The system currently provides the underlying infrastructure for such handle-based systems as Digital Object Identifiers (DOI) and DSpace , which are mainly used to provide access to scholarly, professional and government documents and other information resources. CNRI provides specifications and the source code for reference implementations for the servers and protocols used in the system under a royalty-free \"Public License\", similar to an open source license. [ 5 ] Thousands of handle services are currently running. Over 1000 of these are at universities and libraries, but they are also in operation at national laboratories, research groups, government agencies, and commercial enterprises, receiving over 200 million resolution requests per month. [ citation needed ] The Handle System is defined in informational RFCs 3650, [ 1 ] 3651 [ 6 ] and 3652 [ 7 ] of the Internet Engineering Task Force (IETF) ; it includes an open set of protocols, a namespace, and a reference implementation of the protocols. Documentation, software, and related information is provided by CNRI on a dedicated website [ 8 ] Handles consist of a prefix which identifies a \"naming authority\" and a suffix which gives the \"local name\" of a resource. Similar to domain names, prefixes are issued to naming authorities by one of the \"multi-primary administrators\" of the system upon payment of a fee, which must be renewed annually. A naming authority may create any number of handles, with unique \"local names\", within their assigned prefixes. Two example of handles are: In the first example, which is the handle for the HANDLE.NET software license , 20.1000 is the prefix assigned to the naming authority (in this case, Handle.net itself) and 100 is the local name within that namespace. The local name may consist of any characters from the Unicode UCS-2 character set. The prefix also consists of any UCS-2 characters, other than \"/\". The prefixes consist of one or more naming authority segments, separated by periods, representing a hierarchy of naming authorities. Thus, in the example 20 is the naming authority prefix for CNRI, while 1000 designates a subordinate naming authority within the 20 prefix. Other examples of top-level prefixes for the federated naming authorities of the DONA Foundation are 10 for DOI handles; 11 for handles assigned by the ITU ; 21 for handles issued by the German Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG), the scientific computing center of the University of Göttingen ; and 86 for the Coalition of Handle Services – China. Older \"legacy\" prefixes issued by CNRI before the \"multi-primary administrator\" (MPA) structure was instituted are typically four of five digits, as in the second example above, a handle administered by the University of Leicester . All prefixes must be registered in the Global Handle Registry through an DONA Foundation approved registrar, normally for a fee. As with other uses of handles in computing, the handle is opaque; that is, it encodes no information about the underlying resource and provides only the means to retrieve metadata about the resource. This may be contrasted with a Uniform Resource Locator (URL), which may encode within the identifier such attributes of the resource as the protocol to be used to access the server holding the resource, the server host name and port number, and perhaps even location specifics such as the name of a file in the server file system containing the resource. In the Handle System, these specifics are not encoded in the handle, but are found in the metadata to which the handle is bound. The metadata may include many attributes of the information resource, such as its locations, the forms in which it is available, the types of access (e.g. \"free\" versus \"paid\") offered, and to whom. The processing of the metadata to determine how and where the resource should be accessed, and the provision of the resource to the user, are performed in a separate step, called \"resolution\", using a Resolver, a server which may be different from the ones involved in exchanging the handle for the metadata. Unlike URLs, which may become invalid if the metadata embedded within them becomes invalid, handles do not become invalid and do not need to change when locations or other metadata attributes change. This helps to prevent link rot , as changes in the information resource (such as location) need only be reflected in changes to the metadata, rather than in changes in every reference to the resource. Each handle may have its own administrator and administration of the handles can be done in a distributed environment, similar to DNS domain names. The name-to-value bindings may also be secured, both via signatures to verify the data and via challenge response to verify the transmission of the data, allowing handles to be used in trust management applications. It is possible for the same underlying information resource to be associated with multiple handles, as when two university libraries generate handles (and therefore possibly different sets of metadata) for the same book. The Handle System is compatible with the Domain Name System (DNS), but does not require it, unlike persistent identifiers such as PURLs or ARKs , which are similar to handles, but which utilise domain names. However, unlike these domain-name based approaches, handles do require a separate prefix registration process and handle servers separate from the domain name servers. Handles can be used natively, or expressed as Uniform Resource Identifiers (URIs) through a namespace within the info URI scheme ; [ 9 ] [ 10 ] for example, 20.1000/100 may be written as the URI, info:hdl/20.1000/100 . Some Handle System namespaces, such as Digital Object Identifiers, are \"info:\" URI namespaces in their own right; for example, info:doi/10.1000/182 is another way of writing the handle for the current revision of the DOI Handbook [ 11 ] as a URI. Some Handle System namespaces define special presentation rules. For example, Digital Object Identifiers , which represent a high percentage of the extant handles, are usually presented with a \"doi:\" prefix: doi:10.1000/182 . Any Handle may be expressed as a Uniform Resource Locator (URL) through the use of the generic HTTP proxy server : [ 12 ] Some Handle-based systems offer an HTTP proxy server that is intended for use with their own system such as: Implementation of the Handle System consists of Local Handle Services, each of which is made up of one or more sites that provide the servers that store specific handles. The Global Handle Registry is a unique Local Handle Service which stores information on the prefixes (also known as naming authorities) within the Handle System and can be queried to find out where specific handles are stored on other Local Handle Services within this distributed system. The Handle System website provides a series of implementation tools, notably the HANDLE.NET Software [ 13 ] and HANDLE.NET Client Libraries. [ 14 ] Handle clients can be embedded in end user software (e.g., a web browser) or in server software (e.g., a web server) and extensions are already available for Adobe Acrobat [ 15 ] and Firefox . [ 16 ] Handle client software libraries are available in both C and Java. Some applications have developed specific add-on tools, e.g., for the DOI System. [ 17 ] The interoperable network of distributed handle resolver servers (also known as the Proxy Server System) are linked through a Global Resolver (which is one logical entity though physically decentralised and mirrored). Users of Handle System technology obtain a handle prefix created in the Global Handle Registry. The Global Handle Registry maintains and resolves the prefixes of locally maintained handle services. Any local handle service can, therefore, resolve any handle through the Global Resolver. Handles (identifiers) are passed by a client, as a query of the naming authority/prefix, to the Handle System's Global Handle Registry (GHR). The GHR responds by sending the client the location information for the relevant Local Handle Service (which may consist of multiple servers in multiple sites); a query is then sent to the relevant server within the Local Handle Service. The Local Handle Service returns the information needed to acquire the resource, e.g., a URL which can then be turned into an HTTP redirect. (Note: if the client already has information on the appropriate LHS to query, the initial query to GHR is omitted) Though the original model from which the Handle System derives dealt with management of digital objects, the Handle System does not mandate any particular model of relationships between the identified entities, nor is it limited to identifying only digital objects: non-digital entities may be represented as a corresponding digital object for the purposes of digital object management. Some care is needed in the definition of such objects and how they relate to non-digital entities; there are established models that can aid in such definitions e.g., Functional Requirements for Bibliographic Records (FRBR) , CIDOC CRM , and indecs content model . Some applications have found it helpful to marry such a framework to the handle application: for example, the Advanced Distributed Learning (ADL) Initiative [ 18 ] brings together Handle System application with existing standards for distributed learning content, using a Shareable Content Object Reference Model (SCORM), [ 19 ] and the Digital Object Identifier (DOI) system implementation of the Handle System has adopted it together with the indecs framework to deal with semantic interoperability . The Handle System also makes explicit the importance of organizational commitment to a persistent identifier scheme, but does not mandate one model for ensuring such commitment. Individual applications may choose to establish their own sets of rules and social infrastructure to ensure persistence (e.g., when used in the DSpace application, and the DOI application). [ 20 ] The Handle system is designed to meet the following requirements to contribute to persistence [ 21 ] The identifier string: The identifier resolution mechanism: Among the objects that are currently identified by handles are journal articles, technical reports, books, theses and dissertations, government documents, metadata, distributed learning content, and data sets. Handles are being used in digital watermarking applications, GRID applications, repositories, and more. Although individual users may download and use the HANDLE.NET software independently, many users have found it beneficial to collaborate in developing applications in a federation, using common policy or additional technology to provide shared services. As one of the first persistent identifier schemes, the Handle System has been widely adopted by public and private institutions and proven over several years. (See Paradigm, Persistent identifiers.) [ 22 ] Handle System applications may use handles as simple persistent identifiers (as most commonly used, to resolve to the current URL of an object), or may choose to take advantage of other features. Its support for the simultaneous return as output of multiple pieces of current information related to the object, in defined data structures, enables priorities to be established for the order in which the multiple resolutions will be used. Handles can, therefore, resolve to different digital versions of the same content, to mirror sites, or to different business models (pay vs. free, secure vs. open, public vs. private). They can also resolve to different digital versions of differing content, such as a mix of objects required for a distance-learning course. There are thousands of handle services running today, located in 71 countries, on 6 continents; over 1000 of them run at universities and libraries. Handle services are being run by user federations, national laboratories, universities, computing centers, libraries (national and local), government agencies, contractors, corporations, and research groups. Major publishers use the Handle System for persistent identification of commercially traded and Open Access content through its implementation with the Digital Object Identifier (DOI) system . The number of prefixes, which allow users to assign handles, is growing and stands at over 12,000 as of early 2014. There are six top-level Global Handle Registry servers that receive (on average) 68 million resolution requests per month. Proxy servers known to CNRI, passing requests to the system on the Web, receive (on average) 200 million resolution requests per month. (Statistics from Handle Quick Facts.) In 2010, CNRI and ITU (International Telecommunication Union) entered into an agreement to collaborate on use of the Handle System (and the Digital Object Architecture more generally) and are working on the specific details of that collaboration; in April 2009 ITU listed the Handle System as an \"emerging trend\". [ 23 ] Handle System, HANDLE.NET and Global Handle Registry are trademarks of the Corporation for National Research Initiatives (CNRI), a non-profit research and development corporation in the US. The Handle System is the subject of patents by CNRI, which licenses its Handle System technology through a public license, [ 24 ] similar to an open source license, in order to enable broader use of the technology. Handle System infrastructure is supported by prefix registration and service fees, with the majority coming from single prefix holders. The largest current single contributor is the International DOI Foundation . The Public License allows commercial and non-commercial use at low cost of both its patented technology and the reference implementation of the software, and allows the software to be freely embedded in other systems and products. A Service Agreement [ 5 ] is also available for users who intend to provide identifier or resolution services using the Handle System technology under the Handle System public license. The Handle System represents several components of a long-term digital object architecture. In January 2010 CNRI released its general-purpose Digital Object Repository software, [ 25 ] another major component of this architecture. More information [ 26 ] about the release, including protocol specification, source code and ready-to-use system, clients and utilities, is available. [ 27 ] [ 28 ]",
    "links": [
      "Metadata",
      "Archival Resource Key",
      "Adobe Acrobat",
      "Functional Requirements for Bibliographic Records",
      "Doi (identifier)",
      "Persistent URL",
      "Permalink",
      "World Wide Web",
      "Digital watermarking",
      "Domain Name System",
      "HTTP proxy",
      "Semantic interoperability",
      "Internet Engineering Task Force",
      "Software license",
      "Firefox",
      "Electronic Publishing",
      "Hypertext",
      "Resource Description Framework",
      "Reference implementation",
      "Global Research Identifier Database",
      "International DOI Foundation",
      "Link rot",
      "Request for Comments",
      "Uniform Resource Identifiers",
      "Indecs Content Model",
      "ISSN (identifier)",
      "Digital Library",
      "Unicode",
      "Handle (computing)",
      "Persistent Uniform Resource Locator",
      "Uniform Resource Locator",
      "Open architecture",
      "DSpace",
      "UCS-2",
      "Digital object identifier",
      "Bob Kahn",
      "Digital Object Architecture",
      "ITU",
      "Persistent identifier",
      "Defense Advanced Research Projects Agency",
      "Info URI scheme",
      "Institutional Repository",
      "Uniform Resource Name",
      "Wikidata",
      "OpenURL",
      "University of Göttingen",
      "Linked Data",
      "Corporation for National Research Initiatives",
      "D-Lib Magazine",
      "Semantic Web",
      "University of Leicester",
      "CIDOC CRM",
      "Open standard"
    ]
  },
  "Information seeking": {
    "url": "https://en.wikipedia.org/wiki/Information_seeking",
    "title": "Information seeking",
    "content": "Information seeking is the process or activity of attempting to obtain information in both human and technological contexts. [ 1 ] Information seeking is related to, but different from, information retrieval (IR). Traditionally, IR tools have been designed for IR professionals to enable them to effectively and efficiently retrieve information from a source. It is assumed that the information exists in the source and that a well-formed query will retrieve it (and nothing else). It has been argued that laypersons' information seeking on the internet is very different from information retrieval as performed within the IR discourse. Yet, internet search engines are built on IR principles. Since the late 1990s a body of research on how casual users interact with internet search engines has been forming, but the topic is far from fully understood. IR can be said to be technology-oriented, focusing on algorithms and issues such as precision and recall . Information seeking may be understood as a more human-oriented and open-ended process than information retrieval. In information seeking, one does not know whether there exists an answer to one's query, so the process of seeking may provide the learning required to satisfy one's information need . Much library and information science (LIS) research has focused on the information-seeking practices of practitioners within various fields of professional work. Studies have been carried out into the information-seeking behaviors of librarians, [ 2 ] academics, [ 3 ] medical professionals, [ 4 ] engineers, [ 5 ] lawyers [ 6 ] [ 7 ] and mini-publics [ 8 ] (among others). Much of this research has drawn on the work done by Leckie, Pettigrew (now Fisher) and Sylvain, who in 1996 conducted an extensive review of the LIS literature (as well as the literature of other academic fields) on professionals' information seeking. The authors proposed an analytic model of professionals' information seeking behaviour, intended to be generalizable across the professions, thus providing a platform for future research in the area. The model was intended to \"prompt new insights... and give rise to more refined and applicable theories of information seeking\" (1996, p. 188). The model has been adapted by Wilkinson (2001) who proposes a model of the information seeking of lawyers. Recent studies in this topic address the concept of information-gathering that \"provides a broader perspective that adheres better to professionals' work-related reality and desired skills.\" [ 9 ] (Solomon & Bronstein, 2021). A variety of theories of information behavior – e.g. Zipf 's Principle of Least Effort , Brenda Dervin 's Sense Making, Elfreda Chatman 's Life in the Round – seek to understand the processes that surround information seeking. In addition, many theories from other disciplines have been applied in investigating an aspect or whole process of information seeking behavior. [ 10 ] [ 11 ] A review of the literature on information seeking behavior shows that information seeking has generally been accepted as dynamic and non-linear (Foster, 2005; Kuhlthau 2006). People experience the information search process as an interplay of thoughts, feelings and actions ( Kuhlthau, 2006 ). Donald O. Case (2007) also wrote a good book that is a review of the literature. Information seeking has been found to be linked to a variety of interpersonal communication behaviors beyond question-asking, to include strategies such as candidate answers. Robinson's (2010) [ 12 ] research suggests that when seeking information at work, people rely on both other people and information repositories (e.g., documents and databases), and spend similar amounts of time consulting each (7.8% and 6.4% of work time, respectively; 14.2% in total). However, the distribution of time among the constituent information seeking stages differs depending on the source. When consulting other people, people spend less time locating the information source and information within that source, similar time understanding the information, and more time problem solving and decision making, than when consulting information repositories. Furthermore, the research found that people spend substantially more time receiving information passively (i.e., information that they have not requested) than actively (i.e., information that they have requested), and this pattern is also reflected when they provide others with information. The concepts of information seeking, information retrieval, and information behaviour are objects of investigation of information science . Within this scientific discipline a variety of studies has been undertaken analyzing the interaction of an individual with information sources in case of a specific information need , task, and context. The research models developed in these studies vary in their level of scope. Wilson (1999) therefore developed a nested model of conceptual areas, which visualizes the interrelation of the here mentioned central concepts. Wilson defines models of information behavior to be \"statements, often in the form of diagrams, that attempt to describe an information-seeking activity, the causes and consequences of that activity, or the relationships among stages in information-seeking behaviour\" (1999: 250).",
    "links": [
      "Library and information science",
      "Precision (information retrieval)",
      "Source text",
      "Information access",
      "Cultural studies",
      "Intellectual property",
      "Information society",
      "Categorization",
      "Carol Kuhlthau",
      "Recall (information retrieval)",
      "Doi (identifier)",
      "Taxonomy",
      "Information architecture",
      "Elfreda Chatman",
      "Informatics",
      "Information need",
      "FBI Seeking Information – War on Terrorism list",
      "Social information seeking",
      "S2CID (identifier)",
      "Browse",
      "CiteSeerX (identifier)",
      "Censorship",
      "Outline of information science",
      "Information behavior",
      "Information retrieval",
      "Knowledge organization",
      "Collaborative information seeking",
      "ISBN (identifier)",
      "Information seeking behavior",
      "Intellectual freedom",
      "Science and technology studies",
      "Memory",
      "Bibliometrics",
      "Information foraging",
      "PMID (identifier)",
      "Brenda Dervin",
      "George Kingsley Zipf",
      "Privacy",
      "Principle of least effort",
      "Onboarding",
      "Data modeling",
      "Thomas D. Wilson",
      "Ontology (information science)",
      "Layman",
      "Quantum information science",
      "Philosophy of information",
      "Algorithm",
      "Preservation (library and archival science)",
      "Information management",
      "Computer data storage",
      "Information science",
      "Information technology",
      "Library classification"
    ]
  },
  "Nicholas Jardine": {
    "url": "https://en.wikipedia.org/wiki/Nicholas_Jardine",
    "title": "Nicholas Jardine",
    "content": "Nicholas Jardine FBA (born 4 September 1943) is a British mathematician, philosopher of science and its history, historian of astronomy and natural history, and amateur mycologist . He is Emeritus Professor at the Department of History and Philosophy of Science (HPS) at the University of Cambridge . [ 1 ] Jardine was educated at Monkton Combe School in Somerset and read natural sciences at King's College, Cambridge . He then worked as a King's College and Royal Society Research Fellow on the automation of classification and information retrieval and its applications to biological taxonomy and diagnosis. In 1975 he moved to Darwin College, Cambridge and to the Department of History and Philosophy of Science. Since then he has developed a question-based pragmatic philosophy of science (inspired by the work of Ian Hacking ), as well as studying the history of early-modern astronomy and natural history, and reflecting on the methodology of the history of the sciences. From 1987–2011 he was Senior Editor of Studies in History and Philosophy of Science and from 1998 of Studies in History and Philosophy of Biological and Biomedical Sciences. Since his retirement in 2010 he has been Senior Consultant to the Cambridge Scientific Heritage Project (2010–13) and principal investigator of the project Diagrams, Figures and the Transformation of Astronomy, 1450–1650 (2008–14). [ 2 ] Jardine was a founding member in 1988 of the popular research seminar \"The Cabinet of Natural History (Cambridge Group for the History of Natural History and the Environmental Sciences)\". This is organised by staff and students of the Department of History and Philosophy of Science, and in term time it holds weekly seminars led by academic speakers. Jardine is a keen amateur mycologist and for over twenty years has led the annual HPS fungus hunt. [ 3 ]",
    "links": [
      "International Plant Names Index",
      "Author citation (botany)",
      "King's College, Cambridge",
      "Fellow of the British Academy",
      "Department of History and Philosophy of Science, Cambridge",
      "Doi (identifier)",
      "Fungus",
      "S2CID (identifier)",
      "JSTOR (identifier)",
      "Mycology",
      "Botanical name",
      "List of botanists by author abbreviation (A)",
      "Darwin College, Cambridge",
      "Kenneth L. Taylor",
      "ISBN (identifier)",
      "Roy Porter",
      "John Raven",
      "Monkton Combe School",
      "Somerset",
      "Emeritus Professor",
      "University of Cambridge",
      "Ian Hacking",
      "James A. Secord"
    ]
  },
  "Sergey Brin": {
    "url": "https://en.wikipedia.org/wiki/Sergey_Brin",
    "title": "Sergey Brin",
    "content": "Sergey Mikhailovich Brin [ a ] (Russian: Сергей Михайлович Брин ; born August 21, 1973) is an American computer scientist and businessman who co-founded Google with Larry Page . He was the president of Google's parent company, Alphabet Inc. , until stepping down from the role on December 3, 2019. [ 1 ] He and Page remain at Alphabet as co-founders, controlling shareholders , and board members . Brin is a centibillionaire and among the richest people in the world . [ 2 ] [ 3 ] Brin immigrated to the United States from the Soviet Union at the age of six. He earned his bachelor's degree at the University of Maryland, College Park , following in his father's and grandfather's [ 4 ] [ 5 ] footsteps by studying mathematics as well as computer science. After graduation, in September 1993, he enrolled in Stanford University to pursue a PhD in computer science. There he met Page, with whom he built a web search engine . The program became popular at Stanford, and he discontinued his PhD studies to start Google in Susan Wojcicki 's garage in Menlo Park . [ 6 ] In December 2023, he came out of retirement to contribute to AI research at Alphabet Inc. . [ 7 ] Sergey Mikhailovich Brin was born into a Jewish family on August 21, 1973, in the Russian city of Moscow . Russia was a constituent republic of the Soviet Union at the time of his birth. [ 8 ] Both of Brin's parents, Mikhail and Eugenia Brin (1948–2024), had graduated from Moscow State University (MSU). [ 9 ] His father is a retired mathematics professor at the University of Maryland , [ 10 ] and his mother was a researcher at NASA 's Goddard Space Flight Center . [ 11 ] [ 12 ] [ 13 ] The Brin family lived in a three-room apartment in central Moscow, which they also shared with Sergey's paternal grandmother. [ 11 ] In 1977, after his father returned from a mathematics conference in Warsaw, Poland, Mikhail Brin announced because of anti-Semitism in Soviet Union, that it was time for the family to emigrate. [ 11 ] [ 14 ] They formally applied for their exit visa in September 1978, and as a result, his father was \"promptly fired\". For related reasons, his mother had to leave her job. For the next eight months, without any steady income, they were forced to take on temporary jobs as they waited, afraid their request would be denied as it was for many refuseniks . In May 1979, they were granted their official exit visas and were allowed to leave the country. [ 11 ] The Brin family lived in Vienna and Paris while Mikhail Brin secured a teaching position at the University of Maryland with help from Anatole Katok . During this time, the Brin family received support and assistance from the Hebrew Immigrant Aid Society . They arrived in the United States on October 25, 1979. [ 11 ] [ 15 ] He attended Eleanor Roosevelt High School . [ 11 ] In September 1990, Brin enrolled in the University of Maryland, where he received his Bachelor of Science from the Department of Computer Science in 1993 with honors in computer science and high honors in mathematics at the age of 19. [ 16 ] In 1993, he interned at Wolfram Research , the developers of Mathematica . [ 16 ] Brin began his graduate study in computer science at Stanford University on a graduate fellowship from the National Science Foundation , receiving an M.S. in computer science in 1995. [ 17 ] As of 2008 [update] , he was on leave from his PhD studies at Stanford. [ 18 ] During an orientation for new students at Stanford, he met Larry Page . The two men seemed to disagree on most subjects, but after spending time together they \"became intellectual soul-mates and close friends.\" Brin's focus was on developing data mining systems while Page's was on extending \"the concept of inferring the importance of a research paper from its citations in other papers\". [ 19 ] Together, they authored a paper titled \"The Anatomy of a Large-Scale Hypertextual Web Search Engine\". [ 20 ] To convert the backlink data gathered by Backrub's web crawler into a measure of importance for a given web page, Brin and Page developed the PageRank algorithm , and realized that it could be used to build a search engine far superior to those existing at the time. [ 21 ] The new algorithm relied on a new kind of technology that analyzed the relevance of the backlinks that connected one Web page to another, and allowed the number of links and their rank, to determine the rank of the page. [ 22 ] Combining their ideas, they began utilizing Page's dormitory room as a machine laboratory, and extracted spare parts from inexpensive computers to create a device that they used to connect the nascent search engine with Stanford's broadband campus network. [ 21 ] After filling Page's room with equipment, they then converted Brin's dorm room into an office and programming center, where they tested their new search engine designs on the web. The rapid growth of their project caused Stanford's computing infrastructure to experience problems. [ 23 ] Page and Brin used Page's basic HTML programming skills to set up a simple search page for users, as they did not have a web page developer to create anything visually elaborate. They also began using any computer part they could find to assemble the necessary computing power to handle searches by multiple users. As their search engine grew in popularity among Stanford users, it required additional servers to process the queries. In August 1996, the initial version of Google was made available on the Stanford Web site. [ 21 ] By early 1997, the Backrub page described the state as follows: BackRub already exhibited the rudimentary functions and characteristics of a search engine: a query input was entered and it provided a list of backlinks ranked by importance. Page recalled: \"We realized that we had a querying tool. It gave you a good overall ranking of pages and ordering of follow-up pages.\" [ 25 ] Page said that in mid-1998 they finally realized the further potential of their project: \"Pretty soon, we had 10,000 searches a day. And we figured, maybe this is really real.\" [ 23 ] Some compared Page and Brin's vision to the impact of Johannes Gutenberg , the inventor of modern printing: In 1440, Johannes Gutenberg introduced Europe to the mechanical printing press, printing Bibles for mass consumption. The technology allowed for books and manuscripts‍—‌originally replicated by hand‍—‌to be printed at a much faster rate, thus spreading knowledge and helping to usher in the European Renaissance ... Google has done a similar job. [ 26 ] The comparison was also noted by the authors of The Google Story : \"Not since Gutenberg ... has any new invention empowered individuals, and transformed access to information, as profoundly as Google.\" [ 27 ] Also, not long after the two \"cooked up their new engine for web searches, they began thinking about information that was at the time beyond the web,\" such as digitizing books and expanding health information. [ 23 ] In June 2008, Brin invested $4.5 million in Space Adventures , a Virginia -based space tourism company. [ 28 ] Brin and Page jointly own a customized Boeing 767–200 and a Dassault/Dornier Alpha Jet , [ 29 ] and pay $1.3 million a year to house them and two Gulfstream V jets owned by Google executives at Moffett Federal Airfield . The aircraft has scientific equipment installed by NASA to allow experimental data to be collected in flight. [ 30 ] [ 31 ] Brin is a backer of LTA Research & Exploration LLC, an airship maker company. [ 32 ] In October 2023, LTA's 124-meter long flagship, Pathfinder 1, became the largest airship since the Hindenburg to receive clearance for flight testing, permitted over the boundaries of Moffett Field and neighboring Palo Alto Airport 's airspaces, at a height of up to 460 meters. [ 33 ] Brin was raised Jewish, but is not religious. [ 34 ] [ better source needed ] In May 2007, Brin married biotech analyst and entrepreneur Anne Wojcicki in the Bahamas . [ 35 ] [ 36 ] They had a son in late 2008 and a daughter in late 2011. [ 37 ] In August 2013, it was announced that Brin and his wife were living separately after Brin had an extramarital affair with a Google Glass colleague. [ 38 ] [ 39 ] [ 40 ] In June 2015, Brin and Wojcicki finalized their divorce. [ 41 ] On November 7, 2018, he married Nicole Shanahan , a legal tech founder. [ 42 ] They have a daughter, born in late 2018. [ 43 ] Brin and Shanahan separated on December 15, 2021, and Brin filed for divorce on January 4, 2022. [ 42 ] In summer 2023, the divorce was finalized. [ 44 ] The Wall Street Journal reported that a reason for the breakup was a \"brief affair\" in 2021 between Shanahan and Elon Musk . [ 45 ] Brin's mother, Eugenia, has been diagnosed with Parkinson's disease . In 2008, he decided to make a donation to the University of Maryland School of Medicine , where his mother has received treatment. [ 46 ] According to Forbes , Brin has donated over $1 billion to fund research on the disease. [ 47 ] Brin and Wojcicki, although separated, jointly ran The Brin Wojcicki Foundation until 2014. Since then, Brin has used the Sergey Brin Family Foundation and a donor-advised fund for his philanthropic giving. [ 48 ] They donated extensively to The Michael J. Fox Foundation and in 2009 gave $1 million to support the Hebrew Immigrant Aid Society . [ 15 ] Brin is a donor to Democratic Party candidates and organizations, having donated $5,000 to Barack Obama 's 2012 reelection campaign and $30,800 to the DNC . [ 49 ] Brin attended the second inauguration of Donald Trump , sitting alongside Donald Trump supporters and other tech moguls. [ 50 ] [ 51 ] he was a featured speaker at the World Economic Forum and the Technology, Entertainment and Design Conference. ... PC Magazine has praised Google in the Top 100 Web Sites and Search Engines (1998) and awarded Google the Technical Excellence Award, for Innovation in Web Application Development in 1999. In 2000, Google earned a Webby Award , a People's Voice Award for technical achievement, and in 2001, was awarded Outstanding Search Service, Best Image Search Engine, Best Design, Most Webmaster Friendly Search Engine, and Best Search Feature at the Search Engine Watch Awards. [ 61 ] Notes References",
    "links": [
      "List of mergers and acquisitions by Alphabet",
      "Russian Soviet Federative Socialist Republic",
      "Google Moderator",
      "Google I/O",
      "History of YouTube",
      "Chromebox",
      "Mosley v SARL Google",
      "Google Voice",
      "YouTube Music Awards",
      "Sawzall (programming language)",
      "Titan Security Key",
      "Tenor (website)",
      "Gizmo5",
      "Angular (web framework)",
      "Gmail interface",
      "Protocol Buffers",
      "Bazel (software)",
      "C-SPAN",
      "Marconi Prize",
      "Lighthouse (software)",
      "Android Marshmallow",
      "Google Health",
      "Crashlytics",
      "Kaltix",
      "Google Swiffy",
      "Google Schemer",
      "Google Flights",
      "Where Is My Train",
      "Rick Osterloh",
      "Google News & Weather",
      "Google Opinion Rewards",
      "Pixel 7",
      "Actifio",
      "Reqwireless",
      "Google Earth",
      "Google Translate",
      "Google PowerMeter",
      "Blockly",
      "Digital News Initiative",
      "James R. Thompson Center",
      "People Cards",
      "Googlefight",
      "Wing Aviation",
      "Google Business Groups",
      "Google services outages",
      "Garcia v. Google, Inc.",
      "Timnit Gebru",
      "2011 Slovenian YouTube incident",
      "Gridcentric",
      "Postini",
      "Google.org",
      "Pixel Watch",
      "Waze",
      "PC Magazine",
      "Gson",
      "Actions on Google",
      "Urban Engines",
      "Metaweb",
      "Google logo",
      "Google Antigravity",
      "Jetpack Compose",
      "Quickoffice",
      "Makani (company)",
      "2018 Google data breach",
      "ImageAmerica",
      "AI Overviews",
      "YouTube Premium",
      "Cpplint",
      "Roger W. Ferguson Jr.",
      "Chinchilla (language model)",
      "Forbes",
      "Patronymic",
      "Great Immigrants Award",
      "Business Insider",
      "Google Trends",
      "SageTV",
      "Blogger (service)",
      "Google Expeditions",
      "Google Lively",
      "Pixel 2",
      "Aardvark (search engine)",
      "Google Street View in Europe",
      "Google Question Hub",
      "Biotechnology",
      "Democratic Party (United States)",
      "Hypertext",
      "Google Building Maker",
      "Alphabet Inc",
      "YouTube Next Lab and Audience Development Group",
      "Google Mapathon",
      "Google Search Console",
      "Google Desktop",
      "Google Duo",
      "GRPC",
      "Knowledge Graph (Google)",
      "Google Shopping",
      "Google Sidewiki",
      "Firebase Studio",
      "CiteSeerX (identifier)",
      "Pixel Buds",
      "Meka Robotics",
      "History of Gmail",
      "Find Hub",
      "Nexus 6",
      "Isomorphic Labs",
      "Malseed, Mark",
      "Rachel Whetstone",
      "BigQuery",
      "Google Chrome Frame",
      "Google Analytics",
      "Google Developer Day",
      "Google Web Accelerator",
      "Gboard",
      "List of most-viewed YouTube channels",
      "Moment Magazine",
      "Google Code Jam",
      "Google Apps Script",
      "Google Checkout",
      "Ann Mather",
      "Wolfram Research",
      "Global IP Solutions",
      "Google Optimize",
      "American Fuzzy Lop (software)",
      "Imagen (text-to-image model)",
      "Google Developer Expert",
      "GV (company)",
      "Central Saint Giles",
      "Nexus One",
      "Pixel 8a",
      "Sidewalk Labs",
      "Google Reader",
      "Hal Varian",
      "Urs Hölzle",
      "AlphaGo",
      "Pixel 9 Pro Fold",
      "Bump (application)",
      "Noto fonts",
      "Dassault/Dornier Alpha Jet",
      "Google Messages",
      "Endoxon",
      "Project Zero",
      "AlphaGo versus Fan Hui",
      "Blink (browser engine)",
      "Baseline Study",
      "Dialogflow",
      "Ganeti",
      "Blocking of YouTube videos in Germany",
      "Internet Speech Audio Codec",
      "Pixel 10 Pro Fold",
      "Android Dev Phone",
      "Google Spaces",
      "The Google Story",
      "Judgement of the German Federal Court of Justice on Google's autocomplete function",
      "Relationship between Google and Wikipedia",
      "David Cheriton",
      "Wolfram Mathematica",
      "Android 10",
      "Internet Low Bitrate Codec",
      "Google Ads",
      "BookTube",
      "My Tracks",
      "Google Family Link",
      "The Bahamas",
      "Doodle4Google",
      "Android 11",
      "Softcard",
      "Google Scholar",
      "Apk (file format)",
      "BebaPay",
      "ALTS",
      "OpenSocial",
      "VideoPoet",
      "Pixel 3a",
      ".dev",
      "NSF Graduate Research Fellowship Program",
      "Google Arts & Culture",
      "List of most-viewed YouTube videos",
      "Google bombing",
      "Master of Science",
      "IBM/Google Cloud Computing University Initiative",
      "Unity (cable system)",
      "Moscow State University",
      "Anatole Katok",
      "Borg (cluster manager)",
      "Android Automotive",
      "Vint Cerf",
      "YouTube Space Lab",
      "Google News",
      "Poly (website)",
      "Google Sync",
      "Android Froyo",
      "Impermium",
      "OR-Tools",
      "Panoramio",
      "Joffe v. Google, Inc.",
      "Stackdriver",
      "Flutter (software)",
      "Google Person Finder",
      "Google Buzz",
      "Google Hangouts",
      "Google.by",
      "Pixel 5",
      "Timeline of Google Search",
      "Backlink",
      "The New York Times",
      "Libvpx",
      "Project Ara",
      "Android Ice Cream Sandwich",
      "Jedi Blue",
      "Google Blog Search",
      "GYP (software)",
      "Oyster (company)",
      "Nest Thermostat",
      "Schema.org",
      "Google Directory",
      "Google Offers",
      "Algorithm",
      "Adscape",
      "Photomath",
      "Fastboot",
      "Fflick",
      "HTML",
      "Boeing 767",
      "Democratic National Committee",
      "Nexus S",
      "Google Chrome Experiments",
      "Chromecast",
      "DoubleClick",
      "Google AdSense",
      "Pixel 7a",
      "T5 (language model)",
      "Google Now",
      "Google Video",
      "Google's Ideological Echo Chamber",
      "United States v. Google Inc.",
      "Google Public DNS",
      "Rooting (Android)",
      "Rajen Sheth",
      "Sitemaps",
      "Wayback Machine",
      "Google Brain",
      "Dragonfly (search engine)",
      "2020 Korean YouTube backdoor advertising controversy",
      "Socratic (Google)",
      "Trendalyzer",
      "Tesseract (software)",
      "NASA",
      "YouTube Rewind",
      "Pixel 6",
      "Urchin (software)",
      "ISSN (identifier)",
      "Live Transcribe",
      "Android recovery mode",
      "Project Nimbus",
      "Accelerated Linear Algebra",
      "Green Throttle Games",
      "ISBN (identifier)",
      "Matter (standard)",
      "Google Vids",
      "Pixel C",
      "V8 (JavaScript engine)",
      "Controlling shareholder",
      "YouTube Music",
      "Android Honeycomb",
      "Google Chrome App",
      "Google Fit",
      "Google Currents (social app)",
      "Marconi Foundation",
      "Google Guice",
      "Dart (programming language)",
      "Google Drawings",
      "Google Lens",
      "List of most-liked YouTube videos",
      "List of most-subscribed YouTube channels",
      "Gato (DeepMind)",
      "Orkut",
      "Me at the zoo",
      "BeyondCorp",
      "Frances Arnold",
      "Google Notebook",
      "Nearby Share",
      "Google Data Protocol",
      "Google Lunar X Prize",
      "Firebase Cloud Messaging",
      "Keyhole Markup Language",
      "Android Jelly Bean",
      "Pixel 4",
      "Side project time",
      "Gulfstream V",
      "Gayglers",
      "Google Primer",
      "AlphaFold",
      "The Billion Dollar Code",
      "ChromeOS Flex",
      "Chief executive officer",
      "Google Maps",
      "AppSheet",
      "Project Sunroof",
      "Google Docs",
      "Google Clips",
      "AlphaGeometry",
      "Google Search",
      "Pixel 5a",
      "List of Android smartphones",
      "Google Fonts",
      "Read Along",
      "Quick, Draw!",
      "Google Patents",
      "Nexus 6P",
      "Privacy concerns with Google",
      "2012 Kohistan video case",
      "WebM",
      "Crowdsource (app)",
      "Google Contact Lens",
      "Word Lens",
      "S2CID (identifier)",
      "AlphaGo Zero",
      "Nest Wifi",
      "Titan Aerospace",
      "Sanjay Ghemawat",
      "Federated Learning of Cohorts",
      "SafeSearch",
      "Gemma (language model)",
      "Reactions to Innocence of Muslims",
      "Inbox by Gmail",
      "List of most-downloaded Google Play applications",
      "Rescuecom Corp. v. Google Inc.",
      "Legal technology",
      "Google Street View in the United States",
      "United States v. Google LLC (2023)",
      "Google Pay Send",
      "Comparison of Google Nexus tablets",
      "YouTube Studio",
      "Content ID",
      "The Creepy Line",
      "Google Pay (payment method)",
      "Liquid Galaxy",
      "Verily",
      "Open Location Code",
      "ReCAPTCHA",
      "AppJet",
      "G-Day",
      "GOOG-411",
      "YouTube Creator Awards",
      "Android Developer Day",
      "Street View Trusted",
      "Google litigation",
      "List of most-viewed Pakistani YouTube videos",
      "Pixelbook",
      "RankBrain",
      "Nexus 7 (2013)",
      "Pixelbook Go",
      "Googleplex",
      "DigiPivot",
      "Android TV",
      "Dodgeball (service)",
      "Android Oreo",
      "BumpTop",
      "Google Squared",
      "Google Station",
      "Authors Guild, Inc. v. Google, Inc.",
      "Second inauguration of Donald Trump",
      "United States v. Google LLC (2020)",
      "Google Wave Federation Protocol",
      "List of Android TV devices",
      "Google Code Search",
      "Google Mashup Editor",
      "Nexus 10",
      "Google Daydream",
      "Google Fi Wireless",
      "Nexus Q",
      "Carbon (programming language)",
      "YouTube BrandConnect",
      "Google Street View in Israel",
      "CNN/YouTube presidential debates",
      "Google Contributor",
      "Viewdle",
      "Android Debug Bridge",
      "Android KitKat",
      "Google Play",
      "East Slavic naming customs",
      "Chrome Web Store",
      "Barack Obama 2012 presidential campaign",
      "Google Science Fair",
      "Comparison of Google Nexus smartphones",
      "Google King's Cross",
      "Google Doodle",
      "Googlebot",
      "The Economist",
      "President (corporate title)",
      "Site reliability engineering",
      "List of YouTube features",
      "YouTube Space",
      "San Francisco tech bus protests",
      "Illegal flower tribute",
      "Shirley M. Tilghman",
      "Magic Cat Academy",
      "Android 14",
      "FeedBurner",
      "GeoEye-1",
      "Google WiFi",
      "Google OnHub",
      "Google Cloud Shell",
      "Chief investment officer",
      "Pixel Slate",
      "YouTube copyright issues",
      "Binoculars Building",
      "Comparison of Android Go products",
      "Google Flu Trends",
      "Google Workspace",
      "Googlization",
      "Android Lollipop",
      "Matt Cutts",
      "Google Catalogs",
      "Google Labs",
      "YouTube moderation",
      "Neal Mohan",
      "Chief legal officer",
      "Google Web Toolkit",
      "Google Docs Editors",
      "Cameyo",
      "Android version history",
      "Google Cloud Search",
      "Google Play Services",
      "Gemini (language model)",
      "Google Base",
      "Nexus 5",
      "Pixel Camera",
      "Academy of Achievement",
      "Google Chat",
      "Nano Banana",
      "Objectives and key results",
      "Beauty YouTuber",
      "Made with Code",
      "Sensorvault",
      "Google Insights for Search",
      "Chromebook Pixel",
      "Google Student Ambassador Program",
      "Google Web Server",
      "Frank v. Gaos",
      "Skia Graphics Engine",
      "Google Public Data Explorer",
      "Project Shield",
      "Attention Is All You Need",
      "Google Chrome",
      "StudyTube",
      "Wavii",
      "Google Street View in Asia",
      "Web Open Font Format",
      "Google Me (Teyana Taylor song)",
      "Quest Visual",
      "VP3",
      "Google Closure Tools",
      "Chelsea Market",
      "Google Page Creator",
      "JAX (software)",
      "Jigsaw (company)",
      "Shoploop",
      "Fitbit",
      "GreenBorder",
      "YouTube Symphony Orchestra",
      "Typhoon Studios",
      "The World's Billionaires",
      "RechargeIT",
      "111 Eighth Avenue",
      "PageRank",
      "Epic Games v. Google",
      "Google Hummingbird",
      "Google Nest (smart speakers)",
      "IEEE",
      "Sycamore processor",
      "Google Programmable Search Engine",
      "Google Security Operations",
      "VTuber",
      "RenderScript",
      "Google Maps pin",
      "Bayshore Global Management",
      "Android One",
      "Chrome Remote Desktop",
      "Google barges",
      "Sitelink",
      "Google Street View in Argentina",
      "List of most-viewed French music videos on YouTube",
      "Lyra (codec)",
      "Invite Media",
      "Android XR",
      "YouTube Awards",
      "Google Pay (mobile app)",
      "Google Wave",
      "Social impact of YouTube",
      "Bachelor of Science",
      "Jeff Dean",
      "Pixel Watch 3",
      "Viacom International, Inc. v. YouTube, Inc.",
      "Freebase (database)",
      "YouTube Shorts",
      "Astro Teller",
      "Google App Maker",
      "Google Kythe",
      "Inception (deep learning architecture)",
      "MapReduce",
      "Reunion (advertisement)",
      "Republics of the Soviet Union",
      "AV1",
      "Google Map Maker",
      "Google Street View",
      "Anne Wojcicki",
      "Vanity Fair (magazine)",
      "Google Native Client",
      "Sidewalk Toronto",
      "Android Developer Challenge",
      "Anat Ashkenazi",
      "Google Input Tools",
      "HIAS",
      "RightsFlow",
      "BufferBox",
      "Google Sheets",
      "Eleanor Roosevelt High School (Maryland)",
      "Google Web Light",
      "XLNet",
      "In the Plex",
      "Android Developer Lab",
      "Google Gesture Search",
      "List of Google Play edition devices",
      "List of most-viewed Arabic music videos on YouTube",
      "Perfect 10, Inc. v. Amazon.com, Inc.",
      "Citation",
      "Google Image Swirl",
      "Parkinson's disease",
      "Sparrow (chatbot)",
      "YouTube TV",
      "YouTube Live",
      "Accelerated Mobile Pages",
      "Transformer (deep learning architecture)",
      ".app (top-level domain)",
      "Pixel (1st generation)",
      "Rosetta Stone Ltd. v. Google, Inc.",
      "Pixel Tablet",
      "PostRank",
      "Menlo Park, California",
      "Google Code-in",
      "Google Play Music",
      "Pac-Man Google Doodle",
      "ARCore",
      "Reply (Google)",
      "Sparrow (email client)",
      "Google Street View coverage",
      "Wear OS",
      "Google Hacks",
      "Googleshare",
      "Sergey Brin Family Foundation",
      "Google Digital Garage",
      "Apache Beam",
      "GLOP",
      "Gerrit (software)",
      "Google Talk",
      "Hibnick v. Google, Inc.",
      "Chrome Zone",
      "Centibillionaire",
      "Google APIs",
      "Nexus 4",
      "Technology Review",
      "Speech Recognition & Synthesis",
      "YouTube Kids",
      "Google Pixel",
      "Google Drive",
      "Google Penguin",
      "Proceratium google",
      "Croscore fonts",
      "Elsagate",
      "Google Behind the Screen",
      "Elon Musk",
      "Grace Hopper (submarine communications cable)",
      "Google Get Your Business Online",
      "Android Donut",
      "Palo Alto Airport",
      "Google App Engine",
      "EfficientNet",
      "Google Stadia",
      "Nexus 7 (2012)",
      "The Michael J. Fox Foundation",
      "Webby Award",
      "Omid Kordestani",
      "Google Public Alerts",
      "Google SearchWiki",
      "Vise, David",
      "Waymo",
      "Google Shell",
      "BreadTube",
      "Alphabet Inc.",
      "Computer scientist",
      "Predictions of the end of Google",
      "Weave (protocol)",
      "G.co",
      "Google Street View in Colombia",
      "World Chess Championship 2024",
      "List of Google products",
      "Space tourism",
      "Gears (software)",
      "Google Nest",
      "YouTuber",
      "MIT App Inventor",
      "Alphabet Workers Union",
      "MIT Technology Review",
      "Google Street View privacy concerns",
      "WebRTC",
      "Google Dataset Search",
      "Innovators Under 35",
      "Google Marketing Platform",
      "AlphaGo versus Lee Sedol",
      "Google Attribution",
      "CapitalG",
      "Google (verb)",
      "R. Martin Chavez",
      "Google for Education",
      "Google Safe Browsing",
      "Susan Wojcicki",
      "Master (software)",
      "Pier 57",
      "Pixel Fold",
      "Vevo",
      "Nik Software",
      "Google One",
      "Google Classroom",
      "Google Fusion Tables",
      "History of Google",
      "Goobuntu",
      "BeatThatQuote.com",
      "Roboto",
      "Space Adventures",
      "WDYL (search engine)",
      "Google Express",
      "MuZero",
      "Wall Street Journal",
      "Anvato",
      "Google Founders' Award",
      "Google Gadgets",
      "Grasshopper (mobile app)",
      "Namebench",
      "Google File System",
      "Criticism of Google",
      "Marratech",
      "Liftware",
      "Amit Singhal",
      "Google Pigeon",
      "Pixel Visual Core",
      "University of Maryland, College Park",
      "Songza",
      "Fast Pair",
      "SafetyNet",
      "TalkBack",
      "Google TV (service)",
      "LZ 129 Hindenburg",
      "Data Transfer Project",
      "Android Things",
      "Google and trade unions",
      "Google Tasks",
      "Bigtable",
      "Business Week",
      "University of Maryland",
      "Google Santa Tracker",
      "Web 2.0",
      "American Academy of Achievement",
      "Google Browser Sync",
      "Google Authenticator",
      "Google matrix",
      "Google Search Appliance",
      "Android software development",
      "Mandiant",
      "Flutter (American company)",
      "Orbitera",
      "Meebo",
      "Android 16",
      "Knol",
      "Pixel Watch 2",
      "Pixel Watch 4",
      "Outline of Google",
      "Sun Ultra series",
      "Android 13",
      "Google Street View in Oceania",
      "Google Spain v AEPD and Mario Costeja González",
      "Google Goggles",
      "Krishna Bharat",
      "GVisor",
      "Google Domains",
      "List of custom Android distributions",
      "List of most-viewed Chinese music videos on YouTube",
      "Airship",
      "Eric Schmidt",
      "Fuchsia (operating system)",
      "Goddard Space Flight Center",
      "Stanford University",
      "Hash Code (programming competition)",
      "Android NDK",
      "Google Me (film)",
      "IE Business School",
      "Johannes Gutenberg",
      "David Drummond (businessman)",
      "FairSearch",
      "AdMob",
      "Google Answers",
      "Google News Archive",
      "Google Maps Road Trip",
      "Google Helpouts",
      "Google Tensor",
      "Gonzalez v. Google LLC",
      "YouTube and privacy",
      "Product Sans",
      "Sound Amplifier",
      "Robot Constitution",
      "WebP",
      "Google Images",
      "Andy Bechtolsheim",
      "Neatx",
      "Piper (source control system)",
      "I'm Feeling Lucky (book)",
      "Google Play Books",
      "Google Fiber",
      "Future of Go Summit",
      "University of Maryland School of Medicine",
      "Smartphone patent wars",
      "Pixel 4a",
      "Quick Share",
      "National Academy of Engineering",
      "Doodle Champion Island Games",
      "List of Fitbit products",
      "Google hacking",
      "Google Silicon Initiative",
      "Google Image Labeler",
      "Google Cast",
      "Google Personalized Search",
      "Google Street View in Canada",
      "Material Design",
      "Google",
      "Moscow",
      "Google AI",
      "History of the Jews in Russia",
      "Google Web Designer",
      "Kaggle",
      "Google Home (platform)",
      "Google Sites",
      "Loon LLC",
      "Diane Greene",
      "St. John's Terminal",
      "Kubernetes",
      "Android Gingerbread",
      "High-Tech Employee Antitrust Litigation",
      "Ray Kurzweil",
      "David Krane",
      "Guetzli",
      "Google Account",
      "Google Cardboard",
      "Alan Eustace",
      "List of YouTubers",
      "Google Play Newsstand",
      "Google Glass",
      "Exit visa",
      "No Tech for Apartheid",
      "Google Voice Search",
      "Booting process of Android devices",
      "Feldman v. Google, Inc.",
      "Solve for X",
      "VisBug",
      "AlphaGo versus Ke Jie",
      "Soviet Union",
      "UTM parameters",
      "Google News Lab",
      "Wiz, Inc.",
      "Google Friend Connect",
      "Honorary degree",
      "Fantastic Adventures scandal",
      "Google Dashboard",
      "Dunant (submarine communications cable)",
      "Surname",
      "Google Cloud Datastore",
      "List of features in Android",
      "Google Registry",
      "Google+",
      "Like.com",
      "YouTube (YouTube channel)",
      "Gmail",
      "Files (Google)",
      "Comparison of Google Pixel smartphones",
      "Google TV (operating system)",
      "Living Stories",
      "Pyra Labs",
      "Google Cloud Platform",
      "Google Cloud Connect",
      "Google Groups",
      "Picnik",
      "Google LLC v Defteros",
      "Go (programming language)",
      ".google",
      "Google Currents (news app)",
      "Linux",
      "Google Data Liberation Front",
      "Google Ad Manager",
      "Google Slides",
      "Google Allo",
      "Google LLC v. Oracle America, Inc.",
      "YouTube headquarters shooting",
      "Google Japanese Input",
      "Google China",
      "Google Street View in South America",
      "ChromeOS",
      "Digital Unlocked",
      "Doi (identifier)",
      "Google Mobile Services",
      "Dinosaur Game",
      "Ram Shriram",
      "Tilt Brush",
      "LaMDA",
      "How Google Works",
      "2015 YouTube Music Awards",
      "Google Toolbar",
      "Android Cupcake",
      "Goddard v. Google, Inc.",
      "Data Commons",
      "Mobilegeddon",
      "Google Street View in Antarctica",
      "Refuseniks",
      "Looker Studio",
      "Caja project",
      "WaveNet",
      "Google for Startups",
      "ITA Software",
      "Ivanpah Solar Power Facility",
      "Apigee",
      "Googlewhack",
      "YouTube",
      "Google Calendar",
      "CrossCheck (project)",
      "YouTube Comedy Week",
      "AlphaGo (film)",
      "Google Chart API",
      "Google Bookmarks",
      "Google Wallet",
      "Snapseed",
      "Android 15",
      "Android Go",
      "Google Assistant",
      "SlickLogin",
      "Google Free Zone",
      "NotebookLM",
      "VP9",
      "National Science Foundation",
      "Sputnik (JavaScript conformance test)",
      "Glass OS",
      "Google Compute Engine",
      "ZygoteBody",
      "Google Latitude",
      "PaLM",
      "Widevine",
      "Dropcam",
      "Google Street View in Chile",
      "Quantum Artificial Intelligence Lab",
      "List of most-viewed Indian YouTube videos",
      "Where on Google Earth is Carmen Sandiego?",
      "Pixel 9a",
      "Search engine manipulation effect",
      "Google Cloud Dataflow",
      "Picasa",
      "Google Nexus",
      "Chromebook",
      "ChromiumOS",
      "Android Runtime",
      "Android Beam",
      "List of Google April Fools' Day jokes",
      "Polymer (library)",
      "Google Fast Flip",
      "Umar Javeed, Sukarma Thapar, Aaqib Javeed vs. Google LLC and Ors.",
      "Ultra HDR",
      "Tango (platform)",
      "Bionic (software)",
      "Is Google Making Us Stupid?",
      "List of Android apps by Google",
      "Toontastic 3D",
      "Google Pack",
      "Sundar Pichai",
      "Android Studio",
      "GLinux",
      "Google tax",
      ".zip (top-level domain)",
      "Google Books Ngram Viewer",
      "Google data centers",
      "Google Affiliate Network",
      "Google Street View in North America",
      "Google Pinyin",
      "Google Store",
      "Google URL Shortener",
      "Google One Pass",
      "Project Nightingale",
      "Google Forms",
      "Google Cloud Storage",
      "AlphaZero",
      "Tony Fadell",
      "Google Developers",
      "Pixel 8",
      "Google Gemini",
      "Android Nougat",
      "Google and the World Brain",
      "Google Finance",
      "Google Translator Toolkit",
      "Google effect",
      "Russia",
      "Google Crisis Response",
      "YouTube VR",
      "Don't be evil",
      "Chief financial officer",
      "Google App Runtime for Chrome",
      "YouTube copyright strike",
      "Moffett Federal Airfield",
      "Android (operating system)",
      "Owlchemy Labs",
      "Lmctfy",
      "Nexus 5X",
      "YouTube Poop",
      "Tez (software)",
      "Bitium",
      "Neverware",
      "Tables (Google)",
      "Alan Mulally",
      "OpenRefine",
      "Google ATAP",
      "Android Auto",
      "Web crawler",
      "Ruth Porat",
      "VirusTotal",
      "Android Eclair",
      "Android 12",
      "Veo (text-to-video model)",
      "Pixel 6a",
      "Google Photos",
      "Paul Otellini",
      "X Development",
      "Google Maps Navigation",
      "Board members",
      "John Doerr",
      "LevelDB",
      "Pixel 10",
      "AlphaStar (software)",
      "Field v. Google, Inc.",
      "Webdriver Torso",
      "PaperofRecord.com",
      "Google Travel",
      "Pixel 3",
      "John L. Hennessy",
      "Google Contacts",
      "Wildfire Interactive",
      "Al Gore",
      "Generative pre-trained transformer",
      "List of YouTube Premium original programming",
      "Google Neural Machine Translation",
      "Nomulus",
      "DreamBooth",
      "Carnegie Corporation of New York",
      "Antitrust cases against Google by the European Union",
      "Neotonic Software",
      "Apture",
      "Arthur D. Levinson",
      "YouTube Select",
      "Rocky Mountain Bank v. Google, Inc.",
      "YouTube in education",
      "AngularJS",
      "Google Play Pass",
      "DigiKavach",
      "VP6",
      "Google Alerts",
      "Jaiku",
      "Nexus 9",
      "Boston Dynamics",
      "Chromebit",
      "Dalvik (software)",
      "2018 Google walkouts",
      "Censorship by Google",
      "Google Cloud Print",
      "Google Keep",
      "Android Cloud to Device Messaging",
      "Andrew Conrad",
      "Chromium (web browser)",
      "Calico (company)",
      "Picasa Web Albums",
      "Server (computing)",
      "Charlie Rose (talk show)",
      "Looker (company)",
      "Google Plugin for Eclipse",
      "Google Play Games",
      "BERT (language model)",
      "Google Quick Search Box",
      "Barack Obama",
      "Spanner (database)",
      "The Internship",
      "Google Surveys",
      "Virginia",
      "Google Workspace Marketplace",
      "Google Cloud Messaging",
      "Political Google bombs in the 2004 U.S. presidential election",
      "Google Charts",
      "VP8",
      "Google Feud",
      "Google Guava",
      "Google DeepMind",
      "Lenz v. Universal Music Corp.",
      "FlatBuffers",
      "Google Meet",
      "Chronicle Security",
      "TensorFlow",
      "Project Iris",
      "World Economic Forum",
      "Nexus Player",
      "Vicarious (company)",
      "YouTube Original Channel Initiative",
      "List of Google Easter eggs",
      "Search engine",
      "Mayfield Mall",
      "Jamboard",
      "ElgooG",
      "Area 120",
      "Patrick Pichette",
      "University of California, Berkeley",
      "Tensor Processing Unit",
      "Google Street View in Africa",
      "Android App Bundle",
      "Pixel 9",
      "Galaxy Nexus",
      "The Washington Post",
      "Akwan Information Technologies",
      "Google Summer of Code",
      "Google Books",
      "Google Developers Live",
      "Android SDK",
      "Privacy Sandbox",
      "Google PageSpeed Tools",
      "List of Stadia games",
      "Web search engine",
      "YouTube Theater",
      "DeGoogle",
      "Firebase",
      "Google Energy",
      "Google Dictionary",
      "Google Me (Kim Zolciak song)",
      "List of Year in Search top searches",
      "Android Pie",
      "Etherpad",
      "IGoogle",
      "Google Panda",
      "2013 YouTube Music Awards",
      "Google Beam",
      "BandPage",
      "List of most-disliked YouTube videos",
      "Android lawn statues",
      "Google, Inc. v. American Blind & Wallpaper Factory, Inc.",
      "Salar Kamangar",
      "Larry Page",
      "The MANIAC",
      "AlphaDev",
      "Exposure Notification",
      "Google Podcasts",
      "MobileNet",
      "Google Takeout",
      "Nicole Shanahan",
      "Google Test",
      "Voice Access",
      "Androidland"
    ]
  },
  "Yahoo! Inc. (1995–2017)": {
    "url": "https://en.wikipedia.org/wiki/Yahoo!_Inc._(1995–2017)",
    "title": "Yahoo! Inc. (1995–2017)",
    "content": "Yahoo! Inc. [ 3 ] was an American multinational technology company headquartered in Sunnyvale, California. Yahoo was founded by Jerry Yang and David Filo in January 1994 and was incorporated on March 2, 1995. [ 4 ] [ 5 ] Yahoo was one of the pioneers of the early internet era in the 1990s. [ 6 ] Marissa Mayer , a former Google executive, was CEO and president of Yahoo from July 2012 until June 2017. [ 7 ] It was globally known for its Web portal , search engine Yahoo! Search , and related services, including Yahoo! Directory , Yahoo! Mail , Yahoo! News , Yahoo! Finance , Yahoo! Groups , Yahoo! Answers , advertising , online mapping , video sharing , fantasy sports, and its social media website. At its height, it was one of the most popular sites in the United States. [ 8 ] According to third-party web analytics providers, Alexa and SimilarWeb , Yahoo! was the highest-read news and media website, with over seven billion views per month, being the sixth most visited website globally in 2016 [ 9 ] [ 10 ] [ 11 ] According to news sources, roughly 700 million people visited Yahoo websites every month. [ 12 ] [ 13 ] Yahoo itself claimed it attracted \"more than half a billion consumers every month in more than 30 languages\". [ 14 ] Once the most popular website in the U.S., Yahoo slowly started to decline since the late 2000s, [ 15 ] and on February 21, 2017, Verizon Communications announced its intent to acquire old Yahoo's internet business (excluding its stakes in Alibaba Group and Yahoo! Japan ) for $4.48 billion [ 16 ] —the company was once valued at over $100 billion. Before the transaction was completed, the company expected to change its name to Altaba Inc. [ 17 ] Verizon completed its acquisition of the old iteration of Yahoo! Inc's internet business on June 13, 2017. Verizon announced that the old Yahoo! Inc's internet assets would be combined under a new subsidiary, Oath, which later became known as Verizon Media in 2019 and eventually renamed to the current iteration of Yahoo! Inc. in 2021. [ 18 ] [ 19 ] In January 1994, Yang and Filo were electrical engineering graduate students at Stanford University , when they created a website named \"Jerry and David's guide to the World Wide Web\". [ 20 ] [ 21 ] The site was a directory of other websites, organized in a hierarchy, as opposed to a searchable index of pages. In March 1994, \"Jerry and David's Guide to the World Wide Web\" was renamed \"Yahoo!\", [ 22 ] [ 23 ] the human-edited Yahoo! Directory , provided for users to surf through the internet, being their first product and original purpose. [ 24 ] [ 25 ] The \"yahoo.com\" domain was created on January 18, 1995. [ 26 ] The word \"yahoo\" is a backronym for \"Yet Another Hierarchically Organized Oracle\" [ 27 ] or \" Yet Another Hierarchical Officious Oracle\". [ 28 ] The term \"hierarchical\" described how the Yahoo database was arranged in layers of subcategories. The term \"oracle\" was intended to mean \"source of truth and wisdom\", and the term \"officious\", rather than being related to the word's normal meaning, described the many office workers who would use the Yahoo database while surfing from work. [ 29 ] However, Filo and Yang insist they mainly selected the name because they liked the slang definition of a \"yahoo\" (used by college students in David Filo's native Louisiana in the late 1980s and early 1990s to refer to an unsophisticated, rural Southerner): \"rude, unsophisticated, uncouth.\" [ 30 ] This meaning derives from the Yahoo race of fictional beings from Gulliver's Travels . Yahoo grew rapidly throughout the 1990s. Like many search engines and web directories, Yahoo added a web portal. By 1998, Yahoo was the most popular starting point for web users [ 31 ] and the human-edited Yahoo Directory the most popular search engine. [ 24 ] It also made many high-profile acquisitions. Yahoo!'s initial public offering at the NASDAQ was on April 12, 1996, closing at US$33.00—up 270 percent from the IPO price—after peaking at $43.00 for the day. Its stock price skyrocketed during the dot-com bubble , closing at an all-time high of $118.75 a share on January 3, 2000. However, after the dot-com bubble burst , it reached a post-bubble low of $8.11 on September 26, 2001. [ 32 ] Yahoo began using Google for search in 2000. Over the next four years, it developed its own search technologies, which it began using in 2004. In response to Google's Gmail, Yahoo began to offer unlimited email storage in 2007. The company struggled through 2008, with several large layoffs . [ 33 ] In February 2008, Microsoft made an unsolicited bid to acquire Yahoo for $44.6 billion. Yahoo formally rejected the bid, claiming that it \"substantially undervalues\" the company and was not in the interest of its shareholders. Three years later, Yahoo had a market capitalization of $22.24 billion. [ 34 ] Carol Bartz replaced Yang as CEO in January 2009. [ 35 ] In September 2011, she was removed from her position at Yahoo by the company's chairman Roy Bostock, and CFO Tim Morse was named as Interim CEO of the company. In early 2012, after the appointment of Scott Thompson as CEO, rumors began to spread about looming layoffs. Several key executives, such as Chief Product Officer Blake Irving , left. [ 36 ] On April 4, 2012, Yahoo announced a cut of 2,000 jobs or about 14 percent of its 14,100 workers. The cut was expected to save around $375 million annually after the layoffs were completed at end of 2012. [ 37 ] In an email sent to employees in April 2012, Thompson reiterated his view that customers should come first at Yahoo. He also completely reorganized the company. [ 38 ] On May 13, 2012, Yahoo issued a press release stating that Thompson was no longer with the company, and would immediately be replaced on an interim basis by Ross Levinsohn , recently appointed head of Yahoo's new Media group. [ 38 ] [ 39 ] [ 40 ] Thompson's total compensation for his 130-day tenure with Yahoo was at least $7.3 million. [ 41 ] On July 16, 2012, Marissa Mayer was appointed president and CEO of Yahoo, effective the following day. [ 42 ] On May 19, 2013, the Yahoo board approved a $1.1 billion purchase of blogging site Tumblr . Tumblr's CEO and founder David Karp would remain a large shareholder. The announcement reportedly signified a changing trend in the technology industry, as large corporations like Yahoo, Facebook, and Google acquired start-up Internet companies that generated low amounts of revenue as a way in which to connect with sizeable, fast-growing online communities. The Wall Street Journal stated that the purchase of Tumblr would satisfy Yahoo's need for \"a thriving social-networking and communications hub.\" [ 43 ] [ 44 ] On May 20, the company announced the acquisition of Tumblr officially. [ 45 ] The company also announced plans to open a San Francisco office in July 2013. [ 46 ] On August 2, 2013, Yahoo acquired Rockmelt ; its staff was retained, but all of its existing products were terminated. [ 47 ] Data collated by comScore during July 2013, revealed that more people in the U.S. visited Yahoo websites during the month in comparison to Google; the occasion was the first time that Yahoo outperformed Google since 2011. The data did not count mobile usage, nor Tumblr. [ 48 ] In November 2014, Yahoo! announced that it would acquire the video advertising provider BrightRoll for $640 million. [ 49 ] On November 21, 2014, it was announced that Yahoo had acquired Cooliris . [ 50 ] By the fourth quarter of 2013, the company's share price had more than doubled since Marissa Mayer took over as president in July 2012; however, the share price peaked at about $35 in November 2013. [ 51 ] It did go up to $36.04 in the mid afternoon of December 2, 2015, perhaps on news that the board of directors was meeting to decide on the future of Mayer, whether to sell the struggling internet business, [ 52 ] and whether to continue with the spinoff of its stake in China's Alibaba e-commerce site. [ 53 ] Not all had gone well during Mayer's tenure, including the $1.1 billion acquisition of Tumblr that had yet to prove beneficial and the forays into original video content that led to a $42 million write-down. Sydney Finkelstein , a professor at Dartmouth College's Tuck School of Business , told The Washington Post that sometimes, \"the single best thing you can do ... is sell the company.\" [ 54 ] The closing price of Yahoo! Inc. on December 7, 2015, was $34.68. [ 55 ] The Wall Street Journal ' s Douglas MacMillan reported on February 2, 2016, that Yahoo's CEO Marissa Mayer was expected to cut 15% of its workforce. [ 56 ] [ 57 ] On July 25, 2016, Verizon Communications announced that it had agreed to purchase Yahoo's core internet business for $4.83 billion. [ 58 ] [ 59 ] [ 60 ] Those assets were merged with AOL to form a new entity known as Oath; Yahoo, AOL, and Huffington Post continued to operate under their own names, under the Oath umbrella. [ 61 ] The deal excluded Yahoo's 15% stake in Alibaba Group and 35.5% stake in Yahoo! Japan , which were retained under the name Altaba, with a new executive team. [ 62 ] [ 63 ] On September 22, 2016, Yahoo disclosed a data breach that occurred in late 2014, in which information associated with at least 500 million user accounts, [ 64 ] one of the largest breaches reported to date. [ 65 ] The United States have indicted four men, including two employees of Russia's Federal Security Service (FSB), for their involvement in the hack. [ 66 ] [ 67 ] On December 14, 2016, the company revealed that another separate data breach had occurred in 2014, with hackers obtaining sensitive account information, including security questions, to at least one billion accounts. [ 68 ] The company stated that hackers had utilized stolen internal software to \" forge \" cookies . [ 69 ] In response to these breaches, Bloomberg News reported that Verizon was attempting to re-negotiate the deal to reduce the purchase price by $250 million, [ 70 ] causing a 2% increase in Yahoo stock prices. [ 71 ] On February 21, 2017, Verizon agreed to lower its purchase price for Yahoo! by $350 million, and share liabilities regarding the investigation into the data breaches. [ 71 ] On June 8, 2017, Yahoo shareholders approved the company's sale of some of its Internet assets to Verizon for $4.48 billion. The deal officially closed on June 13, 2017. [ 72 ] [ 73 ] [ 17 ] [ 18 ] [ 55 ] Eight chief executives and interim leaders have led the Yahoo company from 1995 until 2017. They are: For the CEOs of the \"new\" Yahoo, see Yahoo! Inc. (2017–present)#Chief Executive Officers . Yahoo operated a portal that provides the latest news, entertainment, and sports information. The portal also gave users access to other Yahoo services like Yahoo! Search , Yahoo Mail, Yahoo Maps, Yahoo Finance , Yahoo Groups and Yahoo Messenger . Yahoo provided Internet communication services such as Yahoo Messenger and Yahoo Mail . As of May 2007, its e-mail service would offer unlimited storage. [ 75 ] Yahoo provided social networking services and user-generated content, including products such as My Web , Yahoo Personals , Yahoo 360° , Delicious , Flickr , and Yahoo Buzz . Yahoo closed Yahoo Buzz, MyBlogLog, and numerous other products on April 21, 2011. [ 76 ] Yahoo Photos was closed on September 20, 2007, in favor of Flickr. On October 16, 2007, Yahoo announced that it would discontinue Yahoo 360°, including bug repairs; the company explained that in 2008 it would instead establish a \"universal profile\" similar to the Yahoo Mash experimental system. [ 77 ] Yahoo partners with numerous content providers in products such as Yahoo Sports , Yahoo Finance, Yahoo Music , Yahoo Movies , Yahoo Weather , Yahoo News , Yahoo! Answers and Yahoo Games to provide news and related content. Yahoo provides a personalization service, My Yahoo , which enables users to combine their favorite Yahoo features, content feeds and information onto a single page. On March 31, 2008, Yahoo launched Shine, a site tailored for women seeking online information and advice between the ages of 25 and 54. [ 78 ] Yahoo developed partnerships with broadband providers such as AT&T Inc. (via Prodigy , BellSouth & SBC), [ 79 ] [ 80 ] Verizon Communications , [ 81 ] [ 82 ] Rogers Communications , [ 83 ] [ 84 ] and British Telecom , offering a range of free and premium Yahoo content and services to subscribers. Yahoo Mobile offers services for email, instant messaging, and mobile blogging , as well as information services, searches and alerts. Services for the camera phone include entertainment and ring tones. Yahoo introduced its Internet search system, called OneSearch, for mobile phones on March 20, 2007. The results include news headlines, images from Flickr, business listings, local weather and links to other sites. Instead of showing only, for example, popular movies or some critical reviews, OneSearch lists local theaters that at the moment are playing the movie, along with user ratings and news headlines regarding the movie. A zip code or city name is required for OneSearch to start delivering local search results. The results of a Web search are listed on a single page and are prioritized into categories. [ 85 ] As of 2012, Yahoo used Novarra 's mobile content transcoding service for OneSearch. [ 86 ] On October 8, 2010, Yahoo announced plans to bring video chat to mobile phones via Yahoo Messenger. [ 87 ] Yahoo offers shopping services such as Yahoo! Shopping, Yahoo Autos, Yahoo Real Estate and Yahoo Travel , which enables users to gather relevant information and make commercial transactions and purchases online. Yahoo Auctions were discontinued in 2007 except for Asia. [ 88 ] Yahoo Shopping is a price comparison service which uses the Kelkoo price comparison service it acquired in April 2004. [ 89 ] Yahoo provides business services such as Yahoo DomainKeys , Yahoo Web Hosting , Yahoo Merchant Solutions, Yahoo Business Email and Yahoo Store to small business owners and professionals allowing them to build their own online stores using Yahoo's tools. [ citation needed ] Yahoo Search Marketing provides services such as Sponsored Search, Local Advertising and Product/Travel/Directory Submit that let different businesses advertise their products and services on the Yahoo network. Following the closure of a \"beta\" version on April 30, 2010, the Yahoo Publisher Network was re-launched as an advertising tool that allows online publishers to monetize their websites through the use of site-relevant advertisements. [ 90 ] Yahoo launched its new Internet advertisement sales system on February 5, 2007, called Panama . It allows advertisers to bid for search terms to trigger their ads on search results pages. The system considers bids, ad quality, clickthrough rates and other factors in ranking ads. Through Panama, Yahoo aims to provide more relevant search results to users, a better overall experience, and to increase monetization. [ 91 ] On April 7, 2008, Yahoo announced APT from Yahoo , which was originally called AMP from Yahoo, [ 92 ] an online advertising management platform. [ 93 ] The platform simplifies advertising sales by unifying buyer and seller markets. The service was launched in September 2008. [ 94 ] In September 2011, Yahoo formed an ad selling strategic partnership with 2 of its top competitors, AOL and Microsoft. [ 95 ] But by 2013, this was found to be underperforming in market share and revenue, as Microsoft simply skimmed off four percent of the search market from Yahoo, without growing their combined share. [ 96 ] On September 27, 2023, Yahoo partnered with Good-Loop to offer carbon neutral private marketplace (PMP) media opportunities to help advertisers be more sustainable. [ 97 ] Yahoo offers cartographic and geographic services via GeoPlanet . Yahoo Next is an incubation ground for future Yahoo technologies currently undergoing testing. It contains forums for Yahoo users to give feedback to assist in the development of these future Yahoo technologies. [ 98 ] Yahoo Search BOSS is a service that allows developers to build search applications based on Yahoo's search technology. [ 99 ] Early Partners in the program include Hakia , Me.dium, Delver , Daylife and Yebol . [ 100 ] In early 2011, the program switched to a paid model using a cost-per-query model from $0.40 to $0.75 CPM (cost per 1000 BOSS queries). The price, as Yahoo explained, depends on whether the query is of web, image, news or other information. [ 101 ] Yahoo Meme is a beta social service, similar to the popular social networking sites Twitter and Jaiku . Y!Connect enables individuals to leave comments in online publication boards by using their Yahoo ID, instead of having to register with individual publications. The Wall Street Journal reported that Yahoo plans to mimic this strategy used by rival Facebook Inc. to help drive traffic to its site. [ 102 ] Yahoo has invested resources to increase and improve access to the Internet for the disabled community through the Yahoo Accessibility Lab. [ 103 ] Yahoo Axis is a desktop web browser extension and mobile browser for iOS devices created and developed by Yahoo. The browser made its public debut on May 23, 2012. [ 104 ] A copy of the private key used to sign official Yahoo browser extensions for Google Chrome was accidentally leaked in the first public release of the Chrome extension. [ 105 ] Yahoo SearchMonkey (often misspelled Search Monkey) was a Yahoo service which allowed developers and site owners to use structured data to make Yahoo Search results more useful and visually appealing, and drive more relevant traffic to their sites. The service was shut down in October 2010 along with other Yahoo services as part of the Microsoft and Yahoo search deal. The name SearchMonkey is an homage to Greasemonkey. Officially the product name has no space and two capital letters. Yahoo SearchMonkey was selected as one of the top 10 Semantic Web Products of 2008. [ 106 ] GeoCities was a popular web hosting service founded in 1995 and was one of the first services to offer web pages to the public. At one point it was the third-most-browsed site on the World Wide Web . [ 107 ] Yahoo purchased GeoCities in 1999 and ten years later the web host was closed, deleting some seven million web pages. [ 108 ] A great deal of information was lost but many of those sites and pages were mirrored at the Internet Archive , [ 109 ] OOcities.com, and other such databases. [ 110 ] Yahoo Go , a Java-based phone application with access to most of Yahoo services, was closed on January 12, 2010. [ 111 ] Yahoo 360° was a blogging/social networking beta service launched in March 2005 by Yahoo and closed on July 13, 2009. [ 112 ] Yahoo Mash beta was another social service closed after one year of operation prior to leaving beta status. [ 113 ] Yahoo Photos was shut down on September 20, 2007, in favor of integration with Flickr. Yahoo Tech was a website that provided product information and setup advice to users. Yahoo launched the website in May 2006. On March 11, 2010, Yahoo closed down the service and redirected users to Yahoo's technology news section. [ 114 ] Other discontinued services include Farechase, My Web, Audio Search, Pets, Live, Kickstart, Briefcase, and Yahoo for Teachers. [ 115 ] Hotjobs was acquired by and merged with Monster.com . Yahoo Koprol was an Indonesian geo-tagging website that allowed users to share information about locations without the use of a GPS device. Koprol was acquired by Yahoo a year following its inception and, in 2011, 1.5 million people were utilizing the website, with users also based in Singapore, the Philippines and Vietnam. However, eighty percent of users were Indonesian. [ 116 ] Yahoo officially discontinued Koprol on August 28, 2012, because it did \"not meaningfully drive revenue or engagement\". [ 117 ] Yahoo Mail Classic was announced as to be shut down in April 2013. Yahoo made a notice that, starting in June 2013, Mail Classic and other old versions of Yahoo Mail will be shut down. All users of Mail Classic are expected to switch to the new Yahoo Mail, use IMAP, or switch to another email service. [ 118 ] In addition, April 2013 brought the closure of Upcoming , Yahoo Deals, Yahoo SMS Alerts, Yahoo Kids , Yahoo Mail and Messenger feature phone (J2ME). [ 119 ] In early July 2013, Yahoo announced the scheduled closure of the task management service Astrid . Yahoo had acquired the company in May 2013 and was to discontinue the service on August 5, 2013. The team at Astrid has supplied its customers with a data export tool and recommended former competitors such as Wunderlist and Sandglaz. [ 120 ] [ 121 ] On December 15, 2010, one day after Yahoo announced layoffs of 4% of its workers across their portfolio, MyBlogLog founder Eric Marcoullier posted a slide from a Yahoo employee on Twitter. The slide was visible during an employee-only strategy webcast indicating changes in Yahoo's offerings. [ 122 ] The following services were in a column under \"Sunset\": Yahoo Picks, AltaVista , MyM, AlltheWeb , Yahoo Bookmarks, Yahoo Buzz, del.icio.us , and MyBlogLog. Under the \"Merge\" column were: Upcoming, FoxyTunes , Yahoo Events, Yahoo People Search, Sideline, and FireEagle. 11 other properties were listed that Yahoo was interested in developing into feature sites within the portal to take the place of the \"Sunset\" and \"Merge\" vacancies, including the prior feature services (before the new Yahoo Mail was launched), were Yahoo Address Book, Calendar, and Notepad. [ 123 ] Despite Notepad being listed as a feature service instead of sunset or merge in 2010, Yahoo has since taken steps to de-emphasize Notepad. For example, in January 2013, Notepad was no longer linked within the new Yahoo mail service, although it continued to be linked in the older Classic version. Also, starting in mid- to late January 2013, Notepad was no longer searchable. [ citation needed ] The blog on the del.icio.us website released a post by Chris Yeh after the slide was leaked in which Yeh stated that \"Sunset\" doesn't necessarily mean that Yahoo is closing down the site. Yeh further explained that other possibilities—including del.icio.us leaving Yahoo (through sale or spinoff)—were still being considered: \"We can only imagine how upsetting the news coverage over the past 24 hours has been to many of you. Speaking for our team, we were very disappointed by the way that this appeared in the press.\" [ 124 ] On April 27, 2011, Yahoo's sale of del.icio.us to Avos was announced. [ 125 ] Yahoo Buzz was closed down on April 21, 2011, without an official announcement from Yahoo. [ 126 ] MyBlogLog was then discontinued by Yahoo on May 24, 2011. [ 127 ] In September 2013, Yahoo's transparency report said the company received 29 thousand requests for information about users from governments in the first six months of 2013. Over 12 thousand of the requests came from the United States. [ 128 ] In October 2013, The Washington Post reported that the U.S. National Security Agency intercepted communications between Yahoo's data centers, as part of a program named Muscular . [ 129 ] [ 130 ] In late January 2014, Yahoo announced on its company blog that it had detected a \"coordinated effort\" to hack into possibly millions of Yahoo Mail accounts. The company prompted users to reset their passwords, but did not elaborate on the scope of the possible breach, citing an ongoing federal investigation. [ 131 ] Working with comScore, The New York Times found that Yahoo was able to collect far more data about users than its competitors from its Web sites and advertising network. By one measure, on average Yahoo had the potential in December 2007 to build a profile of 2,500 records per month about each of its visitors. [ 132 ] Yahoo retains search requests for a period of 13 months. However, in response to European regulators, Yahoo obfuscates the IP address of users after three months by deleting its last eight bits. [ 133 ] On March 29, 2012, Yahoo announced that it would introduce a \" Do Not Track \" feature that summer, allowing users to opt out of Web-visit tracking and customized advertisements. [ 134 ] However, on April 30, 2014, Yahoo announced that it would no longer support the \"Do Not Track\" browser setting. [ 135 ] According to a 2008 article in Computerworld , Yahoo has a 2-petabyte, specially built data warehouse that it uses to analyze the behavior of its half-billion Web visitors per month, processing 24 billion daily events. [ 136 ] In contrast, the United States Internal Revenue Service (IRS) database of all United States taxpayers weighs in at only 150 terabytes. [ 136 ] In September 2016, it was reported that data from at least 500 million Yahoo accounts was stolen in 2014. [ 137 ] In October 2016, Reuters reported that in 2015, Yahoo! created a software to search their customers e-mail at the request of NSA or FBI. [ 138 ] In 2000, Yahoo was taken to court in France by parties seeking to prevent French citizens from purchasing memorabilia relating to the Nazi Party. [ 139 ] In March 2004, Yahoo launched a paid inclusion program whereby commercial websites were guaranteed listings on the Yahoo search engine. [ 140 ] Yahoo discontinued the program at the end of 2009. [ 141 ] Yahoo was criticized for providing ads via the Yahoo ad network to companies who display them through spyware and adware . [ 142 ] [ 143 ] Yahoo, as well as other search engines, cooperated with the Chinese government in censoring search results. In April 2005, dissident Shi Tao was sentenced to 10 years in prison for \"providing state secrets to foreign entities\" [ 144 ] as a result of being identified by IP address by Yahoo. [ 145 ] Human rights organizations and the company's general counsel disputed the extent of Yahoo's foreknowledge of Shi's fate. [ 146 ] Human rights groups also accuse Yahoo of aiding authorities in the arrest of dissidents Li Zhi and Jiang Lijun . In April 2017, Yahoo was sued for failing to uphold settlement agreements in this case. Yahoo pledged to give support to the families of those arrested and create a relief fund for those persecuted for expressing their views online with Yahoo Human Rights Trust. Of the $17.3 million allotted to this fund, $13 million had been used for a townhouse in Washington, D.C., and other purchases. [ 147 ] In September 2003, dissident Wang Xiaoning was convicted of charges of \"incitement to subvert state power\" and was sentenced to ten years in prison. Yahoo Hong Kong connected Wang's group to a specific Yahoo e-mail address. [ 148 ] Both Xiaoning's wife and the World Organization for Human Rights [ 149 ] sued Yahoo under human rights laws on behalf of Wang and Shi. [ 150 ] As a result of media scrutiny relating to Internet child predators and a lack of significant ad revenues, Yahoo's \"user created\" chatrooms were closed down in June 2005. [ 151 ] On May 25, 2006, Yahoo's image search was criticized for bringing up sexually explicit images even when SafeSearch was active. [ 152 ] Yahoo was [ when? ] a 40% (24% in September 2013) owner of Alibaba Group , [ 153 ] which was a subject of controversy for allowing the sale of shark-derived products. The company banned the sale of shark fin products on all its e-commerce platforms effective January 1, 2009. On November 30, 2009, Yahoo was criticized by the Electronic Frontier Foundation for sending a DMCA notice to whistle-blower website \"Cryptome\" for publicly posting details, prices, and procedures on obtaining private information pertaining to Yahoo's subscribers. [ 154 ] After some concerns over censorship of private emails regarding a website affiliated with Occupy Wall Street protests were raised, [ 155 ] [ 156 ] Yahoo responded with an apology and explained it as an accident. [ 157 ] Scott Ard, a prominent editorial director, fired from Yahoo in 2015 has filed a lawsuit accusing Mayer of leading a sexist campaign to purge male employees. Ard, a male employee, stated \"Mayer encouraged and fostered the use of (an employee performance-rating system) to accommodate management's subjective biases and personal opinions, to the detriment of Yahoo's male employees\". In the suit Ard claimed prior to his firing, he had received \"fully satisfactory\" performance reviews since starting at the company in 2011 as head of editorial programming for Yahoo's home page, however, he was relieved of his role that was given to a woman who had been recently hired by Megan Lieberman, the editor-in-chief of Yahoo News. [ 158 ] [ 159 ] The lawsuit states: \"Liberman stated that she was terminating (Ard) because she had not received a requested breakdown of (his) duties. (Ard) had already provided that very information as requested, however, and reminded Liberman that he had done so. Liberman's excuse for terminating (Ard) was a pretext.\" [ 159 ] A second sexual discrimination lawsuit was filed separately by Gregory Anderson, who was fired in 2014, alleging the company's performance management system was arbitrary and unfair, making it the second sexism lawsuit Yahoo and Meyer has faced in 2016. [ 160 ] [ 161 ] [ 162 ] Former chief operating officer Henrique de Castro departed from the company in January 2014 after Mayer, who initially hired him after her appointment as CEO, dismissed him. De Castro, who previously worked for Google and McKinsey & Company , was employed to revive Yahoo's advertising business. [ 168 ] Yahoo offers a multilingual interface. The site is available in over 20 languages. Yahoo held a 34.75% minority stake in Yahoo Japan , while SoftBank held 35.45%. [ 169 ] Yahoo!Xtra in New Zealand, which Yahoo!7 held 51% of and Telecom New Zealand held 49% of. Yahoo!7 in Australia was a 50–50 agreement between Yahoo and the Seven Network . Historically, Yahoo entered into joint venture agreements with SoftBank for the major European sites (UK, France and Germany) and well as South Korea and Japan. In November 2005, Yahoo purchased the minority interests that SoftBank owned in Europe and Korea. On March 8, 2011, Yahoo launched its Romania local service after years of delay due to the 2008 financial crisis . [ 170 ] [ 171 ] [ 172 ] [ 173 ] [ 174 ] Yahoo officially entered the MENA region when it acquired Maktoob , a pan-regional, Arabic-language hosting and social services online portal, on August 25, 2009. [ 175 ] Since the service is pan-regional, Yahoo officially became Yahoo Maktoob in the region. On December 31, 2012, Yahoo Korea shut down all its services and left the country, with its previous domain saying in Korean, \"Starting from December 31, 2012, Yahoo! Korea has ended. You can go to the original Yahoo! for more Yahoo's information.\" Sooner did that message also disappear, leaving with just an abandoned, empty search bar powered by Bing. [ 176 ] Yahoo used to hold a 40% stake in Alibaba, which manages a web portal in China using the Yahoo brand name. Yahoo in the USA does not have direct control over Alibaba, which operates as a completely independent company. On September 18, 2012, following years of negotiations, Yahoo agreed to sell a 20% stake back to Alibaba for $7.6 billion. [ 177 ] On September 2, 2013, Yahoo China shut down and was redirected to taobao.com, [ 178 ] and has been being redirected to Yahoo Singapore's search page. The first logo appeared when the company was founded in 1994—it was red with three icons on each side. [ 179 ] The logo used on the Yahoo home page formerly consisted of the color red with a black outline and shadow; however, in May 2009, together with a theme redesign, the logo was changed to purple without an outline or shadow. This change also applied to several international Yahoo home pages. In some countries, most notably Yahoo!7 (of Australia), the logo remained red until 2014. [ 180 ] On occasion the logo is abbreviated: \"Y!\" [ 181 ] On August 7, 2013, at around midnight EDT, Yahoo announced that the final version of the new logo would be revealed on September 5, 2013, at 4:00 a.m. UTC. In the period leading up to the unveiling of the new logo, the \"30 Days of Change\" campaign was introduced, whereby a variation of the logo was published every day for the 30 days following the announcement. [ 182 ] [ 183 ] The new logo was eventually launched with an accompanying video that showed its digital construction, and Mayer published a personalized description of the design process on her Tumblr page. [ 184 ] Mayer explains: So, one weekend this summer, I rolled up my sleeves and dove into the trenches with our logo design team ... We spent the majority of Saturday and Sunday designing the logo from start to finish, and we had a ton of fun weighing every minute detail. We knew we wanted a logo that reflected Yahoo – whimsical, yet sophisticated. Modern and fresh, with a nod to our history. Having a human touch, personal. Proud. [ 185 ] On September 19, 2013, Yahoo launched a new version of the \"My Yahoo\" personalized homepage. The redesign allows users to tailor a homepage with widgets that access features such as email accounts, calendars, Flickr and other Yahoo content, and Internet content. Users can also select \"theme packs\" that represent artists such as Polly Apfelbaum and Alec Monopoly, and bands such as Empire of the Sun . [ 186 ] Mayer then explained at a conference in late September 2013 that the logo change was the result of feedback from both external parties and employees. [ 187 ]",
    "links": [
      "Yahoo Finance",
      "Yahoo DomainKeys",
      "The Wall Street Journal",
      "Software bug",
      "Rogers Communications",
      "Yahoo Publisher Network",
      "Verizon Communications",
      "Electronic Frontier Foundation",
      "Yahoo! Messenger Protocol",
      "LiveOps",
      "Wiley-Interscience",
      "Yahoo Messenger",
      "Alto Mail",
      "Yahoo Australia",
      "In2TV",
      "Yahoo Movies",
      "Socialthing",
      "Yahoo (2017–present)",
      "Cooliris",
      "Intel",
      "Polyvore",
      "OSCAR protocol",
      "Times Square",
      "SafeSearch",
      "Carol Bartz",
      "List of mergers and acquisitions by Yahoo",
      "British Telecom",
      "Yahoo! Answers",
      "Ross Levinsohn",
      "Yahoo Mash",
      "AlltheWeb",
      "Shi Tao (journalist)",
      "Blake Irving",
      "My Yahoo",
      "Yahoo Music",
      "One by AOL",
      "Yahoo! data breach",
      "Yahoo 360°",
      "FoxyTunes",
      "Yahoo Query Language",
      "Yahoo Weather",
      "LICRA v. Yahoo",
      "TAC (software)",
      "Yahoo Web Hosting",
      "Del.icio.us",
      "List of Yahoo-owned sites and services",
      "Mobile blogging",
      "Thomas J. McInerney (executive)",
      "Wal-Mart",
      "Yahoo Maktoob",
      "Flickr",
      "Rivals.com",
      "Subsidiary",
      "San Francisco Chronicle",
      "Delicious (website)",
      "Seven Network",
      "DMCA",
      "Ticker symbol",
      "TOC protocol",
      "Panama (ad system)",
      "Upcoming",
      "Internal Revenue Service",
      "Bangalore",
      "Yahoo",
      "Do Not Track",
      "McKinsey & Company",
      "Babel Fish (website)",
      "Maktoob",
      "BrightRoll",
      "MSN TV",
      "Clickthrough rate",
      "Rockmelt",
      "Yahoo Auctions",
      "Yahoo! Sports",
      "Yahoo Search BOSS",
      "OneSearch",
      "PC Magazine",
      "John Wiley & Sons",
      "Monster.com",
      "Dot-com bubble",
      "Variety (magazine)",
      "Tuck School of Business",
      "APT from Yahoo",
      "Delver (search engine)",
      "Yahoo Koprol",
      "Maynard Webb",
      "Koprol",
      "Yahoo China",
      "Yahoo Next",
      "Yahoo Kimo",
      "Federal Security Service",
      "Terry Semel",
      "Netscape",
      "List of search engines",
      "Business Insider",
      "Charles Schwab Corporation",
      "Alexa Internet",
      "Occupy Wall Street",
      "Yahoo Photos",
      "Politics Daily",
      "Kelkoo",
      "Sydney Finkelstein",
      "AOL On",
      "Apollo Global Management",
      "Yahoo! Groups",
      "Verizon",
      "SoftBank",
      "History of Yahoo!",
      "Telecom New Zealand",
      "AOLpress",
      "Singingfish",
      "Yahoo! Maps",
      "Yahoo! litigation",
      "Shark fin soup",
      "Yahoo Tech",
      "BellSouth",
      "Technology company",
      "2008 financial crisis",
      "Tim Morse",
      "NASDAQ",
      "Asset",
      "Gravity (American company)",
      "Max Levchin",
      "Yahoo Maps",
      "Good-Loop",
      "ART image file format",
      "Internet Archive",
      "Public company",
      "Twitter",
      "Yahoo Sports",
      "History of Yahoo",
      "Li Zhi (dissident)",
      "Yahoo Mobile",
      "H. Lee Scott, Jr.",
      "Microsoft",
      "RYOT",
      "Timeline of Yahoo",
      "AOL Community Leader Program",
      "Yahoo!7",
      "Web portal",
      "National Security Agency",
      "Novarra",
      "Yahoo Smart TV",
      "Criticism of Yahoo",
      "Neverwinter Nights (1991 video game)",
      "Forgery",
      "Moviefone",
      "Earnings before interest and taxes",
      "Yahoo News",
      "Price comparison service",
      "Yahoo (disambiguation)",
      "Wang Xiaoning",
      "List of Yahoo!-owned sites and services",
      "HTTP cookie",
      "Yahoo Japan",
      "Propeller.com",
      "Dublin",
      "Yahoo Games",
      "World Wide Web",
      "OpenCorporates",
      "Yahoo! Search",
      "Stanford University",
      "IAC/InterActiveCorp",
      "Flurry (company)",
      "Backronym",
      "Spyware",
      "Executive Vice President",
      "Yahoo Travel",
      "CompuServe",
      "Internet censorship in the People's Republic of China",
      "GameDaily",
      "Go90",
      "Bloomberg News",
      "Yahoo Xtra",
      "AOL Explorer",
      "Empire of the Sun (band)",
      "Jaiku",
      "Tumblr",
      "Yahoo Search",
      "Sunnyvale, California",
      "Yahoo Kids",
      "Yahoo! Video",
      "Computerworld",
      "David Filo",
      "Yahoo Pipes",
      "Yahoo View",
      "Yahoo Personals",
      "Sue James",
      "List of acquisitions by Yahoo!",
      "Equity (finance)",
      "The New York Times",
      "Yahoo Briefcase",
      "Yahoo! Messenger",
      "Muscular (surveillance program)",
      "AltaVista",
      "AIM (software)",
      "Engadget",
      "DMOZ",
      "GeoPlanet",
      "Yahoo Answers",
      "Yahoo Mail",
      "Karen Linder",
      "Alibaba Group",
      "Yahoo! Japan",
      "Google",
      "Net income",
      "United States dollar",
      "Hakia",
      "Yahoo Meme",
      "Yahoo Kickstart",
      "Altaba",
      "David Karp",
      "Yahoo! Inc. (2017-present)",
      "My Web",
      "Internet forum",
      "Video advertising",
      "Timothy Koogle",
      "TechCrunch",
      "Weblogs, Inc.",
      "Yahoo Buzz",
      "Yahoo Go",
      "List of acquisitions by AOL",
      "Yahoo Groups",
      "Yahoo! Mail",
      "Prodigy (online service)",
      "FanHouse",
      "AOL",
      "Gulliver's Travels",
      "Yahoo! Directory",
      "AOL Hometown",
      "Wayback Machine",
      "MENA",
      "ISSN (identifier)",
      "Built by Girls",
      "The Washington Post",
      "GeoCities",
      "Nasdaq",
      "USA Today",
      "Data warehouse",
      "Yahoo! News",
      "SimilarWeb",
      "Yahoo Together",
      "ISBN (identifier)",
      "AOLserver",
      "List of web analytics software",
      "Aol.com",
      "Web search engine",
      "Astrid (application)",
      "Yahoo! Inc. (2017–present)",
      "Initial public offering",
      "Layoff",
      "Yet Another",
      "Scott Thompson (businessman)",
      "Voice of America",
      "Marissa Mayer",
      "Daylife",
      "HuffPost",
      "Yebol",
      "AOL TV",
      "AT&T Inc.",
      "AOL Radio",
      "Charles R. Schwab",
      "AOL Seed",
      "Yahoo (Gulliver's Travels)",
      "Adware",
      "Jerry Yang",
      "Edgecast",
      "Yahoo Time Capsule",
      "Jiang Lijun",
      "Yahoo!Xtra",
      "MapQuest",
      "Yahoo Axis",
      "Buy.at",
      "Ficlets",
      "Radio KOL (Kids Online)",
      "Elwood Edwards",
      "Yahoo Search Marketing",
      "Yahoo! Finance",
      "Yahoo Hong Kong",
      "Ultravox (software)"
    ]
  },
  "Theodor Nelson": {
    "url": "https://en.wikipedia.org/wiki/Theodor_Nelson",
    "title": "Theodor Nelson",
    "content": "Theodor Holm Nelson (born June 17, 1937) is an American pioneer of information technology , philosopher of computer science, and sociologist. He coined the terms hypertext and hypermedia in 1963 [ 1 ] and published them in 1965. [ 2 ] According to his profile published in Forbes in 1997, Nelson \"sees himself as a literary romantic, like a Cyrano de Bergerac , or 'the Orson Welles of software'.\" [ 3 ] Nelson is the son of Emmy Award –winning director Ralph Nelson and Academy Award ‍–‍winning actress Celeste Holm . [ 4 ] His parents' marriage was brief and he was mostly raised by his grandparents, first in Chicago and later in Greenwich Village . [ 5 ] Nelson earned a B.A. in philosophy from Swarthmore College in 1959. Following a year of graduate study in sociology at the University of Chicago , Nelson began graduate work in Social Relations, then a department at Harvard University, specializing in sociology, and ultimately earned a M.A in sociology from the Department of Social Relations in 1962. [ 6 ] After Harvard, Nelson worked as a photographer and filmmaker for a year at John C. Lilly 's Communication Research Institute in Miami, Florida, where he briefly shared an office with Gregory Bateson . From 1964 to 1966, he was an instructor in sociology at Vassar College. During college and graduate school, Nelson began to envision a computer-based writing system that would provide a lasting repository for the world's knowledge, and also permit greater flexibility of drawing connections between ideas. This came to be known as Project Xanadu . [ 7 ] Much later in life, in 2002, Nelson obtained his PhD in media and governance from Keio University . Nelson first conceived of what would become Project Xanadu in the early 1960s, with the goal of creating a computer network with a simple user interface. He started referring to this project to others using the name Xanadu in 1966. [ 7 ] The effort is documented in the books Computer Lib/Dream Machines (1974), The Home Computer Revolution (1977) and Literary Machines (1981). Much of his adult life has been devoted to working on Xanadu and advocating for it. Throughout his career, Nelson supported his work on the Xanadu project through a variety of administrative, academic, and research positions and consultancies, including stints at Harcourt Brace and Company [ 7 ] Brown University (a tumultuous consultancy on the Nelson-inspired Hypertext Editing System and File Retrieval and Editing System with Swarthmore friend Andries van Dam 's group; c. 1967–1969), [ 8 ] Bell Labs (hypertext-related defense research; 1968–1969), [ 9 ] CBS Laboratories (\"writing and photographing interactive slideshows for their AVS-10 instructional device\"; 1968–1969), [ 9 ] the University of Illinois at Chicago (an interdisciplinary staff position; 1973–1976) [ 9 ] and Swarthmore College (visiting lecturer in computing; 1977). [ 9 ] Nelson also conducted research and development under the auspices of the Nelson Organization (founder and president; 1968–1972) and the Computopia Corporation (co-founder; 1977–1978). Clients of the former firm included IBM , Brown University, Western Electric , the University of California , the Jewish Museum , the Fretheim Chartering Corporation and the Deering-Milliken Research Corporation . He has alleged that the Nelson Organization was envisaged as a clandestine funding conduit for the Central Intelligence Agency , which expressed interest in Project Xanadu at an early juncture; however, the promised funds failed to materialize after several benchmarks were met. From 1980 to 1981, he was the editor of Creative Computing . At the behest of Xanadu developers Mark S. Miller and Stuart Greene, Nelson joined San Antonio, Texas -based Datapoint as chief software designer (1981–1982), remaining with the company as a media specialist and technical writer until its Asher Edelman -driven restructuring in 1984. Following several San Antonio-based consultancies and the acquisition of Xanadu technology by Autodesk in 1988, he continued working on the project as a non-managerial Distinguished Fellow in the San Francisco Bay Area until the divestiture of the Xanadu Operating Group in 1992–1993. After holding visiting professorships in media and information science at Hokkaido University (1995–1996), Keio University (1996–2002), the University of Southampton and the University of Nottingham , he was a fellow (2004–2006) and visiting fellow (2006–2008) of the Oxford Internet Institute in conjunction with Wadham College, Oxford . [ 10 ] More recently, he has taught classes at Chapman University and the University of California, Santa Cruz . The Xanadu project failed to flourish, for a variety of reasons which are disputed. Journalist Gary Wolf published an unflattering history of Nelson and his project in the June 1995 issue of Wired , calling it \"the longest-running vaporware project in the history of computing\". [ 11 ] On his own website, Nelson expressed his disgust with the criticisms, referring to Wolf as \"Gory Jackal\", and threatened to sue him. [ 12 ] He also outlined his objections in a letter to Wired , [ 13 ] and released a detailed rebuttal of the article. [ 14 ] As early as 1972, a demonstration iteration developed by Cal Daniels failed to reach fruition when Nelson was forced to return the project's rented Data General Nova minicomputer due to financial exigencies. Nelson has stated that some aspects of his vision were fulfilled by Tim Berners-Lee 's invention of the World Wide Web , but he disliked the World Wide Web, XML , and embedded markup – regarding Berners-Lee's work as a gross over-simplification of his original vision: HTML is precisely what we were trying to PREVENT— ever-breaking links, links going outward only, quotes you can't follow to their origins, no version management , no rights management. [ 15 ] Jaron Lanier explained the difference between the World Wide Web and Nelson's vision, and the implications: A core technical difference between a Nelsonian network and what we have become familiar with online is that [Nelson's] network links were two-way instead of one-way. In a network with two-way links, each node knows what other nodes are linked to it. ... Two-way linking would preserve context. It's a small simple change in how online information should be stored that couldn't have vaster implications for culture and the economy. [ 16 ] In 1957, Nelson co-wrote and co-produced what he describes as a pioneering rock musical entitled \"Anything and Everything\"; it was performed at Swarthmore College . [ 17 ] Two years later, during his senior year at Swarthmore, Nelson made an experimental humorous student film, The Epiphany of Slocum Furlow , in which the titular hero discovers the meaning of life. Musician and composer Peter Schickele , also a student at Swarthmore College, scored the film. [ 18 ] In 1965, Nelson presented the paper \"Complex Information Processing: A File Structure for the Complex, the Changing, and the Indeterminate\" at the ACM National Conference, in which he coined the term \"hypertext\". [ 2 ] In 1976, Nelson co-founded and briefly served as the advertising director of the \"itty bitty machine company\", or \"ibm\", a small computer retail store that operated from 1977 to 1980 in Evanston, Illinois . In 1978, he had a significant impact upon IBM 's thinking when he outlined his vision of the potential of personal computing to the team that three years later launched the IBM PC . [ 19 ] From the 1960s to the mid-2000s, Nelson built an extensive collection of direct advertising mail he received in his mailbox, mainly from companies selling products in IT, print/publishing, aerospace, and engineering. In 2017, the Internet Archive began to publish it online in scanned form, in a collection titled \"Ted Nelson's Junk Mail Cartons\". [ 20 ] [ 21 ] [ 22 ] As of 2011, Nelson was working on a new information structure, ZigZag, [ 23 ] which is described on the Xanadu project website, which also hosts two versions of the Xanadu code. He also developed XanaduSpace, a system for the exploration of connected parallel documents (an early version of this software may be freely downloaded). [ 24 ] In January 1988 Byte magazine published an article about Nelson's ideas, titled \"Managing Immense Storage\". [ 25 ] In 1998, at the Seventh WWW Conference in Brisbane , Australia, Nelson was awarded the Yuri Rubinsky Memorial Award . In 2001, he was knighted by France as Officier des Arts et Lettres . In 2007, he celebrated his 70th birthday by giving an invited lecture at the University of Southampton . [ 26 ] In 2014, ACM SIGCHI honored him with a Special Recognition Award. [ 27 ] In 2014, Nelson was conferred with a Doctor of Science degree, honoris causa, by Chapman University . The ceremony took place during the 'Intertwingled' conference, featuring Nelson and other prominent figures in the field, including Apple Computer founder Steve Wozniak and former Association for Computing Machinery president Wendy Hall . At the conference, Nelson stated confidence in the potential of his Xanadu system, saying 'The world would have been a better place if I had succeeded, but I ain't dead yet.' [ 28 ] Nelson is credited with coining several new words that have come into common usage especially in the world of computing. Among them are: Nelson publishes some of his books through his self-owned Mindful Press. [ 29 ]",
    "links": [
      "Bachelor of Arts",
      "Doctor of Philosophy",
      "Ecogovernmentality",
      "Jaron Lanier",
      "Doi (identifier)",
      "Daniel Miller (anthropologist)",
      "Science, technology and society",
      "Orson Welles",
      "University of Southampton",
      "S2CID (identifier)",
      "Gabriella Coleman",
      "Sadie Plant",
      "Cyrano de Bergerac",
      "Hypertext fiction",
      "Markup language",
      "Digital anthropology",
      "Ecological anthropology",
      "British Computing Society",
      "ZigZag (software)",
      "IBM PC",
      "SIGCHI",
      "Creative Computing (magazine)",
      "Master of Arts",
      "Cyborg anthropology",
      "StretchText",
      "Academy Awards",
      "Brown University",
      "Steve Wozniak",
      "Andries van Dam",
      "Lewis H. Morgan",
      "Environmental anthropology",
      "Hokkaido University",
      "Information technology",
      "Western Electric",
      "Hypertext Editing System",
      "Literary Machines",
      "Microsoft Press",
      "University of Illinois at Chicago",
      "Ordre des Arts et des Lettres",
      "Micropayment",
      "Forbes",
      "Keio University",
      "PhD",
      "Oxford Internet Institute",
      "Marcel Mauss",
      "Hypertext",
      "Mizuko Ito",
      "Chapman University",
      "Virtuality (software design)",
      "Data General Nova",
      "Milliken & Company",
      "Swarthmore College",
      "Internet Archive",
      "Marshall McLuhan",
      "Donna Haraway",
      "Jewish Museum (Manhattan)",
      "Paul Virilio",
      "Version management",
      "Mark Fisher",
      "Edmund Snow Carpenter",
      "Computer Lib/Dream Machines",
      "University of California",
      "Reza Negarestani",
      "Asher Edelman",
      "Fanged Noumena",
      "Nature–culture divide",
      "Ted Nelson (disambiguation)",
      "Nick Land",
      "Cultural ecology",
      "Datapoint",
      "World Wide Web",
      "Transclusion",
      "Roy Ellen",
      "Brisbane",
      "Mark S. Miller",
      "Docuverse",
      "Cybernetic Culture Research Unit",
      "Minicomputer",
      "CBS Laboratories",
      "Media (communication)",
      "Harvard University",
      "Byte (magazine)",
      "Howard Rheingold",
      "Greenwich Village",
      "Illinois",
      "John Leland (journalist)",
      "Rave",
      "Association for Computing Machinery",
      "Ralph Nelson",
      "Mike Wesch",
      "Cybertext",
      "Bruno Latour",
      "XML",
      "Tom Boellstorff",
      "Gary Wolf (journalist)",
      "Google",
      "YouTube video (identifier)",
      "Tim Berners-Lee",
      "Stewart Brand",
      "Bell Labs",
      "University of Chicago",
      "File Retrieval and Editing System",
      "Social anthropology",
      "Central Intelligence Agency",
      "Capitalist Realism",
      "TWiT.tv",
      "Harcourt Brace and Company",
      "Wayback Machine",
      "Wadham College, Oxford",
      "Gregory Bateson",
      "Vaporware",
      "Hari Kunzru",
      "Teledildonics",
      "ISSN (identifier)",
      "John C. Lilly",
      "University of California, Santa Cruz",
      "Emmy Awards",
      "Wired (magazine)",
      "San Antonio, Texas",
      "ISBN (identifier)",
      "Leslie White",
      "Benjamin H. Bratton",
      "Intertwingularity",
      "Autodesk",
      "University of Nottingham",
      "Peter Schickele",
      "Ray Brassier",
      "Cultural anthropology",
      "Yuri Rubinsky Memorial Award",
      "Project Xanadu",
      "André Leroi-Gourhan",
      "Hypermedia",
      "San Francisco Bay Area",
      "Celeste Holm",
      "Evanston, Illinois",
      "Wendy Hall",
      "IBM",
      "Political ecology",
      "Information science",
      "Chicago"
    ]
  },
  "Science and technology studies": {
    "url": "https://en.wikipedia.org/wiki/Science_and_technology_studies",
    "title": "Science and technology studies",
    "content": "Science and technology studies ( STS ) or science, technology, and society is an interdisciplinary field that examines the creation, development, and consequences of science and technology in their historical, cultural, and social contexts. [ 1 ] Like most interdisciplinary fields of study, STS emerged from the confluence of a variety of disciplines and disciplinary subfields, all of which had developed an interest—typically, during the 1960s or 1970s—in viewing science and technology as socially embedded enterprises. [ 2 ] The key disciplinary components of STS took shape independently, beginning in the 1960s, and developed in isolation from each other well into the 1980s, although Ludwik Fleck 's (1935) monograph Genesis and Development of a Scientific Fact anticipated many of STS's key themes. In the 1970s Elting E. Morison founded the STS program at the Massachusetts Institute of Technology (MIT), which served as a model. By 2011, 111 STS research centers and academic programs were counted worldwide. [ 3 ] \"The mid-70s was a sort of formation period, and the early 1990s as a peak of consolidation, and then the 2000s as a period of global diffusion\" ( Sheila Jasanoff ) [ 4 ] . During the 1970s and 1980s, universities in the US, UK, and Europe began drawing these various components together in new, interdisciplinary programs. For example, in the 1970s, Cornell University developed a new program that united science studies and policy-oriented scholars with historians and philosophers of science and technology. Each of these programs developed unique identities due to variations in the components that were drawn together, as well as their location within the various universities. For example, the University of Virginia's STS program united scholars drawn from a variety of fields (with particular strength in the history of technology); however, the program's teaching responsibilities—it is located within an engineering school and teaches ethics to undergraduate engineering students—means that all of its faculty share a strong interest in engineering ethics . [ 6 ] A decisive moment in the development of STS was the mid-1980s addition of technology studies to the range of interests reflected in science. During that decade, two works appeared en seriatim that signaled what Steve Woolgar was to call the \"turn to technology\". [ 7 ] In a seminal 1984 article, Trevor Pinch and Wiebe Bijker showed how the sociology of technology could proceed along the theoretical and methodological lines established by the sociology of scientific knowledge. [ 8 ] This was the intellectual foundation of the field they called the social construction of technology. Donald MacKenzie and Judy Wajcman primed the pump by publishing a collection of articles attesting to the influence of society on technological design ( Social Shaping of Technology , 1985). [ 9 ] Social science research continued to interrogate STS research from this point onward as researchers moved from post-modern to post-structural frameworks of thought, Bijker and Pinch contributing to SCOT knowledge and Wajcman providing boundary work through a feminist lens. [ 10 ] The \"turn to technology\" helped to cement an already growing awareness of underlying unity among the various emerging STS programs. More recently, there has been an associated turn to ecology, nature, and materiality in general, whereby the socio-technical and natural/material co-produce each other. This is especially evident in work in STS analyses of biomedicine (such as Carl May and Annemarie Mol ) and ecological interventions (such as Bruno Latour , Sheila Jasanoff , Matthias Gross , Sara B. Pritchard , and S. Lochlann Jain ). Social constructions are human-created ideas, objects, or events created by a series of choices and interactions. [ 11 ] These interactions have consequences that change the perception that different groups of people have on these constructs. Some examples of social construction include class, race, money, and citizenship. The following also alludes to the notion that not everything is set, a circumstance or result could potentially be one way or the other. According to the article \"What is Social Construction?\" by Ian Hacking, \"Social construction work is critical of the status quo. Social constructionists about X tend to hold that: Very often they go further, and urge that: In the past, there have been viewpoints that were widely regarded as fact until being called to question due to the introduction of new knowledge. Such viewpoints include the past concept of a correlation between intelligence and the nature of a human's ethnicity or race (X may not be at all as it is). [ 12 ] An example of the evolution and interaction of various social constructions within science and technology can be found in the development of both the high-wheel bicycle, or velocipede , and then of the bicycle . The velocipede was widely used in the latter half of the 19th century. In the latter half of the 19th century, a social need was first recognized for a more efficient and rapid means of transportation. Consequently, the velocipede was first developed, which was able to reach higher translational velocities than the smaller non-geared bicycles of the day, by replacing the front wheel with a larger radius wheel. One notable trade-off was a certain decreased stability leading to a greater risk of falling. This trade-off resulted in many riders getting into accidents by losing balance while riding the bicycle or being thrown over the handlebars. The first \"social construction\" or progress of the velocipede caused the need for a newer \"social construction\" to be recognized and developed into a safer bicycle design. Consequently, the velocipede was then developed into what is now commonly known as the \" bicycle \" to fit within society's newer \"social construction,\" the newer standards of higher vehicle safety. Thus the popularity of the modern geared bicycle design came as a response to the first social construction, the original need for greater speed, which had caused the high-wheel bicycle to be designed in the first place. The popularity of the modern geared bicycle design ultimately ended the widespread use of the velocipede itself, as eventually it was found to best accomplish the social needs/social constructions of both greater speed and of greater safety. [ 13 ] With methodology from ANT, feminist STS theorists built upon SCOT's theory of co-construction to explore the relationship between gender and technology, proposing one cannot exist separately from the other. [ 10 ] This approach suggests the material and social are not separate, reality being produced through interactions and studied through representations of those realities. [ 10 ] Building on Steve Woolgar 's boundary work on user configuration, [ 14 ] feminist critiques shifted the focus away from users of technology and science towards whether technology and science represent a fixed, unified reality. [ 15 ] According to this approach, identity could no longer be treated as causal in human interactions with technology as it cannot exist prior to that interaction, feminist STS researchers proposing a \"double-constructivist\" approach to account for this contradiction. [ 16 ] John Law credits feminist STS scholars for contributing material-semiotic approaches to the broader discipline of STS, stating that research not only attempts to describe reality, but enacts it through the research process. [ 10 ] Sociotechnical imaginaries are what certain communities, societies, and nations envision as achievable through the combination of scientific innovation and social changes. These visions can be based on what is possible to achieve for a certain society, and can also show what a certain state or nation desires. [ 17 ] STIs are often bound with ideologies and ambitions of those who create and circulate them. Sociotechnical imaginaries can be created by states and policymakers, smaller groups within society, or can be a result of the interaction of both. [ 17 ] The term was coined in 2009 by Sheila Jasanoff and Sang-Hyun Kim who compared and contrasted sociotechnical imaginaries of nuclear energy in the USA with those of South Korea over the second half of the 20th century. [ 17 ] Jasanoff and Kim analyzed the discourse of government representatives, national policies, and civil society organizations, looked at the technological and infrastructural developments, and social protests, and conducted interviews with experts. They concluded that in South Korea nuclear energy was imagined mostly as the means of national development, while in the US the dominant sociotechnical imaginary framed nuclear energy as risky and in need of containment. [ 17 ] The concept has been applied to several objects of study including biomedical research, [ 18 ] [ 19 ] nanotechnology development [ 20 ] and energy systems and climate change. [ 21 ] [ 22 ] [ 23 ] [ 24 ] [ 25 ] [ 17 ] Within energy systems, research has focused on nuclear energy, [ 17 ] fossil fuels, [ 22 ] [ 25 ] renewables [ 21 ] as well as broader topics of energy transitions, [ 23 ] and the development of new technologies to address climate change. [ 24 ] Social technical systems are an interplay between technologies and humans, this is clearly expressed in the sociotechnical systems theory . To expound on this interplay, humans fulfill and define tasks, then humans in companies use IT and IT supports people, and finally, IT processes tasks and new IT generates new tasks. This IT redefines work practices. This is what we call the sociotechnical systems. [ 26 ] In socio-technical systems, there are two principles to internalize, that is joint optimization and complementarity. Joint optimization puts an emphasis on developing both systems in parallel and it is only in the interaction of both systems that the success of an organization arises. [ 26 ] The principle of complementarity means that both systems have to be optimized. [ 26 ] If you focus on one system and have bias over the other it will likely lead to the failure of the organization or jeopardize the success of a system. Although the above socio-technical system theory is focused on an organization, it is undoubtedly imperative to correlate this theory and its principles to society today and in science and technology studies. Understanding technology in the context of national development: critical reflections discusses how governance frameworks, digital infrastructure, and institutional capacity influence the societal outcomes of technology adoption. [ 27 ] According to Barley and Bailey, there is a tendency for AI designers and scholars of design studies to privilege the technical over the social, focusing more on taking \"humans out of the loop\" paradigm than the \"augmented intelligence\" paradigm. [ 28 ] Recent work on artificial intelligence considers large sociotechnical systems, such as social networks and online marketplaces , as agents whose behavior can be purposeful and adaptive. The behavior of recommender systems can therefore be analyzed in the language and framework of sociotechnical systems, leading also to a new perspective for their legal regulation. [ 29 ] [ 30 ] Technoscience is a subset of Science, Technology, and Society studies that focuses on the inseparable connection between science and technology. It states that fields are linked and grow together, and scientific knowledge requires an infrastructure of technology in order to remain stationary or move forward. Both technological development and scientific discovery drive one another towards more advancement. Technoscience excels at shaping human thoughts and behavior by opening up new possibilities that gradually or quickly come to be perceived as necessities. [ 31 ] \"Technological action is a social process.\" [ 32 ] Social factors and technology are intertwined so that they are dependent upon each other. This includes the aspect that social, political, and economic factors are inherent in technology and that social structure influences what technologies are pursued. In other words, \"technoscientific phenomena combined inextricably with social/political/economic/psychological phenomena, so 'technology' includes a spectrum of artifacts, techniques, organizations, and systems.\" [ 33 ] Winner expands on this idea by saying \"in the late twentieth-century technology and society, technology and culture, technology and politics are by no means separate.\" [ 34 ] Deliberative democracy is a reform of representative or direct democracies which mandates discussion and debate of popular topics which affect society. Deliberative democracy is a tool for making decisions. Deliberative democracy can be traced back all the way to Aristotle's writings . More recently, the term was coined by Joseph Bessette in his 1980 work Deliberative Democracy: The Majority Principle in Republican Government , where he uses the idea in opposition to the elitist interpretations of the United States Constitution with emphasis on public discussion. [ 36 ] Deliberative democracy can lead to more legitimate, credible, and trustworthy outcomes. Deliberative democracy allows for \"a wider range of public knowledge\", and it has been argued that this can lead to \"more socially intelligent and robust\" science. One major shortcoming of deliberative democracy is that many models insufficiently ensure critical interaction. [ 37 ] According to Ryfe, there are five mechanisms that stand out as critical to the successful design of deliberative democracy: Recently, [ when? ] there has been a movement towards greater transparency in the fields of policy and technology. Jasanoff comes to the conclusion that there is no longer a question of if there needs to be increased public participation in making decisions about science and technology, but now there need to be ways to make a more meaningful conversation between the public and those developing the technology. [ 39 ] Bruce Ackerman and James S. Fishkin offered an example of a reform in their paper \"Deliberation Day.\" The deliberation is to enhance public understanding of popular, complex and controversial issues through devices such as Fishkin's deliberative polling , [ 40 ] though implementation of these reforms is unlikely in a large government such as that of the United States. However, things similar to this have been implemented in small, local governments like New England towns and villages. New England town hall meetings are a good example of deliberative democracy in a realistic setting. [ 36 ] An ideal deliberative democracy balances the voice and influence of all participants. While the main aim is to reach consensus, deliberative democracy should encourage the voices of those with opposing viewpoints, concerns due to uncertainties, and questions about assumptions made by other participants. [ 41 ] It should take its time and ensure that those participating understand the topics on which they debate. Independent managers of debates should also have a substantial grasp of the concepts discussed, but must \"[remain] independent and impartial as to the outcomes of the process.\" [ 37 ] In 1968, Garrett Hardin popularised the phrase \"tragedy of the commons.\" It is an economic theory where rational people act against the best interest of the group by consuming a common resource. Since then, the tragedy of the commons has been used to symbolize the degradation of the environment whenever many individuals use a common resource. Although Garrett Hardin was not an STS scholar, the concept of the tragedy of the commons still applies to science, technology, and society. [ 42 ] In a contemporary setting, the Internet acts as an example of the tragedy of the commons through the exploitation of digital resources and private information. Data and internet passwords can be stolen much more easily than physical documents. Virtual spying is almost free compared to the costs of physical spying. [ 43 ] Additionally, net neutrality can be seen as an example of tragedy of the commons in an STS context. The movement for net neutrality argues that the Internet should not be a resource that is dominated by one particular group, specifically those with more money to spend on Internet access. [ 44 ] A counterexample to the tragedy of the commons is offered by Andrew Kahrl. Privatization can be a way to deal with the tragedy of the commons. However, Kahrl suggests that the privatization of beaches on Long Island , in an attempt to combat the overuse of Long Island beaches, made the residents of Long Island more susceptible to flood damage from Hurricane Sandy . The privatization of these beaches took away from the protection offered by the natural landscape. Tidal lands that offer natural protection were drained and developed. This attempt to combat the tragedy of the commons by privatization was counter-productive. Privatization actually destroyed the public good of natural protection from the landscape. [ 45 ] Alternative modernity [ 46 ] [ 47 ] is a conceptual tool conventionally used to represent the state of present western society. Modernity represents the political and social structures of society, the sum of interpersonal discourse, and ultimately a snapshot of society's direction at a point in time. Unfortunately, conventional modernity is incapable of modeling alternative directions for further growth within our society. Also, this concept is ineffective at analyzing similar but unique modern societies such as those found in the diverse cultures of the developing world. Problems can be summarized into two elements: inward failure to analyze the growth potentials of a given society, and outward failure to model different cultures and social structures and predict their growth potentials. [ 48 ] Previously, modernity carried a connotation of the current state of being modern, and its evolution through European colonialism. The process of becoming \"modern\" is believed to occur in a linear, pre-determined way, and is seen by Philip Brey as a way to interpret and evaluate social and cultural formations. This thought ties in with modernization theory , the thought that societies progress from \"pre-modern\" to \"modern\" societies. Within the field of science and technology, there are two main lenses with which to view modernity. The first is as a way for society to quantify what it wants to move towards. In effect, we can discuss the notion of \"alternative modernity\" (as described by Andrew Feenberg) and which of these we would like to move towards. Alternatively, modernity can be used to analyze the differences in interactions between cultures and individuals. From this perspective, alternative modernities exist simultaneously, based on differing cultural and societal expectations of how a society (or an individual within society) should function. Because of different types of interactions across different cultures, each culture will have a different modernity. The pace of innovation is the speed at which technological innovation or advancement is occurring, with the most apparent instances being too slow or too rapid. Both these rates of innovation are extreme and therefore have effects on the people that get to use this technology. [ 49 ] \"No innovation without representation\" is a democratic ideal of ensuring that everyone involved gets a chance to be represented fairly in technological developments. Legacy thinking is defined as an inherited method of thinking imposed from an external source without objection by the individual because it is already widely accepted by society. [ 53 ] Legacy thinking can impair the ability to drive technology for the betterment of society by blinding people to innovations that do not fit into their accepted model of how society works. By accepting ideas without questioning them, people often see all solutions that contradict these accepted ideas as impossible or impractical. Legacy thinking tends to advantage the wealthy, who have the means to project their ideas on the public. It may be used by the wealthy as a vehicle to drive technology in their favor rather than for the greater good. Examining the role of citizen participation and representation in politics provides an excellent example of legacy thinking in society. The belief that one can spend money freely to gain influence has been popularized, leading to public acceptance of corporate lobbying . As a result, a self-established role in politics has been cemented where the public does not exercise the power ensured to them by the Constitution to the fullest extent. This can become a barrier to political progress as corporations who have the capital to spend have the potential to wield great influence over policy. [ 54 ] Legacy thinking, however, keeps the population from acting to change this, despite polls from Harris Interactive that report over 80% of Americans to feel that big business holds too much power in government. [ 55 ] Therefore, Americans are beginning to try to steer away from this line of thought, rejecting legacy thinking, and demanding less corporate, and more public, participation in political decision-making. Additionally, an examination of net neutrality functions as a separate example of legacy thinking. Starting with dial-up , the internet has always been viewed as a private luxury good. [ 56 ] [ 57 ] Internet today is a vital part of modern-day society members. They use it in and out of life every day. [ 58 ] Corporations are able to mislabel and greatly overcharge for their internet resources. Since the American public is so dependent upon the internet there is little for them to do. Legacy thinking has kept this pattern on track despite growing movements arguing that the internet should be considered a utility. Legacy thinking prevents progress because it was widely accepted by others before us through advertising that the internet is a luxury and not a utility. Due to pressure from grassroots movements the Federal Communications Commission (FCC) has redefined the requirements for broadband and internet in general as a utility. [ 58 ] Now AT&T and other major internet providers are lobbying against this action and are in large able to delay the onset of this movement due to legacy thinking's grip on American [ specify ] culture and politics. For example, those who cannot overcome the barrier of legacy thinking may not consider the privatization of clean drinking water as an issue. [ 59 ] This is partial because access to water has become such a given fact of the matter to them. For a person living in such circumstances, it may be widely accepted to not concern themselves with drinking water because they have not needed to be concerned with it in the past. Additionally, a person living within an area that does not need to worry about their water supply or the sanitation of their water supply is less likely to be concerned with the privatization of water. This notion can be examined through the thought experiment of \" veil of ignorance \". [ 60 ] Legacy thinking causes people to be particularly ignorant about the implications behind the \"you get what you pay for\" mentality applied to a life necessity. By utilizing the \"veil of ignorance\", one can overcome the barrier of legacy thinking as it requires a person to imagine that they are unaware of their own circumstances, allowing them to free themselves from externally imposed thoughts or widely accepted ideas. STS is taught in several countries. According to the STS wiki, STS programs can be found in twenty countries, including 45 programs in the United States, three programs in India, and eleven programs in the UK. [ 66 ] STS programs can be found in Canada , [ 67 ] Germany, [ 68 ] [ 69 ] Israel , [ 70 ] Malaysia , [ 71 ] and Taiwan . [ 72 ] Some examples of institutions offering STS programs are Stanford University , [ 73 ] University College London , [ 74 ] Harvard University , [ 75 ] the University of Oxford , [ 76 ] Mines ParisTech , [ 77 ] Bar-Ilan University , [ 78 ] and York University . [ 67 ] In Europe the European Inter-University Association on Society, Science and Technology ( ESST ) offers an MA degree in STS through study programs and student exchanges with over a dozen specializations. The field has professional associations in regions and countries around the world. Notable peer-reviewed journals in STS include: Student journals in STS include:",
    "links": [
      "Society for the History of Technology",
      "Traditional ecological knowledge",
      "Artificial intelligence",
      "Public awareness of science",
      "York University",
      "Demography",
      "Politicization of science",
      "Annemarie Mol",
      "Private law",
      "History of the social sciences",
      "Citizen science",
      "American Association for the History of Medicine",
      "Hdl (identifier)",
      "Israel",
      "Strong programme",
      "American Anthropological Association",
      "Barry Bozeman",
      "Ford Pinto",
      "Bruce Ackerman",
      "Dial-up",
      "Feminist technoscience",
      "Cyborg anthropology",
      "Scientific method",
      "Taiwan",
      "Science, Technology, & Human Values",
      "Massimiano Bucchi",
      "ESST",
      "Women in science",
      "Communication studies",
      "Deliberative polling",
      "Garrett Hardin",
      "Policy studies",
      "Marcel Roche",
      "University College London",
      "Bibcode (identifier)",
      "University of Oxford",
      "Science communication",
      "Mines ParisTech",
      "Traditional knowledge",
      "Sociotechnology",
      "Black swan events",
      "Technological transitions",
      "Sara B. Pritchard",
      "Social work",
      "Technology and society",
      "Behavioural sciences",
      "Management",
      "Indian Institute of Technology Hyderabad",
      "Linear model of innovation",
      "Co-production (society)",
      "Demarcation problem",
      "Arie Rip",
      "Technical change",
      "Social history",
      "Philosophy of Science Association",
      "ArXiv (identifier)",
      "Science and technology studies in India",
      "Physical anthropology",
      "Science",
      "Area studies",
      "Annual Review of Political Science",
      "Social network",
      "User innovation",
      "Europe",
      "Quantum social science",
      "S. Lochlann Jain",
      "Women in STEM fields",
      "Philosophy of social science",
      "Metascience",
      "Technological innovation system",
      "Scientocracy",
      "Science in Action (book)",
      "History",
      "Scientific language (linguistic classification)",
      "Sheila Jasanoff",
      "Science studies",
      "Science wars",
      "Transition management (governance)",
      "Engineering studies",
      "Ian Hacking",
      "South Korea",
      "Rural sociology",
      "Public health",
      "Brazil",
      "Paradigm",
      "Philosophy and economics",
      "Bicycle",
      "Women in engineering",
      "Trading zones",
      "ISSN (identifier)",
      "National Women's Studies Association",
      "Open University Press",
      "ISBN (identifier)",
      "Volkswagen",
      "Empiricism",
      "New England",
      "PMC (identifier)",
      "Cultural anthropology",
      "Scientific community",
      "Political ecology",
      "Lobbying",
      "S. Barry Barnes",
      "Media studies",
      "Scientific controversy",
      "Physical geography",
      "Pace of innovation",
      "Mexico",
      "Technology policy",
      "Psychology",
      "Tapuya",
      "S2CID (identifier)",
      "Human science",
      "Sociology of scientific ignorance",
      "Environmental studies",
      "World history (field)",
      "Japan",
      "Global studies",
      "Scientific consensus",
      "Deliberative democracy",
      "Mapping controversies",
      "Digital anthropology",
      "Right to science and culture",
      "Argentina",
      "Social science",
      "History and philosophy of science",
      "Technology",
      "Critical animal studies",
      "Ulrike Felt",
      "Pseudoscience",
      "Business studies",
      "Transhumanism",
      "Michelle Murphy",
      "American Political Science Association",
      "Scientific literacy",
      "Deakin University",
      "Index of sociology articles",
      "Political sociology",
      "Sociology of knowledge",
      "The Journal of Political Philosophy",
      "Scientific dissent",
      "Geography",
      "Normal science",
      "Interdisciplinary",
      "Cornell University",
      "Technoscience",
      "International relations",
      "Emma Kowal",
      "Relationship between religion and science",
      "Historians of science",
      "Social shaping of technology",
      "Neo-Luddism",
      "Public law",
      "Scientism",
      "James S. Fishkin",
      "Technology dynamics",
      "Research ethics",
      "Historical sociology",
      "Science Museum, London",
      "Women's studies",
      "Harvard University",
      "The Structure of Scientific Revolutions",
      "Matthias Gross",
      "Vegan studies",
      "Michel Callon",
      "History of Science Society",
      "Humanities",
      "Military history",
      "Bruno Latour",
      "Technology assessment",
      "Double hermeneutic",
      "Westminster",
      "Corpus Aristotelicum",
      "American Sociological Association",
      "Linguistics",
      "Social anthropology",
      "Cultural history",
      "Cultural lag",
      "José Leite Lopes",
      "Innovation system",
      "Education",
      "Abnormal psychology",
      "Technological optimism",
      "Legal history",
      "Bernard Stiegler",
      "Gender studies",
      "United States Constitution",
      "Development studies",
      "Sociology",
      "Technological change",
      "Modernity",
      "Technological convergence",
      "Boundary-work",
      "Political economy",
      "Diffusion of innovations",
      "Velocipede",
      "Economic history",
      "Donald Angus MacKenzie",
      "Archaeology",
      "JSTOR (identifier)",
      "Rachel Carson",
      "Post-normal science",
      "Social constructivism",
      "Scientific misconduct",
      "Horizon scanning",
      "Design studies",
      "Social Studies of Science",
      "Digital divide",
      "Macroeconomics",
      "Business administration",
      "Water privatization",
      "Scientometrics",
      "Criminology",
      "Karen Barad",
      "Languages of science",
      "Recommender system",
      "Science of team science",
      "Cognitive psychology",
      "Representative democracy",
      "Thomas P. Hughes (historian)",
      "History of science",
      "Scientific enterprise",
      "Fuzzy logic",
      "Andrew Feenberg",
      "Sociology of the history of science",
      "Auxiliary sciences of history",
      "Simon Schaffer",
      "European Inter-University Association on Society, Science and Technology",
      "Law",
      "Anthrozoology",
      "Bar-Ilan University",
      "Food studies",
      "Lee Iacocca",
      "Hurricane Sandy",
      "Developmental psychology",
      "PMID (identifier)",
      "Veil of ignorance",
      "Hype cycle",
      "Urban sociology",
      "Technology transfer",
      "Sandra Harding",
      "Reverse salient",
      "Political history",
      "Science of science policy",
      "OCLC (identifier)",
      "Factor 10",
      "Knowledge production modes",
      "Stanford University",
      "Lucy Suchman",
      "Economics",
      "Technological revolution",
      "Langdon Winner",
      "Engineering ethics",
      "Semiotics",
      "Urban planning",
      "Integrated geography",
      "Geisteswissenschaft",
      "Paradigm shift",
      "Steven L. Goldman",
      "Policy",
      "Ludwik Fleck",
      "International Journal of Technoethics",
      "Critique of technology",
      "Philosophy of history",
      "Regional planning",
      "Canada",
      "Mathematical economics",
      "Elting E. Morison",
      "Technology and Culture",
      "John Law (sociologist)",
      "United States",
      "No innovation without representation",
      "Assemblage (philosophy)",
      "Antipositivism",
      "Anthropology of technology",
      "Anthropology",
      "Skunkworks project",
      "Technological determinism",
      "Philosophy of technology",
      "Ford Motor Company",
      "Cognitive science",
      "List of social science journals",
      "Jurisprudence",
      "Trevor Pinch",
      "Direct democracy",
      "Bibliometrics",
      "Early adopter",
      "Anthropocene",
      "Modernization theory",
      "MIT Press",
      "Human ecology",
      "Information science",
      "Tragedy of the commons",
      "Motorola",
      "Judy Wajcman",
      "Sociotechnical system",
      "Philosophy of science",
      "Doi (identifier)",
      "Feminist science and technology studies",
      "Leapfrogging",
      "History of science policy",
      "Technical geography",
      "Steve Fuller (sociologist)",
      "Science policy",
      "Regulation of science",
      "Environmental social science",
      "Rhetoric of science",
      "Online marketplace",
      "Innovation",
      "Funding of science",
      "Academic bias",
      "Political science",
      "Regional science",
      "Positivism",
      "Federal Communications Commission",
      "History of technology",
      "DDT",
      "Unity of science",
      "Carl May",
      "Community studies",
      "Actor–network theory",
      "India",
      "Evidence-based policy",
      "Sal Restivo",
      "Economics of scientific knowledge",
      "Sociology of the Internet",
      "Social psychology",
      "Antiscience",
      "Scientific skepticism",
      "Uruguay",
      "Philosophy",
      "Donna Haraway",
      "Economics of science",
      "Outline of social science",
      "Neo-colonial science",
      "Dematerialization (products)",
      "Wiebe Bijker",
      "History of medicine",
      "Human geography",
      "Robin Williams (academic)",
      "David F. Noble",
      "Public policy",
      "Philosophy of psychology",
      "Net neutrality",
      "Society for Social Studies of Science",
      "Francisco Sagasti",
      "Silent Spring",
      "Social epistemology",
      "Criticism of technology",
      "Comparative politics",
      "Science education",
      "Political philosophy",
      "Jorge Sabato",
      "Thomas Kuhn",
      "Public administration",
      "Scientific priority",
      "Massachusetts Institute of Technology",
      "Financial technology",
      "Technocracy",
      "History of science and technology",
      "David Bloor",
      "Normalization process theory",
      "Helen Verran",
      "Peru",
      "Science and technology in Israel",
      "Academic freedom",
      "Long Island",
      "Disruptive innovation",
      "Venezuela",
      "Postpositivism",
      "Systems engineering",
      "Personality psychology",
      "Microeconomics",
      "Cultural studies",
      "Malaysia",
      "Psychology of science",
      "Steve Woolgar",
      "Econometrics",
      "Historical materialism",
      "List of national legal systems",
      "University of California, Berkeley",
      "Johan Schot",
      "Social construction of technology",
      "Theories of technology",
      "Consilience",
      "Land-use planning",
      "Digital media use and mental health",
      "Sociology of scientific knowledge",
      "Harry Collins",
      "Criticism of science",
      "Shobita Parthasarathy"
    ]
  },
  "Information retrieval applications": {
    "url": "https://en.wikipedia.org/wiki/Information_retrieval_applications",
    "title": "Information retrieval applications",
    "content": "Information retrieval ( IR ) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need . The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science [ 1 ] of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds. Cross-modal retrieval implies retrieval across modalities. Automated information retrieval systems are used to reduce what has been called information overload . An IR system is a software system that provides access to books, journals and other documents; it also stores and manages those documents. Web search engines are the most visible IR applications. An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval, a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevance . An object is an entity that is represented by information in a content collection or database . User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching. [ 2 ] Depending on the application the data objects may be, for example, text documents, images, [ 3 ] audio, [ 4 ] mind maps [ 5 ] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata . Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. [ 6 ] there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute — J. E. Holmstrom, 1948 The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. [ 7 ] It would appear that Bush was inspired by patents for a 'statistical machine' – filed by Emanuel Goldberg in the 1920s and 1930s – that searched for documents stored on film. [ 8 ] The first description of a computer searching for information was described by Holmstrom in 1948, [ 9 ] detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy Desk Set . In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). [ 7 ] Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s. In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further. By the late 1990s, the rise of the World Wide Web fundamentally transformed information retrieval. While early search engines such as AltaVista (1995) and Yahoo! (1994) offered keyword-based retrieval, they were limited in scale and ranking refinement. The breakthrough came in 1998 with the founding of Google , which introduced the PageRank algorithm, [ 10 ] using the web's hyperlink structure to assess page importance and improve relevance ranking. During the 2000s, web search systems evolved rapidly with the integration of machine learning techniques. These systems began to incorporate user behavior data (e.g., click-through logs), query reformulation, and content-based signals to improve search accuracy and personalization. In 2009, Microsoft launched Bing , introducing features that would later incorporate semantic web technologies through the development of its Satori knowledge base. Academic analysis [ 11 ] have highlighted Bing's semantic capabilities, including structured data use and entity recognition, as part of a broader industry shift toward improving search relevance and understanding user intent through natural language processing. A major leap occurred in 2018, when Google deployed BERT ( B idirectional E ncoder R epresentations from T ransformers) to better understand the contextual meaning of queries and documents. This marked one of the first times deep neural language models were used at scale in real-world retrieval systems. [ 12 ] BERT's bidirectional training enabled a more refined comprehension of word relationships in context, improving the handling of natural language queries. Because of its success, transformer-based models gained traction in academic research and commercial search applications. [ 13 ] Simultaneously, the research community began exploring neural ranking models that outperformed traditional lexical-based methods. Long-standing benchmarks such as the T ext RE trieval C onference ( TREC ), initiated in 1992, and more recent evaluation frameworks Microsoft MARCO( MA chine R eading CO mprehension) (2019) [ 14 ] became central to training and evaluating retrieval systems across multiple tasks and domains. MS MARCO has also been adopted in the TREC Deep Learning Tracks, where it serves as a core dataset for evaluating advances in neural ranking models within a standardized benchmarking environment. [ 15 ] As deep learning became integral to information retrieval systems, researchers began to categorize neural approaches into three broad classes: sparse , dense , and hybrid models. Sparse models, including traditional term-based methods and learned variants like SPLADE, rely on interpretable representations and inverted indexes to enable efficient exact term matching with added semantic signals. [ 16 ] Dense models, such as dual-encoder architectures like ColBERT, use continuous vector embeddings to support semantic similarity beyond keyword overlap. [ 17 ] Hybrid models aim to combine the advantages of both, balancing the lexical (token) precision of sparse methods with the semantic depth of dense models. This way of categorizing models balances scalability, relevance, and efficiency in retrieval systems. [ 18 ] As IR systems increasingly rely on deep learning, concerns around bias, fairness, and explainability have also come to the picture. Research is now focused not just on relevance and efficiency, but on transparency, accountability, and user trust in retrieval algorithms. Areas where information retrieval techniques are employed include (the entries are in alphabetical order within each category): Methods/Techniques in which information retrieval techniques are employed include: In order to effectively retrieve relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model. In addition to the theoretical distinctions, modern information retrieval models are also categorized on how queries and documents are represented and compared, using a practical classification distinguishing between sparse, dense and hybrid models. [ 16 ] This classification has become increasingly common in both academic and the real world applications and is getting widely adopted and used in evaluation benchmarks for Information Retrieval models. [ 18 ] [ 19 ] The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval [ clarification needed ] or top-k retrieval, include precision and recall . All measures assume a ground truth notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevance.",
    "links": [
      "Latent Dirichlet allocation",
      "Information access",
      "Tf–idf",
      "Cross-modal retrieval",
      "Categorization",
      "Digital libraries",
      "European Conference on Information Retrieval",
      "Doi (identifier)",
      "Allen Kent",
      "Dimension reduction",
      "Grateful Med",
      "Automatic summarization",
      "Ricardo Baeza-Yates",
      "Pearl growing",
      "Personal information management",
      "Social information seeking",
      "Alvin Weinberg",
      "International World Wide Web Conference",
      "Text Retrieval Conference",
      "Retrievability",
      "S2CID (identifier)",
      "JSTOR (identifier)",
      "Search engines",
      "Ground truth",
      "Apache Lucene",
      "Video retrieval",
      "Latent semantic analysis",
      "As We May Think",
      "Spam filtering",
      "Xapian",
      "OSTI (identifier)",
      "Tabulating machine",
      "Knowledge organization",
      "Collaborative information seeking",
      "Hdl (identifier)",
      "Information seeking",
      "Nicholas Jardine",
      "Sergey Brin",
      "Yahoo! Inc. (1995–2017)",
      "Theodor Nelson",
      "Science and technology studies",
      "Memory",
      "Computer memory",
      "Information Retrieval Facility",
      "Query understanding",
      "Privacy",
      "Federated search",
      "Probabilistic relevance model",
      "Data modeling",
      "Univac",
      "Enterprise search",
      "Social search",
      "Orthogonality",
      "Semantic Web",
      "Joseph Marie Jacquard",
      "Search engine indexing",
      "Preservation (library and archival science)",
      "Information filtering",
      "Sphinx (search engine)",
      "Information technology",
      "Atlantic Monthly",
      "Software engineering",
      "Terrier Search Engine",
      "Information society",
      "C. J. van Rijsbergen",
      "Taxonomy",
      "Music information retrieval",
      "Web mining",
      "Hypertext",
      "Independence (mathematical logic)",
      "Information needs",
      "Database",
      "Punched cards",
      "Word embedding",
      "Apache Solr",
      "SPLADE",
      "Data mining",
      "CiteSeerX (identifier)",
      "Censorship",
      "Outline of information science",
      "Data retrieval",
      "Don Swanson",
      "Relevance feedback",
      "Learning to rank",
      "Information science",
      "Ill-posed",
      "Computational linguistics",
      "Controlled vocabulary",
      "Vertical search",
      "Information extraction",
      "Communications of the ACM",
      "Language model",
      "PMID (identifier)",
      "Temporal information retrieval",
      "Nicholas J. Belkin",
      "Citation index",
      "Microsoft",
      "Legal information retrieval",
      "Precision and recall",
      "Generalized vector space model",
      "National Bureau of Standards",
      "Desk Set",
      "Computer data storage",
      "Fuzzy retrieval",
      "Desktop search",
      "Lemur Project",
      "Natural language user interface",
      "Library classification",
      "CERN",
      "ArXiv (identifier)",
      "Ranking (information retrieval)",
      "Uncertain inference",
      "Microsoft Bing",
      "Eugene Garfield",
      "Relevance (information retrieval)",
      "Mind maps",
      "Herman Hollerith",
      "OCLC (identifier)",
      "Science",
      "Term Discrimination",
      "World Wide Web",
      "Wikipedia",
      "MEDLARS",
      "Information architecture",
      "Robert R. Korfhage",
      "Co-occurrence",
      "Elasticsearch",
      "Probabilistic relevance model (BM25)",
      "Recommender systems",
      "Vector space model",
      "Information overload",
      "Compound term processing",
      "Information behavior",
      "Multi-document summarization",
      "Bill Maron",
      "XML retrieval",
      "Case Western Reserve University",
      "Computing",
      "SMART Information Retrieval System",
      "Divergence-from-randomness model",
      "Text corpora",
      "Grateful Dead",
      "PageRank",
      "Association for Computing Machinery",
      "National Institute of Standards and Technology",
      "BERT (language model)",
      "3D retrieval",
      "Robert M. Hayes (information scientist)",
      "Subject indexing",
      "AltaVista",
      "Emanuel Goldberg",
      "Quantum information science",
      "Philosophy of information",
      "Topic-based vector space model",
      "Google",
      "Multimedia information retrieval",
      "Library and information science",
      "Karen Spärck Jones Award",
      "J. C. R. Licklider",
      "Intellectual property",
      "Cultural studies",
      "Metadata",
      "Hans Peter Luhn",
      "Latent semantic indexing",
      "Tim Berners-Lee",
      "Calvin Mooers",
      "Knowledge visualization",
      "Jacquard loom",
      "MIT",
      "Vannevar Bush",
      "Adversarial information retrieval",
      "Melvin Earl Maron",
      "Human–computer information retrieval",
      "Special Interest Group on Information Retrieval",
      "Informatics",
      "Cornelis J. van Rijsbergen",
      "Wayback Machine",
      "JASIS",
      "Cyril W. Cleverdon",
      "ISSN (identifier)",
      "Bayes' theorem",
      "Karen Spärck Jones",
      "Image retrieval",
      "Set (mathematics)",
      "Standard Boolean model",
      "Donald A.B. Lindberg",
      "Keypunch",
      "ISBN (identifier)",
      "Web search engine",
      "Question answering",
      "National Library of Medicine",
      "Intellectual freedom",
      "Bibliometrics",
      "Geographic information retrieval",
      "F. Wilfrid Lancaster",
      "Gerard Salton",
      "Document classification",
      "Extended Boolean model",
      "PMC (identifier)",
      "Nearest centroid classifier",
      "Scalability",
      "Ontology (information science)",
      "Gerard Salton Award",
      "Full-text search",
      "Larry Page",
      "Cross-language information retrieval",
      "Information management",
      "Information system",
      "1890 US census",
      "Evaluation measures (information retrieval)",
      "Binary Independence Model",
      "Tony Kent Strix award"
    ]
  },
  "Memory": {
    "url": "https://en.wikipedia.org/wiki/Memory",
    "title": "Memory",
    "content": "Memory is the faculty of the mind by which data or information is encoded , stored, and retrieved when needed. It is the retention of information over time for the purpose of influencing future action . [ 1 ] If past events could not be remembered, it would be impossible for language, relationships, or personal identity to develop. [ 2 ] Memory loss is usually described as forgetfulness or amnesia . [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] Memory is often understood as an informational processing system with explicit and implicit functioning that is made up of a sensory processor , short-term (or working ) memory, and long-term memory . [ 9 ] This can be related to the neuron . The sensory processor allows information from the outside world to be sensed in the form of chemical and physical stimuli and attended to various levels of focus and intent. Working memory serves as an encoding and retrieval processor. Information in the form of stimuli is encoded in accordance with explicit or implicit functions by the working memory processor. The working memory also retrieves information from previously stored material. Finally, the function of long-term memory is to store through various categorical models or systems. [ 9 ] Declarative, or explicit memory , is the conscious storage and recollection of data. [ 10 ] Under declarative memory resides semantic and episodic memory . Semantic memory refers to memory that is encoded with specific meaning. [ 2 ] Meanwhile, episodic memory refers to information that is encoded along a spatial and temporal plane. [ 11 ] [ 12 ] [ 13 ] Declarative memory is usually the primary process thought of when referencing memory. [ 2 ] Non-declarative, or implicit, memory is the unconscious storage and recollection of information. [ 14 ] An example of a non-declarative process would be the unconscious learning or retrieval of information by way of procedural memory , or a priming phenomenon. [ 2 ] [ 14 ] [ 15 ] Priming is the process of subliminally arousing specific responses from memory and shows that not all memory is consciously activated, [ 15 ] whereas procedural memory is the slow and gradual learning of skills that often occurs without conscious attention to learning. [ 2 ] [ 14 ] Memory is not a perfect processor and is affected by many factors. The ways by which information is encoded, stored, and retrieved can all be corrupted. Pain, for example, has been identified as a physical condition that impairs memory, and has been noted in animal models as well as chronic pain patients. [ 16 ] [ 17 ] [ 18 ] [ 19 ] The amount of attention given new stimuli can diminish the amount of information that becomes encoded for storage. [ 2 ] Also, the storage process can become corrupted by physical damage to areas of the brain that are associated with memory storage, such as the hippocampus. [ 20 ] [ 21 ] Finally, the retrieval of information from long-term memory can be disrupted because of decay within long-term memory. [ 2 ] Normal functioning, decay over time, and brain damage all affect the accuracy and capacity of the memory. [ 22 ] [ 23 ] Sensory memory holds information, derived from the senses, less than one second after an item is perceived. The ability to look at an item and remember what it looked like with just a split second of observation, or memorization, is an example of sensory memory. It is out of cognitive control and is an automatic response. With very short presentations, participants often report that they seem to \"see\" more than they can actually report. The first precise experiments exploring this form of sensory memory were conducted by George Sperling (1963) [ 24 ] using the \"partial report paradigm.\" Subjects were presented with a grid of 12 letters, arranged into three rows of four. After a brief presentation, subjects were then played either a high, medium or low tone, cuing them which of the rows to report. Based on these partial report experiments, Sperling was able to show that the capacity of sensory memory was approximately 12 items, but that it degraded very quickly (within a few hundred milliseconds). Because this form of memory degrades so quickly, participants would see the display but be unable to report all of the items (12 in the \"whole report\" procedure) before they decayed. This type of memory cannot be prolonged via rehearsal. Three types of sensory memories exist. Iconic memory is a fast decaying store of visual information, a type of sensory memory that briefly stores an image that has been perceived for a small duration. Echoic memory is a fast decaying store of auditory information, also a sensory memory that briefly stores sounds that have been perceived for short durations. [ 25 ] [ 26 ] Haptic memory is a type of sensory memory that represents a database for touch stimuli. How we encode information has a big impact on how well we remember it later. Research on levels of processing shows that memory is better when encoding emphasizes “deep” aspects of a stimulus (such as meaning) vs. “shallow” aspects of a stimulus (such as a place). “Deep” encoding leads to a highly distributed representation and “shallow” encoding leads to a sparse representation. Thinking about meaning creates a richer, more connected memory, while shallow processing leads to weaker, less lasting traces. [ 27 ] Memory also works best when the cues we use to recall information match the ones present during learning which connects to the principle called encoding specificity. [ 28 ] Similarly, context dependence shows that remembering is easier when the environment at study matches the environment at test. For example, studying in a “wet” lab can help if the test is in the same setting. [ 29 ] Overall, these findings show that both the way we think about information and the context in which we learn it play a major role in memory. Short-term memory, not to be confused with working memory, allows recall for a period of several seconds to a minute without rehearsal. Its capacity, however, is very limited. In 1956, George A. Miller (1920–2012), when working at Bell Laboratories , conducted experiments showing that the store of short-term memory was 7±2 items. (Hence, the title of his famous paper, \"The Magical Number 7±2.\" ) Modern perspectives estimate the capacity of short-term memory to be lower, typically on the order of 4–5 items, [ 30 ] or argue for a more flexible limit based on information instead of items. [ 31 ] Memory capacity can be increased through a process called chunking . [ 32 ] For example, in recalling a ten-digit telephone number , a person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456), and, last, a four-digit chunk (7890). This method of remembering telephone numbers is far more effective than attempting to remember a string of 10 digits; this is because we are able to chunk the information into meaningful groups of numbers. This is reflected in some countries' tendencies to display telephone numbers as several chunks of two to four numbers. Short-term memory is believed to rely mostly on an acoustic code for storing information, and to a lesser extent on a visual code. Conrad (1964) [ 33 ] found that test subjects had more difficulty recalling collections of letters that were acoustically similar, e.g., E, P, D. Confusion with recalling acoustically similar letters rather than visually similar letters implies that the letters were encoded acoustically. Conrad's (1964) study, however, deals with the encoding of written text. Thus, while the memory of written language may rely on acoustic components, generalizations to all forms of memory cannot be made. The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration. This means that information is not retained indefinitely. By contrast, while the total capacity of long-term memory has yet to be established, it can store much larger quantities of information. Furthermore, it can store this information for a much longer duration, potentially for a whole life span. For example, given a random seven-digit number, one may remember it for only a few seconds before forgetting, suggesting it was stored in short-term memory. On the other hand, one can remember telephone numbers for many years through repetition; this information is said to be stored in long-term memory. While short-term memory encodes information acoustically, long-term memory encodes it semantically: Baddeley (1966) [ 34 ] discovered that, after 20 minutes, test subjects had the most difficulty recalling a collection of words that had similar meanings (e.g. big, large, great, huge) long-term. Another part of long-term memory is episodic memory, \"which attempts to capture information such as 'what', 'when' and 'where ' \". [ 35 ] With episodic memory, individuals are able to recall specific events such as birthday parties and weddings. Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex ) and the parietal lobe . Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. It was thought that without the hippocampus new memories were unable to be stored into long-term memory and that there would be a very short attention span , as first gleaned from patient Henry Molaison [ 36 ] [ 37 ] after what was thought to be the full removal of both his hippocampi. More recent examination of his brain, post-mortem, shows that the hippocampus was more intact than first thought, throwing theories drawn from the initial data into question. The hippocampus may be involved in changing neural connections for a period of three months or more after the initial learning. Research has suggested that long-term memory storage in humans may be maintained by DNA methylation , [ 38 ] and the 'prion' gene . [ 39 ] [ 40 ] Further research investigated the molecular basis for long-term memory . By 2015 it had become clear that long-term memory requires gene transcription activation and de novo protein synthesis . [ 41 ] Long-term memory formation depends on both the activation of memory promoting genes and the inhibition of memory suppressor genes, and DNA methylation / DNA demethylation was found to be a major mechanism for achieving this dual regulation. [ 42 ] Rats with a new, strong long-term memory due to contextual fear conditioning have reduced expression of about 1,000 genes and increased expression of about 500 genes in the hippocampus 24 hours after training, thus exhibiting modified expression of 9.17% of the rat hippocampal genome. Reduced gene expressions were associated with methylations of those genes. [ 43 ] Considerable further research into long-term memory has illuminated the molecular mechanisms by which methylations are established or removed, as reviewed in 2022. [ 44 ] These mechanisms include, for instance, signal-responsive TOP2B -induced double-strand breaks in immediate early genes . Also the messenger RNAs of many genes that had been subjected to methylation-controlled increases or decreases are transported by neural granules ( messenger RNP ) to the dendritic spines . At these locations the messenger RNAs can be translated into the proteins that control signaling at neuronal synapses . [ 44 ] The transition of a memory from short term to long term is called memory consolidation . Little is known about the physiological processes involved. Two propositions of how the brain achieves this task are backpropagation or backprop and positive feedback from the endocrine system. Backprop has been proposed as a mechanism the brain uses to achieve memory consolidation and has been used, for example by Geoffrey E. Hinton , Nobel Prize laureate for Physics in 2024, to build AI software. It implies a feedback to neurons consolidating a given memory to erase that information when the brain learns that that information is misleading or wrong. However, empirical evidence of its existence is not available. [ 45 ] On the contrary, positive feedback for consolidating a certain short-term memory registered in neurons, and considered by the neuro-endocrine systems to be useful, will make that short-term memory to consolidate into a permanent one. This has been shown to be true experimentally first in insects, [ 46 ] [ 47 ] [ 48 ] [ 49 ] [ 50 ] which use arginine and nitric oxide levels in their brains and endorphin receptors for this task. The involvement of arginine and nitric oxide in memory consolidation has been confirmed in birds, mammals and other creatures, including humans. [ 51 ] Glial cells have also an important role in memory formation, although how they do their work remains to be unveiled. [ 52 ] [ 53 ] Other mechanisms for memory consolidation can not be discarded. The multi-store model (also known as Atkinson–Shiffrin memory model ) was first described in 1968 by Atkinson and Shiffrin . The multi-store model has been criticised for being too simplistic. For instance, long-term memory is believed to be actually made up of multiple subcomponents, such as episodic and procedural memory . It also proposes that rehearsal is the only mechanism by which information eventually reaches long-term storage, but evidence shows us capable of remembering things without rehearsal. The model also shows all the memory stores as being a single unit whereas research into this shows differently. For example, short-term memory can be broken up into different units such as visual information and acoustic information. In a study by Zlonoga and Gerber (1986), patient 'KF' demonstrated certain deviations from the Atkinson–Shiffrin model. Patient KF was brain damaged , displaying difficulties regarding short-term memory. Recognition of sounds such as spoken numbers, letters, words, and easily identifiable noises (such as doorbells and cats meowing) were all impacted. Visual short-term memory was unaffected, suggesting a dichotomy between visual and audial memory. [ 54 ] In 1974 Baddeley and Hitch proposed a \"working memory model\" that replaced the general concept of short-term memory with active maintenance of information in short-term storage. In this model, working memory consists of three basic stores: the central executive, the phonological loop, and the visuo-spatial sketchpad. In 2000 this model was expanded with the multimodal episodic buffer ( Baddeley's model of working memory ). [ 55 ] The central executive essentially acts as an attention sensory store. It channels information to the three component processes: the phonological loop, the visuospatial sketchpad, and the episodic buffer. The phonological loop stores auditory information by silently rehearsing sounds or words in a continuous loop: the articulatory process (for example the repetition of a telephone number over and over again). A short list of data is easier to remember. The phonological loop is occasionally disrupted. Irrelevant speech or background noise can impede the phonological loop. Articulatory suppression can also confuse encoding and words that sound similar can be switched or misremembered through the phonological similarity effect. the phonological loop also has a limit to how much it can hold at once which means that it is easier to remember a lot of short words rather than a lot of long words, according to the word length effect. The visuospatial sketchpad stores visual and spatial information. It is engaged when performing spatial tasks (such as judging distances) or visual ones (such as counting the windows on a house or imagining images). Those with aphantasia will not be able to engage the visuospatial sketchpad. The episodic buffer is dedicated to linking information across domains to form integrated units of visual, spatial, and verbal information and chronological ordering (e.g., the memory of a story or a movie scene). The episodic buffer is also assumed to have links to long-term memory and semantic meaning. The working memory model explains many practical observations, such as why it is easier to do two different tasks, one verbal and one visual, than two similar tasks, and the aforementioned word-length effect. Working memory is also the premise for what allows us to do everyday activities involving thought. It is the section of memory where we carry out thought processes and use them to learn and reason about topics. [ 55 ] Researchers distinguish between recognition and recall memory. Recognition memory tasks require individuals to indicate whether they have encountered a stimulus (such as a picture or a word) before. Recall memory tasks require participants to retrieve previously learned information. For example, individuals might be asked to produce a series of actions they have seen before or to say a list of words they have heard before. Topographical memory involves the ability to orient oneself in space, to recognize and follow an itinerary, or to recognize familiar places. [ 56 ] Getting lost when traveling alone is an example of the failure of topographic memory. [ 57 ] Flashbulb memories are clear episodic memories of unique and highly emotional events. [ 58 ] People remembering where they were or what they were doing when they first heard the news of President Kennedy 's assassination in 1963 [ 59 ] the 9/11 or Sydney Siege are examples of flashbulb memories. Anderson (1976) [ 60 ] divides long-term memory into declarative (explicit) and procedural (implicit) memories. Declarative memory requires conscious recall , in that some conscious process must call back the information. It is sometimes called explicit memory , since it consists of information that is explicitly stored and retrieved. Declarative memory can be further sub-divided into semantic memory , concerning principles and facts taken independent of context; and episodic memory , concerning information specific to a particular context, such as a time and place. Semantic memory allows the encoding of abstract knowledge about the world, such as \"Paris is the capital of France\". Episodic memory, on the other hand, is used for more personal memories, such as the sensations, emotions, and personal associations of a particular place or time. Episodic memories often reflect the \"firsts\" in life such as a first kiss, first day of school or first time winning a championship. These are key events in one's life that can be remembered clearly. Research suggests that declarative memory is supported by several functions of the medial temporal lobe system which includes the hippocampus. [ 61 ] Autobiographical memory – memory for particular events within one's own life – is generally viewed as either equivalent to, or a subset of, episodic memory. Visual memory is part of memory preserving some characteristics of our senses pertaining to visual experience. One is able to place in memory information that resembles objects, places, animals or people in sort of a mental image . Visual memory can result in priming and it is assumed some kind of perceptual representational system underlies this phenomenon. [ 61 ] In contrast, procedural memory (or implicit memory ) is not based on the conscious recall of information, but on implicit learning . It can best be summarized as remembering how to do something. Procedural memory is primarily used in learning motor skills and can be considered a subset of implicit memory. It is revealed when one does better in a given task due only to repetition – no new explicit memories have been formed, but one is unconsciously accessing aspects of those previous experiences. Procedural memory involved in motor learning depends on the cerebellum and basal ganglia . [ 62 ] A characteristic of procedural memory is that the things remembered are automatically translated into actions, and thus sometimes difficult to describe. Some examples of procedural memory include the ability to ride a bike or tie shoelaces. [ 63 ] Another major way to distinguish different memory functions is whether the content to be remembered is in the past, retrospective memory , or in the future, prospective memory . John Meacham introduced this distinction in a paper presented at the 1975 American Psychological Association annual meeting and subsequently included by Ulric Neisser in his 1982 edited volume, Memory Observed: Remembering in Natural Contexts . [ 64 ] [ 65 ] Thus, retrospective memory as a category includes semantic, episodic and autobiographical memory. In contrast, prospective memory is memory for future intentions, or remembering to remember (Winograd, 1988). Prospective memory can be further broken down into event- and time-based prospective remembering. Time-based prospective memories are triggered by a time-cue, such as going to the doctor (action) at 4pm (cue). Event-based prospective memories are intentions triggered by cues, such as remembering to post a letter (action) after seeing a mailbox (cue). Cues do not need to be related to the action (as the mailbox/letter example), and lists, sticky-notes, knotted handkerchiefs, or string around the finger all exemplify cues that people use as strategies to enhance prospective memory. Infants do not have the language ability to report on their memories and so verbal reports cannot be used to assess very young children's memory. Throughout the years, however, researchers have adapted and developed a number of measures for assessing both infants' recognition memory and their recall memory. Habituation and operant conditioning techniques have been used to assess infants' recognition memory and the deferred and elicited imitation techniques have been used to assess infants' recall memory. Techniques used to assess infants' recognition memory include the following: Techniques used to assess infants' recall memory include the following: Researchers use a variety of tasks to assess older children and adults' memory. Some examples are: Brain areas involved in the neuroanatomy of memory such as the hippocampus , the amygdala , the striatum , or the mammillary bodies are thought to be involved in specific types of memory. For example, the hippocampus is believed to be involved in spatial learning and declarative learning , while the amygdala is thought to be involved in emotional memory . [ 80 ] Damage to certain areas in patients and animal models and subsequent memory deficits is a primary source of information. However, rather than implicating a specific area, it could be that damage to adjacent areas, or to a pathway traveling through the area is actually responsible for the observed deficit. Further, it is not sufficient to describe memory, and its counterpart, learning , as solely dependent on specific brain regions. Learning and memory are usually attributed to changes in neuronal synapses , thought to be mediated by long-term potentiation and long-term depression . In general, the more emotionally charged an event or experience is, the better it is remembered; this phenomenon is known as the memory enhancement effect . Patients with amygdala damage, however, do not show a memory enhancement effect. [ 81 ] [ 82 ] Hebb distinguished between short-term and long-term memory. He postulated that any memory that stayed in short-term storage for a long enough time would be consolidated into a long-term memory. Later research showed this to be false. Research has shown that direct injections of cortisol or epinephrine help the storage of recent experiences. This is also true for stimulation of the amygdala. This proves that excitement enhances memory by the stimulation of hormones that affect the amygdala. Excessive or prolonged stress (with prolonged cortisol) may hurt memory storage. Patients with amygdalar damage are no more likely to remember emotionally charged words than nonemotionally charged ones. The hippocampus is important for explicit memory. The hippocampus is also important for memory consolidation. The hippocampus receives input from different parts of the cortex and sends its output out to different parts of the brain also. The input comes from secondary and tertiary sensory areas that have processed the information a lot already. Hippocampal damage may also cause memory loss and problems with memory storage. [ 83 ] This memory loss includes retrograde amnesia which is the loss of memory for events that occurred shortly before the time of brain damage. [ 79 ] Cognitive neuroscientists consider memory as the retention, reactivation, and reconstruction of the experience-independent internal representation. The term of internal representation implies that such a definition of memory contains two components: the expression of memory at the behavioral or conscious level, and the underpinning physical neural changes (Dudai 2007). The latter component is also called engram or memory traces (Semon 1904). Some neuroscientists and psychologists mistakenly equate the concept of engram and memory, broadly conceiving all persisting after-effects of experiences as memory; others argue against this notion that memory does not exist until it is revealed in behavior or thought (Moscovitch 2007). One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved. Considering that there are several kinds of memory, depending on types of represented knowledge, underlying mechanisms, processes functions and modes of acquisition, it is likely that different brain areas support different memory systems and that they are in mutual relationships in neuronal networks: \"components of memory representation are distributed widely across different parts of the brain as mediated by multiple neocortical circuits\". [ 84 ] APP and LTP in Alzheimer disease.png Study of the genetics of human memory is in its infancy though many genes have been investigated for their association to memory in humans and non-human animals. A notable initial success was the association of APOE with memory dysfunction in Alzheimer's disease . The search for genes associated with normally varying memory continues. One of the first candidates for normal variation in memory is the protein KIBRA , [ 85 ] [ medical citation needed ] which appears to be associated with the rate at which material is forgotten over a delay period. There has been some evidence that memories are stored in the nucleus of neurons. [ 86 ] [ 87 ] Several genes , proteins and enzymes have been extensively researched for their association with memory. Long-term memory, unlike short-term memory, is dependent upon the synthesis of new proteins. [ 88 ] This occurs within the cellular body, and concerns the particular transmitters, receptors, and new synapse pathways that reinforce the communicative strength between neurons. The production of new proteins devoted to synapse reinforcement is triggered after the release of certain signaling substances (such as calcium within hippocampal neurons) in the cell. In the case of hippocampal cells, this release is dependent upon the expulsion of magnesium (a binding molecule) that is expelled after significant and repetitive synaptic signaling. The temporary expulsion of magnesium frees NMDA receptors to release calcium in the cell, a signal that leads to gene transcription and the construction of reinforcing proteins. [ 89 ] For more information, see long-term potentiation (LTP). One of the newly synthesized proteins in LTP is also critical for maintaining long-term memory. This protein is an autonomously active form of the enzyme protein kinase C (PKC), known as PKMζ . PKMζ maintains the activity-dependent enhancement of synaptic strength and inhibiting PKMζ erases established long-term memories, without affecting short-term memory or, once the inhibitor is eliminated, the ability to encode and store new long-term memories is restored. Also, BDNF is important for the persistence of long-term memories. [ 90 ] The long-term stabilization of synaptic changes is also determined by a parallel increase of pre- and postsynaptic structures such as axonal bouton , dendritic spine and postsynaptic density . [ 91 ] On the molecular level, an increase of the postsynaptic scaffolding proteins PSD-95 and HOMER1c has been shown to correlate with the stabilization of synaptic enlargement. [ 91 ] The cAMP response element-binding protein ( CREB ) is a transcription factor which is believed to be important in consolidating short-term to long-term memories, and which is believed to be downregulated in Alzheimer's disease. [ 92 ] Rats exposed to an intense learning event may retain a life-long memory of the event, even after a single training session. The long-term memory of such an event appears to be initially stored in the hippocampus , but this storage is transient. Much of the long-term storage of the memory seems to take place in the anterior cingulate cortex . [ 93 ] When such an exposure was experimentally applied, more than 5,000 differently methylated DNA regions appeared in the hippocampus neuronal genome of the rats at one and at 24 hours after training. [ 94 ] These alterations in methylation pattern occurred at many genes that were downregulated , often due to the formation of new 5-methylcytosine sites in CpG rich regions of the genome. Furthermore, many other genes were upregulated, likely often due to hypomethylation. Hypomethylation often results from the removal of methyl groups from previously existing 5-methylcytosines in DNA. Demethylation is carried out by several proteins acting in concert, including the TET enzymes as well as enzymes of the DNA base excision repair pathway (see Epigenetics in learning and memory ). The pattern of induced and repressed genes in brain neurons subsequent to an intense learning event likely provides the molecular basis for a long-term memory of the event. Studies of the molecular basis for memory formation indicate that epigenetic mechanisms operating in neurons in the brain play a central role in determining this capability. Key epigenetic mechanisms involved in memory include the methylation and demethylation of neuronal DNA, as well as modifications of histone proteins including methylations , acetylations and deacetylations . Stimulation of brain activity in memory formation is often accompanied by the generation of damage in neuronal DNA that is followed by repair associated with persistent epigenetic alterations. In particular the DNA repair processes of non-homologous end joining and base excision repair are employed in memory formation. [ 95 ] During a new learning experience, a set of genes is rapidly expressed in the brain. This induced gene expression is considered to be essential for processing the information being learned. Such genes are referred to as immediate early genes (IEGs). DNA topoisomerase 2-beta (TOP2B) activity is essential for the expression of IEGs in a type of learning experience in mice termed associative fear memory. [ 96 ] Such a learning experience appears to rapidly trigger TOP2B to induce double-strand breaks in the promoter DNA of IEG genes that function in neuroplasticity . Repair of these induced breaks is associated with DNA demethylation of IEG gene promoters allowing immediate expression of these IEG genes. [ 96 ] The double-strand breaks that are induced during a learning experience are not immediately repaired. About 600 regulatory sequences in promoters and about 800 regulatory sequences in enhancers appear to depend on double strand breaks initiated by topoisomerase 2-beta (TOP2B) for activation. [ 97 ] [ 98 ] The induction of particular double-strand breaks are specific with respect to their inducing signal. When neurons are activated in vitro , just 22 of TOP2B-induced double-strand breaks occur in their genomes. [ 99 ] Such TOP2B-induced double-strand breaks are accompanied by at least four enzymes of the non-homologous end joining (NHEJ) DNA repair pathway (DNA-PKcs, KU70, KU80, and DNA LIGASE IV) (see Figure). These enzymes repair the double-strand breaks within about 15 minutes to two hours. [ 99 ] [ 100 ] The double-strand breaks in the promoter are thus associated with TOP2B and at least these four repair enzymes. These proteins are present simultaneously on a single promoter nucleosome (there are about 147 nucleotides in the DNA sequence wrapped around a single nucleosome) located near the transcription start site of their target gene. [ 100 ] The double-strand break introduced by TOP2B apparently frees the part of the promoter at an RNA polymerase-bound transcription start site to physically move to its associated enhancer (see regulatory sequence ). This allows the enhancer, with its bound transcription factors and mediator proteins , to directly interact with the RNA polymerase paused at the transcription start site to start transcription . [ 99 ] [ 101 ] Contextual fear conditioning in the mouse causes the mouse to have a long-term memory and fear of the location in which it occurred. Contextual fear conditioning causes hundreds of DSBs in mouse brain medial prefrontal cortex (mPFC) and hippocampus neurons (see Figure: Brain regions involved in memory formation). These DSBs predominately activate genes involved in synaptic processes, that are important for learning and memory. [ 102 ] Up until the mid-1980s it was assumed that infants could not encode, retain, and retrieve information. [ 103 ] A growing body of research now indicates that infants as young as 6-months can recall information after a 24-hour delay. [ 104 ] Furthermore, research has revealed that as infants grow older they can store information for longer periods of time; 6-month-olds can recall information after a 24-hour period, 9-month-olds after up to five weeks, and 20-month-olds after as long as twelve months. [ 105 ] In addition, studies have shown that with age, infants can store information faster. Whereas 14-month-olds can recall a three-step sequence after being exposed to it once, 6-month-olds need approximately six exposures in order to be able to remember it. [ 70 ] [ 104 ] Although 6-month-olds can recall information over the short-term, they have difficulty recalling the temporal order of information. It is only by 9 months of age that infants can recall the actions of a two-step sequence in the correct temporal order – that is, recalling step 1 and then step 2. [ 106 ] [ 107 ] In other words, when asked to imitate a two-step action sequence (such as putting a toy car in the base and pushing in the plunger to make the toy roll to the other end), 9-month-olds tend to imitate the actions of the sequence in the correct order (step 1 and then step 2). Younger infants (6-month-olds) can only recall one step of a two-step sequence. [ 104 ] Researchers have suggested that these age differences are probably due to the fact that the dentate gyrus of the hippocampus and the frontal components of the neural network are not fully developed at the age of 6-months. [ 71 ] [ 108 ] [ 109 ] In fact, the term 'infantile amnesia' refers to the phenomenon of accelerated forgetting during infancy. Importantly, infantile amnesia is not unique to humans, and preclinical research (using rodent models) provides insight into the precise neurobiology of this phenomenon. A review of the literature from behavioral neuroscientist Jee Hyun Kim suggests that accelerated forgetting during early life is at least partly due to rapid growth of the brain during this period. [ 110 ] One of the key concerns of older adults is the experience of memory loss , especially as it is one of the hallmark symptoms of Alzheimer's disease . However, memory loss is qualitatively different in normal aging from the kind of memory loss associated with a diagnosis of Alzheimer's (Budson & Price, 2005). Research has revealed that individuals' performance on memory tasks that rely on frontal regions declines with age. Older adults tend to exhibit deficits on tasks that involve knowing the temporal order in which they learned information, [ 111 ] source memory tasks that require them to remember the specific circumstances or context in which they learned information, [ 112 ] and prospective memory tasks that involve remembering to perform an act at a future time. Older adults can manage their problems with prospective memory by using appointment books, for example. Gene transcription profiles were determined for the human frontal cortex of individuals from age 26 to 106 years. Numerous genes were identified with reduced expression after age 40, and especially after age 70. [ 113 ] Genes that play central roles in memory and learning were among those showing the most significant reduction with age. There was also a marked increase in DNA damage , likely oxidative damage , in the promoters of those genes with reduced expression. It was suggested that DNA damage may reduce the expression of selectively vulnerable genes involved in memory and learning. [ 113 ] Much of the current knowledge of memory has come from studying memory disorders , particularly loss of memory, known as amnesia . Amnesia can result from extensive damage to: (a) the regions of the medial temporal lobe, such as the hippocampus, dentate gyrus, subiculum, amygdala, the parahippocampal, entorhinal, and perirhinal cortices [ 114 ] or the (b) midline diencephalic region, specifically the dorsomedial nucleus of the thalamus and the mammillary bodies of the hypothalamus. [ 115 ] There are many sorts of amnesia, and by studying their different forms, it has become possible to observe apparent defects in individual sub-systems of the brain's memory systems, and thus hypothesize their function in the normally working brain. Other neurological disorders such as Alzheimer's disease and Parkinson's disease [ 116 ] [ 117 ] can also affect memory and cognition. [ 118 ] Hyperthymesia , or hyperthymesic syndrome, is a disorder that affects an individual's autobiographical memory, essentially meaning that they cannot forget small details that otherwise would not be stored. [ 119 ] [ 120 ] [ 121 ] Korsakoff's syndrome , also known as Korsakoff's psychosis, amnesic-confabulatory syndrome, is an organic brain disease that adversely affects memory by widespread loss or shrinkage of neurons within the prefrontal cortex. [ 79 ] While not a disorder, a common temporary failure of word retrieval from memory is the tip-of-the-tongue phenomenon. Those with anomic aphasia (also called nominal aphasia or Anomia), however, do experience the tip-of-the-tongue phenomenon on an ongoing basis due to damage to the frontal and parietal lobes of the brain . Memory dysfunction can also occur after viral infections. [ 122 ] Many patients recovering from COVID-19 experience memory lapses . Other viruses can also elicit memory dysfunction, including SARS-CoV-1 , MERS-CoV , Ebola virus and even influenza virus . [ 122 ] [ 123 ] Interference can hamper memorization and retrieval. There is retroactive interference , when learning new information makes it harder to recall old information [ 124 ] and proactive interference , where prior learning disrupts recall of new information. Although interference can lead to forgetting, it is important to keep in mind that there are situations when old information can facilitate learning of new information. Knowing Latin, for instance, can help an individual learn a related language such as French – this phenomenon is known as positive transfer. [ 125 ] Stress has a significant effect on memory formation and learning. In response to stressful situations, the brain releases hormones and neurotransmitters (ex. glucocorticoids and catecholamines) which affect memory encoding processes in the hippocampus. Behavioural research on animals shows that chronic stress produces adrenal hormones which impact the hippocampal structure in the brains of rats. [ 126 ] An experimental study by German cognitive psychologists L. Schwabe and O. Wolf demonstrates how learning under stress also decreases memory recall in humans. [ 127 ] In this study, 48 healthy female and male university students participated in either a stress test or a control group. Those randomly assigned to the stress test group had a hand immersed in ice cold water (the reputable SECPT or 'Socially Evaluated Cold Pressor Test') for up to three minutes, while being monitored and videotaped. Both the stress and control groups were then presented with 32 words to memorize. Twenty-four hours later, both groups were tested to see how many words they could remember (free recall) as well as how many they could recognize from a larger list of words (recognition performance). The results showed a clear impairment of memory performance in the stress test group, who recalled 30% fewer words than the control group. The researchers suggest that stress experienced during learning distracts people by diverting their attention during the memory encoding process. However, memory performance can be enhanced when material is linked to the learning context, even when learning occurs under stress. A separate study by cognitive psychologists Schwabe and Wolf shows that when retention testing is done in a context similar to or congruent with the original learning task (i.e., in the same room), memory impairment and the detrimental effects of stress on learning can be attenuated. [ 128 ] Seventy-two healthy female and male university students, randomly assigned to the SECPT stress test or to a control group, were asked to remember the locations of 15 pairs of picture cards – a computerized version of the card game \"Concentration\" or \"Memory\". The room in which the experiment took place was infused with the scent of vanilla, as odour is a strong cue for memory. Retention testing took place the following day, either in the same room with the vanilla scent again present, or in a different room without the fragrance. The memory performance of subjects who experienced stress during the object-location task decreased significantly when they were tested in an unfamiliar room without the vanilla scent (an incongruent context); however, the memory performance of stressed subjects showed no impairment when they were tested in the original room with the vanilla scent (a congruent context). All participants in the experiment, both stressed and unstressed, performed faster when the learning and retrieval contexts were similar. [ 129 ] This research on the effects of stress on memory may have practical implications for education, for eyewitness testimony and for psychotherapy: students may perform better when tested in their regular classroom rather than an exam room, eyewitnesses may recall details better at the scene of an event than in a courtroom, and persons with post-traumatic stress may improve when helped to situate their memories of a traumatic event in an appropriate context. Stressful life experiences may be a cause of memory loss as a person ages. Glucocorticoids that are released during stress cause damage to neurons that are located in the hippocampal region of the brain. Therefore, the more stressful situations that someone encounters, the more susceptible they are to memory loss later on. The CA1 neurons found in the hippocampus are destroyed due to glucocorticoids decreasing the release of glucose and the reuptake of glutamate . This high level of extracellular glutamate allows calcium to enter NMDA receptors which in return kills neurons. Stressful life experiences can also cause repression of memories where a person moves an unbearable memory to the unconscious mind. [ 79 ] This directly relates to traumatic events in one's past such as kidnappings, being prisoners of war or sexual abuse as a child. The more long term the exposure to stress is, the more impact it may have. However, short term exposure to stress also causes impairment in memory by interfering with the function of the hippocampus. Research shows that subjects placed in a stressful situation for a short amount of time still have blood glucocorticoid levels that have increased drastically when measured after the exposure is completed. When subjects are asked to complete a learning task after short term exposure they often have difficulties. Prenatal stress also hinders the ability to learn and memorize by disrupting the development of the hippocampus and can lead to unestablished long term potentiation in the offspring of severely stressed parents. Although the stress is applied prenatally, the offspring show increased levels of glucocorticoids when they are subjected to stress later on in life. [ 130 ] One explanation for why children from lower socioeconomic backgrounds tend to display poorer memory performance than their higher-income peers is the effects of stress accumulated over the course of the lifetime. [ 131 ] The effects of low income on the developing hippocampus is also thought be mediated by chronic stress responses which may explain why children from lower and higher-income backgrounds differ in terms of memory performance. [ 131 ] Making memories occurs through a three-step process, which can be enhanced by sleep . The three steps are as follows: Sleep affects memory consolidation. During sleep, the neural connections in the brain are strengthened. This enhances the brain's abilities to stabilize and retain memories. There have been several studies which show that sleep improves the retention of memory, as memories are enhanced through active consolidation. System consolidation takes place during slow-wave sleep (SWS). [ 132 ] [ 133 ] This process implicates that memories are reactivated during sleep, but that the process does not enhance every memory. It also implicates that qualitative changes are made to the memories when they are transferred to long-term store during sleep. During sleep, the hippocampus replays the events of the day for the neocortex. The neocortex then reviews and processes memories, which moves them into long-term memory. When one does not get enough sleep it makes it more difficult to learn as these neural connections are not as strong, resulting in a lower retention rate of memories. Sleep deprivation makes it harder to focus, resulting in inefficient learning. [ 132 ] Furthermore, some studies have shown that sleep deprivation can lead to false memories as the memories are not properly transferred to long-term memory. One of the primary functions of sleep is thought to be the improvement of the consolidation of information, as several studies have demonstrated that memory depends on getting sufficient sleep between training and test. [ 134 ] Additionally, data obtained from neuroimaging studies have shown activation patterns in the sleeping brain that mirror those recorded during the learning of tasks from the previous day, [ 134 ] suggesting that new memories may be solidified through such rehearsal. [ 135 ] Although people often think that memory operates like recording equipment, this is not the case. The molecular mechanisms underlying the induction and maintenance of memory are very dynamic and comprise distinct phases covering a time window from seconds to even a lifetime. [ 136 ] In fact, research has revealed that our memories are constructed: \"current hypotheses suggest that constructive processes allow individuals to simulate and imagine future episodes, [ 137 ] happenings, and scenarios. Since the future is not an exact repetition of the past, simulation of future episodes requires a complex system that can draw on the past in a manner that flexibly extracts and recombines elements of previous experiences – a constructive rather than a reproductive system.\" [ 84 ] People can construct their memories when they encode them and/or when they recall them. To illustrate, consider a classic study conducted by Elizabeth Loftus and John Palmer (1974) [ 138 ] in which people were instructed to watch a film of a traffic accident and then asked about what they saw. The researchers found that the people who were asked, \"How fast were the cars going when they smashed into each other?\" gave higher estimates than those who were asked, \"How fast were the cars going when they hit each other?\" Furthermore, when asked a week later whether they had seen broken glass in the film, those who had been asked the question with smashed were twice more likely to report that they had seen broken glass than those who had been asked the question with hit (there was no broken glass depicted in the film). Thus, the wording of the questions distorted viewers' memories of the event. Importantly, the wording of the question led people to construct different memories of the event – those who were asked the question with smashed recalled a more serious car accident than they had actually seen. The findings of this experiment were replicated around the world, and researchers consistently demonstrated that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect . [ 139 ] Research has revealed that asking individuals to repeatedly imagine actions that they have never performed or events that they have never experienced could result in false memories. For instance, Goff and Roediger [ 140 ] (1998) asked participants to imagine that they performed an act (e.g., break a toothpick) and then later asked them whether they had done such a thing. Findings revealed that those participants who repeatedly imagined performing such an act were more likely to think that they had actually performed that act during the first session of the experiment. Similarly, Garry and her colleagues (1996) [ 141 ] asked college students to report how certain they were that they experienced a number of events as children (e.g., broke a window with their hand) and then two weeks later asked them to imagine four of those events. The researchers found that one-fourth of the students asked to imagine the four events reported that they had actually experienced such events as children. That is, when asked to imagine the events they were more confident that they experienced the events. Research reported in 2013 revealed that it is possible to artificially stimulate prior memories and artificially implant false memories in mice. Using optogenetics , a team of RIKEN- MIT scientists caused the mice to incorrectly associate a benign environment with a prior unpleasant experience from different surroundings. Some scientists believe that the study may have implications in studying false memory formation in humans, and in treating PTSD and schizophrenia . [ 142 ] [ 143 ] [ medical citation needed ] Memory reconsolidation is when previously consolidated memories are recalled or retrieved from long-term memory to your active consciousness. During this process, memories can be further strengthened and added to but there is also risk of manipulation involved. We like to think of our memories as something stable and constant when they are stored in long-term memory but this is not the case. There are a large number of studies that found that consolidation of memories is not a singular event but are put through the process again, known as reconsolidation. [ 144 ] This is when a memory is recalled or retrieved and placed back into your working memory. The memory is now open to manipulation from outside sources and the misinformation effect which could be due to misattributing the source of the inconsistent information, with or without an intact original memory trace. [ 145 ] [ 146 ] One thing that can be sure is that memory is malleable. This new research into the concept of reconsolidation has opened the door to methods to help those with unpleasant memories or those that struggle with memories. An example of this is if you had a truly frightening experience and recall that memory in a less arousing environment, the memory will be weaken the next time it is retrieved. [ 144 ] \"Some studies suggest that over-trained or strongly reinforced memories do not undergo reconsolidation if reactivated the first few days after training, but do become sensitive to reconsolidation interference with time.\" [ 144 ] This, however does not mean that all memory is susceptible to reconsolidation. There is evidence to suggest that memory that has undergone strong training and whether or not is it intentional is less likely to undergo reconsolidation. [ 147 ] There was further testing done with rats and mazes that showed that reactivated memories were more susceptible to manipulation, in both good and bad ways, than newly formed memories. [ 148 ] It is still not known whether or not these are new memories formed and it is an inability to retrieve the proper one for the situation or if it is a reconsolidated memory. Because the study of reconsolidation is still a newer concept, there is still debate on whether it should be considered scientifically sound. A UCLA research study published in the June 2008 issue of the American Journal of Geriatric Psychiatry found that people can improve cognitive function and brain efficiency through simple lifestyle changes such as incorporating memory exercises, healthy eating , physical fitness and stress reduction into their daily lives. This study examined 17 subjects, (average age 53) with normal memory performance. Eight subjects were asked to follow a \"brain healthy\" diet, relaxation, physical, and mental exercise (brain teasers and verbal memory training techniques). After 14 days, they showed greater word fluency (not memory) compared to their baseline performance. No long-term follow-up was conducted; it is therefore unclear if this intervention has lasting effects on memory. [ 149 ] Exercise, even at light intensity, significantly improves memory across all age groups, with the greatest benefits observed in children and adolescents. Even low- to moderate-intensity exercise and shorter interventions (1–3 months) can produce meaningful cognitive improvements. [ 150 ] There are a loosely associated group of mnemonic principles and techniques that can be used to vastly improve memory known as the art of memory . The International Longevity Center released in 2001 a report [ 151 ] which includes in pages 14–16 recommendations for keeping the mind in good functionality until advanced age. Some of the recommendations are: Memorization is a method of learning that allows an individual to recall information verbatim. Rote learning is the method most often used. Methods of memorizing things have been the subject of much discussion over the years with some writers, such as Cosmos Rossellius using visual alphabets . The spacing effect shows that an individual is more likely to remember a list of items when rehearsal is spaced over an extended period of time. In contrast to this is cramming : an intensive memorization in a short period of time. The spacing effect is exploited to improve memory in spaced repetition flashcard training. Also relevant is the Zeigarnik effect , which states that people remember uncompleted or interrupted tasks better than completed ones. The so-called Method of loci uses spatial memory to memorize non-spatial information. [ 152 ] Research on sex differences in episodic memory has produced mixed findings. A recent meta-analysis revealed a small overall female advantage, with task-specific variations. Women outperformed men on cued recall and free recall, while men showed an advantage in complex span tasks. No sex differences were observed in serial recall or simple span tasks. Factors such as recall direction, stimulus type, presentation format, response format, and age accounted for variance in results. Importantly, no publication bias was detected, although effect sizes varied by sample source and study reporting. Neuroimaging studies using activation likelihood estimation (ALE) indicated male > female activity in the lateral prefrontal cortex, visual regions, parahippocampal cortex, and cerebellum during long-term memory retrieval. [ 153 ] These findings suggest meaningful sex differences in both behavior and brain function, highlighting the need for cautious interpretation and further controlled research. [ 154 ] Plants lack a specialized organ devoted to memory retention, so plant memory has been a controversial topic in recent years. New advances in the field have identified the presence of neurotransmitters in plants, adding to the hypothesis that plants are capable of remembering. [ 155 ] Action potentials , a physiological response characteristic of neurons , have been shown to have an influence on plants as well, including in wound responses and photosynthesis . [ 155 ] In addition to these homologous features of memory systems in both plants and animals, plants have also been observed to encode, store and retrieve basic short-term memories. One of the most well-studied plants to show rudimentary memory is the Venus flytrap . Native to the subtropical wetlands of the eastern United States , Venus flytraps have evolved the ability to obtain meat for sustenance, likely due to the lack of nitrogen in the soil. [ 156 ] This is done by two trap-forming leaf tips that snap shut once triggered by a potential prey. On each lobe, three trigger hairs await stimulation. In order to maximize the benefit-to-cost ratio, the plant enables a rudimentary form of memory in which two trigger hairs must be stimulated within thirty seconds in order to result in trap closure. [ 156 ] This system ensures that the trap only closes when potential prey is within grasp. The time lapse between trigger hair stimulations suggests that the plant can remember an initial stimulus long enough for a second stimulus to initiate trap closure. This memory is not encoded in a brain, as plants lack this specialized organ. Rather, information is stored in the form of cytoplasmic calcium levels. The first trigger causes a subthreshold cytoplasmic calcium influx. [ 156 ] This initial trigger is not enough to activate trap closure, so a subsequent stimulus allows for a secondary influx of calcium. The latter calcium rise superimposes on the initial one, creating an action potential that passes threshold, resulting in trap closure. [ 156 ] Researchers, to prove that an electrical threshold must be met to stimulate trap closure, excited a single trigger hair with a constant mechanical stimulus using Ag/AgCl electrodes. [ 157 ] The trap closed after only a few seconds. This experiment demonstrated that the electrical threshold, not necessarily the number of trigger hair stimulations, was the contributing factor in Venus flytrap memory. It has been shown that trap closure can be blocked using uncouplers and inhibitors of voltage-gated channels . [ 157 ] After trap closure, these electrical signals stimulate glandular production of jasmonic acid and hydrolases , allowing for digestion of prey. [ 158 ] Many other plants exhibit the capacity to remember, including Mimosa pudica . [ 159 ] An experimental apparatus was designed to drop potted mimosa plants repeatedly from the same distance and at the same speed. It was observed that the plants' defensive response of curling up their leaves decreased over the sixty times the experiment was repeated. To confirm that this was a mechanism of memory rather than exhaustion , some of the plants were shaken post experiment and displayed normal defensive responses of leaf curling. This experiment demonstrated long-term memory in the plants, as it was repeated a month later, and the plants were observed to remain unfazed by the dropping. [ 159 ]",
    "links": [
      "Indirect tests of memory",
      "Childhood memory",
      "Memory implantation",
      "Source-monitoring error",
      "Intelligence and personality",
      "Habituation",
      "Impact of health on intelligence",
      "Personal identity",
      "Body memory",
      "Transactive memory",
      "DNA repair",
      "Inhibitory postsynaptic potential",
      "BBC News Online",
      "George Sperling",
      "American Psychological Association",
      "Visual memory",
      "Storage (memory)",
      "Karl H. Pribram",
      "Neuropsychology",
      "Tip of the tongue",
      "DNA methylation",
      "Nucleosome",
      "Hdl (identifier)",
      "University of Washington",
      "Forecasting",
      "Sleep",
      "Non-homologous end joining",
      "Mnemonic major system",
      "Dentate gyrus",
      "Emotion and memory",
      "Emotional intelligence",
      "Geoffrey Loftus",
      "Memory rehearsal",
      "RGB color model",
      "Repressed memory",
      "Neuroanatomy of memory",
      "Phineas Gage",
      "Histone",
      "Lateralization of brain function",
      "Bibcode (identifier)",
      "Kent Cochrane",
      "Procedural memory",
      "Neurophysiology",
      "Deese–Roediger–McDermott paradigm",
      "Context-dependent memory",
      "State-dependent memory",
      "Social perception",
      "Exosomatic memory",
      "Antonio Damasio",
      "Henry L. Roediger III",
      "Mark Rosenzweig (psychologist)",
      "Environment and intelligence",
      "NMDA receptors",
      "CiteSeerX (identifier)",
      "Jee Hyun Kim",
      "Speech perception",
      "Genome",
      "Knowledge",
      "Object recognition (cognitive science)",
      "Foresight (psychology)",
      "Transcription factor",
      "Glutamate",
      "Height and intelligence",
      "Patricia Goldman-Rakic",
      "Memories of a traumatic event",
      "Memory and aging",
      "Memory & Cognition",
      "Linguistic intelligence",
      "Enhancer (genetics)",
      "Politics of memory",
      "Learning",
      "Executive functions",
      "Improving memory",
      "Cryptomnesia",
      "G factor (psychometrics)",
      "Retrograde amnesia",
      "Motor skill",
      "Animal memory",
      "Histone methylation",
      "Consciousness",
      "Dominic O'Brien",
      "Peripheral vision",
      "Pitch (music)",
      "2014 Sydney hostage crisis",
      "Transcription (biology)",
      "Cerebellum",
      "Thomas Jefferson Building",
      "Luria–Nebraska Neuropsychological Battery",
      "Eyewitness memory",
      "Pasko Rakic",
      "Fear conditioning",
      "Declarative memory",
      "Baddeley's model of working memory",
      "Ability",
      "Retrospective memory",
      "Hippocampus",
      "Neuron",
      "Alzheimer's disease",
      "Sex differences in intelligence",
      "World Memory Championships",
      "Method of loci",
      "DNA damage (naturally occurring)",
      "Understanding",
      "Striatum",
      "Semantic similarity",
      "Mnemonic",
      "Eastern United States",
      "Alexander Luria",
      "Excitatory postsynaptic potential",
      "Alan Baddeley",
      "The Seven Sins of Memory",
      "Intention",
      "Cultural memory",
      "Apolipoprotein E",
      "Cognitive neuropsychology",
      "Learning & Memory",
      "Anterograde amnesia",
      "Reflex",
      "Heritability of IQ",
      "ISSN (identifier)",
      "Operant conditioning",
      "Arthur P. Shimamura",
      "ISBN (identifier)",
      "Protein kinase C",
      "Wetland",
      "Autobiographical memory",
      "Continuous performance task",
      "Memory consolidation",
      "Hyperthymesia",
      "Wakefulness",
      "Short-term memory",
      "Spike-timing-dependent plasticity",
      "PMC (identifier)",
      "List of memory biases",
      "Information",
      "Unconscious mind",
      "Art of memory",
      "Memory erasure",
      "Creativity",
      "Human memory",
      "Cosmos Rossellius",
      "Orthomyxoviridae",
      "Posthypnotic amnesia",
      "Skill",
      "Stanford Encyclopedia of Philosophy",
      "Social intelligence",
      "Gene",
      "Lost in the mall technique",
      "Mini–mental state examination",
      "Amnesia",
      "S2CID (identifier)",
      "Critical thinking",
      "Synapse",
      "Dorsolateral prefrontal cortex",
      "Decay theory",
      "Neuroscience and intelligence",
      "Harmonic",
      "Motor learning",
      "Somatosensory evoked potentials",
      "Rey–Osterrieth complex figure",
      "Language acquisition",
      "Aphantasia",
      "Aging",
      "Rote learning",
      "Sentence processing",
      "Action potential",
      "Neurotransmission",
      "Endel Tulving",
      "Jasmonic acid",
      "Attention",
      "Long-term memory",
      "Misinformation effect",
      "Cognitive epidemiology",
      "Human brain",
      "Trends Neurosci",
      "Axon",
      "Visual system",
      "Interference theory",
      "Richard Shiffrin",
      "Sensory processor",
      "Intermediate-term memory",
      "Georges Chapouthier",
      "Daniel Schacter",
      "Institute of Higher Nervous Activity",
      "Bouton (synapse)",
      "Subliminal stimuli",
      "Episodic memories",
      "Stephen J. Ceci",
      "Human intelligence",
      "P300 (neuroscience)",
      "Photographic memory",
      "Ironic process theory",
      "Maryanne Garry",
      "Shas Pollak",
      "Immediate early gene",
      "Amodal perception",
      "Atkinson–Shiffrin memory model",
      "Subtropical",
      "Retrieval-induced forgetting",
      "Personal-event memory",
      "Encoding (memory)",
      "Hindsight bias",
      "Robert A. Bjork",
      "5-methylcytosine",
      "John Robert Anderson (psychologist)",
      "Natural language",
      "Motivated forgetting",
      "Memory error",
      "Susumu Tonegawa",
      "SECPT stress test",
      "Robert Stickgold",
      "Chronaxie",
      "HOMER1",
      "Problem solving",
      "Eric Kandel",
      "TED (conference)",
      "Working memory",
      "Olin Levi Warner",
      "Set (psychology)",
      "Memory loss",
      "Evolution of human intelligence",
      "Epigenetics",
      "Washington, D.C.",
      "Selective amnesia",
      "Norman Geschwind",
      "Abstraction",
      "Lynn Nadel",
      "Mental representation",
      "Benjamin Libet",
      "Outline of human intelligence",
      "Stress management",
      "Hans-Lukas Teuber",
      "BDNF",
      "Postsynaptic density",
      "Affective forecasting",
      "Data",
      "Jonathan Hancock",
      "Fertility and intelligence",
      "Epinephrine",
      "Psychonomic Bulletin & Review",
      "Memory conformity",
      "Halstead–Reitan Neuropsychological Battery",
      "Triarchic theory of intelligence",
      "Ulric Neisser",
      "Andriy Slyusarchuk",
      "Bell Labs",
      "Messenger RNA",
      "Touch",
      "Glucocorticoids",
      "Cristina Alberini",
      "Recall (memory)",
      "DNA damage",
      "Myelin",
      "Collective memory",
      "Mental image",
      "Dementia",
      "Imagination inflation",
      "Selective retention",
      "Transcription preinitiation complex",
      "Metamemory",
      "Wechsler Memory Scale",
      "Lobes of the brain",
      "Paul R. McHugh",
      "Memory and social interactions",
      "Muscle memory",
      "Spatial visualization ability",
      "Susan Clancy",
      "Severe acute respiratory syndrome coronavirus",
      "Nature Reviews Neuroscience",
      "Immediate early genes",
      "Regulatory sequence",
      "Parkinson's disease",
      "Venus flytrap",
      "Edward N. Zalta",
      "Sigmund Freud",
      "Action (philosophy)",
      "Three-stratum theory",
      "Wechsler Adult Intelligence Scale",
      "Spacing effect",
      "Intuition",
      "Gene expression",
      "List of regions in the human brain",
      "False memory syndrome",
      "Clive Wearing",
      "JSTOR (identifier)",
      "Cognition",
      "Basal ganglia",
      "Haptic memory",
      "Engram (neuropsychology)",
      "Concept",
      "Depth perception",
      "Immunological memory",
      "Photosynthesis",
      "Donald O. Hebb",
      "Verbal memory",
      "Relational frame theory",
      "Planning",
      "Psychometrics",
      "Scholarpedia",
      "Long memory",
      "Imagination",
      "Medial temporal lobe",
      "Myelinogenesis",
      "Reason",
      "Misattribution of memory",
      "Neurotransmitter",
      "Epigenetics in learning and memory",
      "Impact of COVID-19 on neurological, psychological and other mental health outcomes",
      "Free recall",
      "Visual evoked potential",
      "Arthur Lester Benton",
      "Middle East respiratory syndrome–related coronavirus",
      "Cognitive psychology",
      "Recovered-memory therapy",
      "Flashbulb memory",
      "Intertrial priming",
      "Memory (disambiguation)",
      "Parietal lobe",
      "Elsevier",
      "Fluid and crystallized intelligence",
      "The Magical Number Seven, Plus or Minus Two",
      "Geoffrey Hinton",
      "Sleep and memory",
      "Alphabets",
      "Richard C. Atkinson",
      "Memory sport",
      "Radboud University",
      "New Scientist",
      "Declarative learning",
      "Wisconsin Card Sorting Test",
      "Memory and trauma",
      "Larry Squire",
      "Repression (psychoanalysis)",
      "Fatigue",
      "Nutrition and memory",
      "Psychoacoustics",
      "Kenneth Heilman",
      "Implicit learning",
      "Lexical decision task",
      "Color vision",
      "Long-term potentiation",
      "PMID (identifier)",
      "Psycholinguistics",
      "Communication",
      "Intracranial pressure",
      "National Institutes of Health",
      "Brain damage",
      "Eleanor Maguire",
      "Visuospatial sketchpad",
      "Association (psychology)",
      "Base excision repair",
      "Current Directions in Psychological Science",
      "Intellectual giftedness",
      "Aesthetic interpretation",
      "OCLC (identifier)",
      "Irrelevant speech effect",
      "Mind",
      "Flashback (psychology)",
      "Neuroanatomy",
      "Convergence-divergence zone",
      "Methods used to study memory",
      "Echoic memory",
      "Elaborative encoding",
      "Attention span",
      "Pattern recognition",
      "Rehabilitation (neuropsychology)",
      "Hydrolase",
      "Neuropsychological test",
      "Positive feedback",
      "Forgetting",
      "CA1 neurons",
      "Spatial intelligence (psychology)",
      "Dendritic spine",
      "Ben Pridmore",
      "Long-term depression",
      "Muriel Lezak",
      "Cytoplasm",
      "PKMζ",
      "Iconic memory",
      "Translation (biology)",
      "Levels of Processing model",
      "CREB",
      "Effects of stress on memory",
      "United States",
      "Active recall",
      "Rodolfo Llinás",
      "Korsakoff's syndrome",
      "George Armitage Miller",
      "Priming (psychology)",
      "Thought suppression",
      "Bereitschaftspotential",
      "Adaptive memory",
      "Hermann Ebbinghaus",
      "Gland",
      "Neuropsychological assessment",
      "Approximate number system",
      "President Kennedy",
      "Explicit memory",
      "Memory reconsolidation",
      "Information processing (psychology)",
      "Anne Treisman",
      "Numerosity adaptation effect",
      "Implicit memory",
      "Telephone number",
      "Involuntary memory",
      "Promoter (genetics)",
      "Wikisource",
      "Recollection",
      "Amygdala",
      "Haptic perception",
      "Stroop effect",
      "Benton Visual Retention Test",
      "Awareness",
      "Encyclopedia Americana",
      "Arousal",
      "Edith Kaplan",
      "Socioeconomic status and memory",
      "Post-traumatic stress disorder",
      "Cognitive liberty",
      "Dissociative amnesia",
      "Patient N.A.",
      "Prefrontal cortex",
      "Memorization",
      "Doi (identifier)",
      "Elizabeth Loftus",
      "Memory inhibition",
      "Eidetic memory",
      "Neuroregeneration",
      "Clinical neuropsychology",
      "Healthy eating",
      "Articulatory suppression",
      "Cognitive neuroscience",
      "Sensory nervous system",
      "YouTube",
      "Visual perception",
      "Prenatal memory",
      "9/11",
      "Annals of the New York Academy of Sciences",
      "Synaptic plasticity",
      "Mediator (coactivator)",
      "Axoplasmic transport",
      "Spaced repetition",
      "Evoked potential",
      "Histone acetylation and deacetylation",
      "Confabulation",
      "Memory development",
      "Marcia K. Johnson",
      "David Bohm",
      "Episodic memory",
      "Intelligence",
      "Exceptional memory",
      "Postsynaptic potential",
      "Nervous system",
      "Prion",
      "Ephraim Moses Lilien",
      "International Longevity Center",
      "Henry Molaison",
      "Cortisol",
      "Transient global amnesia",
      "Memory disorder",
      "Anomic aphasia",
      "Number sense",
      "DNA oxidation",
      "Brain",
      "Conscious",
      "Voltage-gated ion channel",
      "Prospection",
      "Scientific American",
      "Neuroplasticity",
      "DNA demethylation",
      "Ivan Izquierdo",
      "TET enzymes",
      "Traumatic brain injury",
      "Mimosa pudica",
      "Howard Eichenbaum",
      "Cattell–Horn–Carroll theory",
      "Membrane potential",
      "Frontiers in Behavioral Neuroscience",
      "Absent-mindedness",
      "Event perception",
      "Optogenetics",
      "Sensory memory",
      "Messenger RNP",
      "Hallucination (artificial intelligence)",
      "Cognitive flexibility",
      "Roger Wolcott Sperry",
      "Childhood amnesia",
      "Collective intelligence",
      "Weapon focus",
      "Synaptic transmission",
      "Cram school",
      "Hans Markowitsch",
      "Physical fitness",
      "TOP2B",
      "Assassination of John F. Kennedy",
      "Massachusetts Institute of Technology",
      "Outline of thought",
      "KIBRA",
      "Race and intelligence",
      "Oliver Sacks",
      "Number sense in animals",
      "Semantic memory",
      "Volition (psychology)",
      "Brenda Milner",
      "Parallel individuation system",
      "Eyewitness testimony",
      "Meaningful learning",
      "Experiential avoidance",
      "Library of Congress",
      "Schizophrenia",
      "Zaire ebolavirus",
      "Intellect",
      "Anterior cingulate cortex",
      "Hayling and Brixton tests",
      "Spatial memory",
      "Memory improvement",
      "Elkhonon Goldberg",
      "PSD-95",
      "Intelligence quotient",
      "Motor coordination",
      "Journal of Verbal Learning & Verbal Behavior",
      "False memory",
      "James McGaugh",
      "Perception",
      "Post-traumatic stress",
      "Decision-making",
      "Prospective memory",
      "Neurology",
      "Cellular and Molecular Life Sciences",
      "Effects of alcohol on memory",
      "Chunking (psychology)",
      "Downregulation and upregulation",
      "Forgetting curve",
      "Mammillary bodies",
      "De novo protein synthesis theory of memory formation",
      "Post-traumatic amnesia",
      "Frontal lobe",
      "Zeigarnik effect",
      "Hippocampal",
      "Face perception",
      "Form perception",
      "Thought",
      "Theory of multiple intelligences",
      "PASS theory of intelligence"
    ]
  },
  "Computer memory": {
    "url": "https://en.wikipedia.org/wiki/Computer_memory",
    "title": "Computer memory",
    "content": "Computer memory stores information, such as data and programs, for immediate use in the computer . [ 2 ] The term memory is often synonymous with the terms RAM , main memory , or primary storage . Archaic synonyms for main memory include core (for magnetic core memory) and store . [ 3 ] Main memory operates at a high speed compared to mass storage which is slower but less expensive per bit and higher in capacity. Besides storing opened programs and data being actively processed, computer memory serves as a mass storage cache and write buffer to improve both reading and writing performance. Operating systems typically borrow RAM capacity for caching so long as it is not needed by running software. [ 4 ] If needed, contents of the computer memory can be transferred to storage; a common way of doing this is through a memory management technique called virtual memory . Modern computer memory is implemented as semiconductor memory , [ 5 ] [ 6 ] where data is stored within memory cells built from MOS transistors and other components on an integrated circuit . [ 7 ] There are two main kinds of semiconductor memory: volatile and non-volatile . Examples of non-volatile memory are flash memory and ROM , PROM , EPROM , and EEPROM memory. Examples of volatile memory are dynamic random-access memory (DRAM) used for primary storage and static random-access memory (SRAM) used mainly for CPU cache . Most semiconductor memory is organized into memory cells each storing one bit (0 or 1). Flash memory organization includes both one bit per memory cell and a multi-level cell capable of storing multiple bits per cell. The memory cells are grouped into words of fixed word length , for example, 1, 2, 4, 8, 16, 32, 64 or 128 bits. Each word can be accessed by a binary address of N bits, making it possible to store 2 N words in the memory. In the early 1940s, memory technology often permitted a capacity of a few bytes. The first electronic programmable digital computer , the ENIAC , using thousands of vacuum tubes , could perform simple calculations involving 20 numbers of ten decimal digits stored in the vacuum tubes. The next significant advance in computer memory came with acoustic delay-line memory , developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through the mercury, with the quartz crystals acting as transducers to read and write bits. Delay-line memory was limited to a capacity of up to a few thousand bits. Two alternatives to the delay line, the Williams tube and Selectron tube , originated in 1946, both using electron beams in glass tubes as means of storage. Using cathode-ray tubes , Fred Williams invented the Williams tube, which was the first random-access computer memory . The Williams tube was able to store more information than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and was less expensive. The Williams tube was nevertheless frustratingly sensitive to environmental disturbances. Efforts began in the late 1940s to find non-volatile memory . Magnetic-core memory allowed for memory recall after power loss. It was developed by Frederick W. Viehe and An Wang in the late 1940s, and improved by Jay Forrester and Jan A. Rajchman in the early 1950s, before being commercialized with the Whirlwind I computer in 1953. [ 8 ] Magnetic-core memory was the dominant form of memory until the development of MOS semiconductor memory in the 1960s. [ 9 ] The first semiconductor memory was implemented as a flip-flop circuit in the early 1960s using bipolar transistors . [ 9 ] Semiconductor memory made from discrete devices was first shipped by Texas Instruments to the United States Air Force in 1961. In the same year, the concept of solid-state memory on an integrated circuit (IC) chip was proposed by applications engineer Bob Norman at Fairchild Semiconductor . [ 10 ] The first bipolar semiconductor memory IC chip was the SP95 introduced by IBM in 1965. [ 9 ] While semiconductor memory offered improved performance over magnetic-core memory, it remained larger and more expensive and did not displace magnetic-core memory until the late 1960s. [ 9 ] [ 11 ] The invention of the metal–oxide–semiconductor field-effect transistor ( MOSFET ) enabled the practical use of metal–oxide–semiconductor (MOS) transistors as memory cell storage elements. MOS memory was developed by John Schmidt at Fairchild Semiconductor in 1964. [ 12 ] In addition to higher performance, MOS semiconductor memory was cheaper and consumed less power than magnetic core memory. [ 13 ] In 1965, J. Wood and R. Ball of the Royal Radar Establishment proposed digital storage systems that use CMOS (complementary MOS) memory cells, in addition to MOSFET power devices for the power supply , switched cross-coupling, switches and delay-line storage . [ 14 ] The development of silicon-gate MOS integrated circuit (MOS IC) technology by Federico Faggin at Fairchild in 1968 enabled the production of MOS memory chips . [ 15 ] NMOS memory was commercialized by IBM in the early 1970s. [ 16 ] MOS memory overtook magnetic core memory as the dominant memory technology in the early 1970s. [ 13 ] The two main types of volatile random-access memory (RAM) are static random-access memory (SRAM) and dynamic random-access memory (DRAM). Bipolar SRAM was invented by Robert Norman at Fairchild Semiconductor in 1963, [ 9 ] followed by the development of MOS SRAM by John Schmidt at Fairchild in 1964. [ 13 ] SRAM became an alternative to magnetic-core memory, but requires six transistors for each bit of data. [ 17 ] Commercial use of SRAM began in 1965, when IBM introduced their SP95 SRAM chip for the System/360 Model 95 . [ 9 ] Toshiba introduced bipolar DRAM memory cells for its Toscal BC-1411 electronic calculator in 1965. [ 18 ] [ 19 ] While it offered improved performance, bipolar DRAM could not compete with the lower price of the then dominant magnetic-core memory. [ 20 ] MOS technology is the basis for modern DRAM. In 1966, Robert H. Dennard at the IBM Thomas J. Watson Research Center was working on MOS memory. While examining the characteristics of MOS technology, he found it was possible to build capacitors , and that storing a charge or no charge on the MOS capacitor could represent the 1 and 0 of a bit, while the MOS transistor could control writing the charge to the capacitor. This led to his development of a single-transistor DRAM memory cell. [ 17 ] In 1967, Dennard filed a patent for a single-transistor DRAM memory cell based on MOS technology. [ 21 ] This led to the first commercial DRAM IC chip, the Intel 1103 in October 1970. [ 22 ] [ 23 ] [ 24 ] Synchronous dynamic random-access memory (SDRAM) later debuted with the Samsung KM48SL2000 chip in 1992. [ 25 ] [ 26 ] The term memory is also often used to refer to non-volatile memory including read-only memory (ROM) through modern flash memory . Programmable read-only memory (PROM) was invented by Wen Tsing Chow in 1956, while working for the Arma Division of the American Bosch Arma Corporation. [ 27 ] [ 28 ] In 1967, Dawon Kahng and Simon Sze of Bell Labs proposed that the floating gate of a MOS semiconductor device could be used for the cell of a reprogrammable ROM, which led to Dov Frohman of Intel inventing EPROM (erasable PROM) in 1971. [ 29 ] EEPROM (electrically erasable PROM) was developed by Yasuo Tarui, Yutaka Hayashi and Kiyoko Naga at the Electrotechnical Laboratory in 1972. [ 30 ] Flash memory was invented by Fujio Masuoka at Toshiba in the early 1980s. [ 31 ] [ 32 ] Masuoka and colleagues presented the invention of NOR flash in 1984, [ 33 ] and then NAND flash in 1987. [ 34 ] Toshiba commercialized NAND flash memory in 1987. [ 35 ] [ 36 ] [ 37 ] Developments in technology and economies of scale have made possible so-called very large memory (VLM) computers. [ 37 ] Volatile memory is computer memory that requires power to maintain the stored information. Most modern semiconductor volatile memory is either static RAM (SRAM) or dynamic RAM (DRAM). [ a ] DRAM dominates for desktop system memory. SRAM is used for CPU cache . SRAM is also found in small embedded systems requiring little memory. SRAM retains its contents as long as the power is connected and may use a simpler interface, but commonly uses six transistors per bit . Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs. [ 2 ] [ 23 ] [ 37 ] Non-volatile memory can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory , flash memory , most types of magnetic computer storage devices (e.g. hard disk drives , floppy disks and magnetic tape ), optical discs , and early computer storage methods such as magnetic drum , paper tape and punched cards . [ 37 ] Non-volatile memory technologies under development include ferroelectric RAM , programmable metallization cell , Spin-transfer torque magnetic RAM , SONOS , resistive random-access memory , racetrack memory , Nano-RAM , 3D XPoint , and millipede memory . A third category of memory is semi-volatile . The term is used to describe a memory that has some limited non-volatile duration after power is removed, but then data is ultimately lost. A typical goal when using a semi-volatile memory is to provide the high performance and durability associated with volatile memories while providing some benefits of non-volatile memory. For example, some non-volatile memory types experience wear when written. A worn cell has increased volatility but otherwise continues to work. Data locations which are written frequently can thus be directed to use worn circuits. As long as the location is updated within some known retention time, the data stays valid. After a period of time without update, the value is copied to a less-worn circuit with longer retention. Writing first to the worn area allows a high write rate while avoiding wear on the not-worn circuits. [ 38 ] As a second example, an STT-RAM can be made non-volatile by building large cells, but doing so raises the cost per bit and power requirements and reduces the write speed. Using small cells improves cost, power, and speed, but leads to semi-volatile behavior. In some applications, the increased volatility can be managed to provide many benefits of a non-volatile memory, for example by removing power but forcing a wake-up before data is lost; or by caching read-only data and discarding the cached data if the power-off time exceeds the non-volatile threshold. [ 39 ] The term semi-volatile is also used to describe semi-volatile behavior constructed from other memory types, such as nvSRAM , which combines SRAM and a non-volatile memory on the same chip , where an external signal copies data from the volatile memory to the non-volatile memory, but if power is removed before the copy occurs, the data is lost. Another example is battery-backed RAM , which uses an external battery to power the memory device in case of external power loss. If power is off for an extended period of time, the battery may run out, resulting in data loss. [ 37 ] Proper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs or slow performance. Improper management of memory is a common cause of bugs and security vulnerabilities, including the following types: Virtual memory is a system where physical memory is managed by the operating system typically with assistance from a memory management unit , which is part of many modern CPUs . It allows multiple types of memory to be used. For example, some data can be stored in RAM while other data is stored on a hard drive (e.g. in a swapfile ), functioning as an extension of the cache hierarchy . This offers several advantages. Computer programmers no longer need to worry about where their data is physically stored or whether the user's computer will have enough memory. The operating system will place actively used data in RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving data from RAM to disk and back than it does accomplishing tasks; this is known as thrashing . Protected memory is a system where each program is given an area of memory to use and is prevented from going outside that range. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated (or otherwise restricted or redirected). This way, only the offending program crashes, and other programs are not affected by the misbehavior (whether accidental or intentional). Use of protected memory greatly enhances both the reliability and security of a computer system. Without protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be rebooted . At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers. It may also be used benignly by desirable programs which are intended to modify other programs, debuggers , for example, to insert breakpoints or hooks.",
    "links": [
      "Data integrity",
      "Sysinfo",
      "Volatile memory",
      "Electronic calculator",
      "Light pen",
      "Capacitors",
      "Whirlwind I",
      "RAM",
      "Transducer",
      "Software rot",
      "BRL-CAD",
      "Central processing unit",
      "Silicon-gate",
      "Software entropy",
      "Phase-change memory",
      "Quantum memory",
      "Quad Data Rate SRAM",
      "WorldBench",
      "Core dump",
      "Computer case",
      "Random-access memory",
      "History of computing hardware (1960s–present)",
      "Iometer",
      "Voltage regulator module",
      "Programmable read-only memory",
      "8 mm video format",
      "Ferroelectric RAM",
      "Expansion card",
      "CD-ROM",
      "Bytes",
      "Disk pack",
      "Volume (computing)",
      "Magnetic drum",
      "Blu-ray",
      "Bubble memory",
      "Springer Science & Business Media",
      "Network-attached storage",
      "LaserDisc",
      "Spin-transfer torque magnetic RAM",
      "Millipede memory",
      "Synchronous dynamic random-access memory",
      "Bibcode (identifier)",
      "Fairchild Semiconductor",
      "Paper tape",
      "HD DVD",
      "Clustered file system",
      "Database",
      "Output device",
      "Fhourstones",
      "Cloud computing",
      "EPROM",
      "Tak (function)",
      "Computer History Museum",
      "File copying",
      "HD Tach",
      "Magnetic-core memory",
      "DDR SDRAM",
      "Multi-level cell",
      "Replication (computing)",
      "Cathode-ray tube",
      "USB flash drive",
      "VGA connector",
      "Benchmark (computing)",
      "MacOS",
      "Z-RAM",
      "Computer speakers",
      "Videotape",
      "CPU power dissipation",
      "Quadruplex videotape",
      "Computer mouse",
      "Softcam",
      "Sound card",
      "Scratchpad memory",
      "Object storage",
      "Data deduplication",
      "XQD card",
      "RDRAM",
      "Backup",
      "Non-RAID drive architectures",
      "Persistent data structure",
      "NMOS logic",
      "Database transaction",
      "Write once read many",
      "IBM iSeries benchmarks",
      "Cycles per byte",
      "CD Video",
      "Hierarchical INTegration",
      "Video CD",
      "Performance Rating",
      "ARM architecture family",
      "Memory management unit",
      "Tape drive",
      "Optical disc",
      "Buffer overflow",
      "Super PI",
      "NOR flash",
      "Microprocessor",
      "Tsung",
      "Racetrack memory",
      "Digital Visual Interface",
      "Cache coherence",
      "IEEE 1394",
      "Adjusted Peak Performance",
      "Physical memory",
      "MOS integrated circuit",
      "Disk array",
      "Metadata",
      "Digital rights management",
      "Single-instance storage",
      "Selectron tube",
      "Microphone",
      "Memory card",
      "HDMI",
      "Debugger",
      "Phone connector (audio)",
      "Shared resource",
      "Flash memory",
      "Wayback Machine",
      "Futuremark",
      "EEMBC",
      "NAS Parallel Benchmarks",
      "ISSN (identifier)",
      "Image scanner",
      "1T-SRAM",
      "Integrated circuit",
      "Samsung",
      "DVD-RAM",
      "ISBN (identifier)",
      "Input/output",
      "Hard drive",
      "Linear Tape-Open",
      "Floating-point unit",
      "IOzone",
      "Floppy disk",
      "Electric battery",
      "Swapfile",
      "Hyper CD-ROM",
      "Virtual memory",
      "Memory Stick",
      "Memory-mapped file",
      "Locality of reference",
      "List of pioneers in computer science",
      "GUID Partition Table",
      "DVD card",
      "In-memory processing",
      "Power devices",
      "Serial port",
      "Smithsonian Institution",
      "Multiuser",
      "Webcam",
      "VHS",
      "Super Video CD",
      "5D optical data storage",
      "Integer",
      "Transaction Processing over XML",
      "Directory (computing)",
      "Computer",
      "MOS memory",
      "Information transfer",
      "Alan Turing",
      "Bank switching",
      "Tape library",
      "S-VHS",
      "Cryptography",
      "Data validation and reconciliation",
      "STT-RAM",
      "Floating-gate MOSFET",
      "DEISA Benchmark Suite",
      "Intel 1103",
      "Memory map",
      "IBM System/360",
      "Punched card",
      "Nonvolatile BIOS memory",
      "Electrochemical RAM",
      "Motherboard",
      "NBench",
      "Object file",
      "Data communication",
      "ROM",
      "Touchscreen",
      "Operating system",
      "CPU cache",
      "Vacuum tube",
      "IBM FlashSystem",
      "Bipolar transistors",
      "SmartMedia",
      "Dual-ported RAM",
      "Giga-updates per second",
      "Areal density (computer storage)",
      "XDR DRAM",
      "Data corruption",
      "Computational RAM",
      "Peripherals",
      "Solid-state electronics",
      "Plugboard",
      "Cloud storage",
      "Printer (computing)",
      "Power supply unit (computer)",
      "United States Air Force",
      "Magnetic tape",
      "Reboot (computing)",
      "YCSB",
      "RAID",
      "TATP Benchmark",
      "Fog computing",
      "Magnetic-tape data storage",
      "D-VHS",
      "Twistor memory",
      "Dov Frohman",
      "LAPACK",
      "Academic Press",
      "Data store",
      "Electronic quantum holography",
      "Data",
      "Power supply",
      "IEEE",
      "Performance per watt",
      "Thunderbolt (interface)",
      "Digital computer",
      "MicroSDHC",
      "SATA",
      "Holographic Versatile Disc",
      "SDET",
      "MOS transistor",
      "Disk image",
      "Paper data storage",
      "Game controller",
      "Data structure",
      "Memistor",
      "Disk partitioning",
      "Recursion",
      "DNA digital data storage",
      "In-memory database",
      "CPU",
      "Trackball",
      "Bonnie++",
      "Main memory",
      "Patterned media",
      "Ethernet",
      "File deletion",
      "Non-volatile memory",
      "Unstructured data",
      "Solid-state storage",
      "Universal memory",
      "VHS-C",
      "LPDDR",
      "BogoMips",
      "JavaScript",
      "Refreshable braille display",
      "SxS",
      "Bit",
      "Drum memory",
      "Microchip",
      "Memory chip",
      "Federico Faggin",
      "Memory coherence",
      "Digital signal processor",
      "Continuous availability",
      "Semiconductor",
      "Fe FET",
      "Segmentation fault",
      "Prime95",
      "Magnetic recording",
      "SysSpeed",
      "Average CPU power",
      "Memory cell (computing)",
      "IBM Thomas J. Watson Research Center",
      "DDR4 SDRAM",
      "History of computing hardware",
      "Electronic visual display",
      "SPECpower",
      "Intel",
      "LINPACK benchmarks",
      "VMmark",
      "Electronic Design",
      "Java (software platform)",
      "NVM Express",
      "Memory protection",
      "Filesystems",
      "Disk aggregation",
      "Ultra HD Blu-ray",
      "An Wang",
      "Big data",
      "Processor register",
      "Copy protection",
      "Pointing device",
      "Jan A. Rajchman",
      "Data compression",
      "Read-only memory",
      "Dynamic random-access memory",
      "Information repository",
      "Graphics card",
      "CD-R",
      "UltraRAM",
      "Coremark",
      "Bits of information",
      "Content-addressable memory",
      "EDRAM",
      "Programmable ROM",
      "Toshiba",
      "Vacuum tubes",
      "Motorola 68k",
      "X86",
      "Programmable metallization cell",
      "Vision Electronic Recording Apparatus",
      "Data storage",
      "Phonograph record",
      "Memory segmentation",
      "Non-volatile",
      "Persistence (computer science)",
      "MultiMediaCard",
      "DV (video format)",
      "Page cache",
      "Computer monitor",
      "Memory management",
      "Distributed file system for cloud",
      "J. Presper Eckert",
      "CMOS",
      "Plated-wire memory",
      "Storage virtualization",
      "Touchpad",
      "MicroMV",
      "Metal–oxide–semiconductor",
      "SONOS",
      "3D XPoint",
      "Betamax",
      "Boot sector",
      "MOSFET",
      "Universal Flash Storage",
      "PS/2 port",
      "Memory organization",
      "Non-volatile random-access memory",
      "High Bandwidth Memory",
      "Web server benchmarking",
      "Primary storage",
      "Direct-attached storage",
      "International Electron Devices Meeting",
      "Memory hierarchy",
      "DVD-Video",
      "Block (data storage)",
      "Analog recording",
      "File sharing",
      "MiniDV",
      "Solid-state hybrid drive",
      "ROM cartridge",
      "SD card",
      "Data center infrastructure efficiency",
      "Mass storage",
      "Virtual machine",
      "Parallel computing",
      "Solid-state drive",
      "Plotter",
      "Data recovery",
      "NvSRAM",
      "Knowledge base",
      "Jay Forrester",
      "Flip-flop (electronics)",
      "Optical trackpad",
      "Data cleansing",
      "Nintendo optical discs",
      "Computer storage",
      "DVD",
      "Cassette tape",
      "EWeek",
      "Computer network",
      "Mellon optical memory",
      "Whetstone (benchmark)",
      "Free software",
      "Temporary file",
      "CD-RW",
      "Input device",
      "OpenSTA",
      "FLOPS",
      "Pointing stick",
      "Server Efficiency Rating Tool",
      "Graphics processing unit",
      "IBM",
      "Delay-line memory",
      "Linux",
      "Battery-backed memory",
      "Computer keyboard",
      "Mini CD",
      "Proprietary software",
      "AnTuTu",
      "HPC Challenge Benchmark",
      "Nuclear weapon",
      "NoSQL",
      "Switches",
      "A-RAM",
      "Doi (identifier)",
      "Dew computing",
      "Royal Radar Establishment",
      "Floating gate",
      "Dhrystone",
      "Distributed data store",
      "Data bank",
      "Write buffer",
      "Removable media",
      "Thrashing (computer science)",
      "NAND flash",
      "Mercury (element)",
      "Optical storage",
      "Standard Performance Evaluation Corporation",
      "File system",
      "Fax modem",
      "Word length",
      "Memristor",
      "Princeton Application Repository for Shared-Memory Computers",
      "Novabench",
      "Computer port (hardware)",
      "Curl-loader",
      "Network interface controller",
      "Parallel port",
      "Arithmetic logic unit",
      "Embedded systems",
      "Memory access pattern",
      "Encyclopedia Britannica",
      "TPC-W",
      "Data model",
      "Video random access memory",
      "Robert H. Dennard",
      "Diode matrix",
      "Apache JMeter",
      "Grid computing",
      "Texas Instruments",
      "Hard disk drive",
      "Memory paging",
      "Moore's law",
      "Sound chip",
      "Data redundancy",
      "Simon Sze",
      "Semiconductor device",
      "Game port",
      "Resistive random-access memory",
      "T-RAM",
      "Core rope memory",
      "Volume boot record",
      "Edge computing",
      "SPECint",
      "Data cluster",
      "Holographic data storage",
      "SPECfp",
      "Data validation",
      "Punched tape",
      "USB",
      "Electrotechnical Laboratory",
      "Embedded system",
      "Computer hardware",
      "SPECvirt",
      "DisplayPort",
      "Video RAM (dual-ported DRAM)",
      "IAS computer",
      "Computer data storage",
      "Data degradation",
      "EEPROM",
      "MicroP2",
      "Storage area network",
      "3D optical data storage",
      "Static random-access memory",
      "Block-level storage",
      "U-matic",
      "ICOMP (index)",
      "Browser speed test",
      "Hex dump",
      "IBM 602",
      "Semiconductor memory",
      "Magnetic storage",
      "Thin-film memory",
      "Phonograph cylinder",
      "Storage record",
      "GDDR SDRAM",
      "Memory refresh",
      "Httperf",
      "Cache hierarchy",
      "AIM Multiuser Benchmark",
      "Switched-mode power supply",
      "Static RAM",
      "Memory geometry",
      "Williams tube",
      "Flash Core Module",
      "Data security",
      "Dynamic RAM",
      "Amdahl's law",
      "Logical disk",
      "Compact Disc Digital Audio",
      "Gibibyte",
      "Livermore loops",
      "Magnetoresistive RAM",
      "Samsung Electronics",
      "Discrete device",
      "Graphics tablet",
      "Master boot record",
      "Transaction Processing Performance Council",
      "SUPS",
      "SuperPrime",
      "SIM card",
      "MiniDVD",
      "Fujio Masuoka",
      "CompactFlash",
      "Digital Data Storage",
      "Computer file",
      "Phoronix Test Suite",
      "Disk mirroring",
      "Dekatron",
      "ENIAC",
      "Nano-RAM",
      "Wen Tsing Chow",
      "CoreMark",
      "Optical mouse",
      "Distributed database",
      "PC Card",
      "Diskspd",
      "Compact disc",
      "Applications engineers",
      "Time crystal",
      "Power MOSFET"
    ]
  },
  "Information Retrieval Facility": {
    "url": "https://en.wikipedia.org/wiki/Information_Retrieval_Facility",
    "title": "Information Retrieval Facility",
    "content": "The Information Retrieval Facility ( IRF ), founded 2006 and located in Vienna , Austria , was a research platform for networking and collaboration for professionals in the field of information retrieval . It ceased operations in 2012.",
    "links": [
      "Austria",
      "EScience",
      "Information retrieval",
      "Vienna"
    ]
  },
  "Query understanding": {
    "url": "https://en.wikipedia.org/wiki/Query_understanding",
    "title": "Query understanding",
    "content": "Query understanding is the process of inferring the intent of a search engine user by extracting semantic meaning from the searcher’s keywords. [ 1 ] Query understanding methods generally take place before the search engine retrieves and ranks results. It is related to natural language processing but specifically focused on the understanding of search queries. Many languages inflect words to reflect their role in the utterance they appear in. The variation between various forms of a word is likely to be of little importance for the relatively coarse-grained model of meaning involved in a retrieval system, and for this reason the task of conflating the various forms of a word is a potentially useful technique to increase recall of a retrieval system. [ 2 ] Stemming algorithms, also known as stemmers, typically use a collection of simple rules to remove suffixes intended to model the language’s inflection rules. [ 3 ] For some languages, there are simple lemmatisation methods to reduce a word in query to its lemma or root form or its stem ; for others, this operation involves non-trivial string processing and may require recognizing the word's part of speech or referencing a lexical database . The effectiveness of stemming and lemmatization varies across languages. [ 4 ] [ 5 ] Query segmentation is a key component of query understanding, aiming to divide a query into meaningful segments. Traditional approaches, such as the bag-of-words model , treat individual words as independent units, which can limit interpretative accuracy. For languages like Chinese, where words are not separated by spaces, segmentation is essential, as individual characters often lack standalone meaning. Even in English, the BOW model may not capture the full meaning, as certain phrases—such as \"New York\"—carry significance as a whole rather than as isolated terms. By identifying phrases or entities within queries, query segmentation enhances interpretation, enabling search engines to apply proximity and ordering constraints, ultimately improving search accuracy and user satisfaction. [ 6 ] Entity recognition is the process of locating and classifying entities within a text string. Named-entity recognition specifically focuses on named entities , such as names of people, places, and organizations. In addition, entity recognition includes identifying concepts in queries that may be represented by multi-word phrases. Entity recognition systems typically use grammar-based linguistic techniques or statistical machine learning models. [ 7 ] Query rewriting is the process of automatically reformulating a search query to more accurately capture its intent. Query expansion adds additional query terms, such as synonyms, in order to retrieve more documents and thereby increase recall. Query relaxation removes query terms to reduce the requirements for a document to match the query, thereby also increasing recall . Other forms of query rewriting, such as automatically converting consecutive query terms into phrases and restricting query terms to specific fields , aim to increase precision . Automatic spelling correction is a critical feature of modern search engines, designed to address common spelling errors in user queries. Such errors are especially frequent as users often search for unfamiliar topics. By correcting misspelled queries, search engines enhance their understanding of user intent, thereby improving the relevance and quality of search results and overall user experience. [ 8 ]",
    "links": [
      "Search engine (computing)",
      "Ranking (information retrieval)",
      "Natural language processing",
      "Suffix",
      "Bag-of-words model",
      "Doi (identifier)",
      "Machine learning",
      "Stemming",
      "Named-entity recognition",
      "User intent",
      "Lemmatisation",
      "Root (linguistics)",
      "Information retrieval",
      "Field (computer science)",
      "Inflection",
      "Word stem",
      "Query expansion",
      "Lexical database",
      "Phrase",
      "Part of speech",
      "Named entity",
      "Spell checker",
      "Lemma (morphology)"
    ]
  },
  "Privacy": {
    "url": "https://en.wikipedia.org/wiki/Privacy",
    "title": "Privacy",
    "content": "Privacy ( UK : / ˈ p r ɪ v ə s i / , US : / ˈ p r aɪ -/ ) [ 1 ] [ 2 ] is the ability of an individual or group to seclude themselves or information about themselves, and thereby express themselves selectively. The domain of privacy partially overlaps with security , which can include the concepts of appropriate use and protection of information . Privacy may also take the form of bodily integrity . Throughout history, there have been various conceptions of privacy. Most cultures acknowledge the right of individuals to keep aspects of their personal lives out of the public domain. The right to be free from unauthorized invasions of privacy by governments, corporations, or individuals is enshrined in the privacy laws of many countries and, in some instances, their constitutions. With the rise of technology, the debate regarding privacy has expanded from a bodily sense to include a digital sense. In most countries, the right to digital privacy is considered an extension of the original right to privacy , and many countries have passed acts that further protect digital privacy from public and private entities. There are multiple techniques to invade privacy, which may be employed by corporations or governments for profit or political reasons. Conversely, in order to protect privacy, people may employ encryption or anonymity measures. The word privacy is derived from the Latin word and concept of ' privatus ', which referred to things set apart from what is public; personal and belonging to oneself, and not to the state. [ 3 ] Literally, ' privatus ' is the past participle of the Latin verb ' privere ' meaning 'to be deprived of'. [ 4 ] The concept of privacy has been explored and discussed by numerous philosophers throughout history. Privacy has historical roots in ancient Greek philosophical discussions. The most well-known of these was Aristotle 's distinction between two spheres of life: the public sphere of the polis , associated with political life, and the private sphere of the oikos , associated with domestic life. [ 5 ] Privacy is valued along with other basic necessities of life in the Jewish deutero-canonical Book of Sirach . [ 6 ] Islam's holy text, the Qur'an, states the following regarding privacy: 'Do not spy on one another' (49:12); 'Do not enter any houses except your own homes unless you are sure of their occupants' consent' (24:27). [ 7 ] English philosopher John Locke 's (1632-1704) writings on natural rights and the social contract laid the groundwork for modern conceptions of individual rights, including the right to privacy. In his Second Treatise of Civil Government (1689), Locke argued that a man is entitled to his own self through one's natural rights of life, liberty, and property. [ 8 ] He believed that the government was responsible for protecting these rights so individuals were guaranteed private spaces to practice personal activities. [ 9 ] In the political sphere, philosophers hold differing views on the right of private judgment. German philosopher Georg Wilhelm Friedrich Hegel (1770-1831) makes the distinction between moralität , which refers to an individual's private judgment, and sittlichkeit , pertaining to one's rights and obligations as defined by an existing corporate order. On the contrary, Jeremy Bentham (1748-1832), an English philosopher, interpreted law as an invasion of privacy. His theory of utilitarianism argued that legal actions should be judged by the extent of their contribution to human wellbeing, or necessary utility. [ 10 ] Hegel's notions were modified by prominent 19th century English philosopher John Stuart Mill . Mill's essay On Liberty (1859) argued for the importance of protecting individual liberty against the tyranny of the majority and the interference of the state. His views emphasized the right of privacy as essential for personal development and self-expression. [ 11 ] Discussions surrounding surveillance coincided with philosophical ideas on privacy. Jeremy Bentham developed the phenomenon known as the Panoptic effect through his 1791 architectural design of a prison called Panopticon . The phenomenon explored the possibility of surveillance as a general awareness of being watched that could never be proven at any particular moment. [ 12 ] French philosopher Michel Foucault (1926-1984) concluded that the possibility of surveillance in the instance of the Panopticon meant a prisoner had no choice but to conform to the prison's rules. [ 12 ] As technology has advanced, the way in which privacy is protected and violated has changed with it. In the case of some technologies, such as the printing press or the Internet , the increased ability to share information can lead to new ways in which privacy can be breached. It is generally agreed that the first publication advocating privacy in the United States was the 1890 article by Samuel Warren and Louis Brandeis , \"The Right to Privacy\", [ 13 ] and that it was written mainly in response to the increase in newspapers and photographs made possible by printing technologies. [ 14 ] In 1948, 1984 , written by George Orwell , was published. A classic dystopian novel, 1984 describes the life of Winston Smith in 1984, located in Oceania, a totalitarian state. The all-controlling Party, the party in power led by Big Brother, is able to control power through mass surveillance and limited freedom of speech and thought. George Orwell provides commentary on the negative effects of totalitarianism , particularly on privacy and censorship . [ 15 ] Parallels have been drawn between 1984 and modern censorship and privacy, a notable example being that large social media companies, rather than the government, are able to monitor a user's data and decide what is allowed to be said online through their censorship policies, ultimately for monetary purposes. [ 16 ] In the 1960s, people began to consider how changes in technology were bringing changes in the concept of privacy. [ 17 ] Vance Packard 's The Naked Society was a popular book on privacy from that era and led US discourse on privacy at that time. [ 17 ] In addition, Alan Westin 's Privacy and Freedom shifted the debate regarding privacy from a physical sense, how the government controls a person's body (i.e. Roe v. Wade ) and other activities such as wiretapping and photography. As important records became digitized, Westin argued that personal data was becoming too accessible and that a person should have complete jurisdiction over their data, laying the foundation for the modern discussion of privacy. [ 18 ] New technologies can also create new ways to gather private information. In 2001, the legal case Kyllo v. United States (533 U.S. 27) determined that the use of thermal imaging devices that can reveal previously unknown information without a warrant constitutes a violation of privacy. In 2019, after developing a corporate rivalry in competing voice-recognition software, Apple and Amazon required employees to listen to intimate moments and faithfully transcribe the contents. [ 19 ] Police and citizens often conflict on what degree the police can intrude a citizen's digital privacy. For instance, in 2012, the Supreme Court ruled unanimously in United States v. Jones (565 U.S. 400), in the case of Antoine Jones who was arrested of drug possession using a GPS tracker on his car that was placed without a warrant, that warrantless tracking infringes the Fourth Amendment . The Supreme Court also justified that there is some \"reasonable expectation of privacy\" in transportation since the reasonable expectation of privacy had already been established under Griswold v. Connecticut (1965). The Supreme Court also further clarified that the Fourth Amendment did not only pertain to physical instances of intrusion but also digital instances, and thus United States v. Jones became a landmark case. [ 20 ] In 2014, the Supreme Court ruled unanimously in Riley v. California (573 U.S. 373), where David Leon Riley was arrested after he was pulled over for driving on expired license tags when the police searched his phone and discovered that he was tied to a shooting, that searching a citizen's phone without a warrant was an unreasonable search, a violation of the Fourth Amendment. The Supreme Court concluded that the cell phones contained personal information different from trivial items, and went beyond to state that information stored on the cloud was not necessarily a form of evidence. Riley v. California evidently became a landmark case, protecting the digital protection of citizen's privacy when confronted with the police. [ 21 ] A recent notable occurrence of the conflict between law enforcement and a citizen in terms of digital privacy has been in the 2018 case, Carpenter v. United States (585 U.S. ____). In this case, the FBI used cell phone records without a warrant to arrest Timothy Ivory Carpenter on multiple charges, and the Supreme Court ruled that the warrantless search of cell phone records violated the Fourth Amendment, citing that the Fourth Amendment protects \"reasonable expectations of privacy\" and that information sent to third parties still falls under data that can be included under \"reasonable expectations of privacy\". [ 22 ] Beyond law enforcement, many interactions between the government and citizens have been revealed either lawfully or unlawfully, specifically through whistleblowers. One notable example is Edward Snowden , who released multiple operations related to the mass surveillance operations of the National Security Agency (NSA), where it was discovered that the NSA continues to breach the security of millions of people, mainly through mass surveillance programs whether it was collecting great amounts of data through third party private companies, hacking into other embassies or frameworks of international countries, and various breaches of data, which prompted a culture shock and stirred international debate related to digital privacy. [ 23 ] The Internet and technologies built on it enable new forms of social interactions at increasingly faster speeds and larger scales. Because the computer networks which underlie the Internet introduce such a wide range of novel security concerns, the discussion of privacy on the Internet is often conflated with security . [ 24 ] Indeed, many entities such as corporations involved in the surveillance economy inculcate a security-focused conceptualization of privacy which reduces their obligations to uphold privacy into a matter of regulatory compliance , [ 25 ] while at the same time lobbying to minimize those regulatory requirements. [ 26 ] The Internet's effect on privacy includes all of the ways that computational technology and the entities that control it can subvert the privacy expectations of their users . [ 27 ] [ 28 ] In particular, the right to be forgotten is motivated by both the computational ability to store and search through massive amounts of data as well as the subverted expectations of users who share information online without expecting it to be stored and retained indefinitely. Phenomena such as revenge porn and deepfakes are not merely individual because they require both the ability to obtain images without someone's consent as well as the social and economic infrastructure to disseminate that content widely. [ 28 ] Therefore, privacy advocacy groups such as the Cyber Civil Rights Initiative and the Electronic Frontier Foundation argue that addressing the new privacy harms introduced by the Internet requires both technological improvements to encryption and anonymity as well as societal efforts such as legal regulations to restrict corporate and government power. [ 29 ] [ 30 ] While the Internet began as a government and academic effort up through the 1980s, private corporations began to enclose the hardware and software of the Internet in the 1990s, and now most Internet infrastructure is owned and managed by for-profit corporations. [ 31 ] As a result, the ability of governments to protect their citizens' privacy is largely restricted to industrial policy , instituting controls on corporations that handle communications or personal data . [ 32 ] [ 33 ] Privacy regulations are often further constrained to only protect specific demographics such as children, [ 34 ] or specific industries such as credit card bureaus. [ 35 ] Several online social network sites (OSNs) are among the top 10 most visited websites globally. Facebook for example, as of August 2015, was the largest social-networking site, with nearly 2.7 billion [ 36 ] members, who upload over 4.75 billion pieces of content daily. While Twitter is significantly smaller with 316 million registered users, the US Library of Congress recently announced that it will be acquiring and permanently storing the entire archive of public Twitter posts since 2006. [ 27 ] A review and evaluation of scholarly work regarding the current state of the value of individuals' privacy of online social networking show the following results: \"first, adults seem to be more concerned about potential privacy threats than younger users; second, policy makers should be alarmed by a large part of users who underestimate risks of their information privacy on OSNs; third, in the case of using OSNs and its services, traditional one-dimensional privacy approaches fall short\". [ 37 ] This is exacerbated by deanonymization research indicating that personal traits such as sexual orientation, race, religious and political views, personality, or intelligence can be inferred based on a wide variety of digital footprints , such as samples of text, browsing logs, or Facebook Likes. [ 38 ] Intrusions of social media privacy are known to affect employment in the United States. Microsoft reports that 75 percent of U.S. recruiters and human-resource professionals now do online research about candidates, often using information provided by search engines, social-networking sites, photo/video-sharing sites, personal web sites and blogs, and Twitter . They also report that 70 percent of U.S. recruiters have rejected candidates based on internet information. This has created a need by many candidates to control various online privacy settings in addition to controlling their online reputations, the conjunction of which has led to legal suits against both social media sites and US employers. [ 27 ] Selfies are popular today. A search for photos with the hashtag #selfie retrieves over 23 million results on Instagram and 51 million with the hashtag #me. [ 39 ] However, due to modern corporate and governmental surveillance, this may pose a risk to privacy. [ 40 ] In a research study which takes a sample size of 3763, researchers found that for users posting selfies on social media, women generally have greater concerns over privacy than men, and that users' privacy concerns inversely predict their selfie behavior and activity. [ 41 ] An invasion of someone's privacy may be widely and quickly disseminated over the Internet. When social media sites and other online communities fail to invest in content moderation , an invasion of privacy can expose people to a much greater volume and degree of harassment than would otherwise be possible. Revenge porn may lead to misogynist or homophobic harassment, such as in the suicide of Amanda Todd and the suicide of Tyler Clementi . When someone's physical location or other sensitive information is leaked over the Internet via doxxing , harassment may escalate to direct physical harm such as stalking or swatting . Despite the way breaches of privacy can magnify online harassment, online harassment is often used as a justification to curtail freedom of speech , by removing the expectation of privacy via anonymity , or by enabling law enforcement to invade privacy without a search warrant . In the wake of Amanda Todd's death, the Canadian parliament proposed a motion purporting to stop bullying, but Todd's mother herself gave testimony to parliament rejecting the bill due to its provisions for warrantless breaches of privacy, stating \"I don't want to see our children victimized again by losing privacy rights.\" [ 42 ] [ 43 ] [ 44 ] Even where these laws have been passed despite privacy concerns, they have not demonstrated a reduction in online harassment. When the Korea Communications Commission introduced a registration system for online commenters in 2007, they reported that malicious comments only decreased by 0.9%, and in 2011 it was repealed. [ 45 ] A subsequent analysis found that the set of users who posted the most comments actually increased the number of \"aggressive expressions\" when forced to use their real name. [ 46 ] In the US, while federal law only prohibits online harassment based on protected characteristics such as gender and race, [ 47 ] individual states have expanded the definition of harassment to further curtail speech: Florida's definition of online harassment includes \"any use of data or computer software\" that \"Has the effect of substantially disrupting the orderly operation of a school.\" [ 48 ] Increasingly, mobile devices facilitate location tracking . This creates user privacy problems. A user's location and preferences constitute personal information , and their improper use violates that user's privacy. A recent MIT study by de Montjoye et al. showed that four spatio-temporal points constituting approximate places and times are enough to uniquely identify 95% of 1.5M people in a mobility database. The study further shows that these constraints hold even when the resolution of the dataset is low. Therefore, even coarse or blurred datasets confer little privacy protection. [ 49 ] Several methods to protect user privacy in location-based services have been proposed, including the use of anonymizing servers and blurring of information. Methods to quantify privacy have also been proposed, to calculate the equilibrium between the benefit of obtaining accurate location information and the risks of breaching an individual's privacy. [ 50 ] There have been scandals regarding location privacy. One instance was the scandal concerning AccuWeather , where it was revealed that AccuWeather was selling locational data. This consisted of a user's locational data, even if they opted out within Accuweather, which tracked users' location. Accuweather sold this data to Reveal Mobile, a company that monetizes data related to a user's location. [ 51 ] Other international cases are similar to the Accuweather case. In 2017, a leaky API inside the McDelivery App exposed private data, which consisted of home addresses, of 2.2 million users. [ 52 ] In the wake of these types of scandals, many large American technology companies such as Google, Apple, and Facebook have been subjected to hearings and pressure under the U.S. legislative system. In 2011, US Senator Al Franken wrote an open letter to Steve Jobs , noting the ability of iPhones and iPads to record and store users' locations in unencrypted files. [ 53 ] [ 54 ] Apple claimed this was an unintentional software bug , but Justin Brookman of the Center for Democracy and Technology directly challenged that portrayal, stating \"I'm glad that they are fixing what they call bugs, but I take exception with their strong denial that they track users.\" [ 55 ] In 2021, the U.S. state of Arizona found in a court case that Google misled its users and stored the location of users regardless of their location settings. [ 56 ] The Internet has become a significant medium for advertising, with digital marketing making up approximately half of the global ad spending in 2019. [ 57 ] While websites are still able to sell advertising space without tracking, including via contextual advertising , digital ad brokers such as Facebook and Google have instead encouraged the practice of behavioral advertising , providing code snippets used by website owners to track their users via HTTP cookies . This tracking data is also sold to other third parties as part of the mass surveillance industry . Since the introduction of mobile phones, data brokers have also been planted within apps, resulting in a $350 billion digital industry especially focused on mobile devices. [ 58 ] Digital privacy has become the main source of concern for many mobile users, especially with the rise of privacy scandals such as the Facebook–Cambridge Analytica data scandal . [ 58 ] Apple has received some reactions for features that prohibit advertisers from tracking a user's data without their consent. [ 59 ] Google attempted to introduce an alternative to cookies named FLoC which it claimed reduced the privacy harms, but it later retracted the proposal due to antitrust probes and analyses that contradicted their claims of privacy. [ 60 ] [ 61 ] [ 62 ] The ability to do online inquiries about individuals has expanded dramatically over the last decade. Importantly, directly observed behavior, such as browsing logs, search queries, or contents of a public Facebook profile, can be automatically processed to infer secondary information about an individual, such as sexual orientation, political and religious views, race, substance use, intelligence, and personality. [ 63 ] In Australia, the Telecommunications (Interception and Access) Amendment (Data Retention) Act 2015 made a distinction between collecting the contents of messages sent between users and the metadata surrounding those messages. Most countries give citizens rights to privacy in their constitutions. [ 17 ] Representative examples of this include the Constitution of Brazil , which says \"the privacy, private life, honor and image of people are inviolable\"; the Constitution of South Africa says that \"everyone has a right to privacy\"; and the Constitution of the Republic of Korea says \"the privacy of no citizen shall be infringed.\" [ 17 ] The Italian Constitution also defines the right to privacy. [ 64 ] Among most countries whose constitutions do not explicitly describe privacy rights, court decisions have interpreted their constitutions to intend to give privacy rights. [ 17 ] Many countries have broad privacy laws outside their constitutions, including Australia's Privacy Act 1988 , Argentina's Law for the Protection of Personal Data of 2000, Canada's 2000 Personal Information Protection and Electronic Documents Act , and Japan's 2003 Personal Information Protection Law. [ 17 ] Beyond national privacy laws, there are international privacy agreements. [ 65 ] The United Nations Universal Declaration of Human Rights says \"No one shall be subjected to arbitrary interference with [their] privacy, family, home or correspondence, nor to attacks upon [their] honor and reputation.\" [ 17 ] The Organisation for Economic Co-operation and Development published its Privacy Guidelines in 1980. The European Union's 1995 Data Protection Directive guides privacy protection in Europe. [ 17 ] The 2004 Privacy Framework by the Asia-Pacific Economic Cooperation is a privacy protection agreement for the members of that organization. [ 17 ] Approaches to privacy can, broadly, be divided into two categories: free market or consumer protection . [ 66 ] One example of the free market approach is to be found in the voluntary OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data. [ 67 ] The principles reflected in the guidelines, free of legislative interference, are analyzed in an article putting them into perspective with concepts of the GDPR put into law later in the European Union. [ 68 ] In a consumer protection approach, in contrast, it is claimed that individuals may not have the time or knowledge to make informed choices, or may not have reasonable alternatives available. In support of this view, Jensen and Potts showed that most privacy policies are above the reading level of the average person. [ 69 ] The Privacy Act 1988 is administered by the Office of the Australian Information Commissioner. The initial introduction of privacy law in 1998 extended to the public sector, specifically to Federal government departments, under the Information Privacy Principles. State government agencies can also be subject to state based privacy legislation. This built upon the already existing privacy requirements that applied to telecommunications providers (under Part 13 of the Telecommunications Act 1997 ), and confidentiality requirements that already applied to banking, legal and patient / doctor relationships. [ 70 ] In 2008 the Australian Law Reform Commission (ALRC) conducted a review of Australian privacy law and produced a report titled \"For Your Information\". [ 71 ] Recommendations were taken up and implemented by the Australian Government via the Privacy Amendment (Enhancing Privacy Protection) Bill 2012. [ 72 ] In 2015, the Telecommunications (Interception and Access) Amendment (Data Retention) Act 2015 was passed, to some controversy over its human rights implications and the role of media. Canada is a federal state whose provinces and territories abide by the common law save the province of Quebec whose legal tradition is the civil law . Privacy in Canada was first addressed through the Privacy Act , [ 73 ] a 1985 piece of legislation applicable to personal information held by government institutions. The provinces and territories would later follow suit with their own legislation. Generally, the purposes of said legislation are to provide individuals rights to access personal information; to have inaccurate personal information corrected; and to prevent unauthorized collection, use, and disclosure of personal information. [ 74 ] In terms of regulating personal information in the private sector, the federal Personal Information Protection and Electronic Documents Act [ 75 ] (\"PIPEDA\") is enforceable in all jurisdictions unless a substantially similar provision has been enacted on the provincial level. [ 76 ] However, inter-provincial or international information transfers still engage PIPEDA. [ 76 ] PIPEDA has gone through two law overhaul efforts in 2021 and 2023 with the involvement of the Office of the Privacy Commissioner and Canadian academics. [ 77 ] In the absence of a statutory private right of action absent an OPC investigation, the common law torts of intrusion upon seclusion and public disclosure of private facts, as well as the Civil Code of Quebec may be brought for an infringement or violation of privacy. [ 78 ] [ 79 ] Privacy is also protected under ss. 7 and 8 of the Canadian Charter of Rights and Freedoms [ 80 ] which is typically applied in the criminal law context. [ 81 ] In Quebec, individuals' privacy is safeguarded by articles 3 and 35 to 41 of the Civil Code of Quebec [ 82 ] as well as by s. 5 of the Charter of human rights and freedoms . [ 83 ] In 2016, the European Union passed the General Data Protection Regulation (GDPR), which was intended to reduce the misuse of personal data and enhance individual privacy, by requiring companies to receive consent before acquiring personal information from users. [ 84 ] Although there are comprehensive regulations for data protection in the European Union, one study finds that despite the laws, there is a lack of enforcement in that no institution feels responsible to control the parties involved and enforce their laws. [ 85 ] The European Union also champions the Right to be Forgotten concept in support of its adoption by other countries. [ 86 ] Since the introduction of the Aadhaar project in 2009, which resulted in all 1.2 billion Indians being associated with a 12-digit biometric-secured number. Aadhaar has uplifted the poor in India [ how? ] [ promotion? ] by providing them with a form of identity and preventing the fraud and waste of resources, as normally the government would not be able to allocate its resources to its intended assignees due to the ID issues. [ citation needed ] With the rise of Aadhaar, India has debated whether Aadhaar violates an individual's privacy and whether any organization should have access to an individual's digital profile, as the Aadhaar card became associated with other economic sectors, allowing for the tracking of individuals by both public and private bodies. [ 87 ] Aadhaar databases have suffered from security attacks as well and the project was also met with mistrust regarding the safety of the social protection infrastructures. [ 88 ] In 2017, where the Aadhar was challenged, the Indian Supreme Court declared privacy as a human right, but postponed the decision regarding the constitutionality of Aadhaar for another bench. [ 89 ] In September 2018, the Indian Supreme Court determined that the Aadhaar project did not violate the legal right to privacy. [ 90 ] In the United Kingdom, it is not possible to bring an action for invasion of privacy. An action may be brought under another tort (usually breach of confidence) and privacy must then be considered under EC law. In the UK, it is sometimes a defence that disclosure of private information was in the public interest. [ 91 ] There is, however, the Information Commissioner's Office (ICO), an independent public body set up to promote access to official information and protect personal information. They do this by promoting good practice, ruling on eligible complaints, giving information to individuals and organisations, and taking action when the law is broken. The relevant UK laws include: Data Protection Act 1998 ; Freedom of Information Act 2000 ; Environmental Information Regulations 2004 ; Privacy and Electronic Communications Regulations 2003 . The ICO has also provided a \"Personal Information Toolkit\" online which explains in more detail the various ways of protecting privacy online. [ 92 ] In the United States, more systematic treatises of privacy did not appear until the 1890s, with the development of privacy law in America . [ 93 ] Although the US Constitution does not explicitly include the right to privacy , individual as well as locational privacy may be implicitly granted by the Constitution under the 4th Amendment . [ 94 ] The Supreme Court of the United States has found that other guarantees have penumbras that implicitly grant a right to privacy against government intrusion, for example in Griswold v. Connecticut and Roe v. Wade . Dobbs v. Jackson Women's Health Organization later overruled Roe v. Wade , with Supreme Court Justice Clarence Thomas characterizing Griswold 's penumbral argument as having a \"facial absurdity\", [ 95 ] casting doubt on the validity of a constitutional right to privacy in the United States and of previous decisions relying on it. [ 96 ] In the United States, the right of freedom of speech granted in the First Amendment has limited the effects of lawsuits for breach of privacy. Privacy is regulated in the US by the Privacy Act of 1974 , and various state laws. The Privacy Act of 1974 only applies to federal agencies in the executive branch of the federal government. [ 97 ] Certain privacy rights have been established in the United States via legislation such as the Children's Online Privacy Protection Act (COPPA), [ 98 ] the Gramm–Leach–Bliley Act (GLB), and the Health Insurance Portability and Accountability Act (HIPAA). [ 99 ] Unlike the EU and most EU-member states, the US does not recognize the right to privacy of non-US citizens. The UN's Special Rapporteur on the right to privacy, Joseph A. Cannataci, criticized this distinction. [ 100 ] The theory of contextual integrity , [ 101 ] developed by Helen Nissenbaum , defines privacy as an appropriate information flow, where appropriateness, in turn, is defined as conformance with legitimate, informational norms specific to social contexts. In 1890, the United States jurists Samuel D. Warren and Louis Brandeis wrote \"The Right to Privacy\", an article in which they argued for the \"right to be let alone\", using that phrase as a definition of privacy. [ 102 ] This concept relies on the theory of natural rights and focuses on protecting individuals. The citation was a response to recent technological developments, such as photography, and sensationalist journalism, also known as yellow journalism . [ 103 ] There is extensive commentary over the meaning of being \"let alone\", and among other ways, it has been interpreted to mean the right of a person to choose seclusion from the attention of others if they wish to do so, and the right to be immune from scrutiny or being observed in private settings, such as one's own home. [ 102 ] Although this early vague legal concept did not describe privacy in a way that made it easy to design broad legal protections of privacy, it strengthened the notion of privacy rights for individuals and began a legacy of discussion on those rights in the US. [ 102 ] Limited access refers to a person's ability to participate in society without having other individuals and organizations collect information about them. [ 104 ] Various theorists have imagined privacy as a system for limiting access to one's personal information. [ 104 ] Edwin Lawrence Godkin wrote in the late 19th century that \"nothing is better worthy of legal protection than private life, or, in other words, the right of every man to keep his affairs to himself, and to decide for himself to what extent they shall be the subject of public observation and discussion.\" [ 104 ] [ 105 ] Adopting an approach similar to the one presented by Ruth Gavison [ 106 ] Nine years earlier, [ 107 ] Sissela Bok said that privacy is \"the condition of being protected from unwanted access by others—either physical access, personal information, or attention.\" [ 104 ] [ 108 ] Control over one's personal information is the concept that \"privacy is the claim of individuals, groups, or institutions to determine for themselves when, how, and to what extent information about them is communicated to others.\" Generally, a person who has consensually formed an interpersonal relationship with another person is not considered \"protected\" by privacy rights with respect to the person they are in the relationship with. [ 109 ] [ 110 ] Charles Fried said that \"Privacy is not simply an absence of information about us in the minds of others; rather it is the control we have over information about ourselves.\" [ 111 ] Nevertheless, in the era of big data , control over information is under pressure. [ 112 ] [ 113 ] Alan Westin defined four states—or experiences—of privacy: solitude, intimacy, anonymity, and reserve. Solitude is a physical separation from others; [ 114 ] Intimacy is a \"close, relaxed; and frank relationship between two or more individuals\" that results from the seclusion of a pair or small group of individuals. [ 114 ] Anonymity is the \"desire of individuals for times of 'public privacy.'\" [ 114 ] Lastly, reserve is the \"creation of a psychological barrier against unwanted intrusion\"; this creation of a psychological barrier requires others to respect an individual's need or desire to restrict communication of information concerning themself. [ 114 ] In addition to the psychological barrier of reserve, Kirsty Hughes identified three more kinds of privacy barriers: physical, behavioral, and normative. Physical barriers, such as walls and doors, prevent others from accessing and experiencing the individual. [ 115 ] (In this sense, \"accessing\" an individual includes accessing personal information about them.) [ 115 ] Behavioral barriers communicate to others—verbally, through language, or non-verbally, through personal space, body language, or clothing—that an individual does not want the other person to access or experience them. [ 115 ] Lastly, normative barriers, such as laws and social norms, restrain others from attempting to access or experience an individual. [ 115 ] Psychologist Carl A. Johnson has identified the psychological concept of \"personal control\" as closely tied to privacy. His concept was developed as a process containing four stages and two behavioural outcome relationships, with one's outcomes depending on situational as well as personal factors. [ 116 ] Privacy is described as \"behaviors falling at specific locations on these two dimensions\". [ 117 ] Johnson examined the following four stages to categorize where people exercise personal control: outcome choice control is the selection between various outcomes. Behaviour selection control is the selection between behavioural strategies to apply to attain selected outcomes. Outcome effectance describes the fulfillment of selected behaviour to achieve chosen outcomes. Outcome realization control is the personal interpretation of one's achieved outcome. The relationship between two factors– primary and secondary control, is defined as the two-dimensional phenomenon where one reaches personal control: primary control describes behaviour directly causing outcomes, while secondary control is behaviour indirectly causing outcomes. [ 118 ] Johnson explores the concept that privacy is a behaviour that has secondary control over outcomes. Lorenzo Magnani expands on this concept by highlighting how privacy is essential in maintaining personal control over one's identity and consciousness. [ 119 ] He argues that consciousness is partly formed by external representations of ourselves, such as narratives and data, which are stored outside the body. However, much of our consciousness consists of internal representations that remain private and are rarely externalized. This internal privacy, which Magnani refers to as a form of \"information property\" or \"moral capital,\" is crucial for preserving free choice and personal agency. According to Magnani, [ 120 ] when too much of our identity and data is externalized and subjected to scrutiny, it can lead to a loss of personal control, dignity, and responsibility. The protection of privacy, therefore, safeguards our ability to develop and pursue personal projects in our own way, free from intrusive external forces. Acknowledging other conceptions of privacy while arguing that the fundamental concern of privacy is behavior selection control, Johnson converses with other interpretations including those of Maxine Wolfe and Robert S. Laufer, and Irwin Altman. He clarifies the continuous relationship between privacy and personal control, where outlined behaviours not only depend on privacy, but the conception of one's privacy also depends on his defined behavioural outcome relationships. [ 121 ] Privacy is sometimes defined as an option to have secrecy. Richard Posner said that privacy is the right of people to \"conceal information about themselves that others might use to their disadvantage\". [ 122 ] [ 123 ] In various legal contexts, when privacy is described as secrecy, a conclusion is reached: if privacy is secrecy, then rights to privacy do not apply for any information which is already publicly disclosed. [ 124 ] When privacy-as-secrecy is discussed, it is usually imagined to be a selective kind of secrecy in which individuals keep some information secret and private while they choose to make other information public and not private. [ 124 ] Privacy may be understood as a necessary precondition for the development and preservation of personhood. Jeffrey Reiman defined privacy in terms of a recognition of one's ownership of their physical and mental reality and a moral right to self-determination . [ 125 ] Through the \"social ritual\" of privacy, or the social practice of respecting an individual's privacy barriers, the social group communicates to developing children that they have exclusive moral rights to their bodies—in other words, moral ownership of their body. [ 125 ] This entails control over both active (physical) and cognitive appropriation, the former being control over one's movements and actions and the latter being control over who can experience one's physical existence and when. [ 125 ] Alternatively, Stanley Benn defined privacy in terms of a recognition of oneself as a subject with agency—as an individual with the capacity to choose. [ 126 ] Privacy is required to exercise choice. [ 126 ] Overt observation makes the individual aware of himself or herself as an object with a \"determinate character\" and \"limited probabilities.\" [ 126 ] Covert observation, on the other hand, changes the conditions in which the individual is exercising choice without their knowledge and consent. [ 126 ] In addition, privacy may be viewed as a state that enables autonomy, a concept closely connected to that of personhood. According to Joseph Kufer, an autonomous self-concept entails a conception of oneself as a \"purposeful, self-determining, responsible agent\" and an awareness of one's capacity to control the boundary between self and other—that is, to control who can access and experience him or her and to what extent. [ 127 ] Furthermore, others must acknowledge and respect the self's boundaries—in other words, they must respect the individual's privacy. [ 127 ] The studies of psychologists such as Jean Piaget and Victor Tausk show that, as children learn that they can control who can access and experience them and to what extent, they develop an autonomous self-concept. [ 127 ] In addition, studies of adults in particular institutions, such as Erving Goffman's study of \"total institutions\" such as prisons and mental institutions, [ 128 ] suggest that systemic and routinized deprivations or violations of privacy deteriorate one's sense of autonomy over time. [ 127 ] Privacy may be understood as a prerequisite for the development of a sense of self-identity. Privacy barriers, in particular, are instrumental in this process. According to Irwin Altman, such barriers \"define and limit the boundaries of the self\" and thus \"serve to help define [the self].\" [ 129 ] This control primarily entails the ability to regulate contact with others. [ 129 ] Control over the \"permeability\" of the self's boundaries enables one to control what constitutes the self and thus to define what is the self. [ 129 ] In addition, privacy may be seen as a state that fosters personal growth, a process integral to the development of self-identity. Hyman Gross suggested that, without privacy—solitude, anonymity, and temporary releases from social roles—individuals would be unable to freely express themselves and to engage in self-discovery and self-criticism . [ 127 ] Such self-discovery and self-criticism contributes to one's understanding of oneself and shapes one's sense of identity. [ 127 ] In a way analogous to how the personhood theory imagines privacy as some essential part of being an individual, the intimacy theory imagines privacy to be an essential part of the way that humans have strengthened or intimate relationships with other humans. [ 130 ] Because part of human relationships includes individuals volunteering to self-disclose most if not all personal information, this is one area in which privacy does not apply. [ 130 ] James Rachels advanced this notion by writing that privacy matters because \"there is a close connection between our ability to control who has access to us and to information about us, and our ability to create and maintain different sorts of social relationships with different people.\" [ 130 ] [ 131 ] Protecting intimacy is at the core of the concept of sexual privacy, which law professor Danielle Citron argues should be protected as a unique form of privacy. [ 132 ] Physical privacy could be defined as preventing \"intrusions into one's physical space or solitude.\" [ 133 ] An example of the legal basis for the right to physical privacy is the U.S. Fourth Amendment , which guarantees \"the right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures\". [ 134 ] Physical privacy may be a matter of cultural sensitivity, personal dignity, and/or shyness. There may also be concerns about safety, if, for example one is wary of becoming the victim of crime or stalking . [ 135 ] There are different things that can be prevented to protect one's physical privacy, including people watching (even through recorded images) one's intimate behaviours or intimate parts and unauthorized access to one's personal possessions or places. Examples of possible efforts used to avoid the former, especially for modesty reasons, are clothes , walls , fences , privacy screens, cathedral glass , window coverings , etc. Government agencies, corporations, groups/societies and other organizations may desire to keep their activities or secrets from being revealed to other organizations or individuals, adopting various security practices and controls in order to keep private information confidential. Organizations may seek legal protection for their secrets. For example, a government administration may be able to invoke executive privilege [ 136 ] or declare certain information to be classified , or a corporation might attempt to protect valuable proprietary information as trade secrets . [ 134 ] Privacy self-synchronization is a hypothesized mode by which the stakeholders of an enterprise privacy program spontaneously contribute collaboratively to the program's maximum success. The stakeholders may be customers, employees, managers, executives, suppliers, partners or investors. When self-synchronization is reached, the model states that the personal interests of individuals toward their privacy is in balance with the business interests of enterprises who collect and use the personal information of those individuals. [ 137 ] David Flaherty believes networked computer databases pose threats to privacy. He develops 'data protection' as an aspect of privacy, which involves \"the collection, use, and dissemination of personal information\". This concept forms the foundation for fair information practices used by governments globally. Flaherty forwards an idea of privacy as information control, \"[i]ndividuals want to be left alone and to exercise some control over how information about them is used\". [ 138 ] Richard Posner and Lawrence Lessig focus on the economic aspects of personal information control. Posner criticizes privacy for concealing information, which reduces market efficiency. For Posner, employment is selling oneself in the labour market, which he believes is like selling a product. Any 'defect' in the 'product' that is not reported is fraud. [ 139 ] For Lessig, privacy breaches online can be regulated through code and law. Lessig claims \"the protection of privacy would be stronger if people conceived of the right as a property right\", [ 140 ] and that \"individuals should be able to control information about themselves\". [ 141 ] There have been attempts to establish privacy as one of the fundamental human rights , whose social value is an essential component in the functioning of democratic societies. [ 142 ] Priscilla Regan believes that individual concepts of privacy have failed philosophically and in policy. She supports a social value of privacy with three dimensions: shared perceptions, public values, and collective components. Shared ideas about privacy allows freedom of conscience and diversity in thought. Public values guarantee democratic participation, including freedoms of speech and association, and limits government power. Collective elements describe privacy as collective good that cannot be divided. Regan's goal is to strengthen privacy claims in policy making: \"if we did recognize the collective or public-good value of privacy, as well as the common and public value of privacy, those advocating privacy protections would have a stronger basis upon which to argue for its protection\". [ 143 ] Leslie Regan Shade argues that the human right to privacy is necessary for meaningful democratic participation, and ensures human dignity and autonomy. Privacy depends on norms for how information is distributed, and if this is appropriate. Violations of privacy depend on context. The human right to privacy has precedent in the United Nations Declaration of Human Rights : \"Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.\" [ 144 ] Shade believes that privacy must be approached from a people-centered perspective, and not through the marketplace. [ 145 ] Dr. Eliza Watt, Westminster Law School, University of Westminster in London, UK, proposes application of the International Human Right Law (IHRL) concept of \"virtual control\" as an approach to deal with extraterritorial mass surveillance by state intelligence agencies. Dr. Watt envisions the \"virtual control\" test, understood as a remote control over the individual's right to privacy of communications, where privacy is recognized under the ICCPR, Article 17. This, she contends, may help to close the normative gap that is being exploited by nation states. [ 146 ] The privacy paradox is a phenomenon in which online users state that they are concerned about their privacy but behave as if they were not. [ 147 ] While this term was coined as early as 1998, [ 148 ] it was not used in its current popular sense until the year 2000. [ 149 ] [ 147 ] Susan B. Barnes similarly used the term privacy paradox to refer to the ambiguous boundary between private and public space on social media. [ 150 ] When compared to adults, young people tend to disclose more information on social media . However, this does not mean that they are not concerned about their privacy. Susan B. Barnes gave a case in her article: in a television interview about Facebook, a student addressed her concerns about disclosing personal information online. However, when the reporter asked to see her Facebook page, she put her home address, phone numbers, and pictures of her young son on the page. The privacy paradox has been studied and scripted in different research settings. Several studies have shown this inconsistency between privacy attitudes and behavior among online users. [ 151 ] However, by now an increasing number of studies have also shown that there are significant and at times large correlations between privacy concerns and information sharing behavior, [ 152 ] which speaks against the privacy paradox. A meta-analysis of 166 studies published on the topic reported an overall small but significant relation between privacy concerns and informations sharing or use of privacy protection measures. [ 153 ] So although there are several individual instances or anecdotes where behavior appear paradoxical, on average privacy concerns and privacy behaviors seem to be related, and several findings question the general existence of the privacy paradox. [ 154 ] However, the relationship between concerns and behavior is likely only small, and there are several arguments that can explain why that is the case. According to the attitude-behavior gap , attitudes and behaviors are in general and in most cases not closely related. [ 155 ] A main explanation for the partial mismatch in the context of privacy specifically is that users lack awareness of the risks and the degree of protection. [ 156 ] Users may underestimate the harm of disclosing information online. [ 28 ] On the other hand, some researchers argue that the mismatch comes from lack of technology literacy and from the design of sites. [ 157 ] For example, users may not know how to change their default settings even though they care about their privacy. Psychologists Sonja Utz and Nicole C. Krämer particularly pointed out that the privacy paradox can occur when users must trade-off between their privacy concerns and impression management. [ 158 ] A study conducted by Susanne Barth and Menno D.T. de Jo demonstrates that decision making takes place on an irrational level, especially when it comes to mobile computing. Mobile applications in particular are often built up in such a way that spurs decision making that is fast and automatic without assessing risk factors. Protection measures against these unconscious mechanisms are often difficult to access while downloading and installing apps. Even with mechanisms in place to protect user privacy, users may not have the knowledge or experience to enable these mechanisms. [ 159 ] Users of mobile applications generally have very little knowledge of how their personal data are used. When they decide which application to download, they typically are not able to effectively interpret the information provided by application vendors regarding the collection and use of personal data. [ 160 ] Other research finds that this lack of interpretability means users are much more likely to be swayed by cost, functionality, design, ratings, reviews and number of downloads than requested permissions for usage of their personal data. [ 161 ] The willingness to incur a privacy risk is suspected to be driven by a complex array of factors including risk attitudes, personal value for private information, and general attitudes to privacy (which are typically measured using surveys). [ 162 ] One experiment aiming to determine the monetary value of several types of personal information indicated relatively low evaluations of personal information. [ 160 ] Providing personal data does not only regard monetary value, but also increased ease of use in voluntary interactions in government processes. [ 163 ] Despite claims that ascertaining the value of data requires a \"stock-market for personal information\", [ 164 ] surveillance capitalism and the mass surveillance industry regularly place price tags on this form of data as it is shared between corporations and governments. Users are not always given the tools to live up to their professed privacy concerns, and they are sometimes willing to trade private information for convenience, functionality, or financial gain, even when the gains are very small. [ 165 ] One study suggests that people think their browser history is worth the equivalent of a cheap meal. [ 166 ] Another finds that attitudes to privacy risk do not appear to depend on whether it is already under threat or not. [ 162 ] The methodology of user empowerment describes how to provide users with sufficient context to make privacy-informed decisions. It is suggested by Andréa Belliger and David J. Krieger that the privacy paradox should not be considered a paradox, but more of a privacy dilemma , for services that cannot exist without the user sharing private data. [ 166 ] However, the general public is typically not given the choice whether to share private data or not, [ 19 ] [ 56 ] making it difficult to verify any claim that a service truly cannot exist without sharing private data. The privacy calculus model posits that two factors determine privacy behavior, namely privacy concerns (or perceived risks) and expected benefits. [ 167 ] [ 168 ] By now, the privacy calculus has been supported by several studies. [ 169 ] [ 170 ] As with other conceptions of privacy , there are various ways to discuss what kinds of processes or actions remove, challenge, lessen, or attack privacy. In 1960 legal scholar William Prosser created the following list of activities which can be remedied with privacy protection: [ 171 ] [ 172 ] From 2004 to 2008, building from this and other historical precedents, Daniel J. Solove presented another classification of actions which are harmful to privacy, including collection of information which is already somewhat public, processing of information, sharing information, and invading personal space to get private information. [ 173 ] In the context of harming privacy, information collection means gathering whatever information can be obtained by doing something to obtain it. [ 173 ] Examples include surveillance and interrogation . [ 173 ] Another example is how consumers and marketers also collect information in the business context through facial recognition which has recently caused a concern for things such as privacy. There is currently research being done related to this topic. [ 174 ] Companies like Google and Meta collect vast amounts of personal data from their users through various services and platforms. This data includes browsing habits, search history, location information, and even personal communications. These companies then analyze and aggregate this data to create detailed user profiles, which are sold to advertisers and other third parties. This practice is often done without explicit user consent, leading to an invasion of privacy as individuals have little control over how their information is used. The sale of personal data can result in targeted advertising, manipulation, and even potential security risks, as sensitive information can be exploited by malicious actors. This commercial exploitation of personal data undermines user trust and raises significant ethical and legal concerns regarding data protection and privacy rights. [ 175 ] It can happen that privacy is not harmed when information is available, but that the harm can come when that information is collected as a set, then processed together in such a way that the collective reporting of pieces of information encroaches on privacy. [ 176 ] Actions in this category which can lessen privacy include the following: [ 176 ] Count not him among your friends who will retail your privacies to the world. — Publilius Syrus Information dissemination is an attack on privacy when information which was shared in confidence is shared or threatened to be shared in a way that harms the subject of the information. [ 176 ] There are various examples of this. [ 176 ] Breach of confidentiality is when one entity promises to keep a person's information private, then breaks that promise. [ 176 ] Disclosure is making information about a person more accessible in a way that harms the subject of the information, regardless of how the information was collected or the intent of making it available. [ 176 ] Exposure is a special type of disclosure in which the information disclosed is emotional to the subject or taboo to share, such as revealing their private life experiences, their nudity, or perhaps private body functions. [ 176 ] Increased accessibility means advertising the availability of information without actually distributing it, as in the case of doxing . [ 176 ] Blackmail is making a threat to share information, perhaps as part of an effort to coerce someone. [ 176 ] Appropriation is an attack on the personhood of someone, and can include using the value of someone's reputation or likeness to advance interests which are not those of the person being appropriated. [ 176 ] Distortion is the creation of misleading information or lies about a person. [ 176 ] Invasion of privacy, a subset of expectation of privacy , is a different concept from the collecting, aggregating, and disseminating information because those three are a misuse of available data, whereas invasion is an attack on the right of individuals to keep personal secrets. [ 176 ] An invasion is an attack in which information, whether intended to be public or not, is captured in a way that insults the personal dignity and right to private space of the person whose data is taken. [ 176 ] An intrusion is any unwanted entry into a person's private personal space and solitude for any reason, regardless of whether data is taken during that breach of space. [ 176 ] Decisional interference is when an entity somehow injects itself into the personal decision-making process of another person, perhaps to influence that person's private decisions but in any case doing so in a way that disrupts the private personal thoughts that a person has. [ 176 ] Similarly to actions which reduce privacy , there are multiple angles of privacy and multiple techniques to improve them to varying extents. When actions are done at an organizational level , they may be referred to as cybersecurity . Individuals can encrypt e-mails via enabling either two encryption protocols, S/MIME , which is built into companies like Apple or Outlook and thus most common, or PGP . [ 177 ] The Signal messaging app, which encrypts messages so that only the recipient can read the message, is notable for being available on many mobile devices and implementing a form of perfect forward secrecy . [ 178 ] Signal has received praise from whistleblower Edward Snowden . [ 179 ] Encryption and other privacy-based security measures are also used in some cryptocurrencies such as Monero and ZCash . [ 180 ] [ 181 ] Anonymizing proxies or anonymizing networks like I2P and Tor can be used to prevent Internet service providers (ISP) from knowing which sites one visits and with whom one communicates, by hiding IP addresses and location, but does not necessarily protect a user from third party data mining. Anonymizing proxies are built into a user's device, in comparison to a Virtual Private Network (VPN), where users must download software. [ 182 ] Using a VPN hides all data and connections that are exchanged between servers and a user's computer, resulting in the online data of the user being unshared and secure, providing a barrier between the user and their ISP, and is especially important to use when a user is connected to public Wi-Fi. However, users should understand that all their data does flow through the VPN's servers rather than the ISP. Users should decide for themselves if they wish to use either an anonymizing proxy or a VPN. In a more non-technical sense, using incognito mode or private browsing mode will prevent a user's computer from saving history, Internet files, and cookies, but the ISP will still have access to the users' search history. Using anonymous search engines will not share a user's history, clicks, and will obstruct ad blockers. [ 183 ] Concrete solutions on how to solve paradoxical behavior still do not exist. Many efforts are focused on processes of decision making, like restricting data access permissions during application installation, but this would not completely bridge the gap between user intention and behavior. Susanne Barth and Menno D.T. de Jong believe that for users to make more conscious decisions on privacy matters, the design needs to be more user-oriented. [ 159 ] That being said, delivering on privacy protections is difficult due to the complexity of online consent processes, for example. [ 184 ] In a social sense, simply limiting the amount of personal information that users posts on social media could increase their security, which in turn makes it harder for criminals to perform identity theft. [ 183 ] Moreover, creating a set of complex passwords and using two-factor authentication can allow users to be less susceptible to their accounts being compromised when various data leaks occur. Furthermore, users should protect their digital privacy by using anti-virus software, which can block harmful viruses like a pop-up scanning for personal information on a users' computer. [ 185 ] Although there are laws that promote the protection of users, in some countries, like the U.S., there is no federal digital privacy law and privacy settings are essentially limited by the state of current enacted privacy laws. To further their privacy, users can start conversing with representatives, letting representatives know that privacy is a main concern, which in turn increases the likelihood of further privacy laws being enacted. [ 186 ] David Attenborough , a biologist and natural historian , affirmed that gorillas \"value their privacy\" while discussing a brief escape by a gorilla in London Zoo . [ 187 ] Lack of privacy in public spaces, caused by overcrowding, increases health issues in animals, including heart disease and high blood pressure . Also, the stress from overcrowding is connected to an increase in infant mortality rates and maternal stress. The lack of privacy that comes with overcrowding is connected to other issues in animals, which causes their relationships with others to diminish. How they present themselves to others of their species is a necessity in their life, and overcrowding causes the relationships to become disordered. [ 188 ] For example, David Attenborough claims that the gorilla's right to privacy is being violated when they are looked at through glass enclosures. They are aware that they are being looked at, therefore they do not have control over how much the onlookers can see of them. Gorillas and other animals may be in the enclosures due to safety reasons, however Attenborough states that this is not an excuse for them to be constantly watched by unnecessary eyes. Also, animals will start hiding in unobserved spaces. [ 188 ] Animals in zoos have been found to exhibit harmful or different behaviours due to the presence of visitors watching them: [ 189 ]",
    "links": [
      "Privacy in English law",
      "Window covering",
      "Regulatory compliance",
      "Personal Information Protection and Electronic Documents Act",
      "Alan F. Westin",
      "Media pluralism",
      "Geolocation",
      "Danielle Citron",
      "Office of the Personal Data Protection Committee",
      "Heart disease",
      "Office of the Data Protection Supervisor",
      "Louis Blom-Cooper",
      "Media culture",
      "Pretty Good Privacy",
      "Orangutan",
      "Liberty",
      "Industrial policy",
      "Nineteen Eighty-Four",
      "Telecommunications (Interception and Access) Amendment (Data Retention) Act 2015",
      "Civil law (legal system)",
      "Tabloid journalism",
      "Wikimedia Foundation",
      "Arbitrary arrest and detention",
      "Right to privacy in New Zealand",
      "Panopticon",
      "Post-Fordism",
      "Information privacy law",
      "De-anonymization",
      "Right to development",
      "Charles Fried",
      "Atlantic Monthly",
      "Torture",
      "Bibcode (identifier)",
      "Punk subculture",
      "Visual privacy",
      "Marketplace of ideas",
      "Natural historian",
      "Secret ballot",
      "The Chosun Ilbo",
      "Bank secrecy",
      "Right of asylum",
      "Noam Chomsky",
      "Trial by media",
      "Private browsing",
      "Deuterocanonical books",
      "Freedom of Information Act 2000",
      "Zcash",
      "Streaming media",
      "California Law Review",
      "Apple Inc.",
      "Sexual and reproductive health and rights",
      "National Security Agency",
      "Andréa Belliger",
      "Right to rest and leisure",
      "Abortion",
      "Graffiti",
      "Old media",
      "Protest",
      "William Prosser (academic)",
      "Data re-identification",
      "Digital footprints",
      "Sittlichkeit",
      "Cyber Civil Rights Initiative",
      "Counterculture",
      "American Civil Liberties Union",
      "Wiretapping",
      "Georg Wilhelm Friedrich Hegel",
      "Civil and political rights",
      "Social influence",
      "Fourth Amendment to the United States Constitution",
      "Data Protection Act, 2012",
      "The New York Times",
      "Virtual assistant privacy",
      "Blushing",
      "Mainstream media",
      "Human right to water and sanitation",
      "State media",
      "Metadata",
      "Commission nationale de l'informatique et des libertés",
      "Classified information",
      "Office of the Australian Information Commissioner",
      "Political satire",
      "Right of self-defense",
      "Managing the news",
      "Influence of mass media",
      "Wayback Machine",
      "Family planning",
      "Digital privacy",
      "Common law",
      "Oikos",
      "Printing press",
      "ISSN (identifier)",
      "Public participation",
      "News broadcasting",
      "Right to a fair trial",
      "United States v. Jones (2012)",
      "Self-criticism",
      "ISBN (identifier)",
      "National Privacy Commission",
      "Information Commissioner's Office",
      "The Guardian",
      "Agenda-setting theory",
      "Doxxing",
      "Female genital mutilation",
      "Online harassment",
      "PMC (identifier)",
      "Transgender rights",
      "Right to protest",
      "Edward Bernays",
      "Information",
      "Freedom of speech",
      "Philosophy & Public Affairs",
      "Family rights",
      "John C. Wells",
      "Strike action",
      "Banksy",
      "Lobbying",
      "Personhood",
      "24-hour news cycle",
      "Consumerism",
      "Outlook.com",
      "Open access",
      "Bipartisanship as an ideology",
      "Cult of personality",
      "Media studies",
      "Electronic Frontier Foundation",
      "Helen Nissenbaum",
      "Stanford Encyclopedia of Philosophy",
      "Mediatization (media)",
      "Sissela Bok",
      "Universal Declaration of Human Rights",
      "S2CID (identifier)",
      "Al Franken",
      "Federated Learning of Cohorts",
      "Privacy laws of the United Kingdom",
      "Independent media",
      "Rose Eveleth",
      "Verso Books",
      "Federal Data Protection and Information Commissioner",
      "Personal Data Protection Act (Sri Lanka)",
      "Labor rights",
      "Exploitation of women in mass media",
      "Riley v. California",
      "Carpenter v. United States",
      "Suicide of Amanda Todd",
      "Right to social security",
      "Right to science and culture",
      "Interrogation",
      "Location-based service",
      "Expectation of privacy",
      "Griswold v. Connecticut",
      "New media",
      "Protest paradigm",
      "American English",
      "Data protection (privacy) laws in Russia",
      "Theodor W. Adorno",
      "Fake news",
      "Right to sexuality",
      "Personal Data Protection Act 2012 (Singapore)",
      "Right to Internet access",
      "Surveillance",
      "Cambridge University Press",
      "Censorship",
      "Media transparency",
      "Thermal imaging",
      "Right of access to personal data",
      "Digital media",
      "Media bias",
      "Media development",
      "Stalking",
      "Speedy trial",
      "Canadian Charter of Rights and Freedoms",
      "Kyllo v. United States",
      "Involuntary treatment",
      "Data Protection Commissioner",
      "Microsoft",
      "Federal Commissioner for Data Protection and Freedom of Information",
      "Vance Packard",
      "Suicide of Tyler Clementi",
      "Polish Data Protection Commissioner",
      "Identity theft",
      "Effects of violence in mass media",
      "Information security",
      "Security",
      "British English",
      "Bodily integrity",
      "HTTP cookie",
      "Privacy in education",
      "TED (conference)",
      "Wall",
      "Semiotic democracy",
      "Modesty",
      "Dumbing down",
      "I2P",
      "Transparency (behavior)",
      "Freedom of information",
      "Freedom from discrimination",
      "Spectacle (critical theory)",
      "Media manipulation",
      "Consumer protection",
      "Right to education",
      "Fence",
      "Post-mortem privacy",
      "Computerworld",
      "John Locke",
      "Privacy software",
      "Freedom of assembly",
      "Samuel D. Warren (US attorney)",
      "News media",
      "Police body camera",
      "Personal identifier",
      "Nationality",
      "Gramm–Leach–Bliley Act",
      "Jacques Rancière",
      "Slavery",
      "Facebook",
      "Penumbra (law)",
      "Right to work",
      "Privacy International",
      "Media event",
      "Information privacy",
      "Search warrant",
      "End user",
      "Value-action gap",
      "Freedom of movement",
      "Catch and kill",
      "PRISM",
      "Lion-tailed macaque",
      "Email privacy",
      "SAGE Publishing",
      "Encryption",
      "Civil Code of Quebec",
      "United States Constitution",
      "Privacy-invasive software",
      "Private Information",
      "Ruth Gavison",
      "Media circus",
      "Hypertension",
      "National data protection authority",
      "Electronic media",
      "Norwegian Data Protection Authority",
      "Surveillance capitalism",
      "Deepfakes",
      "Data breach",
      "Alternative media",
      "User (computing)",
      "Solitude",
      "Guy Debord",
      "Human rights",
      "Right to life",
      "Tort",
      "Eavesdropping",
      "Physical intimacy",
      "Gorilla",
      "Revenge porn",
      "Danish Data Protection Agency",
      "Intimate part",
      "Independent contractors",
      "JSTOR (identifier)",
      "Mass media",
      "Right to homeland",
      "Canadian privacy law",
      "European Data Protection Supervisor",
      "First Amendment to the United States Constitution",
      "Constitution of South Africa",
      "United Nations Declaration of Human Rights",
      "Workplace privacy",
      "Sensationalism",
      "Totalitarianism",
      "Right to counsel",
      "Privacy and Electronic Communications (EC Directive) Regulations 2003",
      "Open data",
      "Electronic Privacy Information Center",
      "Police",
      "Recuperation (politics)",
      "Occupation (protest)",
      "Big data",
      "Environmental Information Regulations 2004",
      "Civil disobedience",
      "NOYB",
      "Culture jamming",
      "Privacy concerns with Facebook",
      "Cathedral glass",
      "Right to food",
      "Alan Westin",
      "London Zoo",
      "Right of reply",
      "Dobbs v. Jackson Women's Health Organization",
      "Swatting",
      "S/MIME",
      "Spanish Data Protection Agency",
      "California Consumer Privacy Act",
      "International Association of Privacy Professionals",
      "Media independence",
      "Content moderation",
      "Anonymity",
      "Consumer privacy",
      "IPads",
      "Internet privacy",
      "Personal Information Protection Commission (South Korea)",
      "Self-harm",
      "Personal Data Protection Authority",
      "Freedom of association",
      "AccuWeather",
      "Secrecy",
      "Cruel, inhuman, or degrading treatment",
      "PMID (identifier)",
      "Marshall McLuhan",
      "Pensée unique",
      "Biologist",
      "Civil liberties",
      "Influencer",
      "Dutch Data Protection Authority",
      "Nomination rules",
      "Right to die",
      "Polis",
      "PRISM (surveillance program)",
      "Suffrage",
      "Korea Communications Commission",
      "Tor (network)",
      "OCLC (identifier)",
      "Concentration of media ownership",
      "Media franchise",
      "BBC News",
      "John Esling",
      "Online Etymology Dictionary",
      "Constitution of Brazil",
      "Two Treatises of Government",
      "Mass society",
      "Physical media",
      "Data aggregation",
      "Mass surveillance industry",
      "Call-out culture",
      "Computer Professionals for Social Responsibility",
      "Propaganda",
      "Walter Lippmann",
      "Advertising",
      "Access to justice",
      "Narcotizing dysfunction",
      "Privacy Act 1988",
      "Right to privacy",
      "Online advertising",
      "Global Network Initiative",
      "Right to truth",
      "Steve Jobs",
      "Internet",
      "European Digital Rights",
      "Jane Setter",
      "Cybersecurity",
      "Privacy (disambiguation)",
      "Google",
      "Contextual integrity",
      "Viral phenomena",
      "Yellow journalism",
      "Roe v. Wade",
      "Automotive privacy",
      "Framing (social sciences)",
      "Economic, social and cultural rights",
      "Intersex human rights",
      "Michel Foucault",
      "Health Insurance Portability and Accountability Act",
      "Security of person",
      "Speech recognition",
      "Right of return",
      "Right to an adequate standard of living",
      "Privacy Act of 1974",
      "Digital rights",
      "Reproductive rights",
      "Mobile media",
      "Right to clothing",
      "Sexual and reproductive health",
      "Spin (propaganda)",
      "Global surveillance",
      "Psychologist",
      "Contextual advertising",
      "John Stuart Mill",
      "Collectivism and individualism",
      "Informed consent",
      "Clarence Thomas",
      "GDPR",
      "Future of Privacy Forum",
      "Louis Brandeis",
      "Understanding Privacy",
      "IPhones",
      "Facebook–Cambridge Analytica data scandal",
      "David Attenborough",
      "Natural rights and legal rights",
      "Social media",
      "Right to property",
      "Self-determination",
      "Cellphone surveillance",
      "Crowd manipulation",
      "Right to keep and bear arms",
      "Misogynist",
      "Freedom of religion",
      "Ronald Hamowy",
      "Software bug",
      "Children's Online Privacy Protection Act",
      "Cato Institute",
      "Doi (identifier)",
      "Advanced capitalism",
      "Richard Posner",
      "Adrienne Porter Felt",
      "Culture industry",
      "Constitution of the Republic of Korea",
      "Anonymous proxy",
      "Right to a healthy environment",
      "Boycott",
      "Forced circumcision",
      "American Dream",
      "Identity theft in the United States",
      "Morality",
      "Trade secret",
      "Privacy engineering",
      "Privacy concerns regarding Google",
      "Aadhaar",
      "Signal (messaging app)",
      "Virtual private network",
      "Privacy-enhancing technologies",
      "Privacy law in Denmark",
      "Data Protection Board of India",
      "CCTV camera",
      "Executive privilege",
      "YouTube",
      "Lost media",
      "George Orwell",
      "Global Positioning System",
      "Mass surveillance",
      "Personality rights",
      "Review bomb",
      "Selfies",
      "James Rachels",
      "Jurists",
      "Data Privacy Lab",
      "Personal Information Protection Law of the People's Republic of China",
      "Cotton-top tamarins",
      "Seclusion",
      "Doxing",
      "Digital identity",
      "Privacy concerns with social networking services",
      "Right to be forgotten",
      "Personal data",
      "Clothes",
      "Peter Roach (phonetician)",
      "Privatus",
      "Italian Constitution",
      "Book of Sirach",
      "Privacy Act (Canada)",
      "Intimate relationships",
      "Amazon (company)",
      "The Naked Society",
      "Equal pay for equal work",
      "Edward Snowden",
      "Homophobic",
      "General Personal Data Protection Law",
      "Publilius Syrus",
      "Twitter",
      "Human",
      "Right to family life",
      "Freedom of thought",
      "Data Protection Act 1998",
      "Public relations",
      "Jean Baudrillard",
      "LCCN (identifier)",
      "Right to petition",
      "Privacy law",
      "Supreme Court of the United States",
      "Edwin Lawrence Godkin",
      "Media economics",
      "Republican Party (United States)",
      "Agence France-Presse",
      "Privacy settings",
      "California Privacy Rights Act",
      "Aristotle",
      "Jeremy Bentham",
      "Turkish Data Protection Authority",
      "Privacy in Australian law",
      "David Flaherty",
      "Monero",
      "English Pronouncing Dictionary",
      "Library of Congress",
      "Media policy",
      "Equality before the law",
      "Asia-Pacific Economic Cooperation",
      "General Data Protection Regulation",
      "Organisation for Economic Co-operation and Development",
      "Medical privacy",
      "Data security",
      "De-identification",
      "Center for Democracy and Technology",
      "IEEE Computer Society",
      "Utilitarianism",
      "The Lonely Crowd",
      "Privacy laws of the United States",
      "Bundesdatenschutzgesetz",
      "Remuneration",
      "The Wire",
      "Daniel Jones (phonetician)",
      "Presumption of innocence",
      "Implied consent",
      "Privacy policy",
      "Swedish Data Protection Authority",
      "Right to resist",
      "Perfect forward secrecy",
      "LGBTQ rights by country or territory",
      "Wired (magazine)",
      "Personal information",
      "Quebec Charter of Human Rights and Freedoms",
      "Chimpanzee",
      "Political demonstration",
      "Interpersonal relationship",
      "SSRN (identifier)",
      "Right to health",
      "Right to housing",
      "On Liberty",
      "Cancel culture",
      "Lorenzo Magnani",
      "Privacy concerns with Twitter",
      "Chicago",
      "Right to be Forgotten"
    ]
  },
  "Federated search": {
    "url": "https://en.wikipedia.org/wiki/Federated_search",
    "title": "Federated search",
    "content": "Federated search retrieves information from a variety of sources via a search application built on top of one or more search engines. [ 1 ] A user makes a single query request which is distributed to the search engines , databases or other query engines participating in the federation. The federated search then aggregates the results that are received from the search engines for presentation to the user. Federated search can be used to integrate disparate information resources within a single large organization (\"enterprise\") or for the entire web. Federated search, unlike distributed search, requires centralized coordination of the searchable resources. This involves both coordination of the queries transmitted to the individual search engines and fusion of the search results returned by each of them. Federated search came about to meet the need of searching multiple disparate content sources with one query. This allows a user to search multiple databases at once in real time, arrange the results from the various databases into a useful form and then present the results to the user. As such, it is an information aggregation or integration approach - it provides single point access to many information resources, and typically returns the data in a standard or partially homogenized form. Other approaches include constructing an Enterprise data warehouse , Data lake , or Data hub . Federated Search queries many times in many ways (each source is queried separately) where other approaches import and transform data many times, typically in overnight batch processes. Federated search provides a real-time view of all sources (to the extent they are all online and available). In industrial search engines, such as LinkedIn , federated search is used to personalize vertical preference for ambiguous queries. [ 2 ] For instance, when a user issues a query like \"machine learning\" on LinkedIn, he or she could mean to search for people with machine learning skill, jobs requiring machine learning skill or content about the topic. In such cases, federated search could exploit user intent (e.g., hiring, job seeking or content consuming) to personalize the vertical order for each individual user. As described by Peter Jacso, [ 3 ] federated searching consists of Federated search portals, either commercial or open access , generally search public access bibliographic databases , public access Web-based library catalogues ( OPACs ), Web-based search engines like Google and/or open-access, government-operated or corporate data collections. These individual information sources send back to the portal's interface a list of results from the search query. The user can review this hit list. Some portals will merely screen scrape the actual database results and not directly allow a user to enter the information source's application. More sophisticated ones will de-dupe the results list by merging and removing duplicates. There are additional features available in many portals, but the basic idea is the same: to improve the accuracy and relevance of individual searches as well as reduce the amount of time required to search for resources. This process allows federated search some key advantages when compared with existing crawler-based search engines. Federated search need not place any requirements or burdens on owners of the individual information sources, other than handling increased traffic. Federated searches are inherently as current as the individual information sources, as they are searched in real time. One application of federated searching is the metasearch engine . However, the metasearch approach does not overcome the shortcomings of the component search engines, such as incomplete indexes. Documents that are not indexed by search engines create what is known as the deep Web , or invisible Web. Google Scholar is one example of many projects trying to address this, by indexing electronic documents that search engines ignore. And the metasearch approach, like the underlying search engine technology, only works with information sources stored in electronic form. One of the main challenges of metasearch, is ensuring that the search query is compatible with the component search engines that are being federated and combined. When the search vocabulary or data model of the search system is different from the data model of one or more of the foreign target systems, the query must be translated into each of the foreign target systems. This can be done using simple data-element translation or may require semantic translation . For example, if one search engine allows for quoting of exact strings or n-grams and another does not, the query must be translated to be compatible with each search engine. To translate a quoted exact string query, it can be broken down into a set of overlapping N-grams that are most likely to give the desired search results in each search engine. Another challenge faced in the implementation of federated search engines is scalability. It is difficult to maintain the performance, the response speed, of a federated search engine as it combines more and more information sources together. One implementation of federated search that has begun to address this issue is WorldWideScience , hosted by the US Department of Energy 's Office of Scientific and Technical Information . WorldWideScience [ 4 ] is composed of more than 40 information sources, several of which are federated search portals themselves. One such portal is Science.gov [ 5 ] which itself federates more than 30 information sources representing most of the R&D output of the US Federal government. Science.gov returns its highest ranked results to WorldWideScience, which then merges and ranks these results with the search returned by the other information sources that comprise WorldWideScience. [ 5 ] This approach of cascaded federated search enables large number of information sources to be searched via a single query. Another application Sesam running in both Norway and Sweden has been built on top of an open sourced platform specialised for federated search solutions. Sesat, [ 6 ] an acronym for Sesam Search Application Toolkit , is a platform that provides much of the framework and functionality required for handling parallel and pipelined searches and displaying them elegantly in a user interface, allowing engineers to focus on the index/database configuration tuning. To personalize vertical orders in federated search, LinkedIn search engine [ 2 ] exploits the searcher's profile and recent activities to infer his or her intent, such as hiring, job seeking and content consuming, then uses the intent, along with many other signals, to rank vertical orders that are personally relevant to the individual searcher. SWIRL Search [ 7 ] is an open source federated search engine, released under the Apache 2.0 license. It includes pre-built connectors to popular open source search engines, and re-ranks results using cosine vector similarity. Federated searches present a number of significant challenges, as compared with conventional, single-source searches: When federated search is performed against secure data sources, the users' credentials must be passed on to each underlying search engine, so that appropriate security is maintained. If the user has different login credentials for different systems, there must be a means to map their login ID to each search engine's security domain. [ 8 ] Suppose three real-estate sites are searched, each provides a list of hyperlinked city names to click on, to see matches only in each city. Ideally these facets would be combined into one set, but that presents additional technical challenges. [ 9 ] The system also needs to understand \"next page\" links if it's going to allow the user to page through the combined results. Some of this challenge of mapping to a common form can be solved if the federated resources support linked open data via RDF . Ontologies (rules) can be added to map results to common forms using that technology. Each web resource has its own notion of relevance score, and may support some sorted results orders. Relevance varies greatly among \"federates\" in the search, so knowing how to interleave results to show the most relevant is difficult or impossible. Federated search may have to restrict itself to the minimal set of query capabilities that are common to all federates. E.g. if Google supports negation and quoted phrases, but science.gov does not, it will be impossible for the federated search to support negated, quoted phrases. As the number of federates (federated sources) grows, the likelihood of one or more slow or offline federates becomes high. The federated search must decide when to consider a federate offline, or wait for a slow response. Response times will be dictated by the slowest federate of the bunch. Development groups should typically not hit live, production systems as they do regular work, much less intensive load testing. Also, some resources are secure, and should not be arbitrarily queried and exposed in development due to privacy and security concerns. Therefore, the development, testing and performance test environments must include installation and configuration for many sub-systems to allow safe, secure testing. For the overall federated system to be HA/DR, every sub-system may need to be HA/DR. Similarly, performance modeling and capacity planning for the federated system requires modeling, planning and sometimes expansion of all federates. For all of the above reasons, within an enterprise, a data hub or data lake may be preferable, or a hybrid approach. Data hubs and lakes simplify development and access, but may incur some time lag before data is available (without special synchronizing logic). On the web, federation is more typical.",
    "links": [
      "Open access",
      "IT disaster recovery",
      "Audio search engine",
      "Doi (identifier)",
      "Wide area information server",
      "Bibliographic databases",
      "Metasearch engine",
      "S2CID (identifier)",
      "Document retrieval",
      "Search by sound",
      "Search engine marketing",
      "Distributed web crawling",
      "Focused crawler",
      "Performance prediction",
      "OPAC",
      "Enterprise search",
      "Online search",
      "Social search",
      "Screen scrape",
      "Search/Retrieve via URL",
      "Data model",
      "List of search engines",
      "Linked open data",
      "N-gram",
      "GitHub",
      "WorldWideScience",
      "User intent",
      "Multisearch",
      "Sesam Search Application Toolkit",
      "Representational State Transfer",
      "Selection-based search",
      "Vertical search",
      "Web query classification",
      "Natural language search engine",
      "Data hub",
      "Cross-language search",
      "Multimedia search",
      "Voice search",
      "Capacity planning",
      "Desktop search",
      "Web crawler",
      "Spider trap",
      "ArXiv (identifier)",
      "Website mirroring software",
      "Index (search engine)",
      "Google Scholar",
      "Timeout (computing)",
      "Resource Description Framework",
      "OpenSearch (specification)",
      "Video search engine",
      "Enterprise data warehouse",
      "Deep Web (search indexing)",
      "Internet search",
      "Office of Scientific and Technical Information",
      "Robots exclusion standard",
      "Z39.50",
      "Data lake",
      "Google",
      "Search aggregator",
      "Text mining",
      "Search engine (computing)",
      "Local search (Internet)",
      "Sesam (search engine)",
      "LinkedIn",
      "Search engine",
      "Wayback Machine",
      "Semantic search",
      "Web indexing",
      "Collaborative search engine",
      "Image retrieval",
      "Web search query",
      "ISBN (identifier)",
      "High availability",
      "Web search engine",
      "US Department of Energy",
      "Semantic translation",
      "Search/Retrieve Web Service",
      "Web archiving",
      "Web query",
      "Cross-language information retrieval",
      "Evaluation measures (information retrieval)",
      "Search engine optimization"
    ]
  },
  "Probabilistic relevance model": {
    "url": "https://en.wikipedia.org/wiki/Probabilistic_relevance_model",
    "title": "Probabilistic relevance model",
    "content": "The probabilistic relevance model [ 1 ] [ 2 ] was devised by Stephen E. Robertson and Karen Spärck Jones as a framework for probabilistic models to come. It is a formalism of information retrieval useful to derive ranking functions used by search engines and web search engines in order to rank matching documents according to their relevance to a given search query. It is a theoretical model estimating the probability that a document d j is relevant to a query q . The model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query q . Such an ideal answer set is called R and should maximize the overall probability of relevance to that user. The prediction is that documents in this set R are relevant to the query, while documents not present in the set are non-relevant. s i m ( d j , q ) = P ( R | d → j ) P ( R ¯ | d → j ) {\\displaystyle sim(d_{j},q)={\\frac {P(R|{\\vec {d}}_{j})}{P({\\bar {R}}|{\\vec {d}}_{j})}}} There are some limitations to this framework that need to be addressed by further development: To address these and other concerns, other models have been developed from the probabilistic relevance framework, among them the Binary Independence Model from the same author. The best-known derivatives of this framework are the Okapi (BM25) weighting scheme and its multifield refinement, BM25F.",
    "links": [
      "Stephen Robertson (computer scientist)",
      "Probabilistic relevance model (BM25)",
      "Web search engine",
      "Relevance (information retrieval)",
      "Ranking function",
      "Doi (identifier)",
      "CiteSeerX (identifier)",
      "Statistical model",
      "Binary Independence Model",
      "Karen Spärck Jones",
      "Information retrieval",
      "Search engine"
    ]
  },
  "Data modeling": {
    "url": "https://en.wikipedia.org/wiki/Data_modeling",
    "title": "Data modeling",
    "content": "Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. It may be applied as part of broader Model-driven engineering (MDE) concept. Data modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations. Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system. There are three different types of data models produced while progressing from requirements to the actual database to be used for the information system. [ 2 ] The data requirements are initially recorded as a conceptual data model which is essentially a set of technology independent specifications about the data and is used to discuss initial requirements with the business stakeholders. The conceptual model is then translated into a logical data model , which documents structures of the data that can be implemented in databases. Implementation of one conceptual data model may require multiple logical data models. The last step in data modeling is transforming the logical data model to a physical data model that organizes the data into tables, and accounts for access, performance and storage details. Data modeling defines not just data elements, but also their structures and the relationships between them. [ 3 ] Data modeling techniques and methodologies are used to model data in a standard, consistent, predictable manner in order to manage it as a resource. The use of data modeling standards is strongly recommended for all projects requiring a standard means of defining and analyzing data within an organization, e.g., using data modeling: Data modelling may be performed during various types of projects and in multiple phases of projects. Data models are progressive; there is no such thing as the final data model for a business or application. Instead, a data model should be considered a living document that will change in response to a changing business. The data models should ideally be stored in a repository so that they can be retrieved, expanded, and edited over time. Whitten et al. (2004) determined two types of data modelling: [ 4 ] Data modelling is also used as a technique for detailing business requirements for specific databases . It is sometimes called database modelling because a data model is eventually implemented in a database. [ 4 ] Data models provide a framework for data to be used within information systems by providing specific definitions and formats. If a data model is used consistently across systems then compatibility of data can be achieved. If the same data structures are used to store and access data then different applications can share data seamlessly. The results of this are indicated in the diagram. However, systems and interfaces are often expensive to build, operate, and maintain. They may also constrain the business rather than support it. This may occur when the quality of the data models implemented in systems and interfaces is poor. [ 1 ] Some common problems found in data models are: In 1975 ANSI described three kinds of data-model instance : [ 5 ] According to ANSI, this approach allows the three perspectives to be relatively independent of each other. Storage technology can change without affecting either the logical or the conceptual schema. The table/column structure can change without (necessarily) affecting the conceptual schema. In each case, of course, the structures must remain consistent across all schemas of the same data model. In the context of business process integration (see figure), data modeling complements business process modeling , and ultimately results in database generation. [ 6 ] The process of designing a database involves producing the previously described three types of schemas – conceptual, logical, and physical. The database design documented in these schemas is converted through a Data Definition Language , which can then be used to generate a database. A fully attributed data model contains detailed attributes (descriptions) for every entity within it. The term \"database design\" can describe many different parts of the design of an overall database system . Principally, and most correctly, it can be thought of as the logical design of the base data structures used to store the data. In the relational model these are the tables and views . In an object database the entities and relationships map directly to object classes and named relationships. However, the term \"database design\" could also be used to apply to the overall process of designing, not just the base data structures, but also the forms and queries used as part of the overall database application within the Database Management System or DBMS. In the process, system interfaces account for 25% to 70% of the development and support costs of current systems. The primary reason for this cost is that these systems do not share a common data model . If data models are developed on a system by system basis, then not only is the same analysis repeated in overlapping areas, but further analysis must be performed to create the interfaces between them. Most systems within an organization contain the same basic data, redeveloped for a specific purpose. Therefore, an efficiently designed basic data model can minimize rework with minimal modifications for the purposes of different systems within the organization [ 1 ] Data models represent information areas of interest. While there are many ways to create data models, according to Len Silverston (1997) [ 7 ] only two modeling methodologies stand out, top-down and bottom-up: Sometimes models are created in a mixture of the two methods: by considering the data needs and structure of an application and by consistently referencing a subject-area model. In many environments, the distinction between a logical data model and a physical data model is blurred. In addition, some CASE tools don't make a distinction between logical and physical data models . [ 7 ] There are several notations for data modeling. The actual model is frequently called \"entity–relationship model\", because it depicts data in terms of the entities and relationships described in the data . [ 4 ] An entity–relationship model (ERM) is an abstract conceptual representation of structured data. Entity–relationship modeling is a relational schema database modeling method, used in software engineering to produce a type of conceptual data model (or semantic data model ) of a system, often a relational database , and its requirements in a top-down fashion. These models are being used in the first stage of information system design during the requirements analysis to describe information needs or the type of information that is to be stored in a database . The data modeling technique can be used to describe any ontology (i.e. an overview and classifications of used terms and their relationships) for a certain universe of discourse i.e. the area of interest. Several techniques have been developed for the design of data models. While these methodologies guide data modelers in their work, two different people using the same methodology will often come up with very different results. Most notable are: Generic data models are generalizations of conventional data models . They define standardized general relation types, together with the kinds of things that may be related by such a relation type. The definition of the generic data model is similar to the definition of a natural language. For example, a generic data model may define relation types such as a 'classification relation', being a binary relation between an individual thing and a kind of thing (a class) and a 'part-whole relation', being a binary relation between two things, one with the role of part, the other with the role of whole, regardless the kind of things that are related. Given an extensible list of classes, this allows the classification of any individual thing and to specification of part-whole relations for any individual object. By standardization of an extensible list of relation types, a generic data model enables the expression of an unlimited number of kinds of facts and will approach the capabilities of natural languages. Conventional data models, on the other hand, have a fixed and limited domain scope, because the instantiation (usage) of such a model only allows expressions of kinds of facts that are predefined in the model. The logical data structure of a DBMS, whether hierarchical, network, or relational, cannot totally satisfy the requirements for a conceptual definition of data because it is limited in scope and biased toward the implementation strategy employed by the DBMS. That is unless the semantic data model is implemented in the database on purpose, a choice which may slightly impact performance but generally vastly improves productivity. Therefore, the need to define data from a conceptual view has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure the real world, in terms of resources, ideas, events, etc., is symbolically defined by its description within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world. [ 8 ] The purpose of semantic data modeling is to create a structural model of a piece of the real world, called \"universe of discourse\". For this, three fundamental structural relations are considered: A semantic data model can be used to serve many purposes, such as: [ 8 ] The overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful abstraction concepts known from the artificial intelligence field. The idea is to provide high-level modeling primitives as integral parts of a data model in order to facilitate the representation of real-world situations. [ 10 ]",
    "links": [
      "Computer programming",
      "Outline of the Java programming language",
      "Artificial intelligence",
      "Lonnie D. Bentley",
      "Adaptive software development",
      "Data Definition Language",
      "Information model",
      "International Requirements Engineering Board",
      "Data (computer science)",
      "Outline of the Python programming language",
      "Software Engineering Body of Knowledge",
      "Entity–relationship model",
      "Data-flow diagram",
      "Programming paradigm",
      "Glossary of artificial intelligence",
      "Outline of the C++ programming language",
      "Data Format Description Language",
      "Object-relational mapping",
      "Software documentation",
      "Outline of the JavaScript programming language",
      "Database model",
      "Ontology (computer science)",
      "Conceptual schema",
      "View model",
      "Backward compatibility",
      "Entity Data Model",
      "Data structure diagram",
      "Debugging",
      "UML tool",
      "Systems modeling language",
      "V-model (software development)",
      "Outline of software development",
      "Software maintenance",
      "Outline of the C sharp programming language",
      "Software system",
      "Requirements engineering",
      "Service-oriented architecture",
      "Binary relation",
      "Microsoft Solutions Framework",
      "Acceptance test-driven development",
      "Software verification and validation",
      "Lean software development",
      "Structured analysis",
      "Graphical user interface builder",
      "Bachman diagram",
      "Continuous integration",
      "Metamodeling",
      "Project management",
      "Compiler",
      "Integrated development environment",
      "Risk management",
      "Table (database)",
      "Component-based software engineering",
      "Software engineering",
      "Kanban (development)",
      "Building information modeling",
      "Feature-driven development",
      "Data model",
      "Object model",
      "Software quality",
      "Common data model",
      "Relational Model",
      "Interface (computer science)",
      "Database design",
      "Metadata modeling",
      "Database",
      "Compatibility mode",
      "DevOps",
      "Software development",
      "Logical data model",
      "Software configuration management",
      "Specification by example",
      "Relational database",
      "Enterprise modelling",
      "Semantic data model",
      "Three-schema approach",
      "Copyright status of works by the federal government of the United States",
      "Site reliability engineering",
      "Model-driven engineering",
      "Software archaeology",
      "Information science",
      "Systems development life cycle",
      "Team software process",
      "Empirical software engineering",
      "Architectural pattern",
      "Pair programming",
      "Software design",
      "Physical data model",
      "Top-down and bottom-up design",
      "Build automation",
      "Model-driven development",
      "Personal software process",
      "American National Standards Institute",
      "Compatibility layer",
      "Extreme programming",
      "Software incompatibility",
      "Social software engineering",
      "FCO-IM",
      "Object–role modeling",
      "Software construction",
      "Software testing",
      "Database system",
      "Dynamic systems development method",
      "Abstraction (computer science)",
      "Continuous delivery",
      "Spiral model",
      "Function model",
      "Infrastructure as code",
      "Forward compatibility",
      "Scaled agile framework",
      "Object-oriented programming",
      "Outline of the C programming language",
      "Modeling language",
      "Process modeling",
      "Executable UML",
      "Domain-driven design",
      "Activity diagram",
      "Software prototyping",
      "Stand-up meeting",
      "Computer engineering",
      "View (database)",
      "Relational Model/Tasmania",
      "Formal methods",
      "Rational unified process",
      "Physical schema",
      "Round-trip engineering",
      "Requirement",
      "Database management system",
      "Disciplined agile delivery",
      "Software quality assurance",
      "Logical schema",
      "ISO/IEC 15504",
      "Relational model",
      "Zachman Framework",
      "Capability Maturity Model Integration",
      "Object database",
      "Enhanced entity–relationship model",
      "Core architecture data model",
      "Unified process",
      "Iterative and incremental development",
      "Cleanroom software engineering",
      "Application-release automation",
      "Programming tool",
      "Data",
      "Search-based software engineering",
      "Computer science",
      "Systems engineering",
      "Project Management Body of Knowledge",
      "Rapid application development",
      "Data model (GIS)",
      "Incremental build model",
      "XML schema",
      "Document modelling",
      "Universal Systems Language",
      "Software development methodology",
      "Business process modeling",
      "Data Vault Modeling",
      "SEMAT",
      "Waterfall model",
      "Barker's notation",
      "Comparison of data modeling tools",
      "Scrum (software development)",
      "ISO 9001",
      "Data dictionary",
      "Whitten, Jeffrey L.",
      "Data structure",
      "ITIL",
      "Software architecture",
      "Software deployment",
      "CI/CD",
      "Computer compatibility",
      "User experience",
      "IDEF1X",
      "Enterprise unified process",
      "ISO/IEC JTC 1/SC 7",
      "Debugger",
      "Glossary of computer science",
      "Conceptual modeling",
      "Jeffrey L. Whitten",
      "Data architecture",
      "Extended Backus–Naur form",
      "Essential systems analysis",
      "Wayback Machine",
      "Kevin C. Dittman",
      "Outline of the Rust programming language",
      "Generic data model",
      "Software",
      "Reengineering (software)",
      "Profiling (computer programming)",
      "Morgan Kaufmann Publishers",
      "Enterprise data modelling",
      "Object-Role Modeling",
      "Computer-aided software engineering",
      "Data warehouse",
      "ISBN (identifier)",
      "Software project management",
      "Business process",
      "Enterprise architecture",
      "Software development process",
      "Requirements analysis",
      "Experimental software engineering",
      "Domain of discourse",
      "Object Management Group",
      "Agile software development",
      "Unified Modeling Language",
      "Aspect-oriented programming",
      "Behavior-driven development",
      "Tablespace",
      "IEEE Standards Association",
      "Functional specification",
      "Ontology (information science)",
      "Information technology engineering",
      "Systems analysis",
      "Information",
      "Glossary of electrical and electronics engineering",
      "Systems modeling",
      "Test-driven development",
      "Information management",
      "Information system",
      "IDEF"
    ]
  },
  "Univac": {
    "url": "https://en.wikipedia.org/wiki/Univac",
    "title": "Univac",
    "content": "UNIVAC ( Universal Automatic Computer ) was a line of electronic digital stored-program computers starting with the products of the Eckert–Mauchly Computer Corporation . After capturing the public imagination with the use of the UNIVAC I during the 1952 US Presidential election it was decided to extend the branding to all machines made by the other computing divisions of the Remington Rand company ( Engineering Research Associates and the Norwalk Laboratory of Remington Rand). Subsequently after the merger of Remington Rand with the Sperry Corporation (under name of Sperry Rand) in 1955, it was decided to merge all three divisions along with Remington Rand's tabulator division into one unified organization under the name of the Univac division. [ 1 ] This name would persist until the mid 1980's when it would be renamed to the Sperry Computer Systems Division, [ 2 ] the last UNIVAC-badged system was the UNIVAC 1100/90 which was announced in 1982 and first shipped in late 1983. [ 3 ] J. Presper Eckert and John Mauchly built the ENIAC (Electronic Numerical Integrator and Computer) at the University of Pennsylvania 's Moore School of Electrical Engineering between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed Eckert–Mauchly Computer Corporation (EMCC), based in Philadelphia, Pennsylvania . That company first built a computer called BINAC (BINary Automatic Computer) for Northrop Aviation (which was little used, or perhaps not at all). Afterwards, the development of UNIVAC began in April 1946. [ 4 ] UNIVAC was first intended for the Bureau of the Census , which paid for much of the development, and then was put in production. With the death of EMCC's chairman and chief financial backer Henry L. Straus in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to Leslie Groves , [ 5 ] the retired army general who had previously managed building The Pentagon and led the Manhattan Project . The most famous UNIVAC product was the UNIVAC I mainframe computer of 1951, which became known for predicting the outcome of the 1952 United States presidential election : this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over Adlai Stevenson , whereas the final Gallup poll had Eisenhower winning the popular vote 51–49 in a close contest. [ 6 ] The prediction led CBS 's news boss in New York, Siegfried Mickelson , to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 8–7 odds for an Eisenhower win (the actual prediction was 100–1 in his favor). When the predictions proved true—Eisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College total— Charles Collingwood , the on-air announcer, announced that they had failed to believe the earlier prediction. [ 7 ] The United States Army requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the invasion of Normandy that were based on the relationships of various groups. [ 8 ] The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in Philadelphia, Pennsylvania . [ 1 ] [ 9 ] Remington Rand also had an engineering research lab in Norwalk, Connecticut , and later bought Engineering Research Associates (ERA) in St. Paul, Minnesota . [ 1 ] In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory. [ citation needed ] In 1955 Remington Rand merged with Sperry Corporation to become Sperry Rand. General Douglas MacArthur , then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. [ 10 ] Harry Franklin Vickers , then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. [ 10 ] The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. [ 1 ] William Norris was put in charge as Vice-President and General Manager [ 11 ] reporting to the President of the Remington Rand Division (of Sperry Rand). [ 12 ] The following is a list of the General Managers/Presidents of the Division. There was a some degree of internal organisation turmoil from the period of the creation of Sperry Rand in 1955 right into the early 1960s. This culminated in the resignation of William Norris in 1957 [ 12 ] and would continue until the early 1960s with the decentralisation of the former Remington Group and the promotion of UNIVAC to a full division of Sperry Rand. In the 1960s, UNIVAC was one of the eight major American computer companies in an industry then referred to as \" IBM and the seven dwarfs\" – a play on Snow White and the seven dwarfs, with IBM, by far the largest, being cast as Snow White and the other seven as being dwarfs: Burroughs , Univac, NCR , CDC , GE , RCA and Honeywell . [ 24 ] After GE sold its computer business to Honeywell and RCA sold its to Univac, the analogy to the seven dwarfs became less apt and the remaining small firms became known as the \" BUNCH \" ( B urroughs, U nivac, N CR, C ontrol Data, and H oneywell). During the 1960's Univac was involved in work on automating Air Traffic Control, the first trial system of the \"Automated Radar Terminal System\" (ARTS-I) was installed in Atlanta, followed by ARTS-IA system in New York, both of these systems were built around a UNIVAC 418 minicomputer. In 1969 the Federal Aviation Administration awarded a fixed-price multi-year contract to Univac Federal Systems Division for the installation of 64 ARTS III systems. Each installation consisted of three subsystems, namely the Data acquisition subsystem, the Data processing subsystem and the Data Entry and display subsystem. Burroughs Corporation received the subcontract for the Data acquisition subsystem, while the Data Entry and Display subsystem was subcontracted to Texas Instruments . The Data processing system was built around the 30-bit Univac 8303 IOP (I/O Processor) which could be configured in a multiprocessor configuration, depending on the specific Air traffic control site (New York had the largest) [ 25 ] [ 26 ] These systems would continue in service well into the 1990's. [ 27 ] In 1977, Sperry Rand purchased Varian Data Machines so as to enter the minicomputer market. Varian would be renamed as the Sperry UNIVAC Minicomputer Operation, operating as part of the Sperry UNIVAC division. [ 28 ] [ 29 ] Sperry UNIVAC would continue to market the V77 but never made a significant dent in the minicomputer market. To assist \"corporate identity\" the name was changed to Sperry Univac, along with Sperry Remington, Sperry New Holland , etc. In 1978, Sperry Rand, a conglomerate of various divisions (computers, typewriters, office furniture, hay balers, manure spreaders, gyroscopes, avionics, radar, electric razors), decided to concentrate solely on its computing interests and all of the unrelated divisions were sold. The company dropped the Rand from its title and reverted to Sperry Corporation. By 1981 Sperry Univac was the largest of Sperry's operating divisions, in that year it brought in revenues of $2.7 billion. [ 30 ] Subsequently the distinct Sperry UNIVAC branding was dropped and the division was renamed as the Sperry Computer Systems Division. [ 2 ] [ 31 ] In 1986, Sperry Corporation merged with Burroughs Corporation to become Unisys . After the 1986 merger of Burroughs and Sperry, Unisys evolved from a computer manufacturer to a computer services and outsourcing firm, competing at that time in the same marketplace as IBM , Electronic Data Systems (EDS), and Computer Sciences Corporation . As of 2021 [update] , Unisys continues to design and manufacture enterprise class computers with the ClearPath server lines. [ 32 ] In the course of its history, UNIVAC produced a number of separate model ranges. One early UNIVAC line of vacuum tube computers was based on the ERA 1101 and those models built at ERA were rebadged as UNIVAC 110x; despite the 1100 model numbers, they were not related to the latter 1100/2200 series. The 1103A is credited in the literature as the first computer to have interrupts. The original model range was the UNIVAC I (UNIVersal Automatic Computer I), the second commercial computer made in the United States. [ a ] The main memory consisted of tanks of liquid mercury implementing delay-line memory , arranged in 1,000 words of 12 alphanumeric characters each. The first machine was delivered on 31 March 1951. The Remington Rand 409 was a control panel programmed punched card calculator, designed in 1949, and sold in two models: the UNIVAC 60 (1952) and the UNIVAC 120 (1953). The UNIVAC File Computer was first shipped in 1956. It was equipped with between one and ten large drums each holding 180,000 Alphanumeric characters. [ 33 ] One early application was for an airline reservations system , [ 34 ] which was used by Eastern Air Lines . [ 35 ] It competed mainly against the IBM 650 and the IBM 305 RAMAC and a total of 130 were manufactured. [ 36 ] The UNIVAC II was an improvement to the UNIVAC I that UNIVAC first delivered in 1958. The improvements included magnetic (non-mercury) core memory of 2,000 to 10,000 words, UNISERVO II tape drives, which could use either the old UNIVAC I metal tapes or the new PET film tapes, and some circuits that were transistorized (although it was still a vacuum-tube computer ). It was fully compatible with existing UNIVAC I programs for both code and data. The UNIVAC II also added some instructions to the UNIVAC I's instruction set. The UNIVAC Solid State was a 2-address, decimal computer, with memory on a rotating drum with 5,000 signed 10-digit words, aimed at the general-purpose business market. It came in two versions: the Solid State 80 (IBM-Hollerith 80-column cards) and the Solid State 90 (Remington-Rand 90-column cards). This computer used magnetic logic , not transistors, because the transistors then available had highly variable characteristics and were not sufficiently reliable. Magnetic logic gates were based on magnetic cores with multiple wire windings; unlike vacuum tubes, they were solid-state devices and had a virtually infinite lifetime. The magnetic gates required drive pulses of current produced by a transmitter-type vacuum tube, of a type still used in amateur radio final amplifiers. Thus the Solid State depended, at the heart of its operations, on a vacuum tube, however, only a few tubes were required, instead of thousands, greatly increasing reliability. Sperry Rand began shipment of UNIVAC III in 1962, and produced 96 UNIVAC III systems. Unlike the UNIVAC I and UNIVAC II, it was a binary machine as well as maintaining support for all UNIVAC I and UNIVAC II decimal and alphanumeric data formats for backward compatibility. This was the last of the original UNIVAC machines. The UNIVAC 418 (aka 1219), first shipped in 1962, was an 18-bit word core memory machine. Over the three different models, more than 392 systems were manufactured. The UNIVAC 490 was a 30-bit word core memory machine with 16K or 32K words; 4.8 microsecond cycle time. The UNIVAC 1232 was a military version of the 490. [ 37 ] The UNIVAC 492 is similar to the UNIVAC 490 , but with extended memory to 64K 30-bit words. The UNIVAC 494 was a 30-bit word machine and successor to the UNIVAC 490/492 with faster CPU and 131K (later 262K) core memory. Up to 24 I/O channels were available and the system was usually shipped with UNIVAC FH880 or UNIVAC FH432 or FH1782 magnetic drum storage. Basic operating system was OMEGA (successor to REX for the 490) although custom operating systems were also used (e.g. CONTORTS for airline reservations). The UNIVAC 1050 was an internally programmed computer with up to 32K of six-bit character memory, which was introduced in 1963. It was a one-address machine with 30-bit instructions, had a 4K operating system and was programmed in the PAL assembly language. The 1050 was used extensively by the U.S. Air Force supply system for inventory control (The Standard Base Level Supply System [ 38 ] [ 39 ] ). The UNIVAC 1004 was a plug-board programmed punched-card data processing system, introduced in 1962 by UNIVAC. Total memory was 961 characters (6 bits per character) of core memory . Peripherals were a card reader (400 cards/minute), a card punch (200 cards/minute) using proprietary 90-column, round-hole cards or IBM-compatible, 80-column cards, a drum printer (400 lines/minute) and a Uniservo tape drive. [ 40 ] The 1004 was also supported as a remote card reader & printer via synchronous communication services. A U.S. Navy (Weapons Station, Concord) 1004 was dedicated to printing from tape as a means of offloading the task from their Solid State 80 mainframe, which produced the tapes. A design for an \"Emulator\" board was available that would allow the plugboard 1004 to run programs read from card decks. The board was made by the customers, not by UNIVAC. [ 41 ] However, the Emulator made heavy use of the 1004's program-branching reed relays, called selectors, which caused increased failures, later solved by the use of electronic selectors in the follow-on 1005. The UNIVAC 1005 , an enhanced version of the UNIVAC 1004, was first shipped in February 1966. [ 42 ] The machine saw extensive use by the US Army , including the first use of an electronic computer on the battlefield. Additional peripherals were also available including a paper tape reader and a three pocket stacker selectable card read/punch. The machine had a two-stage assembler (SAAL – Single Address Assembly Language) which was its primary assembler; it also had a three-stage card based compiler for a programming language called SARGE. 1005s were used as some nodes on Autodin . There were actually two versions of the 1005. The Federal Systems (military) version described above and a Commercial Systems version for civilian use. While the two versions shared common memory and peripherals they had two completely different instruction sets. [ citation needed ] The Commercial Systems version had a three pass assembler and a program generator. The UNIVAC 1100/2200 series is a series of compatible 36-bit transistorized computer systems initially made by Sperry Rand. The first true member of the series was the 1107, also known as the Thin-Film Computer due to its use of Thin-film memory for its Control Memory store (128 registers). Delivery of the 1107 was late and this affected sales; the subsequent 1108 was considerably more successful, and helped to establish the series as viable competitors to the IBM System/360 . The series continues to be supported today by Unisys Corporation as the ClearPath Forward Dorado Series. [ 43 ] The UNIVAC 9000 series (9200, 9300, 9400, 9700) was introduced in the mid-1960s to compete with the low end of the IBM 360 series. The 9200 and 9300, which differed in CPU speed and maximum memory capacity (16K for the original 9200 vs 32K for the other variants) implemented the same 16-bit modified subset of the 360 architecture as the Model 20 , while the UNIVAC 9400 implemented a subset of the full 360 instruction set. This did not violate IBM patents or copyrights; Sperry gained the right to \"clone\" the 360 as settlement of a lawsuit concerning IBM's infringement of Remington Rand's core memory patents. The 9400 was roughly equivalent to the IBM 360/30. The 9000 series used plated-wire memory , which functioned somewhat like core memory but used a non-destructive read. Since the 9000 series was intended as direct competitors to IBM, they used 80-column cards and EBCDIC character encoding. Memory capacity started as low as 8K byte primary storage for a batch-configured system. Optionally a disk drive subsystem could be added, with 8414 5 MB disk drives as well as tape drives, using the Uniservo VI. The UNIVAC Series 90 : The 1107 was the first 36-bit, word-oriented machine with an architecture close to that which came to be known as that of the \" 1100 Series .\" It ran the EXEC I or EXEC II operating system, batch-oriented second-generation operating systems , typical of the early to mid-1960s. The 1108 ran EXEC II or EXEC 8 . EXEC 8 allowed simultaneous handling of real-time applications, time-sharing , and background batch work. Transaction Interface Package (TIP), a transaction-processing environment, allowed programs to be written in COBOL whereas similar programs on competing systems were written in assembly language. On later systems, EXEC 8 was renamed OS 1100 and OS 2200 , with modern descendants maintaining backwards compatibility. Some more exotic operating systems ran on the 1108 – one of which was RTOS, a more bare-bones system designed to take better advantage of the hardware. The affordable System 80 series of small mainframes ran the OS/3 operating system which originated on the Univac 90/30 (and later 90/25, and 90/40). The UNIVAC Series 90 first ran with Univac developed OS/9, which was later replaced by RCA's Virtual Memory Operating System (VMOS). RCA originally called this operating system Time Sharing Operating System (TSOS), running on RCA's Spectra 70 line of virtual memory systems and changed its name to VMOS before the Sperry acquisition of RCA CSD. After VMOS was ported to the 90/60, Univac renamed it VS/9 . UNIVAC has been, over the years, a registered trademark of:",
    "links": [
      "Transistor",
      "RCA Spectra 70",
      "Electronic Data Systems",
      "UNIVAC EXEC I",
      "EBCDIC",
      "Sperry Rand",
      "Doi (identifier)",
      "Burroughs MCP",
      "KiB",
      "Raytheon",
      "Bureau of the Census",
      "History of computing hardware",
      "PET film (biaxially oriented)",
      "UNIVAC 1108",
      "S2CID (identifier)",
      "Sperry Rand Corporation",
      "The Pentagon",
      "Central processing unit",
      "UNIVAC 1050",
      "Thornton Carle Fry",
      "Northrop Corporation",
      "IBM System/370",
      "Computer",
      "Remington Rand",
      "CBS",
      "Siegfried Mickelson",
      "Federal Aviation Administration",
      "RCA Corporation",
      "UNIVAC 492",
      "University of Pennsylvania",
      "List of UNIVAC products",
      "Norwalk, Connecticut",
      "UNIVAC I",
      "Honeywell",
      "General Electric",
      "Adlai Stevenson II",
      "Word-oriented",
      "UNIVAC II",
      "Corpus Christi Times",
      "Mainframe computer",
      "FASTRAND",
      "Burroughs Corporation",
      "Batch-oriented",
      "UNIVAC Series 90",
      "UNIVAC 418",
      "IBM System/360",
      "Remington Rand 409",
      "Punched card",
      "NCR Voyix",
      "18-bit",
      "VS/9",
      "1952 United States presidential election",
      "Microcode",
      "Gallup, Inc.",
      "NEWP",
      "Texas Instruments",
      "John C. Dvorak",
      "Word (computer architecture)",
      "Convergent Technologies",
      "OS 2200",
      "Charles Collingwood (journalist)",
      "Operating system",
      "Engineering Research Associates",
      "BUNCH",
      "Magnetic-core memory",
      "Vacuum tube",
      "Eastern Air Lines",
      "UNIVAC 9000 series",
      "William Norris (CEO)",
      "CANDE",
      "COBOL",
      "UNIVAC FASTRAND",
      "Instruction prefetch",
      "ES7000",
      "IBM System/3",
      "Leslie Groves",
      "UNIVAC 1103",
      "J. Presper Eckert",
      "Harry Franklin Vickers",
      "Plated-wire memory",
      "Computer architecture",
      "National Bureau of Standards",
      "UNIVAC EXEC II",
      "Varian Data Machines",
      "Control Data Corporation",
      "UNIVAC III",
      "UNIVAC 490",
      "Time-sharing",
      "UNIVAC Solid State",
      "Unisys MCP programming languages",
      "Plugboard",
      "Multivac",
      "Eckert–Mauchly Computer Corporation",
      "Invasion of Normandy",
      "Sperry Corporation",
      "EXEC 8",
      "Moore School of Electrical Engineering",
      "Thin-film memory",
      "Robert S. Barton",
      "Minicomputer",
      "LINC 4GL",
      "Computer Sciences Corporation",
      "Burroughs Medium Systems",
      "Remington Rand Corporation",
      "Vacuum-tube computer",
      "Automatic Digital Network",
      "Control store",
      "Dwight D. Eisenhower",
      "Manhattan Project",
      "IBM System/360 Model 20",
      "Snow White",
      "System Development Corporation",
      "United States Census Bureau",
      "United States Army",
      "IBM 305 RAMAC",
      "Outsourcing",
      "Airline reservations system",
      "Burroughs Large Systems",
      "Magnetic logic",
      "OS 1100",
      "Hagley Museum and Library",
      "New Holland Agriculture",
      "St. Paul, Minnesota",
      "Wayback Machine",
      "ICON (microcomputer)",
      "Unisys",
      "Charles Babbage Institute",
      "John Mauchly",
      "Electronic Control Company",
      "ISSN (identifier)",
      "Douglas MacArthur",
      "FIELDATA",
      "BINAC",
      "Henry L. Straus",
      "Peter Altabef",
      "ISBN (identifier)",
      "Mainframes",
      "NewspaperARCHIVE",
      "Extended memory",
      "ENIAC",
      "Burroughs B1700",
      "Core memory",
      "HOLMES 2",
      "Burroughs B20",
      "UNIVAC 1100/2200 series",
      "Philadelphia, Pennsylvania",
      "UNIVAC 494",
      "IBM",
      "Delay-line memory",
      "ThoughtCo",
      "IBM 650",
      "Virtual Memory Operating System"
    ]
  },
  "Enterprise search": {
    "url": "https://en.wikipedia.org/wiki/Enterprise_search",
    "title": "Enterprise search",
    "content": "Enterprise search is software technology for searching data sources internal to a company, typically intranet and database content. The search is generally offered only to users internal to the company. [ 1 ] [ 2 ] Enterprise search can be contrasted with web search , which applies search technology to documents on the open web, and desktop search , which applies search technology to the content on a single computer. It can also be contrasted with on-site ecommerce search, which helps shoppers find products within a retail catalog. [ 3 ] Enterprise search systems index data and documents from a variety of sources such as: file systems , intranets , document management systems , e-mail , and databases . Many enterprise search systems integrate structured and unstructured data in their collections. [ 4 ] Enterprise search systems also use access controls to enforce a security policy on their users. [ 5 ] Enterprise search can be seen as a type of vertical search of an enterprise. In an enterprise search system, content goes through various phases from source repository to search results: Content awareness (or \"content collection\") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its APIs . This model is used when real-time indexing is important. In the pull model, the software gathers content from sources using a connector such as a web crawler or a database connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content. [ 6 ] Content from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve recall or precision . These may include stemming , lemmatization , synonym expansion, entity extraction , part of speech tagging. As part of processing and analysis, tokenization is applied to split the content into tokens which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall. The resulting text is stored in an index , which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and term frequency . Using a web page, the user issues a query to the system. The query consists of any terms the user enters as well as navigational actions such as faceting and paging information. The processed query is then compared to the stored index, and the search system returns results (or \"hits\") referencing source documents that match. Some systems are able to present the document as it was indexed.",
    "links": [
      "Precision (information retrieval)",
      "Text mining",
      "Index (search engine)",
      "Knowledge management",
      "List of search engines",
      "Recall (information retrieval)",
      "Doi (identifier)",
      "Stemming",
      "Database",
      "Entity extraction",
      "Intranet",
      "Databases",
      "Unstructured data",
      "Collaborative search engine",
      "Intranets",
      "Enterprise information access",
      "Web search query",
      "Web search",
      "Tokenization (lexical analysis)",
      "ISBN (identifier)",
      "Faceted search",
      "API",
      "Information extraction",
      "Vertical search",
      "E-mail",
      "File systems",
      "Document management system",
      "Data defined storage",
      "Lemmatization",
      "Part of speech",
      "Enterprise bookmarking",
      "Synonym",
      "Term frequency",
      "Desktop search",
      "Web crawler"
    ]
  },
  "Social search": {
    "url": "https://en.wikipedia.org/wiki/Social_search",
    "title": "Social search",
    "content": "Social search is a behavior of retrieving and searching on a social searching engine that mainly searches user-generated content such as news, videos and images related search queries on social media like Facebook , LinkedIn , Twitter , Instagram and Flickr . [ 1 ] It is an enhanced version of web search that combines traditional algorithms . The idea behind social search is that instead of ranking search results purely based on semantic relevance between a query and the results, a social search system also takes into account social relationships between the results and the searcher. [ 2 ] [ 3 ] [ 4 ] The social relationships could be in various forms. For example, in LinkedIn people search engine, the social relationships include social connections between searcher and each result, whether or not they are in the same industries, work for the same companies, belong the same social groups, and go the same schools, etc. [ 2 ] [ 5 ] Social search may not be demonstrably better than algorithm-driven search. [ 6 ] In the algorithmic ranking model that search engines used in the past, relevance of a site is determined after analyzing the text and content on the page and link structure of the document. In contrast, search results with social search highlight content that was created or touched by other users who are in the Social Graph of the person conducting a search. It is a personalized search technology with online community filtering to produce highly personalized results. [ 7 ] Social search takes many forms, ranging from simple shared bookmarks or tagging of content with descriptive labels to more sophisticated approaches that combine human intelligence with computer algorithms. Depending on the feature-set of a particular search engine , these results may then be saved and added to community search results, further improving the relevance of results for future searches of that keyword. The principle behind social search is that human network oriented results would be more meaningful and relevant for the user, instead of computer algorithms deciding the results for specific queries. [ 8 ] [ 9 ] [ 10 ] [ 11 ] Over the years, there have been different studies, researches and some implementations of Social Search. In 2008, there were a few startup companies that focused on ranking search results according to one's social graph on social networks . [ 12 ] [ 13 ] Companies in the social search space include Sproose , Mahalo , Jumper 2.0 , Scour , Wink , Eurekster , and Delver . Former efforts include Wikia Search . In 2008, a story on TechCrunch showed Google potentially adding in a voting mechanism to search results similar to Digg 's methodology. [ 14 ] This suggests growing interest in how social groups can influence and potentially enhance the ability of algorithms to find meaningful data for end users. There are also other services like Sentiment that turn search personal by searching within the users' social circles. In 2009, a startup project called HeyStaks ( www.heystaks.com ) developed a web browser plugin \"HayStaks\". HeyStaks applies social search through collaboration in web search as a way that leads to better search results. [ 15 ] The main motivation for HeyStaks to work on this idea is to provide the user with features that search engines didn't provide at that time. For instance, different searches have indicated that about 70% of the time when user search for something, a friend or a coworker have found it already. Also, studies have shown that approximately, 30% of people who use online search, search for something that they have found before. [ 16 ] The startup believe that they help avoid these kind of issues by providing a shared and rich search experience through a list of recommendations that get generated based on search results. In October 2009, Google rolled out its \"Social Search\"; after a time in beta , the feature was expanded to multiple languages in May 2011. Before the expansion however in 2010 Bing and Google were already taking into account re-tweets and Likes when providing search results. [ 17 ] However, after a search deal with Twitter ended without renewal, Google began to retool its Social Search. In January 2012, Google released \"Search plus Your World\", a further development of Social Search. The feature, which is integrated into Google's regular search as an opt-out feature, pulls references to results from Google+ profiles. The goal was to deliver better, more relevant and personalized search results with this integration. This integration however had some problems in which Google+ still is not wildly adopted or has much usage among many users. [ 18 ] Later on, Google was criticized by Twitter for the perceived potential impact of \"Search plus Your World\" upon web publishers, describing the feature's release to the public as a \"bad day for the web\", while Google replied that Twitter refused to allow deep search crawling by Google of Twitter's content. [ 19 ] By Google integrating Google+ , the company was encouraging users to switch to Google's social networking site in order to improve search results. One famous example occurred when Google showed a link to Mark Zuckerberg 's dormant Google+ account rather than the active Facebook profile. [ 20 ] In November 2014 these accusations started to die down because Google's Knowledge Graph started to finally show links to Facebook, Twitter, and other social media sites. [ 20 ] In December 2008, Twitter had re-introduced their people search feature. [ 21 ] While the interface had since changed significantly, it allows you to search either full names or usernames in a straight-forward search engine. In January 2013, Facebook announced a new search engine called Graph Search still in the beta stages. The goal was to allow users to prioritize results that were popular with their social circle over the general internet. Facebook's Graph search utilized Facebook's user generated content to target users. [ 18 ] Although there have been different researches and studies in social search, social media networks have not vested enough interest in working with search engines . LinkedIn for example has taken steps to improve its own individual search functions in order to stray users from external search engines. Even Microsoft started working with Twitter in order to integrate some tweets into Bing 's search results in November 2013. Yet Twitter has its own search engine which points out how much value their data has and why they would like to keep it in house. [ 22 ] In the end though social search will never be truly comprehensive of the subjects that matter to people unless users opt to be completely public with their information. [ 23 ] Social discovery is the use of social preferences and personal information to predict what content will be desirable to the user. [ 24 ] Technology is used to discover new people and sometimes new experiences shopping, meeting friends or even traveling. [ 25 ] The discovery of new people is often in real-time, enabled by mobile apps . However, social discovery is not limited to meeting people in real-time, it also leads to sales and revenue for companies via social media. [ 26 ] An example of retail would be the addition of social sharing with music, through the iTunes music store. There is a social component to discovering new music [ 27 ] Social discovery is at the basis of Facebook 's profitability, generating ad revenue by targeting the ads to users using the social connections to enhance the commercial appeal. [ 24 ] A social search engine in an aspect can be thought of as a search engine that provides an answer for a question from another answer by identifying a person in the answer. That can happen by retrieving a user submitted query and determining that the query is related to the question; and provides an answer, including the link to the resource, as part of search results that are responsive to the query. [ 28 ] Few social search engines depend only on online communities. Depending on the feature-set of a particular search engine, these results may then be saved and added to community search results, further improving the relevance of results for future searches of that keyword. Social search engines are considered a part of Web 2.0 because they use the collective filtering of online communities to elevate particularly interesting or relevant content using tagging. These descriptive tags add to the meta data embedded in Web pages, theoretically improving the results for particular keywords over time. A user will generally see suggested tags for a particular search term, indicating tags that have previously been added. An implementation of a social search engine is Aardvark . Aardvark is a social search engine that is based on the \"village paradigm\" which is about connecting the user who has a question with friends or friends of friends whom can answer his or her question. [ 29 ] In Aadvark, a user ask a question in different ways that mostly involves online ways such as instant messaging, email, web input or other non-online ways such as text message or voice. The Aardvark algorithm forwards the question to someone in the asker extended social network who has the highest probability in knowing the answer to the question. Aadvark was obtained by Google in 2010 and abandoned later in 2011. Potential drawbacks to social search lie in its open structure, as is the case with other tagged databases. As these are trust-based networks, unintentional or malicious misuse of tags in this context can lead to imprecise search results. There are number of social search engines that mainly based on tracking user information to order to provide related search results. Examples of this types are Smashfuse , SocialMention , Topsy and Social Searcher, [ 30 ] originally linked to Facebook. [ 31 ] Other versions of social engines have been launched, including Google Coop , Eurekster , Sproose , Rollyo , Anoox and Yahoo's MyWeb2.0 . Confirmed to be in testing, a new Facebook app feature called ' Add a Link ' lets users see popular articles they might want to include in their status updates and comments by entering a search query. The results appear to comprise articles that have been well-shared by other Facebook users, with the most recently published given priority over others. The option certainly makes it easier for users to add links without manually searching their News Feed or resorting to a Google query. This new app reduce users' reliance on Google Search . [ 32 ] Twitter announced it is replacing its 'Discover' tab with ' Tailored Trends '. The new Tailored Trends feature, besides showing Twitter trends, will give a short description of each topic. Since trends tend to be abbreviations without context, a description will make it more clear what a trend is about. The new trends experience may also include how many Tweets have been sent and whether a topic is trending up or down. [ 33 ] [ 34 ] Google may be falling behind in terms of social search, but in reality they see the potential and importance of this technology with Web 3.0 and web semantics . The importance of social media lies within how Semantic search works. Semantic search understands much more, including where you are, the time of day, your past history, and many other factors including social connections, and social signals. The first step in order to achieve this will be to teach algorithms to understand the relationship between things. [ 35 ] However this is not possible unless social media sites decide to work with search engines, which is difficult since everyone would like to be the main toll bridge to the internet. As we continue on, and more articles are referred by social media sites, the main concern becomes what good is a search engine without the data of users. One development that seeks to redefine search is the combination of distributed search with social search. The goal is a basic search service whose operation is controlled and maintained by the community itself. This would largely work like Peer to Peer networks in which users provide the data they seems appropriate. Since the data used by search engines belongs to the user they should have absolute control over it. The infrastructure required for a search engine is already available in the form of thousands of idle desktops and extensive residential broadband access. [ 36 ] Despite the advantages of distributed search , it shares several same security concerns as the traditionally centralized case. The security concerns can be classified into three categories: data privacy , data integrity and secure social search. Data privacy protection is defined as the way users can fully control their data and manage its accessibility. The solutions for data privacy include information substitution, attributed based encryption and identity based broadcast encryption. The data integrity is defined as the protection of data from unauthorized or improper modifications and deletions. The solutions for data integrity are digital signature , hash chaining and embedded signing key. The solutions for secure social search are blind signature , zero knowledge proof and resource handler. [ 37 ] [ 38 ] Another issue related to both distributed and centralized search is how to more accurately understand user intent from observed multimedia data. The solutions are based on how to effectively and efficiently leverage social media and search engine. A potential method is to derive a user-image interest graph from social media, and then re-rank image search results by integrating social relevance from the user-image interest graph and visual relevance from general search engines. [ 39 ] [ 40 ] Besides above engineering explorations, a more fundamental and potential method is to develop social search systems based on the understanding of related neural mechanisms. Search problems scale from individuals to societies, however, recent trends across disciplines indicate that the formal properties of these problems share similar structures and, often, similar solutions. Moreover, internal search (e.g., memory search) shows similar characteristics to external search (e.g., spatial foraging), including shared neural mechanisms consistent with a common evolutionary origin across species. For search scenarios, organisms must detect – and climb – noisy, long-range environmental (e.g., temperature, salinity, resource) gradients. Here, social interactions can provide substantial additional benefit by allowing individuals, simply through grouping, to average their imperfect estimates of temporal and spatial cues (the so-called ‘ wisdom-of-crowds ’ effect). Due to the investment necessary to obtain personal information, however, this again sets the scene for producers (searchers) to be exploited by others. [ 41 ]",
    "links": [
      "Data integrity",
      "Google Search",
      "Audio search engine",
      "Doi (identifier)",
      "Social interactions",
      "Wide area information server",
      "Topsy (analytics)",
      "Social information seeking",
      "Metasearch engine",
      "S2CID (identifier)",
      "Document retrieval",
      "Search by sound",
      "Search engine marketing",
      "Social graph",
      "Distributed web crawling",
      "Collaborative information seeking",
      "Focused crawler",
      "Web search",
      "Interest graph",
      "Flickr",
      "Mark Zuckerberg",
      "Distributed search",
      "Federated search",
      "Image search",
      "Enterprise search",
      "Online search",
      "Semantic Web",
      "Sproose",
      "Enterprise bookmarking",
      "Social navigation",
      "Wikia Search",
      "User-generated content",
      "Search/Retrieve via URL",
      "Instagram",
      "Mobile apps",
      "List of search engines",
      "Peer-to-peer",
      "Bibcode (identifier)",
      "Aardvark (search engine)",
      "Jumper 2.0",
      "User intent",
      "Multisearch",
      "Collaborative filtering",
      "Eurekster",
      "Relevance feedback",
      "Representational State Transfer",
      "Blind signature",
      "Rollyo",
      "Selection-based search",
      "Vertical search",
      "Web query classification",
      "Natural language search engine",
      "PMID (identifier)",
      "Twitter",
      "Social computing",
      "Cross-language search",
      "Multimedia search",
      "Social networks",
      "Microsoft",
      "Voice search",
      "Desktop search",
      "Google Coop",
      "Web crawler",
      "Spider trap",
      "ArXiv (identifier)",
      "Website mirroring software",
      "Index (search engine)",
      "Zero-knowledge proof",
      "Mahalo.com",
      "OpenSearch (specification)",
      "Social Graph",
      "Digital signature",
      "Video search engine",
      "Wink Technologies",
      "Delver (Social Search)",
      "Internet search",
      "PageRank",
      "Wisdom of crowds",
      "Robots exclusion standard",
      "Algorithm",
      "Z39.50",
      "Google",
      "Facebook",
      "Data privacy",
      "Search aggregator",
      "Graph Search",
      "TechCrunch",
      "Text mining",
      "Search engine (computing)",
      "Social software",
      "Local search (Internet)",
      "LinkedIn",
      "Search engine",
      "Online community",
      "Wayback Machine",
      "Semantic search",
      "Web indexing",
      "Digg",
      "Collaborative search engine",
      "Bing (search engine)",
      "Image retrieval",
      "ISBN (identifier)",
      "Web search engine",
      "Encryption",
      "Betaware",
      "Google+",
      "PMC (identifier)",
      "Scour Inc.",
      "Search/Retrieve Web Service",
      "Web archiving",
      "Social media",
      "Web query",
      "Cross-language information retrieval",
      "Human search engine",
      "Evaluation measures (information retrieval)",
      "Search engine optimization"
    ]
  },
  "Orthogonality": {
    "url": "https://en.wikipedia.org/wiki/Orthogonality",
    "title": "Orthogonality",
    "content": "Orthogonality is a term with various meanings depending on the context. In mathematics , orthogonality is the generalization of the geometric notion of perpendicularity . Although many authors use the two terms perpendicular and orthogonal interchangeably, the term perpendicular is more specifically used for lines and planes that intersect to form a right angle , whereas orthogonal is used in generalizations, such as orthogonal vectors or orthogonal curves . [ 1 ] [ 2 ] The term is also used in other fields like physics, art, computer science, statistics, and economics. The word comes from the Ancient Greek ὀρθός ( orthós ), meaning \"upright\", [ 3 ] and γωνία ( gōnía ), meaning \"angle\". [ 4 ] The Ancient Greek ὀρθογώνιον ( orthogṓnion ) and Classical Latin orthogonium originally denoted a rectangle . [ 5 ] Later, they came to mean a right triangle . In the 12th century, the post-classical Latin word orthogonalis came to mean a right angle or something related to a right angle. [ 6 ] In mathematics , orthogonality is the generalization of the geometric notion of perpendicularity to linear algebra of bilinear forms . Two elements u and v of a vector space with bilinear form B {\\displaystyle B} are orthogonal when B ( u , v ) = 0 {\\displaystyle B(\\mathbf {u} ,\\mathbf {v} )=0} . Depending on the bilinear form, the vector space may contain null vectors , non-zero self-orthogonal vectors, in which case perpendicularity is replaced with hyperbolic orthogonality . In the case of function spaces , families of functions are used to form an orthogonal basis , such as in the contexts of orthogonal polynomials , orthogonal functions , and combinatorics . In optics , polarization states are said to be orthogonal when they propagate independently of each other, as in vertical and horizontal linear polarization or right-handed and left-handed circular polarization . In special relativity , a time axis determined by a rapidity of motion is hyperbolic-orthogonal to a space axis of simultaneous events, also determined by the rapidity. The theory features relativity of simultaneity . In geometry , given a pair of conjugate hyperbolas , two conjugate diameters are hyperbolically orthogonal . This relationship of diameters was described by Apollonius of Perga and has been modernized using analytic geometry . Hyperbolically orthogonal lines appear in special relativity as temporal and spatial directions that show the relativity of simultaneity . In quantum mechanics , a sufficient (but not necessary) condition that two eigenstates of a Hermitian operator , ψ m {\\displaystyle \\psi _{m}} and ψ n {\\displaystyle \\psi _{n}} , are orthogonal is that they correspond to different eigenvalues. This means, in Dirac notation , that ⟨ ψ m | ψ n ⟩ = 0 {\\displaystyle \\langle \\psi _{m}|\\psi _{n}\\rangle =0} if ψ m {\\displaystyle \\psi _{m}} and ψ n {\\displaystyle \\psi _{n}} correspond to different eigenvalues. This follows from the fact that Schrödinger's equation is a Sturm–Liouville equation (in Schrödinger's formulation) or that observables are given by Hermitian operators (in Heisenberg's formulation). [ citation needed ] In art, the perspective (imaginary) lines pointing to the vanishing point are referred to as \"orthogonal lines\". The term \"orthogonal line\" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as Piet Mondrian and Burgoyne Diller are noted for their exclusive use of \"orthogonal lines\" — not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay of the Thyssen-Bornemisza Museum states that \" Mondrian [...] dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours.\" [ 8 ] Orthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results. [ 9 ] This usage was introduced by Van Wijngaarden in the design of Algol 68 : The number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied “orthogonally” in order to maximize the expressive power of the language while trying to avoid deleterious superfluities. [ 10 ] Orthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the separation of concerns and encapsulation , and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them. An instruction set is said to be orthogonal if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task) [ 11 ] and is designed such that instructions can use any register in any addressing mode . This terminology results from considering an instruction as a vector whose components are the instruction fields. One field identifies the registers to be operated upon and another specifies the addressing mode. An orthogonal instruction set uniquely encodes all combinations of registers and addressing modes. [ 12 ] In telecommunications , multiple access schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different basis functions . One such scheme is time-division multiple access (TDMA), where the orthogonal basis functions are nonoverlapping rectangular pulses (\"time slots\"). Another scheme is orthogonal frequency-division multiplexing (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other. Well known examples include ( a , g , and n ) versions of 802.11 Wi-Fi ; WiMAX ; ITU-T G.hn , DVB-T , the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of ADSL . In OFDM, the subcarrier frequencies are chosen [ how? ] so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required. When performing statistical analysis, independent variables that affect a particular dependent variable are said to be orthogonal if they are uncorrelated, [ 13 ] since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables individually with simple regression or simultaneously with multiple regression . If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions). One econometric formalism that is alternative to the maximum likelihood framework, the Generalized Method of Moments , relies on orthogonality conditions. In particular, the Ordinary Least Squares estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals. In taxonomy , an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive. In chemistry and biochemistry, an orthogonal interaction occurs when there are two pairs of substances and each substance can interact with their respective partner, but does not interact with either substance of the other pair. For example, DNA has two orthogonal pairs: cytosine and guanine form a base-pair, and adenine and thymine form another base-pair, but other base-pair combinations are strongly disfavored. As a chemical example, tetrazine reacts with transcyclooctene and azide reacts with cyclooctyne without any cross-reaction, so these are mutually orthogonal reactions, and so, can be performed simultaneously and selectively. [ 14 ] In organic synthesis , orthogonal protection is a strategy allowing the deprotection of functional groups independently of each other. In supramolecular chemistry the notion of orthogonality refers to the possibility of two or more supramolecular, often non-covalent , interactions being compatible; reversibly forming without interference from the other. In analytical chemistry , analyses are \"orthogonal\" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement. Orthogonal testing thus can be viewed as \"cross-checking\" of results, and the \"cross\" notion corresponds to the etymologic origin of orthogonality . Orthogonal testing is often required as a part of a new drug application . In the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure. In neuroscience , a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map. In philosophy , two topics, authors, or pieces of writing are said to be \"orthogonal\" to each other when they do not substantively cover what could be considered potentially overlapping or competing claims. Thus, texts in philosophy can either support and complement one another, they can offer competing explanations or systems, or they can be orthogonal to each other in cases where the scope, content, and purpose of the pieces of writing are entirely unrelated. [ example needed ] In board games such as chess which feature a grid of squares, 'orthogonal' is used to mean \"in the same row/'rank' or column/'file'\". This is the counterpart to squares which are \"diagonally adjacent\". [ 28 ] In the ancient Chinese board game Go a player can capture the stones of an opponent by occupying all orthogonally adjacent points. In law, orthogonality can refer to interests in a proceeding that are not aligned, but also bear no correlation or effect on each other, so as not to create a conflict of interest. Stereo vinyl records encode both the left and right stereo channels in a single groove. The V-shaped groove in the vinyl has walls that are 90 degrees to each other, with variations in each wall separately encoding one of the two analogue channels that make up the stereo signal. The cartridge senses the motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side. [ 29 ] A pure horizontal motion corresponds to a mono signal, equivalent to a stereo signal in which both channels carry identical (in-phase) signals.",
    "links": [
      "Orthogonal frequency-division multiplexing",
      "A Greek–English Lexicon",
      "Analytical chemistry",
      "Bioorthogonal chemistry",
      "Doi (identifier)",
      "Non-covalent",
      "Oxime",
      "Orthogonality (mathematics)",
      "WiMAX",
      "Schrödinger equation",
      "Dependent and independent variables",
      "S2CID (identifier)",
      "Analytic geometry",
      "Linear algebra",
      "Orthogonal instruction set",
      "Right triangle",
      "Maximum likelihood",
      "Orthogonal polynomials",
      "DNA",
      "Quadricyclane",
      "Oxford University Press",
      "Eigenstates",
      "Orthogonal ligand-protein pair",
      "Processor register",
      "Bilinear form",
      "Worldline",
      "Burgoyne Diller",
      "Nitrone",
      "802.11",
      "Circular polarization",
      "Carolyn R. Bertozzi",
      "DVB-T",
      "Dirac notation",
      "Mathematics",
      "Minkowski space",
      "Special relativity",
      "Merriam-Webster",
      "Linear polarization",
      "Glycan",
      "Orthogonal (book series)",
      "Ancient Greek",
      "New drug application",
      "Speed of light",
      "Bibcode (identifier)",
      "Isocyanide",
      "Expected value",
      "Hermitian operator",
      "Multiple access",
      "Generalized Method of Moments",
      "Apollonius of Perga",
      "Perspective (graphical)",
      "Polarization (waves)",
      "Rectangle",
      "Von Neumann neighborhood",
      "Lipid",
      "Simple linear regression",
      "PMID (identifier)",
      "Philosophy",
      "Multiple regression",
      "Telecommunications",
      "Conjugate hyperbola",
      "Go (game)",
      "Living systems",
      "Vanishing point",
      "Chess",
      "Thyssen-Bornemisza Museum",
      "Geometry",
      "Protein",
      "Basis (linear algebra)",
      "1,3-dipolar cycloaddition",
      "Separation of concerns",
      "Time-division multiple access",
      "Functional group",
      "Hyperbolic rotation",
      "Tetrazine",
      "Correlation",
      "Orthogonality (programming)",
      "Rapidity",
      "Chemical reaction",
      "Sturm–Liouville theory",
      "Piet Mondrian",
      "Organic synthesis",
      "Quantum mechanics",
      "Minkowski spacetime",
      "Aldehyde",
      "Wi-Fi",
      "Adriaan van Wijngaarden",
      "Function space",
      "Chemical ligation",
      "Hydrazone",
      "ITU-T",
      "Ordinary Least Squares",
      "Addressing mode",
      "Vector space",
      "Orthogonal vectors",
      "Hyperbolic angle",
      "Combinatorics",
      "Optics",
      "Perpendicularity",
      "Classical Latin",
      "Azide",
      "G.hn",
      "Relativity of simultaneity",
      "Orthogonal polyhedron",
      "Hyperbolic orthogonality",
      "Up tack",
      "Null vector",
      "Orthogonal functions",
      "Euclidean space",
      "Ketone",
      "Econometrics",
      "Right angle",
      "Angle",
      "Supramolecular chemistry",
      "ISSN (identifier)",
      "Algol 68",
      "Basis function",
      "ISBN (identifier)",
      "Cyclooctyne",
      "Orthogonal protection",
      "Instruction set",
      "Taxonomy (general)",
      "Conjugate diameters",
      "PMC (identifier)",
      "Subcarrier",
      "Neuroscience",
      "Copper-free click chemistry",
      "Oxford English Dictionary",
      "ADSL"
    ]
  },
  "Semantic Web": {
    "url": "https://en.wikipedia.org/wiki/Semantic_Web",
    "title": "Semantic Web",
    "content": "The Semantic Web , sometimes known as Web 3.0 , is an extension of the World Wide Web through standards [ 1 ] set by the World Wide Web Consortium (W3C). The goal of the Semantic Web is to make Internet data machine-readable . To enable the encoding of semantics with the data, technologies such as Resource Description Framework (RDF) [ 2 ] and Web Ontology Language (OWL) [ 3 ] are used. These technologies are used to formally represent metadata . For example, ontology can describe concepts , relationships between entities , and categories of things. These embedded semantics offer significant advantages such as reasoning over data and operating with heterogeneous data sources. [ 4 ] These standards promote common data formats and exchange protocols on the Web, fundamentally the RDF. According to the W3C, \"The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries.\" [ 5 ] The Semantic Web is therefore regarded as an integrator across different content and information applications and systems. The term was coined by Tim Berners-Lee for a web of data (or data web ) [ 6 ] that can be processed by machines [ 7 ] —that is, one in which much of the meaning is machine-readable . While its critics have questioned its feasibility, proponents argue that applications in library and information science , industry, biology and human sciences research have already proven the validity of the original concept. [ 8 ] Berners-Lee originally expressed his vision of the Semantic Web in 1999 as follows: I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers. A \"Semantic Web\", which makes this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines. The \" intelligent agents \" people have touted for ages will finally materialize. [ 9 ] The 2001 Scientific American article by Berners-Lee, Hendler , and Lassila described an expected evolution of the existing Web to a Semantic Web. [ 10 ] In 2006, Berners-Lee and colleagues stated that: \"This simple idea…remains largely unrealized\". [ 11 ] In 2013, more than four million Web domains (out of roughly 250 million total) contained Semantic Web markup. [ 12 ] In the following example, the text \"Paul Schuster was born in Dresden\" on a website will be annotated, connecting a person with their place of birth. The following HTML fragment shows how a small graph is being described, in RDFa -syntax using a schema.org vocabulary and a Wikidata ID: The example defines the following five triples (shown in Turtle syntax). Each triple represents one edge in the resulting graph: the first element of the triple (the subject ) is the name of the node where the edge starts, the second element (the predicate ) the type of the edge, and the last and third element (the object ) either the name of the node where the edge ends or a literal value (e.g. a text, a number, etc.). The triples result in the graph shown in the given figure . One of the advantages of using Uniform Resource Identifiers (URIs) is that they can be dereferenced using the HTTP protocol. According to the so-called Linked Open Data principles, such a dereferenced URI should result in a document that offers further data about the given URI. In this example, all URIs, both for edges and nodes (e.g. http://schema.org/Person , http://schema.org/birthPlace , http://www.wikidata.org/entity/Q1731 ) can be dereferenced and will result in further RDF graphs, describing the URI, e.g. that Dresden is a city in Germany, or that a person, in the sense of that URI, can be fictional. The second graph shows the previous example, but now enriched with a few of the triples from the documents that result from dereferencing https://schema.org/Person (green edge) and https://www.wikidata.org/entity/Q1731 (blue edges). Additionally to the edges given in the involved documents explicitly, edges can be automatically inferred: the triple from the original RDFa fragment and the triple from the document at https://schema.org/Person (green edge in the figure) allow to infer the following triple, given OWL semantics (red dashed line in the second Figure): The concept of the semantic network model was formed in the early 1960s by researchers such as the cognitive scientist Allan M. Collins , linguist Ross Quillian and psychologist Elizabeth F. Loftus as a form to represent semantically structured knowledge. When applied in the context of the modern internet, it extends the network of hyperlinked human-readable web pages by inserting machine-readable metadata about pages and how they are related to each other. This enables automated agents to access the Web more intelligently and perform more tasks on behalf of users. The term \"Semantic Web\" was coined by Tim Berners-Lee , [ 7 ] the inventor of the World Wide Web and director of the World Wide Web Consortium (\" W3C \"), which oversees the development of proposed Semantic Web standards. He defines the Semantic Web as \"a web of data that can be processed directly and indirectly by machines\". Many of the technologies proposed by the W3C already existed before they were positioned under the W3C umbrella. These are used in various contexts, particularly those dealing with information that encompasses a limited and defined domain, and where sharing data is a common necessity, such as scientific research or data exchange among businesses. In addition, other technologies with similar goals have emerged, such as microformats . Many files on a typical computer can be loosely divided into either human-readable documents, or machine-readable data. Examples of human-readable document files are mail messages, reports, and brochures. Examples of machine-readable data files are calendars, address books, playlists, and spreadsheets, which are presented to a user using an application program that lets the files be viewed, searched, and combined. Currently, the World Wide Web is based mainly on documents written in Hypertext Markup Language (HTML), a markup convention that is used for coding a body of text interspersed with multimedia objects such as images and interactive forms. Metadata tags provide a method by which computers can categorize the content of web pages. In the examples below, the field names \"keywords\", \"description\" and \"author\" are assigned values such as \"computing\", and \"cheap widgets for sale\" and \"John Doe\". Because of this metadata tagging and categorization, other computer systems that want to access and share this data can easily identify the relevant values. With HTML and a tool to render it (perhaps web browser software, perhaps another user agent ), one can create and present a page that lists items for sale. The HTML of this catalog page can make simple, document-level assertions such as \"this document's title is 'Widget Superstore ' \", but there is no capability within the HTML itself to assert unambiguously that, for example, item number X586172 is an Acme Gizmo with a retail price of €199, or that it is a consumer product. Rather, HTML can only say that the span of text \"X586172\" is something that should be positioned near \"Acme Gizmo\" and \"€199\", etc. There is no way to say \"this is a catalog\" or even to establish that \"Acme Gizmo\" is a kind of title or that \"€199\" is a price. There is also no way to express that these pieces of information are bound together in describing a discrete item, distinct from other items perhaps listed on the page. Semantic HTML refers to the traditional HTML practice of markup following intention, rather than specifying layout details directly. For example, the use of <em> denoting \"emphasis\" rather than <i> , which specifies italics . Layout details are left up to the browser, in combination with Cascading Style Sheets . But this practice falls short of specifying the semantics of objects such as items for sale or prices. Microformats extend HTML syntax to create machine-readable semantic markup about objects including people, organizations, events and products. [ 13 ] Similar initiatives include RDFa , Microdata and Schema.org . The Semantic Web takes the solution further. It involves publishing in languages specifically designed for data: Resource Description Framework (RDF), Web Ontology Language (OWL), and Extensible Markup Language ( XML ). HTML describes documents and the links between them. RDF, OWL, and XML, by contrast, can describe arbitrary things such as people, meetings, or airplane parts. These technologies are combined in order to provide descriptions that supplement or replace the content of Web documents. Thus, content may manifest itself as descriptive data stored in Web-accessible databases , [ 14 ] or as markup within documents (particularly, in Extensible HTML ( XHTML ) interspersed with XML, or, more often, purely in XML, with layout or rendering cues stored separately). The machine-readable descriptions enable content managers to add meaning to the content, i.e., to describe the structure of the knowledge we have about that content. In this way, a machine can process knowledge itself, instead of text, using processes similar to human deductive reasoning and inference , thereby obtaining more meaningful results and helping computers to perform automated information gathering and research. An example of a tag that would be used in a non-semantic web page: Encoding similar information in a semantic web page might look like this: Tim Berners-Lee calls the resulting network of Linked Data the Giant Global Graph , in contrast to the HTML-based World Wide Web. Berners-Lee posits that if the past was document sharing, the future is data sharing . His answer to the question of \"how\" provides three points of instruction. One, a URL should point to the data. Two, anyone accessing the URL should get data back. Three, relationships in the data should point to additional URLs with data. Tags , including hierarchical categories and tags that are collaboratively added and maintained (e.g. with folksonomies ) can be considered part of, of potential use to or a step towards the semantic Web vision. [ 15 ] [ 16 ] [ 17 ] Unique identifiers , including hierarchical categories and collaboratively added ones, analysis tools and metadata , including tags, can be used to create forms of semantic webs – webs that are to a certain degree semantic. [ 18 ] In particular, such has been used for structuring scientific research i.a. by research topics and scientific fields by the projects OpenAlex , [ 19 ] [ 20 ] [ 21 ] Wikidata and Scholia which are under development and provide APIs , Web-pages, feeds and graphs for various semantic queries . Tim Berners-Lee has described the Semantic Web as a component of Web 3.0. [ 22 ] People keep asking what Web 3.0 is. I think maybe when you've got an overlay of scalable vector graphics – everything rippling and folding and looking misty – on Web 2.0 and access to a semantic Web integrated across a huge space of data, you'll have access to an unbelievable data resource … — Tim Berners-Lee, 2006 \"Semantic Web\" is sometimes used as a synonym for \"Web 3.0\", [ 23 ] though the definition of each term varies. The next generation of the Web is often termed Web 4.0, but its definition is not clear. According to some sources, it is a Web that involves artificial intelligence , [ 24 ] the internet of things , pervasive computing , ubiquitous computing and the Web of Things among other concepts. [ 25 ] According to the European Union, Web 4.0 is \"the expected fourth generation of the World Wide Web. Using advanced artificial and ambient intelligence, the internet of things, trusted blockchain transactions, virtual worlds and XR capabilities, digital and real objects and environments are fully integrated and communicate with each other, enabling truly intuitive, immersive experiences, seamlessly blending the physical and digital worlds\". [ 26 ] Some of the challenges for the Semantic Web include vastness, vagueness, uncertainty, inconsistency, and deceit. Automated reasoning systems will have to deal with all of these issues in order to deliver on the promise of the Semantic Web. This list of challenges is illustrative rather than exhaustive, and it focuses on the challenges to the \"unifying logic\" and \"proof\" layers of the Semantic Web. The World Wide Web Consortium (W3C) Incubator Group for Uncertainty Reasoning for the World Wide Web [ 27 ] (URW3-XG) final report lumps these problems together under the single heading of \"uncertainty\". [ 28 ] Many of the techniques mentioned here will require extensions to the Web Ontology Language (OWL) for example to annotate conditional probabilities. This is an area of active research. [ 29 ] Standardization for Semantic Web in the context of Web 3.0 is under the care of W3C. [ 30 ] The term \"Semantic Web\" is often used more specifically to refer to the formats and technologies that enable it. [ 5 ] The collection, structuring and recovery of linked data are enabled by technologies that provide a formal description of concepts, terms, and relationships within a given knowledge domain . These technologies are specified as W3C standards and include: The Semantic Web Stack illustrates the architecture of the Semantic Web. The functions and relationships of the components can be summarized as follows: [ 31 ] Well-established standards: Not yet fully realized: The intent is to enhance the usability and usefulness of the Web and its interconnected resources by creating semantic web services , such as: Such services could be useful to public search engines, or could be used for knowledge management within an organization. Business applications include: In a corporation, there is a closed group of users and the management is able to enforce company guidelines like the adoption of specific ontologies and use of semantic annotation . Compared to the public Semantic Web there are lesser requirements on scalability and the information circulating within a company can be more trusted in general; privacy is less of an issue outside of handling of customer data. Critics question the basic feasibility of a complete or even partial fulfillment of the Semantic Web, pointing out both difficulties in setting it up and a lack of general-purpose usefulness that prevents the required effort from being invested. In a 2003 paper, Marshall and Shipman point out the cognitive overhead inherent in formalizing knowledge, compared to the authoring of traditional web hypertext : [ 46 ] While learning the basics of HTML is relatively straightforward, learning a knowledge representation language or tool requires the author to learn about the representation's methods of abstraction and their effect on reasoning. For example, understanding the class-instance relationship, or the superclass-subclass relationship, is more than understanding that one concept is a \"type of\" another concept. [...] These abstractions are taught to computer scientists generally and knowledge engineers specifically but do not match the similar natural language meaning of being a \"type of\" something. Effective use of such a formal representation requires the author to become a skilled knowledge engineer in addition to any other skills required by the domain. [...] Once one has learned a formal representation language, it is still often much more effort to express ideas in that representation than in a less formal representation [...]. Indeed, this is a form of programming based on the declaration of semantic data and requires an understanding of how reasoning algorithms will interpret the authored structures. According to Marshall and Shipman, the tacit and changing nature of much knowledge adds to the knowledge engineering problem, and limits the Semantic Web's applicability to specific domains. A further issue that they point out are domain- or organization-specific ways to express knowledge, which must be solved through community agreement rather than only technical means. [ 46 ] As it turns out, specialized communities and organizations for intra-company projects have tended to adopt semantic web technologies greater than peripheral and less-specialized communities. [ 47 ] The practical constraints toward adoption have appeared less challenging where domain and scope is more limited than that of the general public and the World-Wide Web. [ 47 ] Finally, Marshall and Shipman see pragmatic problems in the idea of ( Knowledge Navigator -style) intelligent agents working in the largely manually curated Semantic Web: [ 46 ] In situations in which user needs are known and distributed information resources are well described, this approach can be highly effective; in situations that are not foreseen and that bring together an unanticipated array of information resources, the Google approach is more robust. Furthermore, the Semantic Web relies on inference chains that are more brittle; a missing element of the chain results in a failure to perform the desired action, while the human can supply missing pieces in a more Google-like approach. [...] cost-benefit tradeoffs can work in favor of specially-created Semantic Web metadata directed at weaving together sensible well-structured domain-specific information resources; close attention to user/customer needs will drive these federations if they are to be successful. Cory Doctorow 's critique (\" metacrap \") [ 48 ] is from the perspective of human behavior and personal preferences. For example, people may include spurious metadata into Web pages in an attempt to mislead Semantic Web engines that naively assume the metadata's veracity. This phenomenon was well known with metatags that fooled the Altavista ranking algorithm into elevating the ranking of certain Web pages: the Google indexing engine specifically looks for such attempts at manipulation. Peter Gärdenfors and Timo Honkela point out that logic-based semantic web technologies cover only a fraction of the relevant phenomena related to semantics. [ 49 ] [ 50 ] Enthusiasm about the semantic web could be tempered by concerns regarding censorship and privacy . For instance, text-analyzing techniques can now be easily bypassed by using other words, metaphors for instance, or by using images in place of words. An advanced implementation of the semantic web would make it much easier for governments to control the viewing and creation of online information, as this information would be much easier for an automated content-blocking machine to understand. In addition, the issue has also been raised that, with the use of FOAF files and geolocation meta-data , there would be very little anonymity associated with the authorship of articles on things such as a personal blog. Some of these concerns were addressed in the \"Policy Aware Web\" project [ 51 ] and is an active research and development topic. Another criticism of the semantic web is that it would be much more time-consuming to create and publish content because there would need to be two formats for one piece of data: one for human viewing and one for machines. However, many web applications in development are addressing this issue by creating a machine-readable format upon the publishing of data or the request of a machine for such data. The development of microformats has been one reaction to this kind of criticism. Another argument in defense of the feasibility of semantic web is the likely falling price of human intelligence tasks in digital labor markets, such as Amazon 's Mechanical Turk . [ citation needed ] Specifications such as eRDF and RDFa allow arbitrary RDF data to be embedded in HTML pages. The GRDDL (Gleaning Resource Descriptions from Dialects of Language) mechanism allows existing material (including microformats) to be automatically interpreted as RDF, so publishers only need to use a single format, such as HTML. The first research group explicitly focusing on the Corporate Semantic Web was the ACACIA team at INRIA-Sophia-Antipolis , founded in 2002. Results of their work include the RDF(S) based Corese [ 52 ] search engine, and the application of semantic web technology in the realm of distributed artificial intelligence for knowledge management (e.g. ontologies and multi-agent systems for corporate semantic Web) [ 53 ] and E-learning . [ 54 ] Since 2008, the Corporate Semantic Web research group, located at the Free University of Berlin , focuses on building blocks: Corporate Semantic Search, Corporate Semantic Collaboration, and Corporate Ontology Engineering. [ 55 ] Ontology engineering research includes the question of how to involve non-expert users in creating ontologies and semantically annotated content [ 56 ] and for extracting explicit knowledge from the interaction of users within enterprises. Tim O'Reilly , who coined the term Web 2.0, proposed a long-term vision of the Semantic Web as a web of data, where sophisticated applications are navigating and manipulating it. [ 57 ] The data web transforms the World Wide Web from a distributed file system into a distributed database . [ 58 ]",
    "links": [
      "Artificial intelligence",
      "Frank van Harmelen",
      "Knowledge domain",
      "Web engineering",
      "Differential technological development",
      "Facebook Platform",
      "Entity–relationship model",
      "Calais (Reuters product)",
      "Phase-change memory",
      "Business semantics management",
      "Technology in science fiction",
      "Microformat",
      "Unicode",
      "Semantic service-oriented architecture",
      "Jussi Karlgren",
      "Semantic analysis (machine learning)",
      "HRecipe",
      "EU Open Data Portal",
      "Future-oriented technology analysis",
      "Ferroelectric RAM",
      "Linked Data",
      "Amazon Mechanical Turk",
      "Semantic social network",
      "Distributed computing",
      "Semantic matching",
      "Computational philosophy",
      "Millipede memory",
      "Knowledge representation and reasoning",
      "Bibcode (identifier)",
      "Data sharing",
      "Giant Global Graph",
      "Scalable vector graphics",
      "Hypertext",
      "Digital humanities",
      "Database",
      "Ontology learning",
      "Concurrency semantics",
      "Linked Open Data",
      "Multi-agent systems",
      "SNOMED CT",
      "Augmented reality",
      "Geotagging",
      "Identifier",
      "Computational archaeology",
      "History of the World Wide Web",
      "Elizabeth F. Loftus",
      "Carbon nanotube field-effect transistor",
      "Applications of artificial intelligence",
      "Technological singularity",
      "Web of Things",
      "James Hendler",
      "Semantic parsing",
      "Information overload",
      "Description logic",
      "Lexis (linguistics)",
      "Proactionary principle",
      "Semantic analysis (computational)",
      "RDF/XML",
      "Topic map",
      "Technological paradigm",
      "BIBFRAME",
      "Spamming",
      "SAWSDL",
      "Racetrack memory",
      "Research Resource Identifier",
      "Automation",
      "Metadata Authority Description Schema",
      "Schema.org",
      "XHTML",
      "Semantic similarity",
      "Ethics of technology",
      "HTML",
      "Tim Berners-Lee",
      "Digital classics",
      "Metadata",
      "Knowledge management",
      "Ephemeralization",
      "Knowledge extraction",
      "Computational semantics",
      "Collingridge dilemma",
      "ISSN (identifier)",
      "Altavista",
      "ISBN (identifier)",
      "Reference (computer science)",
      "The Guardian",
      "Web Science Trust",
      "Library science",
      "Tag cloud",
      "Solid (web decentralization project)",
      "PMC (identifier)",
      "Common Logic",
      "Digital ontology",
      "Hyperdata",
      "LAP Lambert Academic Publishing",
      "Nextbio",
      "Compositionality",
      "Formal semantics (logic)",
      "Accelerating change",
      "Electronic literature",
      "Internationalized Resource Identifier",
      "Intelligent text analysis",
      "Human science",
      "S2CID (identifier)",
      "Latent semantic analysis",
      "Bibliographic Ontology",
      "Software-defined radio",
      "SHACL",
      "Semantics",
      "Web pages",
      "Action semantics",
      "Digitization",
      "OpenAlex",
      "Cryptography",
      "New media",
      "Semantic analysis (linguistics)",
      "Knowledge engineering",
      "Force dynamics",
      "Electrochemical RAM",
      "Internet of things",
      "Transhumanism",
      "Reasoning engine",
      "Version control software",
      "Relational database",
      "Semantic heterogeneity",
      "Semantic integration",
      "Formal semantics (linguistics)",
      "Three-dimensional integrated circuit",
      "Linked data",
      "Systems theory",
      "Semantic web service",
      "Simple Knowledge Organization System",
      "Web Ontology Language",
      "Inference",
      "Dataspaces",
      "ActivityPub",
      "E-learning",
      "AGRIS",
      "Language",
      "Text Encoding Initiative",
      "Semantic Web Stack",
      "Information architecture",
      "Statistical semantics",
      "Resource Description Framework",
      "Theory of descriptions",
      "Technology forecasting",
      "HTTP",
      "Structural semantics",
      "Defeasible reasoning",
      "HCard",
      "RDFS",
      "Technology readiness level",
      "HarperSanFrancisco",
      "Neuroethics",
      "Cory Doctorow",
      "Amazon.com",
      "HReview",
      "N-Triples",
      "Uniform Resource Identifier",
      "Prototype theory",
      "Progress in artificial intelligence",
      "Linguistics",
      "Optical computing",
      "Computers and writing",
      "Semantic gap",
      "Semantic search",
      "Machine-readable",
      "Intelligent agent",
      "Deductive reasoning",
      "Social Semantic Web",
      "Entity–attribute–value model",
      "Scalability",
      "Cascading Style Sheets",
      "Technological change",
      "Semantic desktop",
      "DBpedia",
      "Context",
      "Game semantics",
      "Technological convergence",
      "Allan M. Collins",
      "W3C",
      "FOAF (software)",
      "Bioethics",
      "Web resource",
      "Knowledge Navigator",
      "JSTOR (identifier)",
      "User agent",
      "Concept",
      "Information retrieval",
      "Horizon scanning",
      "Machine translation",
      "Pervasive computing",
      "Semantics (computer science)",
      "COinS",
      "Privacy",
      "Semantic query",
      "Robot ethics",
      "World Wide Web Consortium",
      "Semantic reasoner",
      "UltraRAM",
      "Fuzzy logic",
      "Semantic Geospatial Web",
      "Semantic computing",
      "Paraconsistent logic",
      "Rule-based system",
      "Programmable metallization cell",
      "Italics",
      "International Semantic Web Conference",
      "Institut national de recherche en informatique et en automatique",
      "Apress",
      "Semantic analytics",
      "Dublin Core",
      "Lexicology",
      "PMID (identifier)",
      "Wikidata",
      "Peter Gärdenfors",
      "Extensibility",
      "Tacit knowledge",
      "E-research",
      "Web 2.0",
      "Ethics of artificial intelligence",
      "SONOS",
      "Knowledge representation language",
      "Digital history",
      "W3C XML Schema",
      "World Wide Web",
      "Branches of science",
      "Technological unemployment",
      "Digital Medievalist",
      "Lexical semantics",
      "Semantic technology",
      "List of emerging technologies",
      "Predicate transformer semantics",
      "Semantic feature",
      "RDFa",
      "Emerging technologies",
      "Algebraic semantics (computer science)",
      "Internet",
      "Cybertext",
      "Tim O'Reilly",
      "JSON-LD",
      "Semantic network",
      "Knowledge base",
      "DOAP",
      "Semantic triple",
      "Speech recognition",
      "TriX (serialization format)",
      "Cyberethics",
      "Cognitive science",
      "FOAF",
      "Semantic wiki",
      "Medical terminology",
      "Information and communications technology",
      "Psychologist",
      "Digital rhetoric",
      "Categorical logic",
      "Ubiquitous computing",
      "Free University of Berlin",
      "Technology roadmap",
      "Distributed artificial intelligence",
      "MIT Press",
      "Information science",
      "Semantic Sensor Web",
      "Web science",
      "Semantic Web (journal)",
      "Chipless RFID",
      "Doi (identifier)",
      "HCalendar",
      "Transliteracy",
      "International Herald Tribune",
      "TriG (syntax)",
      "Semantic HTML",
      "Cultural analytics",
      "Technological evolution",
      "Radio-frequency identification",
      "Semantic Web Rule Language",
      "Digital physics",
      "Digital religion",
      "File system",
      "HTML element",
      "GRDDL",
      "Digital scholarship",
      "SPARQL",
      "Data model",
      "Semantically Interlinked Online Communities",
      "Principle of explosion",
      "Machine-readable data",
      "Mastodon (software)",
      "Aaron Swartz",
      "HAtom",
      "Moore's law",
      "Automated reasoning system",
      "HProduct",
      "Argüman",
      "Scientific American",
      "Resistive random-access memory",
      "Usability",
      "RDF Schema",
      "Holographic data storage",
      "Metacrap",
      "Biology",
      "Web browser",
      "Resource (computer science)",
      "IXBRL",
      "Smart-M3",
      "Exploratory engineering",
      "Class (programming)",
      "Meaning (linguistics)",
      "Web crawler",
      "Argument map",
      "Semantically-Interlinked Online Communities",
      "3D optical data storage",
      "Notation3",
      "Pascal Hitzler",
      "Collective intelligence",
      "Philosophy of computer science",
      "Ora Lassila",
      "Meta-data",
      "Machine vision",
      "Kialo",
      "Metadata Object Description Schema",
      "General-purpose computing on graphics processing units",
      "Semantic file system",
      "Ontology alignment",
      "Humanistic informatics",
      "Trust service",
      "Semantic mapper",
      "Semantic publishing",
      "Microdata (HTML)",
      "Probabilistic logic",
      "Disruptive innovation",
      "Hyperlink",
      "Web3",
      "XML",
      "Ambient intelligence",
      "Tag (metadata)",
      "Operational semantics",
      "Abstract semantic graph",
      "Library 2.0",
      "Mobile translation",
      "Cybermethodology",
      "Atomtronics",
      "Magnetoresistive RAM",
      "Axiomatic semantics",
      "Denotational semantics",
      "Hypertext Markup Language",
      "IEEE Intelligent Systems",
      "Computational theory of mind",
      "Nature (journal)",
      "Rule Interchange Format",
      "Folksonomy",
      "Digital library",
      "Semantic annotation",
      "Web search engine",
      "API",
      "Abstract interpretation",
      "Technology scouting",
      "Nano-RAM",
      "Distributed database",
      "Internet censorship",
      "Semantic translation",
      "Ontology (information science)",
      "Scholia",
      "Turtle (syntax)",
      "Digital theology",
      "Timo Honkela",
      "Semantic MediaWiki",
      "Credibility"
    ]
  },
  "Joseph Marie Jacquard": {
    "url": "https://en.wikipedia.org/wiki/Joseph_Marie_Jacquard",
    "title": "Joseph Marie Jacquard",
    "content": "Joseph Marie Charles dit (called or nicknamed) Jacquard ( / ˈ dʒ æ k ɑːr d , dʒ ə ˈ k ɑːr d / ; [ 1 ] French: [ʒakaʁ] ; 7 July 1752 – 7 August 1834) was a French weaver and merchant. He played an important role in the development of the earliest programmable loom (the \" Jacquard loom \"), which in turn played an important role in the development of other programmable machines, such as an early version of digital compiler used by IBM to develop the modern day computer . In his grandfather's generation, several branches of the Charles family lived in Lyon's Couzon-Au-Mont d’Or suburb (on Lyon's north side, along the Saône River). To distinguish the various branches, the community gave them nicknames; Joseph's branch was called \"Jacquard\" Charles. Thus, Joseph's grandfather was Bartholomew Charles dit [called] Jacquard. [ 2 ] [ 3 ] Joseph Marie Charles dit Jacquard was born into a conservative Catholic family in Lyon , France, on 7 July 1752. He was one of nine children of Jean Charles dit Jacquard, a master weaver of Lyon, and his wife, Antoinette Rive. However, only Joseph and his sister Clémence (born 7 November 1747) survived to adulthood. Although his father was a man of property, Joseph received no formal schooling and remained illiterate until he was 13. He was finally taught by his brother-in-law, Jean-Marie Barret, who ran a printing and book selling business. Barret also introduced Joseph to learned societies and scholars. [ 4 ] Joseph initially helped his father operate his loom, but the work proved too arduous, so Jacquard was placed first with a bookbinder and then with a maker of printers' type. [ 5 ] His mother died in 1762, and when his father died in 1772, Joseph inherited his father's house, looms and workshop as well as a vineyard and quarry in Couzon-au-Mont d’Or. Joseph then dabbled in real estate. In 1778, he listed his occupations as master weaver and silk merchant. [ 4 ] Jacquard's occupation at this time is problematic because by 1780 most silk weavers did not work independently; instead, they worked for wages from silk merchants, and Jacquard was not registered as a silk merchant in Lyon. [ 6 ] There is some confusion about Jacquard's early work history. British economist Sir John Bowring met Jacquard, who told Bowring that at one time he had been a maker of straw hats. [ 7 ] Eymard claimed that before becoming involved in the weaving of silk, Jacquard was a type-founder (a maker of printers' type), a soldier, a bleacher ( blanchisseur ) of straw hats, and a lime burner (a maker of lime for mortar). [ 8 ] Barlow claims that before marrying, Jacquard had worked for a bookbinder, a type-founder, and a maker of cutlery. After marrying, Jacquard tried cutlery making, type-founding, and weaving. [ 9 ] However, Barlow does not cite any sources for that information. On 26 July 1778, Joseph married Claudine Boichon. She was a middle-class widow from Lyon who owned property and had a substantial dowry. However, Joseph soon fell deeply into debt and was brought to court. Barlow claims that after Jacquard's father died, Jacquard started a figure-weaving business but failed and lost all his wealth. However, Barlow cites no sources to support his claim. [ 9 ] To settle his debts, he was obliged to sell his inheritance and to appropriate his wife's dowry. His wife retained a house in Oullins (on Lyon's south side, along the Rhone River), where the couple resided. On 19 April 1779, the couple had their only child, a son, Jean Marie. [ 4 ] Charles Ballot stated that after the rebellion of Lyon in 1793 was suppressed, Joseph and his son escaped from the city by joining the revolutionary army. They fought together in the Rhine campaign of 1795 , serving in the Rhone-and-Loire battalion under General Jean Charles Pichegru . Joseph's son was killed outside of Heidelberg . However, Ballot repeated rumors and was a sloppy historian. For example, he stated that Jacquard's wife Claudette Boichon was the daughter of Antoine-Hélon Boichon, a master swordsmith, whereas Claudette was a widow who had been married to a Mr. Boichon before she married Jacquard. [ 10 ] By 1800, Joseph began inventing various devices. He invented a treadle loom in 1800, a loom to weave fishing nets in 1803, and starting in 1804, the \"Jacquard\" loom, which would weave patterned silk automatically. However, these early inventions did not operate well and thus were unsuccessful. [ 6 ] In 1801, Jacquard exhibited his invention at the Exposition des produits de l'industrie française in Paris, where he was awarded a bronze medal. [ 11 ] In 1803 he was summoned to Paris and attached to the Conservatoire des Arts et Metiers . A loom by Jacques de Vaucanson on display there suggested various improvements in his own, which he gradually perfected to its final state. The loom was declared public property in 1805, and Jacquard was rewarded with a pension and a royalty on each machine. Although his invention was fiercely opposed by the silk-weavers, who feared that its introduction, owing to the saving of labour, would deprive them of their livelihood, its advantages secured its general adoption, and by 1812 there were 11,000 Jacquard looms in use in France. This claim has been challenged: Initially few Jacquard looms were sold because of problems with the punched card mechanism. Only after 1815 — once Jean Antoine Breton had solved the problems with the punched card mechanism — did sales of looms increase. [ 12 ] [ 13 ] [ 14 ] Jacquard died at Oullins (Rhône), 7 August 1834. [ 15 ] Six years later, a statue was erected to him in Lyon, on the site where his 1801 exhibit loom was destroyed. The Jacquard Loom is a mechanical loom that uses pasteboard cards with punched holes, each card corresponding to one row of the design. Multiple rows of holes are punched in the cards and the many cards that compose the design of the textile are strung together in order. It is based on earlier inventions by the Frenchmen Basile Bouchon (1725), Jean-Baptiste Falcon (1728) and Jacques Vaucanson (1740). [ 16 ] To understand the Jacquard loom, some basic knowledge of weaving is necessary. Parallel threads (the \"warp\") are stretched across a rectangular frame (the \"loom\"). For plain cloth, every other warp thread is raised. Another thread (the \"weft thread\") is then passed (at a right angle to the warp) through the space (the \"shed\") between the lower and the upper warp threads. Then the raised warp threads are lowered, the alternate warp threads are raised, and the weft thread is passed through the shed in the opposite direction. With hundreds of such cycles, the cloth is gradually created. By raising different (not just alternate) warp threads and using colored threads in the weft, the texture, color, design, and pattern can be varied to create varied and highly desirable fabrics. Weaving elaborate patterns or designs manually is a slow, complicated procedure subject to error. Jacquard's loom was intended to automate this process. Jacquard was not the first to try to automate the process of weaving. In 1725 Basile Bouchon invented an attachment for draw looms that used a broad strip of punched paper to select the warp threads that would be raised during weaving. [ 20 ] Specifically, Bouchon's innovation involved a row of hooks. The curved portion of each hook snagged a string that could raise one of the warp threads, whereas the straight portion of each hook pressed against the punched paper, which was draped around a perforated cylinder. Whenever the hook pressed against the solid paper, pushing the cylinder forward would raise the corresponding warp thread; whereas whenever the hook met a hole in the paper, pushing the cylinder forward would allow the hook to slip inside the cylinder and the corresponding warp thread would not be raised. Bouchon's loom was unsuccessful because it could handle only a modest number of warp threads. [ 21 ] [ 22 ] By 1737, a master silk weaver of Lyon , Jean Falcon, had increased the number of warp threads that the loom could handle automatically. He developed an attachment for looms in which Bouchon's paper strip was replaced by a chain of punched cards, which could deflect multiple rows of hooks simultaneously. Like Bouchon, Falcon used a \"cylinder\" (actually, a four-sided perforated tube) to hold each card in place while it was pressed against the rows of hooks. [ 23 ] His loom was modestly successful; about 40 such looms had been sold by 1762. [ 24 ] In 1741, Jacques de Vaucanson , a French inventor who designed and built automated mechanical toys, was appointed inspector of silk factories. [ 25 ] Between 1747 and 1750, [ 26 ] he tried to automate Bouchon's mechanism. In Vaucanson's mechanism, the hooks that were to lift the warp threads were selected by long pins or \"needles\", which were pressed against a sheet of punched paper that was draped around a perforated cylinder. Specifically, each hook passed at a right angle through an eyelet of a needle. When the cylinder was pressed against the array of needles, some of the needles, pressing against solid paper, would move forward, which in turn would tilt the corresponding hooks. The hooks that were tilted would not be raised, so the warp threads that were snagged by those hooks would remain in place; however, the hooks that were not tilted, would be raised, and the warp threads that were snagged by those hooks would also be raised. By placing his mechanism above the loom, Vaucanson eliminated the complicated system of weights and cords (tail cords, simple, pulley box, etc.) that had been used to select which warp threads were to be raised during weaving. Vaucanson also added a ratchet mechanism to advance the punched paper each time that the cylinder was pushed against the row of hooks. [ 27 ] [ 28 ] [ 29 ] [ 30 ] However, Vaucanson's loom was not successful, probably because, like Bouchon's mechanism, it could not control enough warp threads to make sufficiently elaborate patterns to justify the cost of the mechanism. [ 26 ] To stimulate the French textile industry, which was competing with Britain's industrialized industry, Napoleon Bonaparte placed large orders for Lyon's silk, starting in 1802. [ 14 ] In 1804, [ 31 ] at the urging of Lyon fabric maker and inventor Gabriel Dutillieu, Jacquard studied Vaucanson's loom, which was stored at the Conservatoire des Arts et Métiers in Paris. [ 6 ] By 1805 Jacquard had eliminated the paper strip from Vaucanson's mechanism and returned to using Falcon's chain of punched cards. [ 32 ] The potential of Jacquard's loom was immediately recognized. On 12 April 1805, Emperor Napoleon and Empress Josephine visited Lyon and viewed Jacquard's new loom. On 15 April 1805, the emperor granted the patent for Jacquard's loom to the city of Lyon. In return, Jacquard received a lifelong pension of 3,000 francs; furthermore, he received a royalty of 50 francs for each loom that was bought and used during the period from 1805 to 1811. [ 14 ]",
    "links": [
      "Exposition des produits de l'industrie française",
      "Jacquard loom",
      "July Monarchy",
      "Conservatoire des Arts et Metiers",
      "Science Museum (London)",
      "Museum of Science and Industry in Manchester",
      "Heidelberg",
      "Weaving",
      "Basile Bouchon",
      "Loom",
      "Saône",
      "Empress Josephine",
      "Lyon",
      "Kingdom of France",
      "Napoleon Bonaparte",
      "Computer",
      "Rhône (department)",
      "Paris",
      "Oullins",
      "Rhine Campaign of 1795",
      "England",
      "Jean-Charles Pichegru",
      "Analytical Engine",
      "Punch card",
      "Lime (material)",
      "Jacques de Vaucanson",
      "Charles Babbage",
      "Revolt of Lyon against the National Convention",
      "Dictionary.com",
      "IEEE",
      "John Bowring",
      "Jacques Vaucanson",
      "Conservatoire des Arts et Métiers",
      "Abbott Payson Usher",
      "IBM",
      "List of pioneers in computer science",
      "Lyonnais"
    ]
  },
  "Search engine indexing": {
    "url": "https://en.wikipedia.org/wiki/Search_engine_indexing",
    "title": "Search engine indexing",
    "content": "Search engine indexing is the collecting, parsing , and storing of data to facilitate fast and accurate information retrieval . Index design incorporates interdisciplinary concepts from linguistics , cognitive psychology , mathematics, informatics , and computer science . An alternate name for the process, in the context of search engines designed to find web pages on the Internet, is web indexing . Popular search engines focus on the full-text indexing of online, natural language documents. [ 1 ] Media types such as pictures, video, audio, [ 2 ] and graphics [ 3 ] are also searchable. Meta search engines reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the corpus . Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while agent -based search engines index in real time . The purpose of storing an index is to optimize speed and performance in finding relevant documents for a search query. Without an index, the search engine would scan every document in the corpus , which would require considerable time and computing power. For example, while an index of 10,000 documents can be queried within milliseconds, a sequential scan of every word in 10,000 large documents could take hours. The additional computer storage required to store the index, as well as the considerable increase in the time required for an update to take place, are traded off for the time saved during information retrieval. Major factors in designing a search engine's architecture include: Search engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors. A major challenge in the design of search engines is the management of serial computing processes. There are many opportunities for race conditions and coherent faults. For example, a new document is added to the corpus and the index must be updated, but the index simultaneously needs to continue responding to search queries. This is a collision between two competing tasks. Consider that authors are producers of information, and a web crawler is the consumer of this information, grabbing the text and storing it in a cache (or corpus ). The forward index is the consumer of the information produced by the corpus, and the inverted index is the consumer of information produced by the forward index. This is commonly referred to as a producer-consumer model . The indexer is the producer of searchable information and users are the consumers that need to search. The challenge is magnified when working with distributed storage and distributed processing. In an effort to scale with larger amounts of indexed information, the search engine's architecture may involve distributed computing , where the search engine consists of several machines operating in unison. This increases the possibilities for incoherency and makes it more difficult to maintain a fully synchronized, distributed, parallel architecture. [ 13 ] Many search engines incorporate an inverted index when evaluating a search query to quickly locate documents containing the words in a query and then rank these documents by relevance. Because the inverted index stores a list of the documents containing each word, the search engine can use direct access to find the documents associated with each word in the query in order to retrieve the matching documents quickly. The following is a simplified illustration of an inverted index: This index can only determine whether a word exists within a particular document, since it stores no information regarding the frequency and position of the word; it is therefore considered to be a Boolean index. Such an index determines which documents match a query but does not rank matched documents. In some designs the index includes additional information such as the frequency of each word in each document or the positions of a word in each document. [ 14 ] Position information enables the search algorithm to identify word proximity to support searching for phrases; frequency can be used to help in ranking the relevance of documents to the query. Such topics are the central research focus of information retrieval . The inverted index is a sparse matrix , since not all words are present in each document. To reduce computer storage memory requirements, it is stored differently from a two dimensional array . The index is similar to the term document matrices employed by latent semantic analysis . The inverted index can be considered a form of a hash table. In some cases the index is a form of a binary tree , which requires additional storage but may reduce the lookup time. In larger indices the architecture is typically a distributed hash table . [ 15 ] For phrase searching, a specialized form of an inverted index called a positional index is used. A positional index not only stores the ID of the document containing the token but also the exact position(s) of the token within the document in the postings list . The occurrences of the phrase specified in the query are retrieved by navigating these postings list and identifying the indexes at which the desired terms occur in the expected order (the same as the order in the phrase). So if we are searching for occurrence of the phrase \"First Witch\", we would: The postings lists can be navigated using a binary search in order to minimize the time complexity of this procedure. [ 16 ] The inverted index is filled via a merge or rebuild. A rebuild is similar to a merge but first deletes the contents of the inverted index. The architecture may be designed to support incremental indexing, [ 17 ] where a merge identifies the document or documents to be added or updated and then parses each document into words. For technical accuracy, a merge conflates newly indexed documents, typically residing in virtual memory, with the index cache residing on one or more computer hard drives. After parsing, the indexer adds the referenced document to the document list for the appropriate words. In a larger search engine, the process of finding each word in the inverted index (in order to report that it occurred within a document) may be too time consuming, and so this process is commonly split up into two parts, the development of a forward index and a process which sorts the contents of the forward index into the inverted index. The inverted index is so named because it is an inversion of the forward index. The forward index stores a list of words for each document. The following is a simplified form of the forward index: The rationale behind developing a forward index is that as documents are parsed, it is better to intermediately store the words per document. The delineation enables asynchronous system processing, which partially circumvents the inverted index update bottleneck . [ 18 ] The forward index is sorted to transform it to an inverted index. The forward index is essentially a list of pairs consisting of a document and a word, collated by the document. Converting the forward index to an inverted index is only a matter of sorting the pairs by the words. In this regard, the inverted index is a word-sorted forward index. Generating or maintaining a large-scale search engine index represents a significant storage and processing challenge. Many search engines utilize a form of compression to reduce the size of the indices on disk . [ 19 ] Consider the following scenario for a full text, Internet search engine. Given this scenario, an uncompressed index (assuming a non- conflated , simple, index) for 2 billion web pages would need to store 500 billion word entries. At 1 byte per character, or 5 bytes per word, this would require 2500 gigabytes of storage space alone. [ citation needed ] This space requirement may be even larger for a fault-tolerant distributed storage architecture. Depending on the compression technique chosen, the index can be reduced to a fraction of this size. The tradeoff is the time and processing power required to perform compression and decompression. [ citation needed ] Notably, large scale search engine designs incorporate the cost of storage as well as the costs of electricity to power the storage. Thus compression is a measure of cost. [ citation needed ] Document parsing breaks apart the components (words) of a document or other form of media for insertion into the forward and inverted indices. The words found are called tokens , and so, in the context of search engine indexing and natural language processing , parsing is more commonly referred to as tokenization . It is also sometimes called word boundary disambiguation , tagging , text segmentation , content analysis , text analysis, text mining , concordance generation, speech segmentation , lexing , or lexical analysis . The terms 'indexing', 'parsing', and 'tokenization' are used interchangeably in corporate slang. Natural language processing is the subject of continuous research and technological improvement. Tokenization presents many challenges in extracting the necessary information from documents for indexing to support quality searching. Tokenization for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets. [ citation needed ] Unlike literate humans, computers do not understand the structure of a natural language document and cannot automatically recognize words and sentences. To a computer, a document is only a sequence of bytes. Computers do not 'know' that a space character separates words in a document. Instead, humans must program the computer to identify what constitutes an individual or distinct word referred to as a token. Such a program is commonly called a tokenizer or parser or lexer . Many search engines, as well as other natural language processing software, incorporate specialized programs for parsing, such as YACC or Lex . During tokenization, the parser identifies sequences of characters that represent words and other elements, such as punctuation, which are represented by numeric codes, some of which are non-printing control characters. The parser can also identify entities such as email addresses, phone numbers, and URLs . When identifying each token, several characteristics may be stored, such as the token's case (upper, lower, mixed, proper), language or encoding, lexical category (part of speech, like 'noun' or 'verb'), position, sentence number, sentence position, length, and line number. If the search engine supports multiple languages, a common initial step during tokenization is to identify each document's language; many of the subsequent steps are language dependent (such as stemming and part of speech tagging). Language recognition is the process by which a computer program attempts to automatically identify, or categorize, the language of a document. Other names for language recognition include language classification, language analysis, language identification, and language tagging. Automated language recognition is the subject of ongoing research in natural language processing . Finding which language the words belongs to may involve the use of a language recognition chart . If the search engine supports multiple document formats , documents must be prepared for tokenization. The challenge is that many document formats contain formatting information in addition to textual content. For example, HTML documents contain HTML tags, which specify formatting information such as new line starts, bold emphasis, and font size or style . If the search engine were to ignore the difference between content and 'markup', extraneous information would be included in the index, leading to poor search results. Format analysis is the identification and handling of the formatting content embedded within documents which controls the way the document is rendered on a computer screen or interpreted by a software program. Format analysis is also referred to as structure analysis, format parsing, tag stripping, format stripping, text normalization, text cleaning and text preparation. The challenge of format analysis is further complicated by the intricacies of various file formats. Certain file formats are proprietary with very little information disclosed, while others are well documented. Common, well-documented file formats that many search engines support include: Options for dealing with various formats include using a publicly available commercial parsing tool that is offered by the organization which developed, maintains, or owns the format, and writing a custom parser . Some search engines support inspection of files that are stored in a compressed or encrypted file format. When working with a compressed format, the indexer first decompresses the document; this step may result in one or more files, each of which must be indexed separately. Commonly supported compressed file formats include: Format analysis can involve quality improvement methods to avoid including 'bad information' in the index. Content can manipulate the formatting information to include additional content. Examples of abusing document formatting for spamdexing : Some search engines incorporate section recognition, the identification of major parts of a document, prior to tokenization. Not all the documents in a corpus read like a well-written book, divided into organized chapters and pages. Many documents on the web , such as newsletters and corporate reports, contain erroneous content and side-sections that do not contain primary material (that which the document is about). For example, articles on the Wikipedia website display a side menu with links to other web pages. Some file formats, like HTML or PDF, allow for content to be displayed in columns. Even though the content is displayed, or rendered, in different areas of the view, the raw markup content may store this information sequentially. Words that appear sequentially in the raw source content are indexed sequentially, even though these sentences and paragraphs are rendered in different parts of the computer screen. If search engines index this content as if it were normal content, the quality of the index and search quality may be degraded due to the mixed content and improper word proximity. Two primary problems are noted: Section analysis may require the search engine to implement the rendering logic of each document, essentially an abstract representation of the actual document, and then index the representation instead. For example, some content on the Internet is rendered via JavaScript. If the search engine does not render the page and evaluate the JavaScript within the page, it would not 'see' this content in the same way and would index the document incorrectly. Given that some search engines do not bother with rendering issues, many web page designers avoid displaying content via JavaScript or use the Noscript Archived 2020-07-07 at the Wayback Machine tag to ensure that the web page is indexed properly. At the same time, this fact can also be exploited to cause the search engine indexer to 'see' different content than the viewer. Indexing often has to recognize the HTML tags to organize priority. Indexing low priority to high margin to labels like strong and link to optimize the order of priority if those labels are at the beginning of the text could not prove to be relevant. Some indexers like Google and Bing ensure that the search engine does not take the large texts as relevant source due to strong type system compatibility. [ 22 ] Meta tag indexing plays an important role in organizing and categorizing web content. Specific documents often contain embedded meta information such as author, keywords, description, and language. For HTML pages, the meta tag contains keywords which are also included in the index. Earlier Internet search engine technology would only index the keywords in the meta tags for the forward index; the full document would not be parsed. At that time full-text indexing was not as well established, nor was computer hardware able to support such technology. The design of the HTML markup language initially included support for meta tags for the very purpose of being properly and easily indexed, without requiring tokenization. [ 23 ] As the Internet grew through the 1990s, many brick-and-mortar corporations went 'online' and established corporate websites. The keywords used to describe webpages (many of which were corporate-oriented webpages similar to product brochures) changed from descriptive to marketing-oriented keywords designed to drive sales by placing the webpage high in the search results for specific search queries. The fact that these keywords were subjectively specified was leading to spamdexing , which drove many search engines to adopt full-text indexing technologies in the 1990s. Search engine designers and companies could only place so many 'marketing keywords' into the content of a webpage before draining it of all interesting and useful information. Given that conflict of interest with the business goal of designing user-oriented websites which were 'sticky', the customer lifetime value equation was changed to incorporate more useful content into the website in hopes of retaining the visitor. In this sense, full-text indexing was more objective and increased the quality of search engine results, as it was one more step away from subjective control of search engine result placement, which in turn furthered research of full-text indexing technologies. [ citation needed ] In desktop search , many solutions incorporate meta tags to provide a way for authors to further customize how the search engine will index content from various files that is not evident from the file content. Desktop search is more under the control of the user, while Internet search engines must focus more on the full text index. [ citation needed ]",
    "links": [
      "Real time business intelligence",
      "ZIP (file format)",
      "Kurt Mehlhorn",
      "Donald E. Knuth",
      "Audio search engine",
      "Database index",
      "Computer Storage",
      "Mark Overmars",
      "Part-of-speech tagging",
      "Whitespace (computer science)",
      "Postings list",
      "Span and div",
      "Lexical category",
      "Wide area information server",
      "ASCII",
      "Metasearch engine",
      "Document retrieval",
      "Race conditions",
      "Search by sound",
      "Search engine marketing",
      "Language identification",
      "Latent semantic analysis",
      "Web crawling",
      "YACC",
      "Boolean data type",
      "Byte",
      "Lotus Notes",
      "Distributed web crawling",
      "Chinese language",
      "Information retrieval",
      "Adobe Systems",
      "Binary tree",
      "Information literacy",
      "Customer lifetime value",
      "Focused crawler",
      "Uniform Resource Locator",
      "DNA",
      "Unix",
      "Microsoft Windows",
      "Edward H. Sussenguth Jr.",
      "Federated search",
      "Gzip",
      "Meta tag",
      "Search engine technology",
      "Enterprise search",
      "Online search",
      "Social search",
      "Sorting algorithm",
      "Data compression",
      "Replication (computer science)",
      "Conflation",
      "Cognitive psychology",
      "Distributed computing",
      "Search/Retrieve via URL",
      "Concordance (publishing)",
      "PostScript",
      "Gerald Salton",
      "Parsing",
      "List of search engines",
      "RSS",
      "N-gram",
      "Entity extraction",
      "Multisearch",
      "Spamdexing",
      "Merge (SQL)",
      "Bzip2",
      "Hash table",
      "Compressor (software)",
      "Representational State Transfer",
      "Suffix array",
      "Selection-based search",
      "Controlled vocabulary",
      "Information extraction",
      "Sparse matrix",
      "Vertical search",
      "Web page",
      "Email",
      "Web query classification",
      "Natural language search engine",
      "Citation index",
      "Font family",
      "Computer hardware",
      "Partition (database)",
      "Extendible hashing",
      "Lex programming tool",
      "Multimedia search",
      "Cross-language search",
      "Voice search",
      "Speech segmentation",
      "Computer data storage",
      "SGML",
      "Desktop search",
      "Web crawler",
      "Document-term matrix",
      "Serge Abiteboul",
      "Spider trap",
      "Website mirroring software",
      "Index (search engine)",
      "Natural language processing",
      "English language",
      "Relevance (information retrieval)",
      "Sandhya Dwarkadas",
      "Suffix tree",
      "Language",
      "Stemming",
      "Stanford University",
      "Binary data",
      "Random access",
      "OpenSearch (specification)",
      "Trie",
      "Text segmentation",
      "RAR (file format)",
      "Hash function",
      "PDF",
      "Video search engine",
      "Tokenization (lexical analysis)",
      "Tar (computing)",
      "Text corpus",
      "Evaluation measures (information retrieval)",
      "Character encoding",
      "Internet search",
      "Comparison of parser generators",
      "Data",
      "Internet",
      "Computer science",
      "Robots exclusion standard",
      "Part of speech",
      "Multilingual",
      "XML",
      "Z39.50",
      "Burrows–Wheeler transform",
      "Google",
      "ID3",
      "File format",
      "LaTeX",
      "Array data structure",
      "Search aggregator",
      "HTML",
      "CSS",
      "Lexical analysis",
      "Text mining",
      "Font",
      "Distributed hash table",
      "Search engine (computing)",
      "Brick and mortar business",
      "Local search (Internet)",
      "Japanese language",
      "List of archive formats",
      "Multimedia",
      "Linguistics",
      "Media type",
      "Microsoft Word",
      "Computer storage",
      "Search engine",
      "Microsoft PowerPoint",
      "Informatics",
      "Wayback Machine",
      "Web indexing",
      "Semantic search",
      "Microsoft Excel",
      "Key Word in Context",
      "Collaborative search engine",
      "Bing (search engine)",
      "Tokenizer",
      "Image retrieval",
      "Web search query",
      "Intelligent agent",
      "JavaScript",
      "ISBN (identifier)",
      "Web search engine",
      "The Art of Computer Programming",
      "Bibliometrics",
      "Victor Vianu",
      "Text retrieval",
      "Gerard Salton",
      "Site map",
      "Meta data",
      "Inverted index",
      "Search/Retrieve Web Service",
      "Web archiving",
      "Web query",
      "Full-text search",
      "Parser",
      "Literacy",
      "Cross-language information retrieval",
      "UseNet",
      "Content analysis",
      "Search engine optimization",
      "Cabinet (file format)"
    ]
  },
  "Preservation (library and archival science)": {
    "url": "https://en.wikipedia.org/wiki/Preservation_(library_and_archival_science)",
    "title": "Preservation (library and archival science)",
    "content": "In conservation , library and archival science , preservation is a set of preventive conservation activities aimed at prolonging the life of a record, book, or object while making as few changes as possible. Preservation activities vary widely and may include monitoring the condition of items, maintaining the temperature and humidity in collection storage areas, writing a plan in case of emergencies, digitizing items, writing relevant metadata , and increasing accessibility. Preservation, in this definition, is practiced in a library or an archive by a conservator , librarian , archivist , or other professional when they perceive a collection or record is in need of maintenance. Preservation should be distinguished from interventive conservation and restoration , which refers to the treatment and repair of individual items to slow the process of decay, or restore them to a usable state. [ 1 ] \" Preventive conservation \" is used interchangeably with \"preservation\". [ 2 ] A relatively new concept, digitization , has been hailed as a way to preserve historical items for future use. \"Digitizing refers to the process of converting analog materials into digital form.\" [ 3 ] For manuscripts, digitization is achieved through scanning an item and saving it to a digital format. For example, the Google Book Search program has partnered with over forty libraries around the world to digitize books. The goal of this library partnership project is to \"make it easier for people to find relevant books – specifically, books they wouldn't find any other way such as those that are out of print – while carefully respecting authors' and publishers' copyrights.\" [ 4 ] Although digitization seems to be a promising area for future preservation, there are also problems. The main problems are that digital space costs money, media and file formats may become obsolete, and backwards compatibility is not guaranteed. [ 5 ] Higher-quality images take a longer time to scan, but are often more valuable for future use. Fragile items are often more difficult or more expensive to scan, which creates a selection problem for preservationists where they must decide if digital access in the future is worth potentially damaging the item during the scanning process. Other problems include scan quality, redundancy of digitized books among different libraries, and copyright law. [ 6 ] However, many of these problems are being solved through educational initiatives. Educational programs are tailoring themselves to fit preservation needs and help new students understand preservation practices. Programs teaching graduate students about digital librarianship are especially important. [ 7 ] Groups such as the Digital Preservation Network strive to ensure that \"the complete scholarly record is preserved for future generations\". [ 8 ] The Library of Congress maintains a Sustainability of Digital Formats web site that educates institutions on various aspects of preservation: most notably, on approximately 200 digital format types and which are most likely to last into the future. [ 9 ] Digital Preservation is another name for digitization, and is the term more commonly used in archival courses. The main goal of digital preservation is to guarantee that people will have access to the digitally preserved materials long into the future. [ 10 ] When practicing preservation, one has several factors to consider in order to properly preserve a record: 1) the storage environment of the record, 2) the criteria to determine when preservation is necessary, 3) what the standard preservation practices are for that particular institution, 4) research and testing, and 5) if any vendor services will be needed for further preservation and potentially conservation. Environmental controls are necessary to facilitate the preservation of organic materials and are especially important to monitor in rare and special collections . Key environmental factors to watch include temperature , relative humidity , pests, pollutants, and light exposure. In general, the lower the temperature is, the better it is for the collection. However, since books and other materials are often housed in areas with people, a compromise must be struck to accommodate human comfort. A reasonable temperature to accomplish both goals is 65–68˚F (18–20 °C) however, if possible, film and photography collections should be kept in a segregated area at 55 ˚F (13 °C). [ 11 ] Books and other materials take up and give off moisture making them sensitive to relative humidity. Very high humidity encourages mold growth and insect infestations. Low humidity causes materials to lose their flexibility. Fluctuations in relative humidity are more damaging than a constant humidity in the middle or low range. Generally, the relative humidity should be between 30–50% with as little variation as possible, however recommendations on specific levels to maintain vary depending on the type of material, i.e. paper-based, film, etc. [ 12 ] A specialized dew point calculator for book preservation is available. [ 13 ] Pests, such as insects and vermin, eat and destroy paper and the adhesive that secures book bindings. Food and drink in libraries, archives, and museums can increase the attraction of pests. [ 14 ] An Integrated Pest Management system is one way to control pests in libraries. Particulate and gaseous pollutants, such as soot, ozone , sulfur dioxide , oxides of nitrogen, can cause dust, soiling, and irreversible molecular damage to materials. Pollutants are exceedingly small and not easily detectable or removable. A special filtration system in the building's HVAC is a helpful defense. Exposure to light also has a significant effect on materials. It is not only the light visible to humans that can cause damage, but also ultraviolet light and infrared radiation. Measured in lux or the amount of lumens/m 2 , the generally accepted level of illumination with sensitive materials is limited to 50 lux per day. Materials receiving more lux than recommended can be placed in dark storage periodically to prolong the original appearance of the object. [ 15 ] Recent concerns about the impact of climate change on the management of cultural heritage objects as well as the historic environment [ 16 ] has prompted research efforts to investigate alternative climate control methods and strategies [ 17 ] that include the implementation of alternative climate control systems to replace or supplement traditional high-energy consuming HVAC systems as well as the introduction of passive preservation techniques. [ 18 ] Rather than maintaining a flat line, consistent 24/7 condition for a collection's environment, fluctuation can occur within acceptable limits to create a preservation environment while also thinking of energy efficiency and taking advantage of the outside environment. [ 19 ] Bound materials are sensitive to rapid temperature or humidity cycling due to differential expansion of the binding and pages, which may cause the binding to crack and/or the pages to warp. Changes in temperature and humidity should be done slowly so as to minimize the difference in expansion rates. However, an accelerated aging study on the effects of fluctuating temperature and humidity on paper color and strength showed no evidence that cycling of one temperature to another or one RH to another caused a different mechanism of decay. [ 20 ] The preferred method for storing manuscripts , archival records, and other paper documents is to place them in acid-free paper folders which are then placed in acid-free of low-lignin boxes for further protection. [ 21 ] Similarly, books that are fragile, valuable, oddly shaped, or in need of protection can be stored in archival boxes and enclosures. Additionally, housing books can protect them from many of the contributing factors to book damage: pests, light, temperature changes, and water. [ 22 ] Contamination can occur at the time of manufacture, especially with electronic materials. [ 23 ] It must be stopped before it spreads, but it is usually irreversible. Making a proper decision is an important factor before starting preservation practices. Decision making for preservation should be made considering significance and value of materials. Significance is considered to have two major components: importance and quality. [ 25 ] \"Importance\" relates to the collection's role as a record, and \"quality\" covers comprehensiveness, depth, uniqueness, authenticity and reputation of the collection. Moreover, analyzing the significance of materials can be used to uncover more about their meaning. [ 26 ] Assessment of significance can also aid in documenting the provenance and context to argue the case for grant funding for the object and collection. [ 27 ] Forms of significance can be historically, culturally, socially, or spiritually significant. In the preservation context, libraries and archives make decisions in different ways. In libraries, decision-making likely targets existing holding materials, whereas in archives, decisions for preservation are often made when they acquire materials. Therefore, different criteria might be needed on different occasions. In general, for archive criteria, the points include: For archival criteria, the following are evidence of significance: Since the 1970s, the Northeast Document Conservation Center has stated that the study of understanding the needs of the archive/library is inherently important to their survival. To prolong the life of a collection, it is important that a systematic preservation plan is in place. The first step in planning a preservation program is to assess the institution's existing preservation needs. This process entails identifying the general and specific needs of the collection, establishing priorities, and gathering the resources to execute the plan. [ 29 ] Because budget and time limitations require priorities to be set, standards have been established by the profession to determine what should be preserved in a collection. Considerations include existing condition, rarity, and evidentiary and market values. With non-paper formats, the availability of equipment to access the information will be a factor (for example, playback equipment for audio-visual materials, or microform readers). An institution should determine how many, if any, other institutions hold the material, and consider coordinating efforts with those that do. [ 30 ] Institutions should establish an environment that prioritizes preservation and create an understanding among administration and staff. Additionally, the institution's commitment to preservation should be communicated to funders and stakeholders so that funds can be allocated towards preservation efforts. The first steps an institution should implement, according to the NEDCC, are to establish a policy that defines and charts the course of action and create a framework for carrying out goals and priorities. There are three methods for carrying out a preservation survey: general preservation assessment, collection condition surveys, and an item-by-item survey. [ 31 ] General condition surveys can be part of a library inventory . Selection for treatment determines the survival of materials and should be done by a specialist, whether in relation to an established collection development policy or on an item by item basis. [ 32 ] Once an object or collection has been chosen for preservation, the treatment must be determined that is most appropriate to the material and its collecting institution. If the information is most important, reformatting or creation of a surrogate is a likely option. If the artifact itself is of value, it will receive conservation treatment, ideally of a reversible nature. [ 30 ] With old media deteriorating or showing their vulnerabilities and new media becoming available, research remains active in the field of conservation and preservation. Everything from how to preserve paper media to creating and maintaining electronic resources and gauging their digital permanence is being explored by students and professionals in archives/libraries. The two main issues that most institutions tend to face are the rapid disintegration of acidic paper and water damage (due to flooding, plumbing problems, etc.). Therefore, these areas of preservation, as well as new digital technologies, receive much of the research attention. The American Library Association has many scholarly journals that publish articles on preservation topics, such as College and Research Libraries, Information Technology and Libraries, and Library Resources and Technical Services . Scholarly periodicals in this field from other publishers include International Preservation News, Journal of the American Institute for Conservation , and Collection Management among many others. Learning the proper methods of preservation is important and most archivists are educated on the subject at academic institutions that specifically cover archives and preservation. In the United States most repositories require archivists to have a degree from an ALA-accredited library school. [ 33 ] Similar institutions exist in countries outside the US. Since 2010, the Andrew W. Mellon Foundation has enhanced funding for library and archives conservation education in three major conservation programs. [ 34 ] These programs are all part of the Association of North American Graduate Programs in the Conservation of Cultural Property (ANAGPIC). [ 35 ] Another educational resource available to preservationists is the Northeast Document Conservation Center or NEDCC. [ 36 ] The Preservation, Planning and Publications Committee of the Preservation and Reformatting Section (PARS) in the Association for Library Collections & Technical Services has created a Preservation Education Directory of ALA Accredited schools in the U.S. and Canada offering courses in preservation. The directory is updated approximately every three years. The 10th Edition was made available on the ALCTS web site in March 2015. [ 37 ] Additional preservation education is available to librarians through various professional organizations, such as: Limited, tax-driven funding can often interfere with the ability for public libraries to engage in extensive preservation activities. Materials, particularly books, are often much easier to replace than to repair when damaged or worn. Public libraries usually try to tailor their services to meet the needs and desires of their local communities, which could cause an emphasis on acquiring new materials over preserving old ones. Librarians working in public facilities frequently have to make complicated decisions about how to best serve their patrons. Commonly, public library systems work with each other and sometimes with more academic libraries through interlibrary loan programs. By sharing resources, they are able to expand upon what might be available to their own patrons and share the burdens of preservation across a greater array of systems. Archival facilities focus specifically on rare and fragile materials. With staff trained in appropriate techniques, archives are often available to many public and private library facilities as an alternative to destroying older materials. Items that are unique, such as photographs, or items that are out of print, can be preserved in archival facilities more easily than in many library settings. [ 52 ] Because so many museum holdings are unique, including print materials, art, and other objects, preservationists are often most active in this setting; however, since most holdings are usually much more fragile, or possibly corrupted, conservation may be more necessary than preservation. This is especially common in art museums . Museums typically hold to the same practices led by archival institutions. Preservation as a formal profession in libraries and archives dates from the twentieth century, but its philosophy and practice has roots in many earlier traditions. [ 53 ] In many ancient societies, appeals to heavenly protectors were used to preserve books, scrolls and manuscripts from insects, fire and decay. Human record-keeping arguably dates back to the cave painting boom of the Upper Paleolithic , some 32,000–40,000 years ago. More direct antecedents are the writing systems that developed in the 4th millennium BC. Written record keeping and information sharing practices, along with oral tradition , sustain and transmit information from one group to another. This level of preservation has been supplemented over the last century with the professional practice of preservation and conservation in the cultural heritage community. The Paul Banks and Carolyn Harris Preservation Award for outstanding preservation specialists in library and archival science, is given annually by the Association for Library Collections & Technical Services, [ 59 ] a subdivision of the American Library Association . It is awarded in recognition of professional preservation specialists who have made significant contributions to the field. Reformatting, or in any other way copying an item's contents, raises obvious copyright issues. In many cases, a library is allowed to make a limited number of copies of an item for preservation purposes. In the United States, certain exceptions have been made for libraries and archives. [ 60 ] Ethics will play an important role in many aspects of the conservator's activities. When choosing which objects are in need of treatment, the conservator should do what is best for the object in question and not yield to pressure or opinion from outside sources. Conservators should refer to the AIC Code of Ethics and Guidelines for Practice, [ 61 ] which states that the conservation professional must \"strive to attain the highest possible standards in all aspects of conservation.\" One instance in which these decisions may get tricky is when the conservator is dealing with cultural objects. The AIC Code of Ethics and Guidelines for Practice [ 61 ] has addressed such concerns, stating \"All actions of the conservation professional must be governed by an informed respect for cultural property, its unique character and significance and the people or person who created it.\" This can be applied in both the care and long-term storage of objects in archives and institutions. It is important that preservation specialists be respectful of cultural property and the societies that created it, and it is also important for them to be aware of international and national laws pertaining to stolen items. In recent years there has been a rise in nations seeking out artifacts that have been stolen and are now in museums. In many cases museums are working with the nations to find a compromise to balance the need for reliable supervision as well as access for both the public and researchers. [ 62 ] Conservators are not just bound by ethics to treat cultural and religious objects with respect, but also in some cases by law. For example, in the United States, conservators must comply with the Native American Graves Protection and Repatriation Act (NAGPRA). The First Archivists Circle, a group of Native American archivists, has also created Protocols for Native American Archival Materials. The non-binding guidelines are suggestions for libraries and archives with Native American archival materials. The care of cultural and sacred objects often affects the physical storage or the object. For example, sacred objects of the native peoples of the Western United States are supposed to be stored with sage to ensure their spiritual well-being. The idea of storing an object with plant material is inherently problematic to an archival collection because of the possibility of insect infestation. When conservators have faced this problem, they have addressed it by using freeze-dried sage, thereby meeting both conservation and cultural needs. Some individuals in the archival community have explored the possible moral responsibility to preserve all cultural phenomena, in regards to the concept of monumental preservation. [ 63 ] Other advocates argue that such an undertaking is something that the indigenous or native communities that produce such cultural objects are better suited to perform. Currently, however, many indigenous communities are not financially able to support their own archives and museums. Still, indigenous archives are on the rise in the United States. [ 64 ] There is a longstanding tension between preservation of and access to library materials, particularly in the area of special collections . Handling materials promotes their progression to an unusable state, especially if they are handled carelessly. On the other hand, materials must be used in order to gain any benefit from them. In a collection with valuable materials, this conflict is often resolved by a number of measures which can include heightened security, requiring the use of gloves for photographs, restricting the materials researchers may bring with them into a reading room, and restricting use of materials to patrons who are not able to satisfy their research needs with less valuable copies of an item. These restrictions can be considered hindrances to researchers who feel that these measures are in place solely to keep materials out of the hands of the public. [ citation needed ] There is also controversy surrounding preservation methods. A major controversy at the end of the twentieth century centered on the practice of discarding items that had been microfilmed. This was the subject of novelist Nicholson Baker 's book Double Fold , which chronicled his efforts to save many old runs of American newspapers (formerly owned by the British Library) from being sold to dealers or pulped. A similar concern persists over the retention of original documents reformatted by any means, analog or digital. Concerns include scholarly needs and legal requirements for authentic or original records as well as questions about the longevity, quality, and completeness of reformatted materials. [ 65 ] [ 66 ] Retention of originals as a source or fail-safe copy is now a fairly common practice. Another controversy revolving around different preservation methods is that of digitization of original material to maintain the intellectual content of the material while ignoring the physical nature of the book. [ 67 ] Further, the Modern Language Association 's Committee on the Future of the Print Record structured its \"Statement on the Significance of Primary Records\" on the inherent theoretical ideology that there is a need to preserve as many copies of a printed edition as is possible as texts and their textual settings are, quite simply, not separable, just as the artifactual characteristics of texts are as relevant and varied as the texts themselves (in the report mentioned herewith, G. Thomas Tanselle suggests that presently existing book stacks need not be abandoned with emerging technologies; rather they serve as vitally important original (primary) sources for future study). [ 68 ] Many digitized items, such as back issues of periodicals, are provided by publishers and databases on a subscription basis. If these companies were to cease providing access to their digital information, facilities that elected to discard paper copies of these periodicals could face significant difficulties in providing access to these items. Discussion as to the best ways to utilize digital technologies is therefore ongoing, and the practice continues to evolve. Of course, the issues surrounding digital objects and their care in libraries and archives continues to expand as more and more of contemporary culture is created, stored, and used digitally. These born-digital materials raise their own new kinds of preservation challenges and in some cases they may even require use new kinds of tools and techniques. [ 69 ] In her book Sacred Stacks: The Higher Purpose of Libraries and Librarianship , Nancy Kalikow Maxwell discusses how libraries are capable of performing some of the same functions as religion. [ 70 ] Many librarians feel that their work is done for some higher purpose. [ 70 ] The same can be said for preservation librarians. One instance of the library's role as sacred is to provide a sense of immortality : with the ever-changing world outside, the library will remain stable and dependable. [ 70 ] Preservation is a great help in this regard. Through digitization and reformatting, preservation librarians are able to retain material while at the same time adapting to new methods. In this way, libraries can adapt to the changes in user needs without changing the quality of the material itself. Through preservation efforts, patrons can rest assured that although materials are constantly deteriorating over time, the library itself will remain a stable, reliable environment for their information needs. Another sacred ability of the library is to provide information and a connection to the past. [ 70 ] By working to slow down the processes of deterioration and decay of library materials, preservation practices help keep this link to the past alive.",
    "links": [
      "Archaeological science",
      "Information access",
      "Categorization",
      "Oral culture",
      "Renaissance",
      "Carolyn Harris",
      "Illuminated manuscript",
      "Special library",
      "Cultural property imaging",
      "Cataloging (library science)",
      "Ink",
      "Electronic resource management",
      "Conservation and restoration of lighthouses",
      "Hdl (identifier)",
      "Queen's University at Kingston",
      "Anastylosis",
      "Tribal Historic Preservation Officer",
      "Textile stabilization",
      "Paper splitting",
      "List of female librarians",
      "Paperback",
      "Audiobook",
      "International Federation of Film Archives",
      "Used book",
      "List of libraries in the ancient world",
      "Outline of books",
      "Book town",
      "Information technology",
      "Data management",
      "Textbook",
      "Conservation and restoration of textiles",
      "Conservation and restoration of papyrus",
      "Communication studies",
      "Non-fiction",
      "Association of Moving Image Archivists",
      "Cotton paper",
      "Scribe",
      "Conservator-restorer",
      "Bibcode (identifier)",
      "Paintings conservator",
      "HVAC",
      "Academic library",
      "Public library",
      "Traditional knowledge",
      "Publishing",
      "Encyclopedists",
      "Law library",
      "Saint Jerome",
      "Literary award",
      "Knowledge",
      "Bookbreaking",
      "Cultural resource management",
      "Modern and Contemporary Art Research Initiative",
      "Library",
      "Collecting",
      "Cultural heritage",
      "Heritage science",
      "Provenance",
      "University of Delaware",
      "Conservation and restoration of wooden artifacts",
      "Paleo-inspiration",
      "Ethnochoreology",
      "Tool library",
      "Library publishing",
      "Preservation of Illuminated Manuscripts",
      "Book collecting",
      "Conservation and restoration of neon objects",
      "Incunable",
      "Traditional Chinese bookbinding",
      "Cultural heritage management",
      "Association of Research Libraries",
      "Conservation and restoration of bone, horn, and antler objects",
      "Language preservation",
      "Conservation and restoration of new media art",
      "Objects conservator",
      "Library catalog",
      "Oral tradition",
      "Conservation and restoration of lacquerware",
      "Map collection",
      "Parchment repair",
      "Conservation issues of Pompeii and Herculaneum",
      "Born-digital",
      "Shadow library",
      "List of national and state libraries",
      "Rare Book School",
      "Library binding",
      "Cradling (paintings)",
      "World Book Capital",
      "Acid paper",
      "Babylon",
      "Art dealer",
      "Emergency management",
      "Information history",
      "Conservation scientist",
      "History of writing",
      "Glossary of library and information science",
      "Ethnopoetics",
      "Special collections",
      "Folklore studies",
      "Scriptorium",
      "Japanese tissue",
      "Editing",
      "Print culture",
      "American Institute for Conservation",
      "Ultraviolet",
      "Bioarchaeology",
      "American Library Association",
      "Fonds",
      "Family folklore",
      "Marginalia",
      "Philosophy of information",
      "Conservation and restoration of time-based media art",
      "Book publishing in Pakistan",
      "Display case",
      "Library and information science",
      "Age of Enlightenment",
      "Metadata",
      "Knowledge management",
      "Conservation and restoration of leather objects",
      "Slow fire",
      "Conservation and restoration of flags and banners",
      "Bestseller",
      "Wayback Machine",
      "Conservation and restoration of books, manuscripts, documents, and ephemera",
      "Society of American Archivists",
      "Conservation and restoration of movable cultural property",
      "Conservation and restoration of glass objects",
      "ISBN (identifier)",
      "Conservation-restoration of Thomas Eakins' The Gross Clinic",
      "Coffee table book",
      "History of Public Library Advocacy",
      "Books in the United States",
      "Library science",
      "Video game preservation",
      "Bookbinding",
      "Integrated pest management (cultural property)",
      "VisualAudio",
      "Miniature book",
      "Paul N. Banks",
      "Print permanence",
      "Information",
      "Reading",
      "Literacy",
      "Preservationist",
      "Mummy paper",
      "Conservation and restoration of insect specimens",
      "Quetzalcoatl",
      "Cultural property storage",
      "Library and information scientist",
      "Conservation and restoration of musical instruments",
      "Saint Lawrence",
      "Preservation of cultural venues",
      "Conservation and restoration of vinyl discs",
      "List of female archivists",
      "Book illustration",
      "Collection catalog",
      "Book size",
      "Conservation and restoration of film",
      "Books in Spain",
      "S2CID (identifier)",
      "Documentary editing",
      "Traditional medicine",
      "Conservation and restoration of photographic plates",
      "Bookcase",
      "Oral history preservation",
      "Library technical services",
      "Detachment of wall paintings",
      "Library consortium",
      "Book series",
      "Information literacy",
      "Book",
      "Archivist",
      "Science and technology studies",
      "History of printing",
      "Digital repository audit method based on risk assessment",
      "Book burning",
      "Optical media preservation",
      "Music library",
      "Bibliophilia",
      "Periodicals librarian",
      "Digitization",
      "Conservation and restoration of plastic objects",
      "Bibliography",
      "Collections management system",
      "Folk play",
      "G. Thomas Tanselle",
      "Conservation and restoration of rail vehicles",
      "Midden",
      "Novel",
      "Newberry Library",
      "Registrar (cultural property)",
      "Book and paper conservation",
      "Distance education librarian",
      "Private library",
      "Immortality",
      "Desmet method",
      "Traveling library",
      "List of destroyed libraries",
      "Discovery system (bibliographic search)",
      "School library",
      "List of medical libraries",
      "Bookselling",
      "Censorship",
      "George Eastman Museum",
      "Outline of information science",
      "Book preservation in developing countries",
      "Paul Banks and Carolyn Harris Preservation Award",
      "Data storage device",
      "Repatriation (cultural property)",
      "Upper Paleolithic",
      "Conservation and restoration of performance art",
      "Art handler",
      "Carolyn Price Horton",
      "Sulfuric acid",
      "Conservation and restoration of outdoor bronze objects",
      "Amigos Library Services",
      "Preservation of meaning",
      "Books in Germany",
      "Advance copy",
      "Book discussion club",
      "Cultural property exhibition",
      "William Barrow (chemist)",
      "Transportation library",
      "Temperature",
      "Library of Pergamum",
      "Presidential library system",
      "Database preservation",
      "Disaster recovery plan",
      "Preservation surveys",
      "Books in Brazil",
      "Security",
      "Digital preservation",
      "Preservation metadata",
      "Photograph conservator",
      "Conservation and restoration of clocks",
      "Heritage language",
      "Legal deposit",
      "Information architecture",
      "List of book-burning incidents",
      "Library of Alexandria",
      "Book tour",
      "Heritage railway",
      "Music librarianship",
      "Bookmark",
      "The Philobiblon",
      "Librarian",
      "North Bennet Street School",
      "Conservation and restoration of silver objects",
      "Conservation and restoration of stained glass",
      "Information behavior",
      "Risk management (cultural property)",
      "Merton College",
      "Ephemera",
      "Documentation science",
      "Book censorship",
      "Conservation and restoration of archaeological sites",
      "Data",
      "Destruction of the Library of Alexandria",
      "Art conservation and restoration",
      "Deaccessioning",
      "Dance notation",
      "Printing",
      "Quantum information science",
      "Teacher-librarian",
      "Folk instrument",
      "National library",
      "Conservation of South Asian household shrines",
      "Conservation and restoration of taxidermy",
      "Learning Resource Centre",
      "Nazi book burnings",
      "Peter Waters",
      "Decision making",
      "Folk music",
      "Columbia University School of Library Service",
      "Manuscript",
      "Bookworm",
      "Conservation and restoration of illuminated manuscripts",
      "Books in the Netherlands",
      "Cultural property documentation",
      "Indigenous culture",
      "Presidential library",
      "Archaeological site",
      "Reconstruction (architecture)",
      "Language death",
      "Books in the United Kingdom",
      "Typesetting",
      "Folk dance",
      "Carolyn Harris (librarian)",
      "Digital reference",
      "Web archiving",
      "Native American Graves Protection and Repatriation Act",
      "Collection manager",
      "Hand-colouring of photographs",
      "1966 flood of the Arno",
      "Language revitalization",
      "Lists of banned books",
      "Bibliotherapy",
      "Information professional",
      "Museum",
      "Edition (book)",
      "Conservation and restoration of photographs",
      "Digitizing",
      "Bibliomania",
      "Integrated Pest Management",
      "Patricia Battin",
      "Archaeology",
      "Conservation and restoration of Tibetan thangkas",
      "JSTOR (identifier)",
      "Volume (bibliography)",
      "Five laws of library science",
      "World Heritage Site",
      "Ozone",
      "Conservation and restoration of ivory objects",
      "The Book Collector",
      "Information retrieval",
      "Knowledge organization",
      "Christian library",
      "Living history",
      "History of books",
      "World Book Day",
      "Agents of deterioration",
      "Found in collection",
      "Dog ears",
      "List of library associations",
      "Toy library",
      "Disaster preparedness (cultural property)",
      "Book swapping",
      "Historic paint analysis",
      "Privacy",
      "Inventory (library and archive)",
      "Data modeling",
      "Intangible cultural heritage",
      "The Flood of the River Arno in Florence, Italy",
      "Burial",
      "Campbell Center for Historic Preservation Studies",
      "Indigenous language",
      "Preservation of magnetic audiotape",
      "Reference desk",
      "Conservation and restoration of fur objects",
      "Inherent vice",
      "Collection (publishing)",
      "Folio",
      "Information society",
      "Taxonomy",
      "Folklore",
      "Wood-pulp paper",
      "Cultural property radiography",
      "Media preservation",
      "Library circulation",
      "Book scanning",
      "Art museum",
      "Conservation and restoration of feathers",
      "Book design",
      "Curator",
      "Weeding (library)",
      "Book review",
      "Wasōbon",
      "Language Preservation",
      "Sulfur dioxide",
      "Cave painting",
      "Architectural conservation",
      "Conservation-restoration of the Shroud of Turin",
      "Public bookcase",
      "PMID (identifier)",
      "Conservation and restoration of cultural property",
      "Bookmobile",
      "Historic site",
      "Leafcasting",
      "National Bureau of Standards",
      "Heritage asset",
      "Conservation and restoration of human remains",
      "Bookworm (insect)",
      "Archives management",
      "Conservation and restoration of totem poles",
      "Data science",
      "Slipcase",
      "Library branch",
      "Corning Museum of Glass",
      "Lyrasis",
      "Double Fold",
      "OCLC (identifier)",
      "New York University Institute of Fine Arts",
      "Infrared",
      "Sustainable preservation",
      "Collection (museum)",
      "Conservation-restoration of cultural heritage",
      "Kintsugi",
      "Library of things",
      "Library acquisitions",
      "Book packaging",
      "Solander box",
      "Conservation and restoration of wooden furniture",
      "Collective collection",
      "Epistemology",
      "Ruins",
      "Heritage language learning",
      "List of used book conditions",
      "Film preservation",
      "Conservation and restoration of frescos",
      "Foodways",
      "List of best-selling books",
      "Books in France",
      "Computer science",
      "Inpainting",
      "Library Trends",
      "Medical record librarian",
      "Carnegie library",
      "Books in Italy",
      "Early music",
      "Intellectual property",
      "Acid-free paper",
      "Conservation and restoration of ancient Greek pottery",
      "Arrested decay",
      "Restoration of the Sistine Chapel frescoes",
      "Conservation and restoration of metals",
      "Catherine of Alexandria",
      "Dust jacket",
      "E-Science librarianship",
      "List of largest libraries",
      "Medical library",
      "Mass deacidification",
      "Prison library",
      "Effects of climate change",
      "Collection development",
      "Grimoire",
      "Inventory (library)",
      "Readers' advisory",
      "International Organization for Standardization",
      "Bibliometrics",
      "Conservation and restoration of ceramic objects",
      "Sizing",
      "U.S. Declaration of Independence",
      "Endangered language",
      "Book curse",
      "Conservation and restoration of painting frames",
      "Conservation and restoration of road vehicles",
      "Information science",
      "Education for librarianship",
      "History of libraries",
      "Applied folklore",
      "Virtual school libraries in the United States",
      "Doi (identifier)",
      "List of libraries by country",
      "Ancient music",
      "Public Library Advocacy",
      "Modern Language Association",
      "Lending library",
      "Libraries and librarians in fiction",
      "Collections management",
      "Information seeking",
      "Copyright",
      "Subscription library",
      "Memory",
      "Conservation and restoration of woodblock prints",
      "Document",
      "Online public access catalog",
      "Folk process",
      "Hybrid library",
      "Prehistoric music",
      "Light",
      "Cultural artifact",
      "Digital artifactual value",
      "Ensemble librarianship",
      "Digital photograph restoration",
      "Conservation science (cultural property)",
      "Conservation and restoration of outdoor artworks",
      "Mary Lynn Ritzenthaler",
      "Finding aid",
      "List of library science schools",
      "Hardcover",
      "Quipu",
      "Historic preservation",
      "Pamela Darling",
      "Conservation-restoration of the H.L. Hunley",
      "Archival processing",
      "Conservation and restoration of panel paintings",
      "Archive",
      "Conservation and restoration of parchment",
      "Roving reference",
      "Buffalo State College",
      "Environmental monitoring",
      "Business continuity planning",
      "Treasure",
      "Book cover",
      "Conservation and restoration of books, manuscripts, documents and ephemera",
      "ISBN",
      "Transfer of panel paintings",
      "Ebook",
      "Collections maintenance",
      "Computer data storage",
      "Library management",
      "Library history",
      "Conservation and restoration of immovable cultural property",
      "Tsundoku",
      "Library classification",
      "Romanticism",
      "Library Binding",
      "Preservation survey",
      "Conservation Center for Art and Historic Artifacts",
      "Preventive conservation (disambiguation)",
      "Conservation-restoration of Leonardo da Vinci's The Last Supper",
      "Winterthur Museum, Garden and Library",
      "Textile conservator",
      "Conservation and restoration of paintings",
      "Outline of library and information science",
      "Indigenous intellectual property",
      "Conservation and restoration of Pompeian frescoes",
      "Lux",
      "Lining of paintings",
      "History of bookselling",
      "Conservation technician",
      "Scarab (artifact)",
      "Microform",
      "Architectural reprography",
      "List of archivists",
      "Conservation and restoration of copper-based objects",
      "Library assessment",
      "Archival science",
      "Instant book",
      "Overpainting",
      "Imaginary book",
      "List of libraries",
      "Nicholson Baker",
      "Association for Recorded Sound Collections",
      "Codex",
      "Conservation and restoration of iron and steel objects",
      "List of librarians",
      "Conservation-restoration of the Statue of Liberty",
      "Digital permanence",
      "Bookend",
      "Conservation and restoration of historic gardens",
      "Outline of library science",
      "Auction",
      "Ecce Homo (García Martínez and Giménez)",
      "Cultural studies",
      "Museology",
      "Blurb",
      "UVC-based preservation",
      "Mount maker",
      "New York University",
      "Disaster Recovery",
      "Informatics",
      "Cultural property",
      "Conservation and restoration of shipwreck artifacts",
      "Conservation and restoration of Judaica",
      "Mold control and prevention (library and archive)",
      "Exhibition designer",
      "Rissverklebung",
      "Digital library",
      "The Bookworm (painting)",
      "Limited-edition book",
      "Conservation and restoration of outdoor murals",
      "Inventory (museums)",
      "Scroll",
      "Intellectual freedom",
      "Northeast Document Conservation Center",
      "Library instruction",
      "Pop-up book",
      "Aging (artwork)",
      "Ethnomusicology",
      "Informationist",
      "Realia (library science)",
      "Folk art",
      "Ontology (information science)",
      "Research library",
      "Relative humidity",
      "Folk etymology",
      "Relic",
      "Calendar (archives)",
      "Information management",
      "Conservation and restoration of aircraft",
      "Conservation and restoration of herbaria",
      "Nabu"
    ]
  },
  "Information filtering": {
    "url": "https://en.wikipedia.org/wiki/Information_filtering",
    "title": "Information filtering",
    "content": "An information filtering system is a system that removes redundant or unwanted information from an information stream using (semi)automated or computerized methods prior to presentation to a human user. Its main goal is the management of the information overload and increment of the semantic signal-to-noise ratio . To do this the user's profile is compared to some reference characteristics. These characteristics may originate from the information item (the content-based approach) or the user's social environment (the collaborative filtering approach). Whereas in information transmission signal processing filters are used against syntax -disrupting noise on the bit-level, the methods employed in information filtering act on the semantic level. The range of machine methods employed builds on the same principles as those for information extraction . A notable application can be found in the field of email spam filters . Thus, it is not only the information explosion that necessitates some form of filters, but also inadvertently or maliciously introduced pseudo -information. On the presentation level, information filtering takes the form of user-preferences-based newsfeeds , etc. Recommender systems and content discovery platforms are active information filtering systems that attempt to present to the user information items ( film , television , music , books , news , web pages ) the user is interested in. These systems add information items to the information flowing towards the user, as opposed to removing information items from the information flow towards the user. Recommender systems typically use collaborative filtering approaches or a combination of the collaborative filtering and content-based filtering approaches, although content-based recommender systems do exist. Before the advent of the Internet , there are already several methods of filtering information ; for instance, governments may control and restrict the flow of information in a given country by means of formal or informal censorship. Another example of information filtering is the work done by newspaper editors and journalists who provide a service that selects the most valuable information for their clients i.e readers of books, magazines, newspapers, radio listeners and TV viewers. This filtering operation is also present in schools and universities where there is a selection of information to provide assistance based on academic criteria to customers of this service, the students. With the advent of the Internet it is possible for anyone to publish anything they wish at a low-cost. Because of this, the quantity of less useful information has increased considerably and consequently quality of information has improved/disseminated. Due to this problem, work to devise information filtering to obtain the information required for each specific topic easily and efficiently began. A filtering system of this style consists of several tools that help people find the most valuable information, so the limited time you can dedicate to read / listen / view, is correctly directed to the most interesting and valuable documents. These filters are also used to organize and structure information in a correct and understandable way, in addition to group messages on the mail addressed. These filters are essential in the results obtained of the search engines on the Internet. The functions of filtering improves every day to get downloading Web documents and more efficient messages. One of the criteria used in this step is whether the knowledge is harmful or not, whether knowledge allows a better understanding with or without the concept. In this case the task of information filtering to reduce or eliminate the harmful information with knowledge. A system of learning content consists, in general rules, mainly of three basic stages: Currently the problem is not finding the best way to filter information , but the way that these systems require to learn independently the information needs of users. Not only because they automate the process of filtering but also the construction and adaptation of the filter. Some branches based on it, such as statistics, machine learning, pattern recognition and data mining, are the base for developing information filters that appear and adapt in base to experience. To carry out the learning process, part of the information has to be pre-filtered, which means there are positive and negative examples which we named training data, which can be generated by experts, or via feedback from ordinary users. As data is entered, the system includes new rules; if we consider that this data can generalize the training data information, then we have to evaluate the system development and measure the system's ability to correctly predict the categories of new information . This step is simplified by separating the training data in a new series called \"test data\" that we will use to measure the error rate. As a general rule it is important to distinguish between types of errors (false positives and false negatives). For example, in the case on an aggregator of content for children, it doesn't have the same seriousness to allow the passage of information not suitable for them, that shows violence or pornography, than the mistake to discard some appropriated information. To improve the system to lower error rates and have these systems with learning capabilities similar to humans we require development of systems that simulate human cognitive abilities, such as natural-language understanding , capturing meaning Common and other forms of advanced processing to achieve the semantics of information. Nowadays, there are numerous techniques to develop information filters, some of these reach error rates lower than 10% in various experiments. [ citation needed ] Among these techniques there are decision trees, support vector machines, neural networks, Bayesian networks, linear discriminants, logistic regression, etc.. At present, these techniques are used in different applications, not only in the web context, but in thematic issues as varied as voice recognition, classification of telescopic astronomy or evaluation of financial risk.",
    "links": [
      "Algorithmic curation",
      "Semantic",
      "Music",
      "Kalman filter",
      "Information society",
      "Artificial intelligence",
      "Film",
      "Feedback",
      "Content discovery platform",
      "News",
      "Newsfeed",
      "Search engines",
      "Collaborative filtering",
      "Natural-language understanding",
      "Redundancy (information theory)",
      "Information overload",
      "Radio",
      "Knowledge",
      "Filter bubble",
      "Spam filter",
      "Information literacy",
      "Book",
      "Filter (signal processing)",
      "Information extraction",
      "TV",
      "Web page",
      "Syntax",
      "Internet",
      "Reputation management",
      "Information transmission",
      "Recommender system",
      "Information",
      "Pseudo",
      "Collaborative intelligence",
      "Information explosion",
      "Signal-to-noise ratio",
      "Information technology",
      "Television"
    ]
  },
  "Sphinx (search engine)": {
    "url": "https://en.wikipedia.org/wiki/Sphinx_(search_engine)",
    "title": "Sphinx (search engine)",
    "content": "Sphinx is a fulltext search engine that provides text search functionality to client applications. Sphinx can be used either as a stand-alone server or as a storage engine (\"SphinxSE\") for the MySQL family of databases. When run as a stand-alone server, Sphinx operates like a DBMS and can communicate with MySQL , MariaDB , and PostgreSQL through their native protocols or with any ODBC-compliant DBMS via ODBC . MariaDB , a fork of MySQL, is distributed with SphinxSE. [ 2 ] If Sphinx is run as a stand-alone server, it is possible to use SphinxAPI to connect an application to it. Official implementations of the API are available for PHP , Java , Perl , Ruby , and Python languages. Unofficial implementations for other languages, as well as various third-party [ 3 ] plugins and modules, are also available. Other data sources can be indexed via a pipe in a custom XML format. [ 4 ] The Sphinx search daemon supports the MySQL binary network protocol and can be accessed with the regular MySQL API and/or clients. Sphinx supports a subset of SQL known as SphinxQL. It supports standard querying of all index types with SELECT, modifying RealTime indexes with INSERT, REPLACE, and DELETE, and more. Sphinx can also provide a special storage engine for MariaDB and MySQL databases. This allows MySQL and MariaDB to communicate with Sphinx's searchd to run queries and obtain results. Sphinx indices are treated like regular SQL tables. The SphinxSE storage engine is shipped with MariaDB. Sphinx is configured to examine a data set via its Indexer. The Indexer process creates a full-text index (a special data structure that enables quick keyword searches) from the given data/text. Full-text fields are the resulting content that is indexed by Sphinx; they can be (quickly) searched for keywords. Fields are named, and you can limit your searches to a single field (e.g. search through \"title\" only) or a subset of fields (e.g. to \"title\" and \"abstract\" only). Sphinx's index format generally supports up to 256 fields. Note that the original data is not stored in the Sphinx index, but are discarded during the Indexing process; Sphinx assumes that you store those contents elsewhere. Attributes are additional values associated with each document that can be used to perform additional filtering and sorting during search. Attributes are named. Attribute names are case insensitive. Attributes are not full-text indexed; they are stored in the index as is. Currently supported attribute types are: (since 1.10-beta); (since 2.1.1-beta); [ 5 ] [ 6 ] Sphinx, like classic SQL databases , works with a so-called fixed schema , that is, a set of predefined attribute columns. These work well when most of the data stored actually has values: mapping sparse data to static columns can be cumbersome. Assume for example that you're running a price comparison or an auction site with many different products categories. Some of the attributes like the price or the vendor are identical across all goods. But from there, for laptops, you also need to store the weight, screen size, HDD type, RAM size, etc. And, say, for shovels, you probably want to store the color, the handle length, and so on. So it's manageable across a single category, but all the distinct fields that you need for all the goods across all the categories are legion. The JSON field can be used to overcome this. Inside the JSON attribute you don't need a fixed structure. You can have various keys which may or may not be present in all documents. When you try to filter on one of these keys, Sphinx will ignore documents that don't have the key in the JSON attribute and will work only with those documents that have it. Up until version 3, Sphinx is dual licensed ; either: Since version 3, Sphinx has become proprietary, with a promise to release its source code in the future [ 7 ] In 2017, key members of the original Sphinx team forked the project under the name Manticore, [ 17 ] with the intention of fixing bugs and developing new features. [ 18 ] Unlike Sphinx, Manticore continues to be released as open source under version 3 of the GPL . [ 19 ]",
    "links": [
      "Data structure",
      "Batch processing",
      "GNU General Public License",
      "ODBC",
      "UTF-8",
      "Stopword",
      "Index (search engine)",
      "Search engine (computing)",
      "Software release life cycle",
      "MSSQL",
      "Ruby (programming language)",
      "JSON",
      "DBMS",
      "Programmer",
      "Java (programming language)",
      "PHP",
      "MariaDB",
      "Stemming",
      "String (computer science)",
      "Software license",
      "Ordinal number",
      "GPLv2",
      "Databases",
      "Python (programming language)",
      "Operating system",
      "Dual licensed",
      "Database schema",
      "Tokenization (lexical analysis)",
      "C++",
      "PostgreSQL",
      "ISBN (identifier)",
      "Unsigned Integers",
      "API",
      "Full-text index",
      "Okapi BM25",
      "Timestamps",
      "Set (computer science)",
      "GNU General Public License version 2",
      "Unix",
      "Search algorithm",
      "Sphinx (documentation generator)",
      "Scalar (computing)",
      "CMU Sphinx",
      "MySQL",
      "Storage engine",
      "SBCS",
      "XML",
      "SQL",
      "Fulltext",
      "List of information retrieval libraries",
      "Perl",
      "User-defined function",
      "Proprietary software"
    ]
  },
  "Information technology": {
    "url": "https://en.wikipedia.org/wiki/Information_technology",
    "title": "Information technology",
    "content": "Information technology ( IT ) is the study or use of computers , telecommunication systems and other devices to create, process, store, retrieve and transmit information . [ 1 ] While the term is commonly used to refer to computers and computer networks , it also encompasses other information distribution technologies such as television and telephones . Information technology is an application of computer science and computer engineering . An information technology system ( IT system ) is generally an information system , a communications system , or, more specifically speaking, a computer system — including all hardware , software , and peripheral equipment — operated by a limited group of IT users, and an IT project usually refers to the commissioning and implementation of an IT system. [ 2 ] IT systems play a vital role in facilitating efficient data management, enhancing communication networks, and supporting organizational processes across various industries. Successful IT projects require meticulous planning and ongoing maintenance to ensure optimal functionality and alignment with organizational objectives. [ 3 ] Although humans have been storing, retrieving, manipulating, analysing and communicating information since the earliest writing systems were developed, [ 4 ] the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review ; authors Harold J. Leavitt and Thomas L. Whisler commented that \"the new technology does not yet have a single established name. We shall call it information technology (IT).\" [ 5 ] Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making , and the simulation of higher-order thinking through computer programs. [ 5 ] Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450 – 1840), electromechanical (1840 – 1940), and electronic (1940 to present). [ 4 ] Ideas of computer science were first mentioned before the 1950s under the Massachusetts Institute of Technology (MIT) and Harvard University , where they had discussed and began thinking of computer circuits and numerical calculations. As time went on, the field of information technology and computer science became more complex and was able to handle the processing of more data. Scholarly articles began to be published from different organizations. [ 6 ] During the mid-1900s, Alan Turing , J. Presper Eckert , and John Mauchly were some of the pioneers of early computer technology. While their main efforts focused on designing the first digital computer, Turing also began to raise questions about artificial intelligence. [ 7 ] Devices have been used to aid computation for thousands of years, probably initially in the form of a tally stick . [ 8 ] The Antikythera mechanism , dating from about the beginning of the first century BC, is generally considered the earliest known mechanical analog computer , and the earliest known geared mechanism. [ 9 ] Comparable geared devices did not emerge in Europe until the 16th century, and it was not until 1645 that the first mechanical calculator capable of performing the four basic arithmetical operations was developed. [ 10 ] Electronic computers , using either relays or thermionic valves , began to appear in the early 1940s. The electromechanical Zuse Z3 , completed in 1941, was the world's first programmable computer, and by modern standards one of the first machines that could be considered a complete computing machine. During the Second World War , Colossus developed the first electronic digital computer to decrypt German messages. Although it was programmable , it was not general-purpose, being designed to perform only a single task. It could not also store its program in memory; programming was carried out using plugs and switches to alter the internal wiring. [ 11 ] The first recognizably modern electronic digital stored-program computer was the Manchester Baby , which ran its first program on 21 June 1948. [ 12 ] The development of transistors in the late 1940s at Bell Laboratories allowed a new generation of computers to be designed with greatly reduced power consumption. The first commercially available stored-program computer, the Ferranti Mark I , contained 4050 valves and had a power consumption of 25 kilowatts. By comparison, the first transistorized computer developed at the University of Manchester and operational by November 1953, consumed only 150 watts in its final version. [ 13 ] Several other breakthroughs in semiconductor technology include the integrated circuit (IC) invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor in 1959, silicon dioxide surface passivation by Carl Frosch and Lincoln Derick in 1955, [ 14 ] the first planar silicon dioxide transistors by Frosch and Derick in 1957, [ 15 ] the MOSFET demonstration by a Bell Labs team, [ 16 ] [ 17 ] [ 18 ] [ 19 ] the planar process by Jean Hoerni in 1959, [ 20 ] [ 21 ] [ 22 ] and the microprocessor invented by Ted Hoff , Federico Faggin , Masatoshi Shima , and Stanley Mazor at Intel in 1971. These important inventions led to the development of the personal computer (PC) in the 1970s, and the emergence of information and communications technology (ICT). [ 23 ] By 1984, according to the National Westminster Bank Quarterly Review , the term information technology had been redefined as \"the convergence of telecommunications and computing technology (...generally known in Britain as information technology).\" We then begin to see the appearance of the term in 1990, contained within documents for the International Organization for Standardization (ISO). [ 24 ] Innovations in technology have already revolutionized the world by the twenty-first century as people have gained access to different online services. This has changed the workforce drastically, as thirty percent of U.S. workers were already in careers in this profession. 136.9 million people were personally connected to the Internet , which was equivalent to 51 million households. [ 25 ] Along with the Internet, new types of technology were also being introduced across the globe, which have improved efficiency and made things easier across the globe. As technology revolutionizsed society, millions of processes could be completed in seconds. Innovations in communication were crucial as people increasingly relied on computers to communicate via telephone lines and cable networks. The introduction of the email was considered revolutionary as \"companies in one part of the world could communicate by e-mail with suppliers and buyers in another part of the world...\". [ 26 ] Computers and technology have also revolutionized the marketing industry, resulting in more buyers of their products. In 2002, Americans exceeded $28 billion in goods just over the Internet alone, while e-commerce a decade later resulted in $289 billion in sales. [ 26 ] And as computers are rapidly becoming more sophisticated by the day, they are becoming more widely used as people are becoming more reliant on them during the twenty-first century. Electronic data processing or business information processing can refer to the use of automated methods to process commercial data. Typically, this uses relatively simple, repetitive activities to process large volumes of similar information. For example: stock updates applied to an inventory, banking transactions applied to account and customer master files, booking and ticketing transactions to an airline's reservation system, and billing for utility services. The modifier \"electronic\" or \"automatic\" was used with \"data processing\" (DP), especially c. 1960, to distinguish human clerical data processing from that done by computer. [ 27 ] [ 28 ] Early electronic computers such as Colossus made use of punched tape , a long strip of paper on which data was represented by a series of holes, a technology now obsolete. [ 29 ] Electronic data storage, which is used in modern computers, dates from World War II, when a form of delay-line memory was developed to remove the clutter from radar signals, the first practical application of which was the mercury delay line. [ 30 ] The first random-access digital storage device was the Williams tube , which was based on a standard cathode ray tube . [ 31 ] However, the information stored in it and the delay-line memory was volatile in the fact that it had to be continuously refreshed, and thus was lost once power was removed. The earliest form of non-volatile computer storage was the magnetic drum , invented in 1932 [ 32 ] and used in the Ferranti Mark 1 , the world's first commercially available general-purpose electronic computer. [ 33 ] IBM introduced the first hard disk drive in 1956, as a component of their 305 RAMAC computer system. [ 34 ] : 6 Most digital data today is still stored magnetically on hard disks, or optically on media such as CD-ROMs . [ 35 ] : 4–5 Until 2002, most information was stored on analog devices , but that year digital storage capacity exceeded analog for the first time. As of 2007 [update] , almost 94% of the data stored worldwide was held digitally: [ 36 ] 52% on hard disks, 28% on optical devices, and 11% on digital magnetic tape. It has been estimated that the worldwide capacity to store information on electronic devices grew from less than 3 exabytes in 1986 to 295 exabytes in 2007, [ 37 ] doubling roughly every 3 years. [ 38 ] Database Management Systems (DMS) emerged in the 1960s to address the problem of storing and retrieving large amounts of data accurately and quickly. An early such system was IBM 's Information Management System (IMS), [ 39 ] which is still widely deployed more than 50 years later. [ 40 ] IMS stores data hierarchically , [ 39 ] but in the 1970s Ted Codd proposed an alternative relational storage model based on set theory and predicate logic and the familiar concepts of tables, rows, and columns. In 1981, the first commercially available relational database management system (RDBMS) was released by Oracle . [ 41 ] All DMS consist of components; they allow the data they store to be accessed simultaneously by many users while maintaining its integrity. [ 42 ] All databases have a common one point in that the structure of the data they contain is defined and stored separately from the data itself, in a database schema . [ 39 ] In the late 2000s (decade), the extensible markup language (XML) became a popular format for data representation. Although XML data can be stored in normal file systems , it is commonly held in relational databases to take advantage of their \"robust implementation verified by years of both theoretical and practical effort.\" [ 43 ] As an evolution of the Standard Generalized Markup Language (SGML), XML's text-based structure offers the advantage of being both machine- and human-readable . [ 44 ] Data transmission has three aspects: transmission, propagation, and reception. [ 45 ] It can be broadly categorized as broadcasting , in which information is transmitted unidirectionally downstream, or telecommunications , with bidirectional upstream and downstream channels. [ 37 ] XML has been increasingly employed as a means of data interchange since the early 2000s, [ 46 ] particularly for machine-oriented interactions such as those involved in web-oriented protocols such as SOAP , [ 44 ] describing \"data-in-transit rather than... data-at-rest\". [ 46 ] Hilbert and Lopez identify the exponential pace of technological change (a kind of Moore's law ): machines' application-specific capacity to compute information per capita roughly doubled every 14 months between 1986 and 2007; the per capita capacity of the world's general-purpose computers doubled every 18 months during the same two decades; the global telecommunication capacity per capita doubled every 34 months; the world's storage capacity per capita required roughly 40 months to double (every 3 years); and per capita broadcast information has doubled every 12.3 years. [ 37 ] Massive amounts of data are stored worldwide every day, but unless it can be analyzed and presented effectively it essentially resides in what have been called data tombs: \"data archives that are seldom visited\". [ 47 ] To address that issue, the field of data mining — \"the process of discovering interesting patterns and knowledge from large amounts of data\" [ 48 ] — emerged in the late 1980s. [ 49 ] The technology and services IT provides for sending and receiving electronic messages (called \"letters\" or \"electronic letters\") over a distributed (including global) computer network. In terms of the composition of elements and the principle of operation, electronic mail practically repeats the system of regular (paper) mail, borrowing both terms (mail, letter, envelope, attachment, box, delivery, and others) and characteristic features — ease of use, message transmission delays, sufficient reliability, and at the same time no guarantee of delivery. The advantages of e-mail are: easily perceived and remembered by a person addresses of the form user_name@domain_name (for example, somebody@example.com); the ability to transfer both plain text and formatted, as well as arbitrary files; independence of servers (in the general case, they address each other directly); sufficiently high reliability of message delivery; ease of use by humans and programs. The disadvantages of e-mail include: the presence of such a phenomenon as spam (massive advertising and viral mailings); the theoretical impossibility of guaranteed delivery of a particular letter; possible delays in message delivery (up to several days); limits on the size of one message and on the total size of messages in the mailbox (personal for users). A search system is a software and hardware complex with a web interface that provides the ability to look for information on the Internet. A search engine usually means a site that hosts the interface (front-end) of the system. The software part of a search engine is a search engine (search engine) — a set of programs that provides the functionality of a search engine and is usually a trade secret of the search engine developer company. Most search engines look for information on World Wide Web sites, but some systems can look for files on FTP servers, items in online stores, and information on Usenet newsgroups. Improving search is one of the priorities of the modern Internet (see the Deep Web article about the main problems in the work of search engines). Companies in the information technology field are often discussed as a group as the \"tech sector\" or the \"tech industry.\" [ 50 ] [ 51 ] [ 52 ] These titles can be misleading at times and should not be mistaken for \"tech companies,\" which are generally large scale, for-profit corporations that sell consumer technology and software. From a business perspective, information technology departments are a \" cost center \" the majority of the time. A cost center is a department or staff that incurs expenses, or \"costs,\" within a company rather than generating profits or revenue streams. Modern businesses rely heavily on technology for their day-to-day operations, so the expenses delegated to cover technology that facilitates business in a more efficient manner are usually seen as \"just the cost of doing business.\" IT departments are allocated funds by senior leadership and must attempt to achieve the desired deliverables while staying within that budget. Government and the private sector might have different funding mechanisms, but the principles are more or less the same. This is an often overlooked reason for the rapid interest in automation and artificial intelligence , but the constant pressure to do more with less is opening the door for automation to take control of at least some minor operations in large companies. Many companies now have IT departments for managing the computers , networks, and other technical areas of their businesses. Companies have also sought to integrate IT with business outcomes and decision-making through a BizOps or business operations department. [ 53 ] In a business context, the Information Technology Association of America has defined information technology as \"the study, design, development, application, implementation, support, or management of computer-based information systems\". [ 54 ] [ page needed ] The responsibilities of those working in the field include network administration, software development and installation, and the planning and management of an organization's technology life cycle, by which hardware and software are maintained, upgraded, and replaced. Information services is a term somewhat loosely applied to a variety of IT-related services offered by commercial companies, [ 55 ] [ 56 ] [ 57 ] as well as data brokers . The field of information ethics was established by mathematician Norbert Wiener in the 1940s. [ 59 ] : 9 Some of the ethical issues associated with the use of information technology include: [ 60 ] : 20–21 Research suggests that IT projects in business and public administration can easily become significant in scale. Research conducted by McKinsey in collaboration with the University of Oxford suggested that half of all large-scale IT projects (those with initial cost estimates of $15 million or more) often failed to maintain costs within their initial budgets or to complete on time. [ 61 ]",
    "links": [
      "Transistor",
      "Information access",
      "Johns Hopkins University Press",
      "Tablet computer",
      "Mobile phone",
      "Categorization",
      "Artificial intelligence",
      "International Data Corporation",
      "Virtual world",
      "Digital camera",
      "Munich",
      "Networking hardware",
      "Marine electronics",
      "Mind uploading",
      "Computational neuroscience",
      "Cognitive informatics",
      "Stanley Mazor",
      "Data (computing)",
      "IBM Information Management System",
      "Cost centre (business)",
      "Telecommunication",
      "Schematic capture",
      "Random-access memory",
      "Stream processing",
      "CD-ROM",
      "Wire",
      "World War II",
      "Algorithms",
      "Springer Science & Business Media",
      "Computational biology",
      "Avionics",
      "Bibcode (identifier)",
      "Fairchild Semiconductor",
      "University of Oxford",
      "Central heating",
      "Mixed reality",
      "Database",
      "Hierarchical database model",
      "Kitchen stove",
      "Domestic robot",
      "Failure of electronic components",
      "Learning",
      "Control system",
      "Robotic vacuum cleaner",
      "Manchester Baby",
      "Nobel Prize",
      "Harvard Business Review",
      "Printed electronics",
      "Z3 (computer)",
      "Home theater PC",
      "Spyware",
      "Europe",
      "Analog device",
      "Automotive electronics",
      "Electromechanical",
      "Information overload",
      "Norbert Wiener",
      "Neural computation",
      "Cost estimate",
      "University of Manchester",
      "Computing",
      "Artificial brain",
      "Electromechanics",
      "Spamming",
      "Social information processing",
      "Electronics",
      "Microprocessor",
      "IT infrastructure",
      "Quantum electronics",
      "Social influence",
      "Cellular computing",
      "The New York Times",
      "Exabyte",
      "Thermal management (electronics)",
      "Piezotronics",
      "Philosophy of information",
      "Library and information science",
      "Information storage",
      "Information ethics",
      "Washing machine",
      "Planar process",
      "Software",
      "Hacker (computer security)",
      "Pennywise",
      "John Mauchly",
      "ISSN (identifier)",
      "Consumer electronics",
      "Integrated circuit",
      "Human–computer interaction",
      "ISBN (identifier)",
      "Microelectronics",
      "Ferranti Mark I",
      "Brain–computer interface",
      "Statistics",
      "Internet cafe",
      "Information",
      "Inforg",
      "Information system",
      "Electromagnetic warfare",
      "Decoding the Universe",
      "Radio navigation",
      "Neuroinformatics",
      "S2CID (identifier)",
      "Computer",
      "Alan Turing",
      "Science and technology studies",
      "Broadcasting",
      "Attention",
      "Home automation",
      "Preservation (library and archival science)",
      "Outline of information technology",
      "Bioelectronics",
      "Ted Hoff",
      "Punched card",
      "Relational database management system",
      "Data broker",
      "Decision theory",
      "Data mining",
      "Censorship",
      "Relational database",
      "Outline of information science",
      "Timeline of computing 2020–present",
      "Air conditioning",
      "Radar",
      "Multi-agent system",
      "Vacuum tube",
      "Clothes dryer",
      "Systems theory",
      "It (disambiguation)",
      "Home cinema",
      "Information theory",
      "HTTP cookie",
      "Antikythera mechanism",
      "Electronic data processing",
      "Information architecture",
      "Refrigerator",
      "Harvard University",
      "Information behavior",
      "Analogue electronics",
      "Systems biology",
      "Electronic computers",
      "Open-source hardware",
      "Deutsches Museum",
      "Quantum information science",
      "Edgar F. Codd",
      "Mechanical engineering",
      "Small appliance",
      "Portable media player",
      "Multimedia",
      "Colossus computer",
      "Microwave",
      "Machine-readable",
      "Theory of computation",
      "Drum memory",
      "Personal computer",
      "Federico Faggin",
      "Ferranti Mark 1",
      "Semiconductor",
      "Computer programming",
      "Bio-inspired computing",
      "History of computing hardware",
      "Mechanical calculator",
      "Virtual reality",
      "Intel",
      "Oracle Corporation",
      "Microwave oven",
      "Information retrieval",
      "Knowledge organization",
      "Video game console",
      "Nuclear electronics",
      "Stored-program computer",
      "Audio equipment",
      "Masatoshi Shima",
      "Electronics industry",
      "Reason",
      "Privacy",
      "Data modeling",
      "Knowledge society",
      "Process (computing)",
      "Pantheon Books",
      "Technical support",
      "Information society",
      "Jean Hoerni",
      "Taxonomy",
      "Data transmission",
      "Cambridge, Massachusetts",
      "Optoelectronics",
      "Cathode ray tube",
      "Communications protocol",
      "Low-power electronics",
      "Data storage",
      "Spintronics",
      "PMID (identifier)",
      "Peripheral",
      "Data processing",
      "Molecular electronics",
      "J. Presper Eckert",
      "Signal",
      "Telecommunications",
      "Communication",
      "SOAP",
      "Early computer",
      "James Gleick",
      "Mind",
      "World Wide Web",
      "MOSFET",
      "Event processing",
      "Behavior informatics",
      "Human-readable",
      "Semiotics",
      "Instrumentation",
      "Laptop",
      "Freezer",
      "Internet",
      "Computer science",
      "Standard Generalized Markup Language",
      "IBM 305 RAMAC",
      "Organic electronics",
      "Relay",
      "Intellectual property",
      "Morgan Kaufmann",
      "Wireless",
      "Power electronics",
      "Predicate logic",
      "Computer network",
      "Telephones",
      "Information and communications technology",
      "Database schema",
      "International Organization for Standardization",
      "Bibliometrics",
      "Ubiquitous computing",
      "Radio-frequency engineering",
      "Information structure",
      "Set theory",
      "IBM",
      "Delay-line memory",
      "Information science",
      "Jack Kilby",
      "Harold Leavitt",
      "Doi (identifier)",
      "EHealth",
      "Information Technology (constituency)",
      "Information seeking",
      "File system",
      "Tally stick",
      "Telephone",
      "Memory",
      "Oxford University Press",
      "Terahertz radiation",
      "McKinsey & Company",
      "Robert Noyce",
      "Processor (computing)",
      "Physical computing",
      "Intelligence",
      "Tankless water heating",
      "Texas Instruments",
      "Major appliance",
      "Hard disk drive",
      "Analog computer",
      "Moore's law",
      "Punched tape",
      "Digital data",
      "Embedded system",
      "Remote control",
      "Computer hardware",
      "Science (journal)",
      "Radio receiver",
      "Ebook",
      "Neurocomputing",
      "Flexible electronics",
      "Computer data storage",
      "State (computer science)",
      "Library classification",
      "Military",
      "Circuit (computer science)",
      "Computer engineering",
      "DBMS",
      "Public administration",
      "Electronic engineering",
      "Massachusetts Institute of Technology",
      "Carl Frosch",
      "Information processing theory",
      "Home appliance",
      "Nanoelectronics",
      "XML",
      "Williams tube",
      "Television",
      "Cultural studies",
      "Atomtronics",
      "Genome informatics",
      "Communications system",
      "Infosphere",
      "Computational theory of mind",
      "Search engine",
      "Perception",
      "Decision-making",
      "Informatics",
      "Information Technology Association of America",
      "Dishwasher",
      "Infotech Enterprises",
      "Data acquisition",
      "Bell Laboratories",
      "Deep web",
      "Philosophy of artificial intelligence",
      "Intellectual freedom",
      "Photonics",
      "Ontology (information science)",
      "Information management",
      "Digital electronics"
    ]
  },
  "Atlantic Monthly": {
    "url": "https://en.wikipedia.org/wiki/Atlantic_Monthly",
    "title": "Atlantic Monthly",
    "content": "The Atlantic is an American magazine and multi-platform publisher based in Washington, D.C. It features articles on politics, foreign affairs, business and the economy, culture and the arts, technology, and science. It was founded in 1857 in Boston as The Atlantic Monthly , a literary and cultural magazine that published leading writers' commentary on education, the abolition of slavery , and other major political issues of that time. Its founders included Francis H. Underwood [ 3 ] [ 4 ] and prominent writers Ralph Waldo Emerson , Oliver Wendell Holmes Sr. , Henry Wadsworth Longfellow , Harriet Beecher Stowe , and John Greenleaf Whittier . [ 5 ] [ 6 ] James Russell Lowell was its first editor. [ 7 ] During the 19th and 20th centuries, the magazine also published the annual The Atlantic Monthly Almanac . [ 8 ] The magazine was purchased in 1999 by businessman David G. Bradley , who fashioned it into a general editorial magazine primarily aimed at serious national readers and \" thought leaders \"; in 2017, he sold a majority interest in the publication to Laurene Powell Jobs 's Emerson Collective . [ 9 ] [ 10 ] [ 11 ] The magazine was published monthly until 2001, when 11 issues were produced; since 2003, it has published 10 per year. It dropped \"Monthly\" from the cover with the January/February 2004 issue, and officially changed the name in 2007. [ 12 ] In 2024, it announced that it will resume publishing monthly issues in 2025. [ 13 ] [ 14 ] In 2016, the periodical was named Magazine of the Year by the American Society of Magazine Editors . [ 15 ] In 2022, its writers won Pulitzer Prizes for feature writing and, in 2022, 2023, and 2024 The Atlantic won the award for general excellence by the American Society of Magazine Editors. In 2024, it was reported that the magazine had crossed one million subscribers [ 13 ] and become profitable, three years after losing $20 million in a single year and laying off 17% of its staff. As of 2024, the website's executive editor is Adrienne LaFrance , the editor-in-chief is Jeffrey Goldberg , and the CEO is Nicholas Thompson . According to a 2025 Pew Research Center study on educational differences among audiences of 30 major U.S. news outlets , The Atlantic had the highest proportion of college-educated readers, with 62% of its audience holding at least a bachelor's degree . [ 16 ] In the autumn of 1857, Moses Dresser Phillips , a publisher from Boston , created The Atlantic Monthly . The plan for the magazine was launched at a dinner party at the Parker House Hotel in Boston, [ 17 ] which was described in a letter by Phillips: I must tell you about a little dinner-party I gave about two weeks ago. It would be proper, perhaps, to state the origin of it was a desire to confer with my literary friends on a somewhat extensive literary project, the particulars of which I shall reserve till you come. But to the Party: My invitations included only R. W. Emerson , H. W. Longfellow , J. R. Lowell , Mr. Motley (the 'Dutch Republic' man), O. W. Holmes , Mr. Cabot , and Mr. Underwood , our literary man. Imagine your uncle as the head of such a table, with such guests. The above named were the only ones invited, and they were all present. We sat down at three P.M., and rose at eight. The time occupied was longer by about four hours and thirty minutes than I am in the habit of consuming in that kind of occupation, but it was the richest time intellectually by all odds that I have ever had. Leaving myself and 'literary man' out of the group, I think you will agree with me that it would be difficult to duplicate that number of such conceded scholarship in the whole country besides... Each one is known alike on both sides of the Atlantic, and is read beyond the limits of the English language. [ 18 ] At that dinner he announced his idea for the magazine: Mr. Cabot is much wiser than I am. Dr. Holmes can write funnier verses than I can. Mr. Motley can write history better than I. Mr. Emerson is a philosopher and I am not. Mr. Lowell knows more of the old poets than I. But none of you knows the American people as well as I do. [ 18 ] Harriet Beecher Stowe was invited to the dinner party but declined because it served alcohol. [ 17 ] She signed the manifesto that set out the goals of the paper along with Herman Melville and Nathaniel Hawthorne , [ 17 ] and The Atlantic today credits her as one of its founders. [ 19 ] The Atlantic ' s first issue was published in November 1857, and quickly gained notability as one of the finest magazines in the English-speaking world. In 1878, the magazine absorbed The Galaxy , a competitor monthly magazine founded a dozen years previously by William Conant Church and his brother Francis P. Church ; it had published works by Mark Twain , Walt Whitman , Ion Hanford Perdicaris and Henry James . [ 20 ] In 1879, The Atlantic had offices in Winthrop Square in Boston and at 21 Astor Place in New York City . [ 21 ] A leading literary magazine, The Atlantic has published many significant works and authors. It was the first to publish pieces by the abolitionists Julia Ward Howe (\" Battle Hymn of the Republic \" on February 1, 1862), and William Parker , whose slave narrative , \"The Freedman's Story\" was published in February and March 1866. It also published Charles W. Eliot 's \"The New Education\", a call for practical reform that led to his appointment to the presidency of Harvard University in 1869, works by Charles Chesnutt before he collected them in The Conjure Woman (1899), and poetry and short stories, and helped launch many national literary careers. [ citation needed ] In 2005, the magazine won a National Magazine Award for fiction. [ 23 ] Editors have recognized major cultural changes and movements. For example, of the emerging writers of the 1920s, Ernest Hemingway had his short story \" Fifty Grand \" published in the July 1927 edition. Harking back to its abolitionist roots, in its August 1963 edition, at the height of the civil rights movement , the magazine published Martin Luther King Jr. 's defense of civil disobedience , \" Letter from Birmingham Jail \", [ 24 ] under the headline \"The Negro Is Your Brother\". [ 25 ] The magazine has published speculative articles that inspired the development of new technologies. The classic example is Vannevar Bush 's essay \" As We May Think \" (July 1945), which inspired Douglas Engelbart and later Ted Nelson to develop the modern workstation and hypertext technology. [ 26 ] [ 27 ] The Atlantic Monthly founded the Atlantic Monthly Press in 1917; for many years, it was operated in partnership with Little, Brown and Company . Its published books included Drums Along the Mohawk (1936) and Blue Highways (1982). The press was sold in 1986; today it is an imprint of Grove Atlantic . [ 28 ] In addition to publishing notable fiction and poetry, The Atlantic has emerged in the 21st century as an influential platform for longform storytelling and newsmaker interviews. Influential cover stories have included Anne Marie Slaughter 's \"Why Women Still Can't Have It All\" (2012) and Ta-Nehisi Coates 's \"A Case for Reparations\" (2014). [ 29 ] In 2015, Jeffrey Goldberg 's \"Obama Doctrine\" was widely discussed by American media and prompted response by many world leaders. [ 30 ] As of 2022, writers and frequent contributors to the print magazine included James Fallows , Jeffrey Goldberg, Ta-Nehisi Coates, Caitlin Flanagan , Jonathan Rauch , McKay Coppins , Gillian White, Adrienne LaFrance , Vann R. Newkirk II , Derek Thompson , David Frum , Jennifer Senior, George Packer , Ed Yong , and James Parker. On August 2, 2023, it was announced that Jeffrey Goldberg, who had served as editor-in-chief of The Atlantic since 2016, had been named as the tenth moderator of the PBS news program, Washington Week , and that the politics and culture publication would also enter into an editorial partnership with the television program – which was retitled accordingly as Washington Week with The Atlantic – similar to the earlier collaboration with the National Journal . [ 31 ] [ 32 ] [ 33 ] The first episode under the longer title, and with Goldberg as moderator, was the one broadcast on August 11, 2023. [ 34 ] In 1860, three years into publication, The Atlantic ' s then-editor James Russell Lowell endorsed Republican Abraham Lincoln for his first run for president and also endorsed the abolition of slavery . [ 35 ] In 1964, Edward Weeks wrote on behalf of the editorial board in endorsing Democratic President Lyndon B. Johnson and rebuking Republican Barry Goldwater 's candidacy. [ 36 ] In 2016, during the 2016 presidential campaign , the editorial board endorsed a candidate for the third time in the magazine's history, urging readers to support Democratic nominee Hillary Clinton in a rebuke of Republican Donald Trump 's candidacy. [ 37 ] After Trump prevailed in the November 2016 election, the magazine became a strong critic of his. In March 2019, a cover article by editor Yoni Appelbaum called for the impeachment of Donald Trump : \"It's time for Congress to judge the president's fitness to serve.\" [ 38 ] [ 39 ] [ 40 ] In September 2020, it published a story, citing several anonymous sources, reporting that Trump referred to dead American soldiers as \"losers\". [ 41 ] Trump called it a \"fake story\", and suggested the magazine would soon be out of business. [ 42 ] [ 43 ] In 2020, The Atlantic endorsed the Democratic presidential nominee Joe Biden in the 2020 presidential election , and urged its readers to oppose Trump's re-election bid. [ 44 ] In early 2024, The Atlantic published a special 24-article issue titled \"If Trump Wins,\" warning about a potential second term for Trump being worse than his first. [ 45 ] [ 46 ] In October, the publication endorsed Democratic nominee Kamala Harris in her presidential bid against Trump in the 2024 election . [ 47 ] The website is, as of 2025 a four tier freemium model. All paid subscribers get access to unlimited articles including the archives and narrated articles and various other features. The base paid model is \"Digital\" subscriber, the higher tier \"Print & Digital\" includes physical copies of the magazine, and the \"Premium\" subscription includes advertisement free access ($120 per year). [ 48 ] The Atlantic went on-line with AOL in 1993. They created an independent website The Atlantic Monthly on the Web in 1995, becoming \"Atlantic Unbound\" in 1997. [ 49 ] The Atlantic had a paywall, being only available to subscribers to the print edition, until January 2008, when they removed it, concomitant with a sponsorship from Goldman Sachs . [ 50 ] [ 51 ] The website introduced \"soft\" limitations in October 2016, when free readers with adblockers were advised that they could turn off their adblocker, pay ($39.99 per year for advertising free access) or be blocked. [ 52 ] [ 53 ] Closing the warning window, however would allow reading the article, the block wasn't actually \"hard\" implemented until 10 April 2017. [ 52 ] A new paywall was expected to start trials in January 2018, but the project was delayed while platform improvements and staff recruitment were completed. [ 54 ] [ 55 ] The relaunch of the paywall was finally announced in August 2019. [ 55 ] In September 2019 the new paywall was imposed, \"Digital\" subscriptions were $49.99 per year, print and digital $59.99 and \"Premium\" $100. [ 54 ] Free users are no longer permitted five articles per month. They can only read the first two paragraphs or so and are then presented a link to subscribe. [ 54 ] In 2005, The Atlantic and the Aspen Institute launched the Aspen Ideas Festival , a ten-day event in and around the city of Aspen , Colorado. [ 56 ] The annual conference features 350 presenters, 200 sessions, and 3,000 attendees. The event has been called a \"political who's who \" as it often features policymakers, journalists, lobbyists, and think tank leaders. [ 57 ] On January 22, 2008, TheAtlantic.com dropped its subscriber wall and allowed users to freely browse its site, including all past archives. [ 58 ] By 2011 The Atlantic ' s web properties included TheAtlanticWire.com, a news- and opinion-tracking site launched in 2009, [ 59 ] and TheAtlanticCities.com, a stand-alone website started in 2011 that was devoted to global cities and trends. [ 60 ] According to a Mashable profile in December 2011, \"traffic to the three web properties recently surpassed 11 million uniques per month, up a staggering 2500% since The Atlantic brought down its paywall in early 2008.\" [ 61 ] In 2009, the magazine launched The Atlantic Wire as a stand-alone news aggregator site. It was intended as a curated selection of news and opinions from online, print, radio, and television outlets. [ 62 ] [ 63 ] [ 64 ] At its launch, it published op-eds from across the media spectrum and summarized significant positions in each debate. [ 64 ] It later expanded to feature news and original reporting. Regular features in the magazine included \"What I Read\", describing the media diets of people from entertainment, journalism, and politics; and \"Trimming the Times\", the feature editor's summary of the best content in The New York Times . [ 65 ] The Atlantic Wire rebranded itself as The Wire in November 2013, [ 66 ] and was folded back into The Atlantic the following year. [ 67 ] In August 2011, it created its video channel. [ 68 ] Initially created as an aggregator, The Atlantic ' s video component, Atlantic Studios, has since evolved in an in-house production studio that creates custom video series and original documentaries. [ 69 ] In September 2011, The Atlantic launched CityLab , a separate website. Its co-founders included Richard Florida , urban theorist and professor. The stand-alone site has been described as exploring and explaining \"the most innovative ideas and pressing issues facing today's global cities and neighborhoods.\" [ 70 ] In 2014, it was rebranded as CityLab.com , and covers transportation, environment, equity, life, and design. Among its offerings are Navigator, \"a guide to urban life\"; and Solutions, which covers solutions to problems in a dozen topics. [ 71 ] In December 2011, a new Health Channel launched on TheAtlantic.com, incorporating coverage of food, as well as topics related to the mind, body, sex, family, and public health. Its launch was overseen by Nicholas Jackson, who had previously been overseeing the Life channel and initially joined the website to cover technology. [ 72 ] TheAtlantic.com has also expanded to visual storytelling , with the addition of the \"In Focus\" photo blog, curated by Alan Taylor. [ 73 ] In 2015, TheAtlantic.com launched a dedicated Science section [ 74 ] and in January 2016 it redesigned and expanded its politics section in conjunction with the 2016 U.S. presidential race. [ 75 ] In 2015, CityLab and Univision launched CityLab Latino , which features original journalism in Spanish as well as translated reporting from the English language edition of CityLab.com . [ 76 ] The site has not been updated since 2018. In early December 2019, Atlantic Media sold CityLab to Bloomberg Media , [ 77 ] [ 78 ] which promptly laid off half the staff. [ 79 ] The site was relaunched on June 18, 2020, with few major changes other than new branding and linking the site with other Bloomberg verticals and its data terminal. [ 80 ] In September 2019, TheAtlantic.com introduced a digital subscription model, restricting unsubscribed readers' access to five free articles per month. [ 81 ] [ 82 ] In June 2020, The Atlantic released its first full-length documentary, White Noise , a film about three alt-right activists. [ 83 ] In June 2006, the Chicago Tribune named The Atlantic one of the top ten English-language magazines, describing it as the \"150-year-old granddaddy of periodicals\" because \"it keeps us smart and in the know\" with cover stories on the then-forthcoming fight over Roe v. Wade . It also lauded regular features such as \"Word Fugitives\" and \"Primary Sources\" as \"cultural barometers\". [ 84 ] On January 14, 2013, The Atlantic ' s website published \" sponsor content \" promoting David Miscavige , the leader of the Church of Scientology . While the magazine had previously published advertising looking like articles, this was widely criticized. The page comments were moderated by the marketing team, not by editorial staff, and comments critical of the church were being removed. Later that day, The Atlantic removed the piece from its website and issued an apology. [ 85 ] [ 86 ] [ 87 ] In 2019, the magazine published an expose on the allegations against movie director Bryan Singer that \"sent Singer's career into a tailspin\". It was originally contracted to Esquire magazine, but the writers moved it there due to what New York Times reporter Ben Smith described as Hearst magazines ' \"timid\" nature. \"There's not a lot of nuance here\", Jeffrey Goldberg said. \"They spiked a story that should have been published in the public interest for reasons unknown.\" [ 88 ] In June 2020, The Atlantic faced legal action in Japan that claimed defamation and invasion of privacy in the article \"When the Presses Stop\" by Molly Ball , published in the January/February 2018 edition, which led to numerous removals, corrections and clarifications after a settlement was reached in January 2024. The lawsuit highlighted fact-checking and ethical concerns, bringing attention to the magazine's editorial practices. [ 89 ] [ 90 ] [ 91 ] On November 1, 2020, The Atlantic retracted an article, \"The Mad, Mad World of Niche Sports Among Ivy League –Obsessed Parents\", after an inquiry by The Washington Post . An 800-word editor's note said, \"We cannot attest to the trustworthiness and credibility of the author, and therefore we cannot attest to the veracity of the article.\" The note alleged that the article's author, freelancer Ruth Shalit Barrett , had left the staff of The New Republic in 1999 amid allegations of plagiarism . [ 92 ] [ 93 ] On January 7, 2022, Barrett sued the magazine for defamation. The lawsuit claimed The Atlantic misrepresented Barrett's background and destroyed her journalistic career through what it publicly said about her. [ 94 ] [ 95 ] In legal filings, Barrett argued that The Atlantic ' s handling of allegations and errors in another article written by Molly Ball demonstrated inconsistency in the magazine's editorial standards and accountability measures. Barrett asserted that the factual inaccuracies and ethical violations in Ball's piece, as highlighted by a separate defamation lawsuit that resulted in a settlement and numerous retractions and corrections to Ball's story, were \"transgressions far more numerous and incomparably worse\" than any mistakes attributed to her own work. [ 96 ] [ 91 ] In June 2025, after mediation, Barrett and The Atlantic reached a settlement and jointly moved to dismiss the case with prejudice. Court filings showed that on June 26 the magazine updated its online editor’s note to clarify that the pseudonymous source “Sloane” was anonymous, that Barrett says she elected to leave The New Republic , and that she did not ask the magazine to use a novel byline. The revision also changed a statement that she had encouraged “at least one source” to lie to “a source.” The following day, the parties filed their stipulation of dismissal in federal court. Although the settlement terms were not publicly disclosed in court, The New York Times reported that The Atlantic agreed to pay Barrett more than $1 million. TheWrap , citing the Times , also reported the payment. The story remains retracted and, according to an Atlantic spokesperson, the editor’s note will not be updated further. [ 97 ] [ 98 ] On February 5, 2024, The Atlantic cut ties with contributor Yascha Mounk after he was accused of rape. He called the allegation \"categorically untrue.\" [ 99 ] In 2025, national-security leaders in the Donald Trump administration accidentally included The Atlantic editor Jeffrey Goldberg in a group chat where they organized and strategized upcoming military strikes on the Houthis . [ 100 ] By its third year, it was published by Boston publishing house Ticknor and Fields , which later became part of Houghton Mifflin , [ citation needed ] based in the city known for literary culture. The magazine was purchased in 1908 by editor at the time, Ellery Sedgwick , and remained in Boston. In 1980, the magazine was acquired by Mortimer Zuckerman , property magnate and founder of Boston Properties , who became its chairman. On September 27, 1999, Zuckerman transferred ownership of the magazine to David G. Bradley , owner of the National Journal Group , which focused on Washington, D.C. and federal government news. Bradley had promised that the magazine would stay in Boston for the foreseeable future, as it did for the next five-and-a-half years. In April 2005, however, the publishers announced that the editorial offices would be moved from their longtime home at 77 North Washington Street in Boston to join the company's advertising and circulation divisions in Washington, D.C. [ 101 ] Later in August, Bradley told The New York Observer that the move was not made to save money—near-term savings would be $200,000–$300,000, a relatively small amount that would be swallowed by severance-related spending—but instead would serve to create a hub in Washington, D.C., where the top minds from all of Bradley's publications could collaborate under the Atlantic Media Company umbrella. Few of the Boston staff agreed to move, and Bradley then commenced an open search for a new editorial staff. [ 102 ] In 2006, Bradley hired James Bennet , the Jerusalem bureau chief for The New York Times , as editor-in-chief. Bradley also hired Jeffrey Goldberg and Andrew Sullivan as writers for the magazine. [ 103 ] In 2008, Jay Lauf joined the organization as publisher and vice-president; as of 2017, he was publisher and president of Quartz . [ 104 ] In early 2014, Bennet and Bob Cohn became co-presidents of The Atlantic , and Cohn became the publication's sole president in March 2016 when Bennet was tapped to lead The New York Times ' s editorial page. [ 105 ] [ 106 ] Jeffrey Goldberg was named editor-in-chief in October 2016. [ 107 ] On July 28, 2017, The Atlantic announced that Laurene Powell Jobs (the widow of former Apple Inc. chairman and CEO Steve Jobs ) had acquired majority ownership through her Emerson Collective organization, with a staff member of Emerson Collective, Peter Lattman, being immediately named as vice chairman of The Atlantic . David G. Bradley and Atlantic Media retained a minority share position in this sale. [ 108 ] In May 2019, technology journalist Adrienne LaFrance became executive editor. [ 109 ] In December 2020, former Wired editor-in-chief Nicholas Thompson was named CEO of The Atlantic . [ 110 ] In 2022, The Atlantic moved its offices to The Wharf in Washington, D.C.'s Southwest Waterfront neighborhood.",
    "links": [
      "Drums Along the Mohawk (novel)",
      "Freemium",
      "Francis P. Church",
      "Ernest Hemingway",
      "Mortimer Zuckerman",
      "Undergraduate education",
      "Howard Kurtz",
      "Federal government of the United States",
      "Douglas Engelbart",
      "Ellery Sedgwick",
      "Pulitzer Prize for Feature Writing",
      "Boston",
      "United States government group chat leak",
      "Atlantic Ocean",
      "James Russell Lowell",
      "Thought leader",
      "The Hill (newspaper)",
      "Goldman Sachs",
      "Atlantic Media Company",
      "Alliance for Audited Media",
      "David G. Bradley",
      "Civil rights movement",
      "As We May Think",
      "Molly Ball",
      "Foreign Affairs",
      "Jeffrey Goldberg",
      "Filmmaker (magazine)",
      "Nicholas Thompson (editor)",
      "Yascha Mounk",
      "Adrienne LaFrance",
      "Bachelor's degree",
      "Joe Biden",
      "The Galaxy (magazine)",
      "Horace Scudder",
      "Southwest Waterfront",
      "The New Republic",
      "The Wharf (Washington, D.C.)",
      "Civil disobedience",
      "Think tank",
      "Media diet",
      "Vann R. Newkirk II",
      "Walt Whitman",
      "Robert Manning (journalist)",
      "Richard Florida",
      "Native advertising",
      "Erik Wemple",
      "Tweet (social media)",
      "Politico",
      "Astor Place",
      "Alt-right",
      "Washington Week",
      "Winthrop Square (Boston)",
      "Aspen, Colorado",
      "Business Insider",
      "Forbes",
      "Cullen Murphy",
      "Caitlin Flanagan",
      "William Whitworth (journalist)",
      "Democratic Party (United States)",
      "Hypertext",
      "Kamala Harris",
      "Anne Marie Slaughter",
      "Ticknor and Fields",
      "David Frum",
      "Antislavery Movement In America",
      "News aggregator",
      "Andrew Sullivan",
      "New York City",
      "American Society of Magazine Editors",
      "Ted Nelson",
      "Fox News",
      "Houthis",
      "Edward Everett Hale",
      "James Fallows",
      "Laurene Powell Jobs",
      "Jerusalem",
      "Bloomberg L.P.",
      "Almanac",
      "Yoni Appelbaum",
      "Twitter",
      "James T. Fields",
      "William Dean Howells",
      "Fact-checking",
      "Apple Inc.",
      "The New York Observer",
      "Maggie Haberman",
      "Houghton Mifflin",
      "Church of Scientology",
      "Fifty Grand",
      "Quartz (publication)",
      "Michael Kelly (editor)",
      "Henry James",
      "James Bennet (journalist)",
      "Spike (journalism)",
      "2024 United States presidential election",
      "Grove Atlantic",
      "McKay Coppins",
      "Atlantic (disambiguation)",
      "John Greenleaf Whittier",
      "Hearst Communications",
      "Donald Trump",
      "Ivy League",
      "The Boston Globe",
      "Moses Dresser Phillips",
      "OCLC (identifier)",
      "Republican Party (United States)",
      "Ta-Nehisi Coates",
      "Aspen Institute",
      "Broadcasting & Cable",
      "Washington, D.C.",
      "William Conant Church",
      "Abraham Lincoln",
      "Mark Twain",
      "Who's Who",
      "Lyndon B. Johnson",
      "Harvard University",
      "Harriet Beecher Stowe",
      "George Packer",
      "James Elliot Cabot",
      "Univision",
      "Atlantic Media",
      "Little, Brown and Company",
      "Edward A. Weeks",
      "Op-ed",
      "David Miscavige",
      "Aspen Ideas Festival",
      "Steve Jobs",
      "The New York Times",
      "2016 United States presidential election",
      "News media",
      "Emerson Collective",
      "Battle Hymn of the Republic",
      "John Lothrop Motley",
      "Walter Hines Page",
      "Abolition of slavery",
      "National Journal Group",
      "The Conjure Woman",
      "New York Observer",
      "Boston Properties",
      "Ralph Waldo Emerson",
      "Ruth Shalit Barrett",
      "Ben Smith (journalist)",
      "Roe v. Wade",
      "Plagiarism",
      "Bliss Perry",
      "University of Maryland Libraries",
      "Paywall",
      "Ed Yong",
      "Nathaniel Hawthorne",
      "Workstation",
      "National Journal",
      "PBS",
      "Ion Hanford Perdicaris",
      "Vannevar Bush",
      "Charles Chesnutt",
      "Herman Melville",
      "Wayback Machine",
      "Efforts to impeach Donald Trump",
      "Mashable",
      "Visual storytelling",
      "Thomas Bailey Aldrich",
      "ISSN (identifier)",
      "The Washington Post",
      "Francis H. Underwood",
      "Slave narrative",
      "Wired (magazine)",
      "White Noise (2020 film)",
      "Letter from Birmingham Jail",
      "ISBN (identifier)",
      "Esquire (magazine)",
      "Henry Wadsworth Longfellow",
      "Public interest",
      "Pew Research Center",
      "William Parker (abolitionist)",
      "2020 United States presidential election",
      "Jessica Matthews",
      "Criticism of Scientology",
      "Julia Ward Howe",
      "Martin Luther King Jr.",
      "Charles W. Eliot",
      "Chicago Tribune",
      "Oliver Wendell Holmes Sr.",
      "Wikisource",
      "Hillary Clinton",
      "Long-form journalism",
      "Jonathan Rauch",
      "Blue Highways",
      "Barry Goldwater",
      "Bryan Singer",
      "Brian Stelter",
      "Derek Thompson (journalist)"
    ]
  },
  "Software engineering": {
    "url": "https://en.wikipedia.org/wiki/Software_engineering",
    "title": "Software engineering",
    "content": "Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications . [ 1 ] It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs. [ 2 ] [ 3 ] [ 4 ] [ 5 ] In the tech industry, the title software engineer is often used aspirationally, even though many such roles are fundamentally programming positions and lack the formal regulation associated with traditional engineering. [ 6 ] A software engineer applies a software development process , [ 2 ] [ 7 ] that involves defining, implementing, testing , managing , and maintaining software systems, as well as developing the software development process itself. Beginning in the 1960s, software engineering was recognized as a separate field of engineering . [ 8 ] The development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968, NATO organized the first conference on software engineering, which addressed emerging challenges in software development. The event played a key role in formalizing guidelines and best practices for creating reliable and maintainable software. [ 9 ] The origins of the term software engineering have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of \"Computers and Automation\" [ 10 ] and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) in \"President's Letter to the ACM Membership\" by Anthony A. Oettinger. [ 11 ] [ 12 ] [ 13 ] It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer . [ 14 ] Margaret Hamilton described the discipline of \"software engineering\" during the Apollo missions to give what they were doing legitimacy. [ 15 ] At the time, there was perceived to be a \" software crisis \". [ 16 ] [ 17 ] [ 18 ] The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Sessions' keynotes of Frederick Brooks [ 19 ] and Margaret Hamilton . [ 20 ] In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania , United States. [ 21 ] Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process. [ 21 ] The Process Maturity Levels introduced became the Capability Maturity Model Integration for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team. Modern, generally accepted best practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). [ 7 ] Software engineering is considered one of the major computing disciplines. [ 22 ] In modern systems, where concepts such as Edge Computing , Internet of Things and Cyber-physical Systems are prevalent, software is a critical factor. Thus, software engineering is closely related to the Systems Engineering discipline. The Systems Engineering Body of Knowledge claims: Software is prominent in most modern systems architectures and is often the primary means for integrating complex system components. Software engineering and systems engineering are not merely related disciplines; they are intimately intertwined....Good systems engineering is a key factor in enabling good software engineering. Notable definitions of software engineering include: \"An engineering discipline concerned with all aspects of software production.\" — Ian Sommerville [ 25 ] The term has also been used less formally: Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering. [ 32 ] [ 33 ] Steve McConnell has said that it is not, but that it should be. [ 34 ] Donald Knuth has said that programming is an art and a science. [ 35 ] Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused in the United States. [ 36 ] Requirements engineering is about elicitation, analysis, specification, and validation of requirements for software . Software requirements can be functional , non-functional or domain. Functional requirements describe expected behaviors (i.e. outputs). Non-functional requirements specify issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects. [ 37 ] Software design is the process of making high-level plans for the software. Design is sometimes divided into levels: Software construction typically involves programming (a.k.a. coding), unit testing , integration testing , and debugging so as to implement the design. [ 2 ] [ 7 ] \"Software testing is related to, but different from, ... debugging\". [ 7 ] Software testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the software under test. [ 2 ] [ 7 ] Software testing can be viewed as a risk based activity. When described separately from construction, testing typically is performed by test engineers or quality assurance instead of the programmers who wrote it. It is performed at the system level and is considered an aspect of software quality . The testers' goals during the testing process are to minimize the overall number of tests to a manageable set and make well-informed decisions regarding which risks should be prioritized for testing and which can wait. [ 39 ] Program analysis is the process of analyzing computer programs with respect to an aspect such as performance , robustness , and security . Software maintenance refers to supporting the software after release. It may include but is not limited to: error correction , optimization, deletion of unused and discarded features, and enhancement of existing features. [ 2 ] [ 7 ] Usually, maintenance takes up 40% to 80% of project cost. [ 40 ] Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004, the IEEE Computer Society produced the SWEBOK , which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience. [ 41 ] Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery , and updated in 2014. [ 22 ] A number of universities have Software Engineering degree programs; as of 2010 [update] , there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States. In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering. A small but growing number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering bachelor's degree in the world; in the following year, the University of Sheffield established a similar program. [ 42 ] In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States; however, it did not obtain ABET accreditation until 2003, the same year as Rice University , Clarkson University , Milwaukee School of Engineering , and Mississippi State University . [ 43 ] Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004 , was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society . As of 2004 [update] , about 50 universities in the U.S. offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering master's degree was established at Seattle University in 1979. Since then, graduate software engineering degrees have been made available from many more universities. Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs. Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton . Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers. [ 44 ] ETS (École de technologie supérieure) University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge ( SWEBOK ), which has become an ISO standard describing the body of knowledge covered by a software engineer. [ 7 ] Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, [ 45 ] and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society . In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized. [ 46 ] NCEES ended the exam after April 2019 due to lack of participation. [ 47 ] Mandatory licensing is currently still largely debated, and perceived as controversial. [ 48 ] [ 49 ] The IEEE Computer Society and the ACM , the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version , or SWEBOK , defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current version is SWEBOK v4. [ 7 ] The IEEE also promulgates a \"Software Engineering Code of Ethics\". [ 50 ] There are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016. [ 51 ] [ 52 ] Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers . Some organizations have specialists to perform each of the tasks in the software development process . Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns , often university or college students during a summer break, or externships . Specializations include analysts , architects , developers , testers , technical support , middleware analysts, project managers , software product managers , educators , and researchers . Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. [ 53 ] Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, Thrombosis , Obesity , and hand and wrist problems such as carpal tunnel syndrome . [ 54 ] The U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018. [ 55 ] Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees. [ 56 ] The BLS estimates 2024 to 2034 the growth for software engineers is 15% which is lesser than their prediction from 2023 to 2033 that computer software engineering would increase by 17%. [ 57 ] This is down from the 2022 to 2032 BLS estimate of 25% for software engineering. [ 57 ] [ 58 ] And, is further down from their 30% 2010 to 2020 BLS estimate. [ 59 ] Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries. [ 60 ] [ 53 ] In addition, the BLS Job Outlook for Computer Programmers, the U.S. Bureau of Labor Statistics (BLS) Occupational Outlook predicts a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031. [ 60 ] and then a decline of -11 percent from 2022 to 2032. [ 60 ] Currently their prediction for 2024 to 2034 is a decline of -6 percent. Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower. [ 60 ] [ 61 ] [ 62 ] Furthermore, the ratio of women in many software fields has also been declining over the years as compared to other engineering fields. [ 63 ] Then there is the additional concern that recent advances in Artificial Intelligence might impact the demand for future generations of Software Engineers. [ 64 ] [ 65 ] [ 66 ] [ 67 ] [ 68 ] [ 69 ] [ 70 ] However, this trend may change or slow in the future as many current software engineers in the U.S. market flee the profession or age out of the market in the next few decades. [ 60 ] [ 71 ] The Software Engineering Institute offers certifications on specific topics like security , process improvement and software architecture . [ 72 ] IBM , Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. [ 73 ] These certification programs are tailored to the institutions that would employ people who use these technologies. Broader certification of general software engineering skills is available through various professional societies. As of 2006 [update] , the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). [ 74 ] In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). [ 75 ] The ACM and the IEEE Computer Society together examined the possibility of licensing of software engineers as Professional Engineers in the 1990s, but eventually decided that such licensing was inappropriate for the professional industrial practice of software engineering. [ 48 ] John C. Knight and Nancy G. Leveson presented a more balanced analysis of the licensing issue in 2002. [ 49 ] In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP) , available to fully qualified members ( MBCS ). Software engineers may be eligible for membership of the British Computer Society or Institution of Engineering and Technology and so qualify to be considered for Chartered Engineer status through either of those institutions. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP) . [ 76 ] In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's ( Professional Engineers Ontario ) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng. [ 77 ] The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license. The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers . [ 78 ] Additionally, the glut of high-tech workers has led to a wider adoption of the 996 working hour system and ‘007’ schedules as the expected work load. [ 79 ] Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. [ 80 ] Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. [ 81 ] When North Americans leave work, Asians are just arriving to work. When Asians are leaving work, Europeans arrive to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns. While global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). [ 82 ] Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas. There are various prizes in the field of software engineering: Some call for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field. [ 86 ] Some claim that the concept of software engineering is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters. [ 87 ] Some claim that a core issue with software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a \"theoretical environment.\" [ 87 ] Edsger Dijkstra , a founder of many of the concepts in software development today, rejected the idea of \"software engineering\" up until his death in 2002, arguing that those terms were poor analogies for what he called the \"radical novelty\" of computer science : A number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\" [ 88 ]",
    "links": [
      "Microwave engineering",
      "Very-large-scale integration",
      "Unit testing",
      "European Engineer",
      "List of engineering journals and magazines",
      "Video game",
      "Artificial intelligence",
      "Computer network engineering",
      "Architectural engineering",
      "Information engineering",
      "Networking hardware",
      "Engineering physics",
      "Outline of the Python programming language",
      "Software Engineering Body of Knowledge",
      "Entity–relationship model",
      "Rendering (computer graphics)",
      "History of software engineering",
      "Multiprocessing",
      "Glossary of civil engineering",
      "Professional Engineer",
      "Lists of engineers",
      "Programming team",
      "Interaction design",
      "Computational geometry",
      "Industrial process control",
      "Bachelor's degree",
      "Backward compatibility",
      "Professional certification (computer technology)",
      "V-model (software development)",
      "List of discrete event simulation software",
      "Software framework",
      "Fault tolerance",
      "Chemical reaction engineering",
      "System on a chip",
      "Printed circuit board",
      "Digital marketing",
      "Stevens Award",
      "Margaret Hamilton (software engineer)",
      "Software verification and validation",
      "Signal processing",
      "Document management system",
      "Structured analysis",
      "Graphical user interface builder",
      "Engineering mathematics",
      "Information technology",
      "Integrated development environment",
      "Wireless sensor network",
      "Component-based software engineering",
      "Distributed computing",
      "E-commerce",
      "Computational biology",
      "List of engineering awards",
      "Program analysis",
      "Object model",
      "Knowledge representation and reasoning",
      "Detailed design document",
      "Information Technology Architect Certification",
      "List of finite element analysis software",
      "Database",
      "Reinforcement learning",
      "Concurrency (computer science)",
      "Performance engineering",
      "UQAM",
      "Hardware security",
      "Model-driven engineering",
      "Software archaeology",
      "Wernher von Braun",
      "Augmented reality",
      "Unsupervised learning",
      "Software incompatibility",
      "Form factor (design)",
      "Barry Boehm",
      "Model-driven development",
      "Electronic design automation",
      "Structural alignment software",
      "Network architecture",
      "Software testing",
      "Control theory",
      "Software bloat",
      "Internet of Things",
      "Bioinformatics",
      "Function model",
      "ACM SIGSOFT",
      "Cyber-physical system",
      "Object-oriented programming",
      "Modeling language",
      "Telecommunications engineering",
      "Bernd Bruegge",
      "Systems Engineering",
      "Stand-up meeting",
      "Foreign Worker Visa",
      "Comparison of optimization software",
      "Formal methods",
      "Cognitive systems engineering",
      "Computer security",
      "Sports engineering",
      "Comparison of EDA software",
      "List of numerical libraries",
      "Network scheduler",
      "ISO/IEC 15504",
      "Computational mathematics",
      "Clinical engineering",
      "Metallurgy",
      "Computing",
      "Broadcast engineering",
      "Electromechanics",
      "List of computational fluid dynamics software",
      "Fire protection engineering",
      "Surface engineering",
      "Facilities engineering",
      "Instrumentation engineering",
      "Building services engineering",
      "NCEES",
      "Carpal tunnel syndrome",
      "Rapid application development",
      "Seattle University",
      "Algorithm",
      "Watts Humphrey",
      "Release engineering",
      "Cybersecurity engineering",
      "Forensic engineering",
      "Electronic voting",
      "Mathematical software",
      "Friedrich L. Bauer",
      "US Department of Defense",
      "Algorithmic efficiency",
      "Artificial Intelligence",
      "ISO/IEC JTC 1/SC 7",
      "Debugger",
      "Open-source software",
      "Software craftsmanship",
      "Wayback Machine",
      "Outline of the Rust programming language",
      "Software",
      "Health technology",
      "Computational complexity",
      "ISSN (identifier)",
      "Explosives engineering",
      "Integrated circuit",
      "Comparison of nucleic acid simulation software",
      "Human–computer interaction",
      "Quantum computing",
      "ISBN (identifier)",
      "Mobile computing",
      "Software development process",
      "Requirements analysis",
      "Mathematical optimization",
      "Thermal engineering",
      "Intrusion detection system",
      "Statistics",
      "Offshore outsourcing",
      "ABET",
      "Test engineers",
      "List of computer science journals",
      "Engineering disciplines",
      "ACM SIGSOFT Software Engineering Notes",
      "Information system",
      "Interdisciplinarity",
      "Edsger Dijkstra",
      "Open access",
      "Bachelor of Engineering",
      "École de technologie supérieure",
      "System",
      "List of genetic engineering software",
      "Manufacturing engineering",
      "Marine engineering",
      "List of software for nanostructures modeling",
      "Enterprise systems engineering",
      "Information model",
      "Military engineering",
      "International Requirements Engineering Board",
      "S2CID (identifier)",
      "Capability Maturity Model",
      "Non-functional requirement",
      "Transportation engineering",
      "Design review",
      "List of software for nuclear engineering",
      "Outline of the C++ programming language",
      "Audio engineer",
      "Verification and validation",
      "Software developer",
      "Jolt Award",
      "Benjamin S. Blanchard",
      "List of HDL simulators",
      "Programming language theory",
      "Software maintenance",
      "System of systems engineering",
      "Computer animation",
      "Requirements engineering",
      "Microsoft Solutions Framework",
      "Configuration management",
      "Outline of computer engineering",
      "Bureau of Labor Statistics",
      "Thermodynamic computing",
      "Cryptography",
      "Concurrent computing",
      "Genetic engineering",
      "Compiler",
      "Image compression",
      "Geological engineering",
      "Risk management",
      "Educational technology",
      "Molecular design software",
      "Domain-specific language",
      "List of computer size categories",
      "Software quality",
      "Electrochemical engineering",
      "Software repository",
      "Nancy G. Leveson",
      "List of open-source libraries",
      "DevOps",
      "Comparison of system dynamics software",
      "Software configuration management",
      "Master's degree",
      "Specification by example",
      "Systems architect",
      "Security testing",
      "Acoustical engineering",
      "Data mining",
      "Software Engineering Institute",
      "Biomechanical engineering",
      "Operating system",
      "Agricultural engineering",
      "Site reliability engineering",
      "Corrosion engineering",
      "Offshore engineering",
      "Interpreter (computing)",
      "Molecular engineering",
      "Integration testing",
      "Systems development life cycle",
      "Team software process",
      "Software design",
      "Quality function deployment",
      "Patch (computing)",
      "Wind energy software",
      "Build automation",
      "Compatibility layer",
      "Personal software process",
      "Survey engineering",
      "Social computing",
      "Middleware",
      "Social software engineering",
      "Visualization (graphics)",
      "Aerospace engineering",
      "Petroleum engineering",
      "Geotechnical engineering",
      "Spiral model",
      "List of computer-aided engineering software",
      "Paper engineering",
      "List of automotive engineering software",
      "Forward compatibility",
      "System lifecycle",
      "Arthur David Hall III",
      "Information theory",
      "Margaret Hamilton (scientist)",
      "Control flow",
      "Supervised learning",
      "Executable UML",
      "Rice University",
      "Software prototyping",
      "Earth systems engineering and management",
      "Traffic engineering (transportation)",
      "Information security",
      "Machine learning",
      "Bachelor of Science in Information Technology",
      "Model of computation",
      "Numerical analysis",
      "Ceramic engineering",
      "Codification (law)",
      "Wiley (publisher)",
      "Real-time computing",
      "Disciplined agile delivery",
      "Software quality assurance",
      "Theory",
      "SE2004",
      "Systems engineering (disambiguation)",
      "List of gene prediction software",
      "Ian Sommerville (academic)",
      "Enterprise information system",
      "Association for Computing Machinery",
      "List of programming software development tools",
      "Naval architecture",
      "Automated planning and scheduling",
      "IEEE",
      "Hardware acceleration",
      "Incremental build model",
      "Multithreading (computer architecture)",
      "Process engineering",
      "Analysis of algorithms",
      "Software development methodology",
      "ACM Computing Classification System",
      "SEMAT",
      "Data structure",
      "Scrum (software development)",
      "Mechanical engineering",
      "ITIL",
      "Quality assurance",
      "Software deployment",
      "Materials engineering",
      "CI/CD",
      "Computer compatibility",
      "Robert E. Machol",
      "Packaging engineering",
      "Bachelor of Science",
      "Coursera",
      "Comparison of EM simulation software",
      "Geomatics engineering",
      "Probability",
      "Logistics engineering",
      "Application software",
      "British Computer Society",
      "Dependability",
      "Computational complexity theory",
      "Climate engineering",
      "Developed world",
      "Mississippi State University",
      "Freelancer",
      "Pharmaceutical engineering",
      "Biomedical engineering",
      "Theory of computation",
      "Biochemical engineering",
      "Outline of software engineering",
      "Agile software development",
      "Internship",
      "V-model",
      "Outline of computer science",
      "Behavior-driven development",
      "List of sequence alignment software",
      "Environmental engineering",
      "Importance",
      "Systems analysis",
      "Glossary of electrical and electronics engineering",
      "Systems modeling",
      "Test-driven development",
      "Automation engineering",
      "Civil engineering",
      "Compiler construction",
      "Structural engineering",
      "Food engineering",
      "Computer programming",
      "Outline of the Java programming language",
      "List of free electronics circuit simulators",
      "Virtual reality",
      "Computational social science",
      "ACM-AAAI Allen Newell Award",
      "Software crisis",
      "Robustness (computer science)",
      "Biological engineering",
      "Clarkson University",
      "Glossary of artificial intelligence",
      "List of bioinformatics software",
      "Polymer engineering",
      "Information retrieval",
      "Data engineering",
      "View model",
      "Semantics (computer science)",
      "Systems modeling language",
      "List of 3D printing software",
      "Discrete mathematics",
      "Lists of open-source artificial intelligence software",
      "Service-oriented architecture",
      "Harold Chestnut",
      "Data modeling",
      "Word processor",
      "Rochester Institute of Technology",
      "Carnegie Mellon",
      "SWEBOK",
      "Donald Knuth",
      "Legal",
      "Continuous integration",
      "Software product management",
      "Project management",
      "Biological systems engineering",
      "Radhika Nagpal",
      "Merriam-Webster",
      "Green computing",
      "Technical support",
      "Logic in computer science",
      "Mechatronics",
      "Simon Ramo",
      "Feature-driven development",
      "IEEE Software",
      "Computer vision",
      "Stochastic computing",
      "Kathleen Carley",
      "Edsger W. Dijkstra",
      "Chartered Engineer (UK)",
      "Microsoft Certified Professional",
      "Institution of Engineering and Technology",
      "Military service",
      "Edge Computing",
      "Algorithm design",
      "List of numerical analysis software",
      "Geographic information system",
      "List of protein structure prediction software",
      "Quality management",
      "Barbara Grosz",
      "Empirical software engineering",
      "Peripheral",
      "Multimedia database",
      "Enterprise software",
      "ISO",
      "Randomized algorithm",
      "Systems Engineering Body of Knowledge",
      "Software construction",
      "Dynamic systems development method",
      "Power engineering",
      "Nuclear engineering",
      "Katia Sycara",
      "Electronic publishing",
      "Infrastructure as code",
      "Scaled agile framework",
      "Outline of the C programming language",
      "List of data science software",
      "World Wide Web",
      "Rational unified process",
      "Canadian Information Processing Society",
      "Carnegie Mellon University",
      "Externship",
      "Mechatronics engineering",
      "Glossary",
      "Tissue engineering",
      "Engineering ethics",
      "BCS Lovelace Medal",
      "Textile engineering",
      "Work breakdown structure",
      "Operations research",
      "Capability Maturity Model Integration",
      "International Electrotechnical Commission",
      "List of computational chemistry software",
      "Unified process",
      "Design engineer",
      "Iterative and incremental development",
      "Manuela M. Veloso",
      "List of engineering societies",
      "Industrial engineering",
      "Biomaterial",
      "Application-release automation",
      "Programming tool",
      "Engineer",
      "Search-based software engineering",
      "Computer science",
      "Chemical engineering",
      "Photograph manipulation",
      "Power engineering software",
      "Virtual machine",
      "Google",
      "Optical engineering",
      "Unified Modeling Language",
      "Construction engineering",
      "Parallel computing",
      "Canadian Council of Professional Engineers",
      "Network security",
      "ISO 9001",
      "United States",
      "Software requirements",
      "Mathematical analysis",
      "Social software",
      "Thrombosis prevention",
      "Multi-task learning",
      "Comparison of software for molecular mechanics modeling",
      "User experience",
      "List of software programming journals",
      "U.S. Bureau of Labor Statistics",
      "Health informatics",
      "Age wave",
      "Communication protocol",
      "Interface design",
      "Mining engineering",
      "Profiling (computer programming)",
      "Functional Analysis and Allocation",
      "Joseph Francis Shea",
      "Regulation and licensure in engineering",
      "Computing platform",
      "Earthquake engineering",
      "Computer network",
      "List of structural engineering software",
      "Empirical research",
      "Municipal or urban engineering",
      "Software project management",
      "Enterprise architecture",
      "Business process",
      "Experimental software engineering",
      "Engineering design process",
      "Computational problem",
      "Ubiquitous computing",
      "List of software engineering conferences",
      "Radio-frequency engineering",
      "List of engineering branches",
      "Aspect-oriented programming",
      "Computer accessibility",
      "Glossary of structural engineering",
      "Coastal engineering",
      "Ontology engineering",
      "Distributed artificial intelligence",
      "Research",
      "Graphics processing unit",
      "John N. Warfield",
      "Information science",
      "IDEF",
      "Steve McConnell",
      "Researcher",
      "Safety engineering",
      "Human-centered computing",
      "Wolt Fabrycky",
      "Solid modeling",
      "Doi (identifier)",
      "Derek Hitchins",
      "Adaptive software development",
      "California State University, Fullerton",
      "Doctorate",
      "Health systems engineering",
      "Mechanical, electrical, and plumbing",
      "Security hacker",
      "Pittsburgh",
      "Privacy engineering",
      "Programming paradigm",
      "Computational physics",
      "Software documentation",
      "YouTube",
      "Outline of the JavaScript programming language",
      "Security engineering",
      "Materials science",
      "Railway engineering",
      "Debugging",
      "UML tool",
      "Cross-validation (statistics)",
      "Outline of the C sharp programming language",
      "Outline of software development",
      "List of RNA structure prediction software",
      "Follow-the-sun",
      "Software system",
      "Imperial College London",
      "James S. Albus",
      "Acceptance test-driven development",
      "Sustainability",
      "Lean software development",
      "List of building information modeling software",
      "Computability theory",
      "David Parnas",
      "List of computer-aided manufacturing software",
      "Metamodeling",
      "Processor (computing)",
      "Computational engineering",
      "Cyberwarfare",
      "Programming language",
      "Kanban (development)",
      "Tribology",
      "Software engineering professionalism",
      "Data model",
      "Sustainable engineering",
      "Automotive engineering",
      "Artificial intelligence engineering",
      "Electrical engineering",
      "Compatibility mode",
      "Software development",
      "Sanitary engineering",
      "Stanford torus",
      "Systems analyst",
      "Lists of engineering software",
      "Engineering",
      "Department of Computing, Imperial College London",
      "Robotics engineering",
      "Educator",
      "IT security",
      "Network service",
      "Computer graphics",
      "Nanotechnology",
      "Semiconductor device",
      "System dynamics",
      "NATO",
      "Automata theory",
      "Communications of the ACM",
      "Pair programming",
      "Energy engineering",
      "Embedded system",
      "Extreme programming",
      "River engineering",
      "Turing Award",
      "Computer hardware",
      "Computer architecture",
      "Glossary of aerospace engineering",
      "Abstraction (computer science)",
      "Computational chemistry",
      "Library (computing)",
      "Control engineering",
      "Continuous delivery",
      "Computer data storage",
      "Application security",
      "Glossary of engineering",
      "Outline of engineering",
      "Ruzena Bajcsy",
      "Natural language processing",
      "Computer engineering",
      "Domain-driven design",
      "Decision support system",
      "Graduate certificate",
      "Glossary of mechanical engineering",
      "Engineering management",
      "Brian Randell",
      "Hydraulic engineering",
      "Bioresource engineering",
      "Round-trip engineering",
      "Systems Modeling Language",
      "Pankaj Jalote",
      "Electronic engineering",
      "List of aerospace engineering software",
      "Formal language",
      "List of chemical process simulators",
      "List of engineering schools",
      "Computer program",
      "Bachelor of Software Engineering",
      "History of engineering",
      "Computer performance",
      "Cleanroom software engineering",
      "Systems engineering",
      "Project Management Body of Knowledge",
      "Instrumentation and control engineering",
      "Software engineering demographics",
      "Universal Systems Language",
      "996 working hour system",
      "Waterfall model",
      "IEEE Computer Society",
      "Software architecture",
      "Lists of programming software development tools",
      "Functional requirement",
      "Milwaukee School of Engineering",
      "Evans Data Corporation",
      "Obesity",
      "Programmer",
      "Enterprise unified process",
      "Glossary of computer science",
      "Decision-making",
      "Essential systems analysis",
      "Engineering education",
      "Reliability engineering",
      "System integration",
      "Rehabilitation engineering",
      "Electronics engineering",
      "Digital library",
      "Philosophy of artificial intelligence",
      "The Atlantic",
      "Frederick Brooks",
      "University of Sheffield",
      "Security service (telecommunication)",
      "Chichester",
      "Digital art",
      "Engineer's degree",
      "Object Management Group",
      "Engineering drawing",
      "Spare part",
      "IEEE Standards Association",
      "Functional specification",
      "Ontology (information science)",
      "System testing",
      "Network performance",
      "Requirements",
      "Certified Software Development Professional",
      "Theoretical computer science",
      "Ecological engineering"
    ]
  },
  "Terrier Search Engine": {
    "url": "https://en.wikipedia.org/wiki/Terrier_Search_Engine",
    "title": "Terrier Search Engine",
    "content": "The Terrier IR Platform is a modular open source software for the rapid development of large-scale information retrieval applications. Terrier was developed by members of the Information Retrieval Research Group , Department of Computing Science, at the University of Glasgow . A core version of Terrier is available as open source software under the Mozilla Public License (MPL), with the aim to facilitate experimentation and research in the wider information retrieval community. Terrier is written in Java . [ 1 ] This free and open-source software article is a stub . You can help Wikipedia by expanding it .",
    "links": [
      "University of Glasgow",
      "Free and open-source software",
      "Java (programming language)",
      "Mozilla Public License",
      "Information retrieval",
      "Open-source software"
    ]
  },
  "Information society": {
    "url": "https://en.wikipedia.org/wiki/Information_society",
    "title": "Information society",
    "content": "An information society is a society or subculture where the usage, creation , distribution , manipulation and integration of information is a significant activity. [ 1 ] [ 2 ] Its main drivers are information and communication technologies , which have resulted in rapid growth of a variety of forms of information. Proponents of this theory posit that these technologies are impacting most important forms of social organization, including education , economy , [ 3 ] health , government , [ 4 ] warfare , and levels of democracy . [ 5 ] The people who are able to partake in this form of society are sometimes called either computer users or even digital citizens , defined by K. Mossberger as “Those who use the Internet regularly and effectively”. This is one of many dozen internet terms that have been identified to suggest that humans are entering a new and different phase of society. [ 6 ] Some of the markers of this steady change may be technological, economic, occupational, spatial, cultural, or a combination of all of these. [ 7 ] Information society is seen as a successor to industrial society . Closely related concepts are the post-industrial society ( post-fordism ), post-modern society, computer society and knowledge society , telematic society, society of the spectacle ( postmodernism ), Information Revolution and Information Age , network society ( Manuel Castells ) or even liquid modernity . There is currently no universally accepted concept of what exactly can be defined as an information society and what shall not be included in the term. Most theoreticians agree that a transformation can be seen as started somewhere between the 1970s, the early 1990s transformations of the Eastern Bloc nations from socialist to capitalist economies and the 2000s period that formed most of today's net principles and currently as is changing the way societies work fundamentally. Information technology goes beyond the internet , as the principles of internet design and usage influence other areas, and there are discussions about how big the influence of specific media or specific modes of production really is. Frank Webster notes five major types of information that can be used to define information society: technological, economic, occupational, spatial and cultural. [ 7 ] According to Webster, the character of information has transformed the way that we live today. How we conduct ourselves centers around theoretical knowledge and information. [ 8 ] Kasiwulaya and Gomo (Makerere University) allude [ where? ] [ dubious – discuss ] that information societies are those that have intensified their use of IT for economic, social, cultural and political transformation. In 2005, governments reaffirmed their dedication to the foundations of the Information Society in the Tunis Commitment and outlined the basis for implementation and follow-up in the Tunis Agenda for the Information Society. In particular, the Tunis Agenda addresses the issues of financing of ICTs for development and Internet governance that could not be resolved in the first phase. Some people, such as Antonio Negri , characterize the information society as one in which people do immaterial labour. [ 9 ] By this, they appear to refer to the production of knowledge or cultural artifacts. One problem with this model is that it ignores the material and essentially industrial basis of the society. However it does point to a problem for workers, namely how many creative people does this society need to function? For example, it may be that you only need a few star performers, rather than a plethora of non-celebrities, as the work of those performers can be easily distributed, forcing all secondary players to the bottom of the market. It is now common for publishers to promote only their best selling authors and to try to avoid the rest—even if they still sell steadily. Films are becoming more and more judged, in terms of distribution, by their first weekend's performance, in many cases cutting out opportunity for word-of-mouth development. Michael Buckland characterizes information in society in his book Information and Society. Buckland expresses the idea that information can be interpreted differently from person to person based on that individual's experiences. [ 10 ] Considering that metaphors and technologies of information move forward in a reciprocal relationship, we can describe some societies (especially the Japanese society ) as an information society because we think of it as such. [ 11 ] [ 12 ] The word information may be interpreted in many different ways. According to Buckland in Information and Society , most of the meanings fall into three categories of human knowledge: information as knowledge, information as a process, and information as a thing. [ 13 ] Thus, the Information Society refers to the social importance given to communication and information in today's society, where social, economic and cultural relations are involved. [ 14 ] In the Information Society, the process of capturing, processing and communicating information is the main element that characterizes it. Thus, in this type of society, the vast majority of it will be dedicated to the provision of services and said services will consist of the processing, distribution or use of information. [ 14 ] The growth of the amount of technologically mediated information has been quantified in different ways, including society's technological capacity to store information, to communicate information, and to compute information. [ 17 ] It is estimated that, the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986, which is the informational equivalent to less than one 730-MB CD-ROM per person in 1986 (539 MB per person), to 295 (optimally compressed) exabytes in 2007. [ 18 ] This is the informational equivalent of 60 CD-ROM per person in 2007 [ 19 ] and represents a sustained annual growth rate of some 25%. The world's combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007. [ 18 ] The world's combined effective capacity to exchange information through two-way telecommunications networks was 281 petabytes of (optimally compressed) information in 1986, 471 petabytes in 1993, 2.2 (optimally compressed) exabytes in 2000, and 65 (optimally compressed) exabytes in 2007, which is the informational equivalent of 6 newspapers per person per day in 2007. [ 19 ] The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 10^8 MIPS in 1986, to 6.4 x 10^12 MIPS in 2007, experiencing the fastest growth rate of over 60% per year during the last two decades. [ 18 ] James R. Beniger describes the necessity of information in modern society in the following way: “The need for sharply increased control that resulted from the industrialization of material processes through application of inanimate sources of energy probably accounts for the rapid development of automatic feedback technology in the early industrial period (1740-1830)” (p. 174) “Even with enhanced feedback control, industry could not have developed without the enhanced means to process matter and energy, not only as inputs of the raw materials of production but also as outputs distributed to final consumption.”(p. 175) [ 6 ] One of the first people to develop the concept of the information society was the economist Fritz Machlup . In 1933, Fritz Machlup began studying the effect of patents on research. His work culminated in the study The production and distribution of knowledge in the United States in 1962. This book was widely regarded [ 20 ] and was eventually translated into Russian and Japanese . The Japanese have also studied the information society (or jōhōka shakai , 情報化社会 ). The issue of technologies and their role in contemporary society have been discussed in the scientific literature using a range of labels and concepts. This section introduces some of them. Ideas of a knowledge or information economy , post-industrial society , postmodern society, network society , the information revolution , informational capitalism, network capitalism, and the like, have been debated over the last several decades. Fritz Machlup (1962) introduced the concept of the knowledge industry . He began studying the effects of patents on research before distinguishing five sectors of the knowledge sector: education, research and development, mass media, information technologies, information services. Based on this categorization he calculated that in 1959 29% per cent of the GNP in the USA had been produced in knowledge industries. [ 21 ] [ 22 ] [ citation needed ] Peter Drucker has argued that there is a transition from an economy based on material goods to one based on knowledge. [ 23 ] Marc Porat distinguishes a primary (information goods and services that are directly used in the production, distribution or processing of information) and a secondary sector (information services produced for internal consumption by government and non-information firms) of the information economy. [ 24 ] Porat uses the total value added by the primary and secondary information sector to the GNP as an indicator for the information economy. The OECD has employed Porat's definition for calculating the share of the information economy in the total economy (e.g. OECD 1981, 1986). Based on such indicators, the information society has been defined as a society where more than half of the GNP is produced and more than half of the employees are active in the information economy. [ 25 ] For Daniel Bell the number of employees producing services and information is an indicator for the informational character of a society. \"A post-industrial society is based on services. (…) What counts is not raw muscle power, or energy, but information. (…) A post industrial society is one in which the majority of those employed are not involved in the production of tangible goods\". [ 26 ] Alain Touraine already spoke in 1971 of the post-industrial society. \"The passage to postindustrial society takes place when investment results in the production of symbolic goods that modify values, needs, representations, far more than in the production of material goods or even of 'services'. Industrial society had transformed the means of production: post-industrial society changes the ends of production, that is, culture. (…) The decisive point here is that in postindustrial society all of the economic system is the object of intervention of society upon itself. That is why we can call it the programmed society, because this phrase captures its capacity to create models of management, production, organization, distribution, and consumption, so that such a society appears, at all its functional levels, as the product of an action exercised by the society itself, and not as the outcome of natural laws or cultural specificities\" (Touraine 1988: 104). In the programmed society also the area of cultural reproduction including aspects such as information, consumption, health, research, education would be industrialized. That modern society is increasing its capacity to act upon itself means for Touraine that society is reinvesting ever larger parts of production and so produces and transforms itself. This makes Touraine's concept substantially different from that of Daniel Bell who focused on the capacity to process and generate information for efficient society functioning. Jean-François Lyotard [ 27 ] has argued that \"knowledge has become the principle [ sic ] force of production over the last few decades\". Knowledge would be transformed into a commodity. Lyotard says that postindustrial society makes knowledge accessible to the layman because knowledge and information technologies would diffuse into society and break up Grand Narratives of centralized structures and groups. Lyotard denotes these changing circumstances as postmodern condition or postmodern society. Similarly to Bell, Peter Otto and Philipp Sonntag (1985) say that an information society is a society where the majority of employees work in information jobs, i.e. they have to deal more with information, signals, symbols, and images than with energy and matter. Radovan Richta (1977) argues that society has been transformed into a scientific civilization based on services, education, and creative activities. This transformation would be the result of a scientific-technological transformation based on technological progress and the increasing importance of computer technology. Science and technology would become immediate forces of production (Aristovnik 2014: 55). Nico Stehr (1994, 2002a, b) says that in the knowledge society a majority of jobs involves working with knowledge. \"Contemporary society may be described as a knowledge society based on the extensive penetration of all its spheres of life and institutions by scientific and technological knowledge\" (Stehr 2002b: 18). For Stehr, knowledge is a capacity for social action. Science would become an immediate productive force, knowledge would no longer be primarily embodied in machines, but already appropriated nature that represents knowledge would be rearranged according to certain designs and programs (Ibid.: 41-46). For Stehr, the economy of a knowledge society is largely driven not by material inputs, but by symbolic or knowledge-based inputs (Ibid.: 67), there would be a large number of professions that involve working with knowledge, and a declining number of jobs that demand low cognitive skills as well as in manufacturing (Stehr 2002a). Also Alvin Toffler argues that knowledge is the central resource in the economy of the information society: \"In a Third Wave economy, the central resource – a single word broadly encompassing data, information, images, symbols, culture, ideology, and values – is actionable knowledge\" (Dyson/Gilder/Keyworth/Toffler 1994). At the end of the twentieth century, the concept of the network society gained importance in information society theory. For Manuel Castells , network logic is besides information, pervasiveness, flexibility, and convergence a central feature of the information technology paradigm (2000a: 69ff). \"One of the key features of informational society is the networking logic of its basic structure, which explains the use of the concept of 'network society'\" (Castells 2000: 21). \"As an historical trend, dominant functions and processes in the Information Age are increasingly organized around networks. Networks constitute the new social morphology of our societies, and the diffusion of networking logic substantially modifies the operation and outcomes in processes of production, experience, power, and culture\" (Castells 2000: 500). For Castells the network society is the result of informationalism, a new technological paradigm. Jan Van Dijk (2006) defines the network society as a \"social formation with an infrastructure of social and media networks enabling its prime mode of organization at all levels (individual, group/organizational and societal). Increasingly, these networks link all units or parts of this formation (individuals, groups and organizations)\" (Van Dijk 2006: 20). For Van Dijk networks have become the nervous system of society, whereas Castells links the concept of the network society to capitalist transformation, Van Dijk sees it as the logical result of the increasing widening and thickening of networks in nature and society. Darin Barney uses the term for characterizing societies that exhibit two fundamental characteristics: \"The first is the presence in those societies of sophisticated – almost exclusively digital – technologies of networked communication and information management/distribution, technologies which form the basic infrastructure mediating an increasing array of social, political and economic practices. (…) The second, arguably more intriguing, characteristic of network societies is the reproduction and institutionalization throughout (and between) those societies of networks as the basic form of human organization and relationship across a wide range of social, political and economic configurations and associations\". [ 28 ] The major critique of concepts such as information society, postmodern society, knowledge society, network society, postindustrial society, etc. that has mainly been voiced by critical scholars is that they create the impression that we have entered a completely new type of society. \"If there is just more information then it is hard to understand why anyone should suggest that we have before us something radically new\" (Webster 2002a: 259). Critics such as Frank Webster argue that these approaches stress discontinuity, as if contemporary society had nothing in common with society as it was 100 or 150 years ago. Such assumptions would have ideological character because they would fit with the view that we can do nothing about change and have to adapt to existing political realities (kasiwulaya 2002b: 267). These critics argue that contemporary society first of all is still a capitalist society oriented towards accumulating economic, political, and cultural capital . They acknowledge that information society theories stress some important new qualities of society (notably globalization and informatization), but charge that they fail to show that these are attributes of overall capitalist structures. Critics such as Webster insist on the continuities that characterise change. In this way Webster distinguishes between different epochs of capitalism: laissez-faire capitalism of the 19th century, corporate capitalism in the 20th century, and informational capitalism for the 21st century (kasiwulaya 2006). For describing contemporary society based on a new dialectic of continuity and discontinuity, other critical scholars have suggested several terms like: Other scholars prefer to speak of information capitalism (Morris-Suzuki 1997) or informational capitalism ( Manuel Castells 2000, Christian Fuchs 2005, Schmiede 2006a, b). Manuel Castells sees informationalism as a new technological paradigm (he speaks of a mode of development) characterized by \"information generation, processing, and transmission\" that have become \"the fundamental sources of productivity and power\" (Castells 2000: 21). The \"most decisive historical factor accelerating, channelling and shaping the information technology paradigm, and inducing its associated social forms, was/is the process of capitalist restructuring undertaken since the 1980s, so that the new techno-economic system can be adequately characterized as informational capitalism\" (Castells 2000: 18). Castells has added to theories of the information society the idea that in contemporary society dominant functions and processes are increasingly organized around networks that constitute the new social morphology of society (Castells 2000: 500). Nicholas Garnham [ 31 ] is critical of Castells and argues that the latter's account is technologically determinist because Castells points out that his approach is based on a dialectic of technology and society in which technology embodies society and society uses technology (Castells 2000: 5sqq). But Castells also makes clear that the rise of a new \"mode of development\" is shaped by capitalist production, i.e. by society, which implies that technology isn't the only driving force of society. Antonio Negri and Michael Hardt argue that contemporary society is an Empire that is characterized by a singular global logic of capitalist domination that is based on immaterial labour. With the concept of immaterial labour Negri and Hardt introduce ideas of information society discourse into their Marxist account of contemporary capitalism. Immaterial labour would be labour \"that creates immaterial products, such as knowledge, information, communication, a relationship, or an emotional response\" (Hardt/Negri 2005: 108; cf. also 2000: 280-303), or services, cultural products, knowledge (Hardt/Negri 2000: 290). There would be two forms: intellectual labour that produces ideas, symbols, codes, texts, linguistic figures, images, etc.; and affective labour that produces and manipulates affects such as a feeling of ease, well-being, satisfaction, excitement, passion, joy, sadness, etc. (Ibid.). Overall, neo-Marxist accounts of the information society have in common that they stress that knowledge, information technologies, and computer networks have played a role in the restructuration and globalization of capitalism and the emergence of a flexible regime of accumulation ( David Harvey 1989). They warn that new technologies are embedded into societal antagonisms that cause structural unemployment , rising poverty, social exclusion , the deregulation of the welfare state and of labour rights , the lowering of wages, welfare, etc. Concepts such as knowledge society, information society, network society, informational capitalism, postindustrial society, transnational network capitalism, postmodern society, etc. show that there is a vivid discussion in contemporary sociology on the character of contemporary society and the role that technologies, information, communication, and co-operation play in it. [ citation needed ] Information society theory discusses the role of information and information technology in society, the question which key concepts shall be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology. Information society is the means of sending and receiving information from one place to another. [ 32 ] As technology has advanced so too has the way people have adapted in sharing information with each other. \"Second nature\" refers a group of experiences that get made over by culture. [ 33 ] They then get remade into something else that can then take on a new meaning. As a society we transform this process so it becomes something natural to us, i.e. second nature. So, by following a particular pattern created by culture we are able to recognise how we use and move information in different ways. From sharing information via different time zones (such as talking online) to information ending up in a different location (sending a letter overseas) this has all become a habitual process that we as a society take for granted. [ 34 ] However, through the process of sharing information vectors have enabled us to spread information even further. Through the use of these vectors information is able to move and then separate from the initial things that enabled them to move. [ 35 ] From here, something called \"third nature\" has developed. An extension of second nature, third nature is in control of second nature. It expands on what second nature is limited by. It has the ability to mould information in new and different ways. So, third nature is able to ‘speed up, proliferate, divide, mutate, and beam in on us from elsewhere. [ 36 ] It aims to create a balance between the boundaries of space and time (see second nature). This can be seen through the telegraph, it was the first successful technology that could send and receive information faster than a human being could move an object. [ 37 ] As a result different vectors of people have the ability to not only shape culture but create new possibilities that will ultimately shape society. Therefore, through the use of second nature and third nature society is able to use and explore new vectors of possibility where information can be moulded to create new forms of interaction. [ 38 ] In sociology , informational society refers to a post-modern type of society. Theoreticians like Ulrich Beck , Anthony Giddens and Manuel Castells argue that since the 1970s a transformation from industrial society to informational society has happened on a global scale. [ 40 ] As steam power was the technology standing behind industrial society, so information technology is seen as the catalyst for the changes in work organisation, societal structure and politics occurring in the late 20th century. In the book Future Shock , Alvin Toffler used the phrase super-industrial society to describe this type of society. Other writers and thinkers have used terms like \" post-industrial society \" and \"post-modern industrial society\" with a similar meaning. A number of terms in current use emphasize related but different aspects of the emerging global economic order. The Information Society intends to be the most encompassing in that an economy is a subset of a society. The Information Age is somewhat limiting, in that it refers to a 30-year period between the widespread use of computers and the knowledge economy , rather than an emerging economic order. The knowledge era is about the nature of the content, not the socioeconomic processes by which it will be traded. The computer revolution , and knowledge revolution refer to specific revolutionary transitions, rather than the end state towards which we are evolving. The Information Revolution relates with the well-known terms agricultural revolution and Industrial Revolution . One of the central paradoxes of the information society is that it makes information easily reproducible, leading to a variety of freedom/control problems relating to intellectual property . Essentially, business and capital, whose place becomes that of producing and selling information and knowledge, seems to require control over this new resource so that it can effectively be managed and sold as the basis of the information economy. However, such control can prove to be both technically and socially problematic. Technically because copy protection is often easily circumvented and socially rejected because the users and citizens of the information society can prove to be unwilling to accept such absolute commodification of the facts and information that compose their environment. Responses to this concern range from the Digital Millennium Copyright Act in the United States (and similar legislation elsewhere) which make copy protection (see Digital rights management ) circumvention illegal, to the free software , open source and copyleft movements, which seek to encourage and disseminate the \"freedom\" of various information products (traditionally both as in \"gratis\" or free of cost, and liberty, as in freedom to use, explore and share). Caveat: Information society is often used by politicians meaning something like \"we all do internet now\"; the sociological term information society (or informational society) has some deeper implications about change of societal structure. Because we lack political control of intellectual property, we are lacking in a concrete map of issues, an analysis of costs and benefits, and functioning political groups that are unified by common interests representing different opinions of this diverse situation that are prominent in the information society. [ 42 ]",
    "links": [
      "The Control Revolution",
      "Information access",
      "Categorization",
      "Vilém Flusser",
      "The Society of the Spectacle",
      "Doi (identifier)",
      "Eastern Bloc",
      "Information society (disambiguation)",
      "Information distribution",
      "Post-communism",
      "John Bellamy Foster",
      "Lev Manovich",
      "Economy",
      "Telecommunications network",
      "S2CID (identifier)",
      "Labour rights",
      "George Keyworth",
      "Deregulation",
      "Knowledge market",
      "Information retrieval",
      "Knowledge organization",
      "Information seeking",
      "YouTube",
      "International Telecommunication Union",
      "Science and technology studies",
      "Memory",
      "Netizen",
      "Society",
      "Welfare state",
      "Jean-François Lyotard",
      "World-Wide Web",
      "Privacy",
      "Digitization",
      "Copy protection",
      "Data modeling",
      "Digital Millennium Copyright Act",
      "CD-ROM",
      "Knowledge society",
      "Daniel Bell",
      "Post-fordism",
      "Information pollution",
      "Wolfgang Fritz Haug",
      "Post-industrial society",
      "Cyberspace",
      "Preservation (library and archival science)",
      "Information technology",
      "Ellen Meiksins Wood",
      "Social exclusion",
      "Alvin Toffler",
      "Bibcode (identifier)",
      "Social networking",
      "Taxonomy",
      "Electronic commerce",
      "Digital citizen",
      "Darin Barney",
      "Antonio Negri",
      "Hygiene",
      "Michael Hardt",
      "Digital addict",
      "Internet economy",
      "Censorship",
      "Liquid modernity",
      "Outline of information science",
      "Tunis Commitment",
      "Alain Touraine",
      "Warfare",
      "Information culture",
      "Russian language",
      "The Information Society",
      "Information Age",
      "Ulrich Beck",
      "Open source",
      "Information integration",
      "Steam power",
      "Unorganisation",
      "George Gilder",
      "PMID (identifier)",
      "Information Revolution",
      "Nico Stehr",
      "Content (media and publishing)",
      "Science (journal)",
      "Estonia",
      "Martin Hilbert",
      "Computer data storage",
      "Marc Porat",
      "Christian Fuchs (sociologist)",
      "Library classification",
      "Future Shock",
      "Quaternary sector of the economy",
      "Baltic country",
      "Colin Clark (economist)",
      "Jan Van Dijk",
      "World Summit on the Information Society",
      "Copyleft",
      "Internet culture",
      "Information industry",
      "OCLC (identifier)",
      "Digital transformation",
      "Sic",
      "Knowledge economy",
      "Information architecture",
      "Information history",
      "Postmodernism",
      "Content creation",
      "Europe",
      "Digital dark age",
      "Petabytes",
      "Anthony Giddens",
      "Information behavior",
      "Information economy",
      "Information revolution",
      "Industrial society",
      "Robert W. McChesney",
      "Peter Drucker",
      "Exabytes",
      "Internet",
      "Digital phobic",
      "Postmodernity",
      "OECD",
      "Yoneji Masuda",
      "Quantum information science",
      "Electronic business",
      "Philosophy of information",
      "Network society",
      "Simon Buckingham",
      "Affective labor",
      "Structural unemployment",
      "James R. Beniger",
      "David Harvey (geographer)",
      "Library and information science",
      "Corporate capitalism",
      "Intellectual property",
      "Cultural studies",
      "Radovan Richta",
      "Frank Webster (sociologist)",
      "Knowledge management",
      "Computer users",
      "Digital rights management",
      "Japanese language",
      "Broadcast",
      "Nicholas Garnham",
      "The Social Life of Information",
      "Informatics",
      "Manuel Castells",
      "Wayback Machine",
      "Industrial Revolution",
      "Health",
      "Fritz Machlup",
      "James Boyle (academic)",
      "Computer revolution",
      "ISSN (identifier)",
      "Government",
      "Education",
      "Leonid Grinin",
      "Wired (magazine)",
      "Japanese society",
      "Cultural capital",
      "ISBN (identifier)",
      "Commodification",
      "Information ecology",
      "Social Age",
      "John Seely Brown",
      "Intellectual freedom",
      "Free software",
      "Bibliometrics",
      "Ralf Dahrendorf",
      "Information and communication technologies",
      "Peter Glotz",
      "Michael Buckland",
      "PMC (identifier)",
      "Sociology",
      "Esther Dyson",
      "Ontology (information science)",
      "Digital economy",
      "Surveillance capitalism",
      "Information",
      "Subculture",
      "Information management",
      "Information science",
      "Democracy"
    ]
  },
  "C. J. van Rijsbergen": {
    "url": "https://en.wikipedia.org/wiki/C._J._van_Rijsbergen",
    "title": "C. J. van Rijsbergen",
    "content": "C. J. \"Keith\" van Rijsbergen FREng ( Cornelis Joost van Rijsbergen ; born 1943) [ 1 ] is a professor of computer science at the University of Glasgow , where he founded the Glasgow Information Retrieval Group. [ 2 ] He is one of the founders of modern Information Retrieval and the author of the seminal monograph Information Retrieval and of the textbook The Geometry of Information Retrieval . He was born in Rotterdam, and educated in the Netherlands, Indonesia, Namibia and Australia. His first degree is in mathematics from the University of Western Australia , and in 1972 he completed a PhD in computer science at the University of Cambridge . He spent three years lecturing in information retrieval and artificial intelligence at Monash University [ 1 ] before returning to Cambridge to hold a Royal Society Information Research Fellowship. In 1980 he was appointed to the chair of computer science at University College Dublin ; from there he moved in 1986 to Glasgow University . He chaired the Scientific Board of the Information Retrieval Facility from 2007 to 2012. In 2003 he was inducted as a Fellow of the Association for Computing Machinery . In 2004 he was awarded the Tony Kent Strix award . In 2004 he was appointed a Fellow of the Royal Academy of Engineering . [ 3 ] In 2006, he was awarded the Gerard Salton Award for Quantum haystacks . In 2009, he was made an honorary professor at the University of Edinburgh . [ 3 ] This article about a Dutch scientist is a stub . You can help Wikipedia by expanding it . This biographical article relating to a computer scientist is a stub . You can help Wikipedia by expanding it .",
    "links": [
      "University of Western Australia",
      "FREng",
      "Glasgow University",
      "Honorary title (academic)",
      "Royal Academy of Engineering",
      "Wayback Machine",
      "University of Glasgow",
      "University College Dublin",
      "Information Retrieval",
      "Tony Kent Strix award",
      "University of Edinburgh",
      "Information Retrieval Facility",
      "Association for Computing Machinery",
      "Computer scientist",
      "Computer science",
      "Gerard Salton Award",
      "Monash University",
      "Royal Society",
      "F1 score",
      "University of Cambridge",
      "Fellow"
    ]
  },
  "Taxonomy": {
    "url": "https://en.wikipedia.org/wiki/Taxonomy",
    "title": "Taxonomy",
    "content": "Taxonomy is a practice and science concerned with classification or categorization. Typically, there are two parts to it: the development of an underlying scheme of classes (a taxonomy) and the allocation of things to the classes ( classification ). Originally, taxonomy referred only to the classification of organisms on the basis of shared characteristics. Today it also has a more general sense. It may refer to the classification of things or concepts, as well as to the principles underlying such work. Thus a taxonomy can be used to organize species, documents, videos or anything else. A taxonomy organizes taxonomic units known as \"taxa\" (singular \"taxon\"). Many are hierarchies . One function of a taxonomy is to help users more easily find what they are searching for. This may be effected in ways that include a library classification system and a search engine taxonomy . Though often used interchangeably, taxonomies are distinct from typologies in that the former are concerned with empirical and objective characteristics, while the latter are concerned with abstract or subjective criteria. [ 1 ] The word was coined in 1813 by the Swiss botanist A. P. de Candolle and is irregularly compounded from the Greek τάξις , taxis 'order' and νόμος , nomos 'law', connected by the French form -o- ; the regular form would be taxinomy , as used in the Greek reborrowing ταξινομία . [ 2 ] [ 3 ] Wikipedia categories form a taxonomy, [ 4 ] which can be extracted by automatic means. [ 5 ] As of 2009 [update] , it has been shown that a manually-constructed taxonomy, such as that of computational lexicons like WordNet , can be used to improve and restructure the Wikipedia category taxonomy. [ 6 ] In a broader sense, taxonomy also applies to relationship schemes other than parent-child hierarchies, such as network structures . Taxonomies may then include a single child with multi-parents, for example, \"Car\" might appear with both parents \"Vehicle\" and \"Steel Mechanisms\"; to some however, this merely means that 'car' is a part of several different taxonomies. [ 7 ] A taxonomy might also simply be organization of kinds of things into groups, or an alphabetical list; here, however, the term vocabulary is more appropriate. In current usage within knowledge management , taxonomies are considered narrower than ontologies since ontologies apply a larger variety of relation types. [ 8 ] Mathematically, a hierarchical taxonomy is a tree structure of classifications for a given set of objects. It is also named containment hierarchy . At the top of this structure is a single classification, the root node, that applies to all objects. Nodes below this root are more specific classifications that apply to subsets of the total set of classified objects. The progress of reasoning proceeds from the general to the more specific. By contrast, in the context of legal terminology, an open-ended contextual taxonomy is employed—a taxonomy holding only with respect to a specific context. In scenarios taken from the legal domain, a formal account of the open-texture of legal terms is modeled, which suggests varying notions of the \"core\" and \"penumbra\" of the meanings of a concept. The progress of reasoning proceeds from the specific to the more general. [ 9 ] Anthropologists have observed that taxonomies are generally embedded in local cultural and social systems, and serve various social functions. Perhaps the most well-known and influential study of folk taxonomies is Émile Durkheim 's The Elementary Forms of Religious Life . A more recent treatment of folk taxonomies (including the results of several decades of empirical research) and the discussion of their relation to the scientific taxonomy can be found in Scott Atran 's Cognitive Foundations of Natural History. Folk taxonomies of organisms have been found in large part to agree with scientific classification, at least for the larger and more obvious species, which means that it is not the case that folk taxonomies are based purely on utilitarian characteristics. [ 10 ] In the seventeenth century, the German mathematician and philosopher Gottfried Leibniz , following the work of the thirteenth-century Majorcan philosopher Ramon Llull on his Ars generalis ultima , a system for procedurally generating concepts by combining a fixed set of ideas, sought to develop an alphabet of human thought . Leibniz intended his characteristica universalis to be an \"algebra\" capable of expressing all conceptual thought. The concept of creating such a \" universal language \" was frequently examined in the 17th century, also notably by the English philosopher John Wilkins in his work An Essay towards a Real Character and a Philosophical Language (1668), from which the classification scheme in Roget 's Thesaurus ultimately derives. Taxonomy in biology encompasses the description, identification, nomenclature, and classification of organisms. Uses of taxonomy include: Uses of taxonomy in business and economics include: Vegas et al. [ 11 ] make a compelling case to advance the knowledge in the field of software engineering through the use of taxonomies. Similarly, Ore et al. [ 12 ] provide a systematic methodology to approach taxonomy building in software engineering related topics. Several taxonomies have been proposed in software testing research to classify techniques, tools, concepts and artifacts. The following are some example taxonomies: Engström et al. [ 15 ] suggest and evaluate the use of a taxonomy to bridge the communication between researchers and practitioners engaged in the area of software testing. They have also developed a web-based tool [ 16 ] to facilitate and encourage the use of the taxonomy. The tool and its source code are available for public use. [ 17 ] Uses of taxonomy in education and academia include: Uses of taxonomy in safety include: Websites with a well designed taxonomy or hierarchy are easily understood by users, due to the possibility of users developing a mental model of the site structure. [ 18 ] Guidelines for writing taxonomy for the web include: Frederick Suppe [ 19 ] distinguished two senses of classification: a broad meaning, which he called \"conceptual classification\" and a narrow meaning, which he called \"systematic classification\". About conceptual classification Suppe wrote: [ 19 ] : 292 \"Classification is intrinsic to the use of language, hence to most if not all communication. Whenever we use nominative phrases we are classifying the designated subject as being importantly similar to other entities bearing the same designation; that is, we classify them together. Similarly the use of predicative phrases classifies actions or properties as being of a particular kind. We call this conceptual classification, since it refers to the classification involved in conceptualizing our experiences and surroundings\" About systematic classification Suppe wrote: [ 19 ] : 292 \"A second, narrower sense of classification is the systematic classification involved in the design and utilization of taxonomic schemes such as the biological classification of animals and plants by genus and species. Two of the predominant types of relationships in knowledge-representation systems are predication and the universally quantified conditional . Predication relationships express the notion that an individual entity is an example of a certain type (for example, John is a bachelor ), while universally quantified conditionals express the notion that a type is a subtype of another type (for example, \" A dog is a mammal\" , which means the same as \" All dogs are mammals\" ). [ 20 ] The \"has-a\" relationship is quite different: an elephant has a trunk; a trunk is a part, not a subtype of elephant. The study of part-whole relationships is mereology . Taxonomies are often represented as is-a hierarchies where each level is more specific than the level above it (in mathematical language is \"a subset of\" the level above). For example, a basic biology taxonomy would have concepts such as mammal , which is a subset of animal , and dogs and cats , which are subsets of mammal . This kind of taxonomy is called an is-a model because the specific objects are considered as instances of a concept. For example, Fido is-an instance of the concept dog and Fluffy is-a cat . [ 21 ] In linguistics , is-a relations are called hyponymy . When one word describes a category, but another describe some subset of that category, the larger term is called a hypernym with respect to the smaller, and the smaller is called a \"hyponym\" with respect to the larger. Such a hyponym, in turn, may have further subcategories for which it is a hypernym. In the simple biology example, dog is a hypernym with respect to its subcategory collie , which in turn is a hypernym with respect to Fido which is one of its hyponyms. Typically, however, hypernym is used to refer to subcategories rather than single individuals. Researchers reported that large populations consistently develop highly similar category systems. This may be relevant to lexical aspects of large communication networks and cultures such as folksonomies and language or human communication, and sense-making in general. [ 22 ] [ 23 ] Hull (1998) suggested \"The fundamental elements of any classification are its theoretical commitments, basic units and the criteria for ordering these basic units into a classification\". [ 24 ] There is a widespread opinion in knowledge organization and related fields that such classes corresponds to concepts. We can, for example, classify \"waterfowls\" into the classes \"ducks\", \"geese\", and \"swans\"; we can also say, however, that the concept \"waterfowl\" is a generic broader term in relation to the concepts \"ducks\", \"geese\", and \"swans\". This example demonstrates the close relationship between classification theory and concept theory. A main opponent of concepts as units is Barry Smith. [ 25 ] Arp, Smith and Spear (2015) discuss ontologies and criticize the conceptualist understanding. [ 26 ] : 5ff The book writes (7): “The code assigned to France, for example, is ISO 3166 – 2:FR and the code is assigned to France itself — to the country that is otherwise referred to as Frankreich or Ranska. It is not assigned to the concept of France (whatever that might be).” Smith's alternative to concepts as units is based on a realist orientation, when scientists make successful claims about the types of entities that exist in reality, they are referring to objectively existing entities which realist philosophers call universals or natural kinds. Smith's main argument - with which many followers of the concept theory agree - seems to be that classes cannot be determined by introspective methods, but must be based on scientific and scholarly research. Whether units are called concepts or universals, the problem is to decide when a thing (say a \"blackbird\") should be considered a natural class. In the case of blackbirds, for example, recent DNA analysis have reconsidered the concept (or universal) \"blackbird\" and found that what was formerly considered one species (with subspecies) are in reality many different species, which just have chosen similar characteristics to adopt to their ecological niches. [ 27 ] : 141 An important argument for considering concepts the basis of classification is that concepts are subject to change and that they change when scientific revolutions occur. Our concepts of many birds, for example, have changed with recent development in DNA analysis and the influence of the cladistic paradigm - and have demanded new classifications. Smith's example of France demands an explanation. First, France is not a general concept, but an individual concept. Next, the legal definition of France is determined by the conventions that France has made with other countries. It is still a concept, however, as Leclercq (1978) demonstrates with the corresponding concept Europe . [ 28 ] Hull (1998) continued: [ 24 ] \"Two fundamentally different sorts of classification are those that reflect structural organization and those that are systematically related to historical development.\" What is referred to is that in biological classification the anatomical traits of organisms is one kind of classification, the classification in relation to the evolution of species is another (in the section below, we expand these two fundamental sorts of classification to four). Hull adds that in biological classification, evolution supplies the theoretical orientation. [ 24 ] Ereshefsky (2000) presented and discussed three general philosophical schools of classification: \"essentialism, cluster analysis, and historical classification. Essentialism sorts entities according to causal relations rather than their intrinsic qualitative features.\" [ 29 ] These three categories may, however, be considered parts of broader philosophies. Four main approaches to classification may be distinguished: (1) logical and rationalist approaches including \"essentialism\"; (2) empiricist approaches including cluster analysis. (It is important to notice that empiricism is not the same as empirical study, but a certain ideal of doing empirical studies. With the exception of the logical approaches they all are based on empirical studies, but are basing their studies on different philosophical principles). (3) Historical and hermeneutical approaches including Ereshefsky's \"historical classification\" and (4) Pragmatic, functionalist and teleological approaches (not covered by Ereshefsky). In addition, there are combined approaches (e.g., the so-called evolutionary taxonomy \", which mixes historical and empiricist principles). Logical division , [ 30 ] or logical partitioning (top-down classification or downward classification) is an approach that divides a class into subclasses and then divide subclasses into their subclasses, and so on, which finally forms a tree of classes. The root of the tree is the original class, and the leaves of the tree are the final classes. Plato advocated a method based on dichotomy, which was rejected by Aristotle and replaced by the method of definitions based on genus, species, and specific difference. [ 31 ] The method of facet analysis (cf., faceted classification ) is primarily based on logical division. [ 32 ] This approach tends to classify according to \"essential\" characteristics, a widely discussed and criticized concept (cf., essentialism ). These methods may overall be related to the rationalist theory of knowledge. Michelle Bunn notes that logical partitioning uses categories which are established a priori ; data is then collected and used to test the extent to which the classification system can be sustained. [ 33 ] \"Empiricism alone is not enough: a healthy advance in taxonomy depends on a sound theoretical foundation\" [ 34 ] : 548 Phenetics or numerical taxonomy [ 35 ] is by contrast bottom-up classification, where the starting point is a set of items or individuals, which are classified by putting those with shared characteristics as members of a narrow class and proceeding upward. Numerical taxonomy is an approach based solely on observable, measurable similarities and differences of the things to be classified. Classification is based on overall similarity: the elements that are most alike in most attributes are classified together. But it is based on statistics, and therefore does not fulfill the criteria of logical division (e.g. to produce classes, that are mutually exclusive and jointly coextensive with the class they divide). Some people will argue that this is not classification/taxonomy at all, but such an argument must consider the definitions of classification (see above). These methods may overall be related to the empiricist theory of knowledge. Genealogical classification is classification of items according to their common heritage. This must also be done on the basis of some empirical characteristics, but these characteristics are developed by the theory of evolution. Charles Darwin's [ 36 ] main contribution to classification theory was not just his claim \"... all true classification is genealogical ...\" but that he provided operational guidance for classification. [ 37 ] : 90–92 Genealogical classification is not restricted to biology, but is also much used in, for example, classification of languages, and may be considered a general approach to classification.\" These methods may overall be related to the historicist theory of knowledge. One of the main schools of historical classification is cladistics , which is today dominant in biological taxonomy, but also applied to other domains. The historical and hermeneutical approaches is not restricted to the development of the object of classification (e.g., animal species) but is also concerned with the subject of classification (the classifiers) and their embeddedness in scientific traditions and other human cultures. Pragmatic classification (and functional [ 38 ] and teleological classification) is the classification of items which emphasis the goals, purposes, consequences, [ 39 ] interests, values and politics of classification. It is, for example, classifying animals into wild animals, pests, domesticated animals and pets. Also kitchenware (tools, utensils, appliances, dishes, and cookware used in food preparation, or the serving of food) is an example of a classification which is not based on any of the above-mentioned three methods, but clearly on pragmatic or functional criteria. Bonaccorsi, et al. (2019) is about the general theory of functional classification and applications of this approach for patent classification. [ 38 ] Although the examples may suggest that pragmatic classifications are primitive compared to established scientific classifications, it must be considered in relation to the pragmatic and critical theory of knowledge, which consider all knowledge as influences by interests. [ 40 ] Ridley (1986) wrote: [ 41 ] : 191 \"teleological classification. Classification of groups by their shared purposes, or functions, in life - where purpose can be identified with adaptation. An imperfectly worked-out, occasionally suggested, theoretically possible principle of classification that differs from the two main such principles, phenetic and phylogenetic classification \". Natural classification is a concept closely related to the concept natural kind . Carl Linnaeus is often recognized as the first scholar to clearly have differentiated \"artificial\" and \"natural\" classifications [ 42 ] [ 43 ] A natural classification is one, using Plato's metaphor, that is “carving nature at its joints” [ 44 ] Although Linnaeus considered natural classification the ideal, he recognized that his own system (at least partly) represented an artificial classification. John Stuart Mill explained the artificial nature of the Linnaean classification and suggested the following definition of a natural classification: \"The Linnæan arrangement answers the purpose of making us think together of all those kinds of plants, which possess the same number of stamens and pistils; but to think of them in that manner is of little use, since we seldom have anything to affirm in common of the plants which have a given number of stamens and pistils.\" [ 45 ] : 498 \"The ends of scientific classification are best answered, when the objects are formed into groups respecting which a greater number of general propositions can be made, and those propositions more important, than could be made respecting any other groups into which the same things could be distributed.\" [ 45 ] : 499 \"A classification thus formed is properly scientific or philosophical, and is commonly called a Natural, in contradistinction to a Technical or Artificial, classification or arrangement.\" [ 45 ] : 499 Ridley (1986) provided the following definitions: [ 41 ] Stamos (2004) [ 46 ] : 138 wrote: \"The fact is, modern scientists classify atoms into elements based on proton number rather than anything else because it alone is the causally privileged factor [gold is atomic number 79 in the periodic table because it has 79 protons in its nucleus]. Thus nature itself has supplied the causal monistic essentialism. Scientists in their turn simply discover and follow (where \"simply\" ≠ \"easily\").\" The periodic table is the classification of the chemical elements which is in particular associated with Dmitri Mendeleev (cf., History of the periodic table ). An authoritative work on this system is Scerri (2020). [ 47 ] Hubert Feger (2001; numbered listing added) wrote about it: [ 48 ] : 1967–1968 \"A well-known, still used, and expanding classification is Mendeleev's Table of Elements. It can be viewed as a prototype of all taxonomies in that it satisfies the following evaluative criteria: Bursten (2020) wrote, however \"Hepler-Smith, a historian of chemistry, and I, a philosopher whose work often draws on chemistry, found common ground in a shared frustration with our disciplines’ emphases on the chemical elements as the stereotypical example of a natural kind. The frustration we shared was that while the elements did display many hallmarks of paradigmatic kindhood, elements were not the kinds of kinds that generated interesting challenges for classification in chemistry, nor even were they the kinds of kinds that occupied much contemporary critical chemical thought. Compounds, complexes, reaction pathways, substrates, solutions – these were the kinds of the chemistry laboratory, and rarely if ever did they slot neatly into taxonomies in the orderly manner of classification suggested by the Periodic Table of Elements. A focus on the rational and historical basis of the development of the Periodic Table had made the received view of chemical classification appear far more pristine, and far less interesting, than either of us believed it to be.\" [ 49 ] Linnaean taxonomy is the particular form of biological classification ( taxonomy ) set up by Carl Linnaeus , as set forth in his Systema Naturae (1735) and subsequent works. A major discussion in the scientific literature is whether a system that was constructed before Charles Darwin's theory of evolution can still be fruitful and reflect the development of life. [ 50 ] [ 51 ] Astronomy is a fine example on how Kuhn's (1962) theory of scientific revolutions (or paradigm shifts) influences classification. [ 52 ] For example: Hornbostel–Sachs is a system of musical instrument classification devised by Erich Moritz von Hornbostel and Curt Sachs, and first published in 1914. [ 53 ] In the original classification, the top categories are: A fifth top category, Each top category is subdivided and Hornbostel-Sachs is a very comprehensive classification of musical instruments with wide applications. In Wikipedia, for example, all musical instruments are organized according to this classification. In opposition to, for example, the astronomical and biological classifications presented above, the Hornbostel-Sachs classification seems very little influenced by research in musicology and organology . It is based on huge collections of musical instruments, but seems rather as a system imposed upon the universe of instruments than as a system with organic connections to scholarly theory. It may therefore be interpreted as a system based on logical division and rationalist philosophy. Diagnostic and Statistical Manual of Mental Disorders (DSM) is a classification of mental disorders published by the American Psychiatric Association (APA).The first edition of the DSM was published in 1952, [ 54 ] and the newest, fifth edition was published in 2013. [ 55 ] In contrast to, for example, the periodic table and the Hornbostel-Sachs classification, its principles for classification have changed much during its history. The first edition was influenced by psychodynamic theory. The DSM-III, published in 1980, [ 56 ] adopted an atheoretical, “descriptive” approach to classification [ 57 ] The system is very important for all people involved in psychiatry, whether as patients, researchers or therapists (in addition to insurance companies), but it is also strongly criticized and does not have the same scientific status as many other classifications. [ 58 ]",
    "links": [
      "Scott Atran",
      "Hyponymy",
      "University of Karlsruhe",
      "Locomotive classification",
      "Information access",
      "Frank Dignum",
      "Categorization",
      "Doi (identifier)",
      "Taxonomy for search engines",
      "Statistical classification",
      "Classification theorem",
      "Gottfried Leibniz",
      "Military taxonomy",
      "Musicology",
      "Containment hierarchy",
      "S2CID (identifier)",
      "European Semantic Web Conference",
      "JSTOR (identifier)",
      "Numerical taxonomy",
      "Predicate (mathematical logic)",
      "Greek language",
      "Lumpers and splitters",
      "Information retrieval",
      "Knowledge organization",
      "Tree structure",
      "Information seeking",
      "North American Industry Classification System",
      "Ontology (computer science)",
      "Carl Linnaeus",
      "Astronomy",
      "Science and technology studies",
      "The Elementary Forms of Religious Life",
      "Memory",
      "Hornbostel–Sachs",
      "Organology",
      "Springer-Verlag",
      "Longmans, Green, Reader, and Dyer",
      "Bloom's taxonomy",
      "Privacy",
      "Typology",
      "Standard Industrial Classification",
      "Universal language",
      "Data modeling",
      "European Green Deal",
      "Émile Durkheim",
      "Evolutionary taxonomy",
      "Records management taxonomy",
      "Preservation (library and archival science)",
      "Corporate taxonomy",
      "Product classification",
      "Contributor Roles Taxonomy",
      "History of the periodic table",
      "Information technology",
      "Conflation",
      "Classification of swords",
      "Gellish English dictionary",
      "Pavitt's Taxonomy",
      "EU taxonomy for sustainable activities",
      "Information society",
      "Folk taxonomy",
      "Knowledge representation and reasoning",
      "Augustin Pyramus de Candolle",
      "Protégé (software)",
      "Bibcode (identifier)",
      "State University of New York Press",
      "Is-a",
      "Ars generalis ultima",
      "Classification (literature)",
      "Electrophone",
      "Ship classification society",
      "Attribute-value system",
      "GitHub",
      "Global Industry Classification Standard",
      "International Standard Industrial Classification",
      "Natural kind",
      "Industrial process",
      "Classification (general theory)",
      "Membranophone",
      "Taxonomy (biology)",
      "CiteSeerX (identifier)",
      "Cambridge University Press",
      "Censorship",
      "Essentialism",
      "Outline of information science",
      "Hierarchy",
      "Image classification",
      "Classification of wine",
      "Industry Classification Benchmark",
      "Periodic table",
      "Celestial Emporium of Benevolent Recognition",
      "Thesaurus (information retrieval)",
      "Civil service",
      "PMID (identifier)",
      "Peter Mark Roget",
      "Mechanical screening",
      "Academic journal",
      "SOLO taxonomy",
      "Confidential Incident Reporting & Analysis System",
      "Chordophone",
      "Computer data storage",
      "Motion picture rating system",
      "Diagnostic and Statistical Manual of Mental Disorders",
      "Library classification",
      "Medical classification",
      "Knowledge representation",
      "International Society for Knowledge Organization",
      "Systema Naturae",
      "Library classification system",
      "OCLC (identifier)",
      "Classification Society",
      "Information architecture",
      "Thomas Kuhn",
      "Characteristica universalis",
      "Hypernym",
      "Security classification",
      "United Kingdom Standard Industrial Classification of Economic Activities",
      "Indicative conditional",
      "Aristotle",
      "Activity-based costing",
      "Structuralism",
      "Information behavior",
      "Aerophone",
      "James T. Reason",
      "Systematics",
      "WordNet",
      "Thesaurus",
      "Master data management",
      "Job analysis",
      "John Wilkins",
      "Idiophone",
      "Reborrowing",
      "Taxon",
      "Frederick Suppe",
      "Plant taxonomy",
      "Swiss cheese model",
      "Moys Classification Scheme",
      "Alpha taxonomy",
      "Semantic similarity network",
      "Quantum information science",
      "Philosophical language",
      "Philosophy of information",
      "Mereology",
      "Human Factors Analysis and Classification System",
      "Kitchenware",
      "Virus classification",
      "ACM Computing Classification System",
      "Library and information science",
      "Taxonomic rank",
      "Intellectual property",
      "Cultural studies",
      "Cladistics",
      "Knowledge management",
      "Semantic network",
      "Mathematics Subject Classification",
      "Linguistics",
      "Alphabet of human thought",
      "Classified information",
      "Plato",
      "Soil classification",
      "Informatics",
      "Flynn's taxonomy",
      "Wayback Machine",
      "Folksonomy",
      "Decimal classification",
      "Linnaean taxonomy",
      "Nosology",
      "ISSN (identifier)",
      "Economic taxonomy",
      "Chemical classification",
      "Safety taxonomy",
      "A priori",
      "ISBN (identifier)",
      "XBRL Taxonomy",
      "Intellectual freedom",
      "Biological classification",
      "Classification of Instructional Programs",
      "Lexicon",
      "An Essay towards a Real Character and a Philosophical Language",
      "Bibliometrics",
      "Phenetics",
      "Document classification",
      "Classification",
      "Cultural anthropology",
      "SRK taxonomy",
      "Class (set theory)",
      "Scientific classification (disambiguation)",
      "PMC (identifier)",
      "Ontology (information science)",
      "Ronald J. Brachman",
      "Network (mathematics)",
      "Phylogenetics",
      "Faceted classification",
      "Information management",
      "Ramon Llull",
      "Information science",
      "Dmitri Mendeleev"
    ]
  },
  "Music information retrieval": {
    "url": "https://en.wikipedia.org/wiki/Music_information_retrieval",
    "title": "Music information retrieval",
    "content": "Music information retrieval ( MIR ) is the interdisciplinary science of retrieving information from music . Those involved in MIR may have a background in academic musicology , psychoacoustics , psychology , signal processing , informatics , machine learning , optical music recognition , computational intelligence , or some combination of these. Music information retrieval is being used by businesses and academics to categorize, manipulate and even create music. One of the classical MIR research topics is genre classification, which is categorizing music items into one of the pre-defined genres such as classical , jazz , rock , etc. Mood classification , artist classification, instrument identification, and music tagging are also popular topics. Several recommender systems for music already exist, but surprisingly few are based upon MIR techniques, instead of making use of similarity between users or laborious data compilation. Pandora , for example, uses experts to tag the music with particular qualities such as \"female singer\" or \"strong bassline\". Many other systems find users whose listening history is similar and suggests unheard music to the users from their respective collections. MIR techniques for similarity in music are now beginning to form part of such systems. Music source separation is about separating original signals from a mixture audio signal . Instrument recognition is about identifying the instruments involved in music. Various MIR systems have been developed that can separate music into its component tracks without access to the master copy. In this way, for example, karaoke tracks can be created from normal music tracks, though the process is not yet perfect owing to vocals occupying some of the same frequency space as the other instruments. Automatic music transcription is the process of converting an audio recording into symbolic notation, such as a score or a MIDI file . [ 1 ] This process involves several audio analysis tasks, which may include multi-pitch detection, onset detection , duration estimation, instrument identification, and the extraction of harmonic , rhythmic or melodic information. This task becomes more difficult with greater numbers of instruments and a greater polyphony level . The automatic generation of music is a goal held by many MIR researchers. Attempts have been made with limited success in terms of human appreciation of the results. Scores give a clear and logical description of music from which to work, but access to sheet music , whether digital or otherwise, is often impractical. [ 2 ] MIDI music has also been used for similar reasons, but some data is lost in the conversion to MIDI from any other format, unless the music was written with the MIDI standards in mind, which is rare. Digital audio formats such as WAV , mp3 , and ogg are used when the audio itself is part of the analysis. Lossy formats such as mp3 and ogg work well with the human ear but may be missing crucial data for study. Additionally some encodings create artifacts which could be misleading to any automatic analyser. Despite this the ubiquity of the mp3 has meant much research in the field involves these as the source material. Increasingly, metadata mined from the web is incorporated in MIR for a more rounded understanding of the music within its cultural context, and this recently consists of analysis of social tags for music. Analysis can often require some summarising, [ 3 ] and for music (as with many other forms of data) this is achieved by feature extraction , especially when the audio content itself is analysed and machine learning is to be applied. The purpose is to reduce the sheer quantity of data down to a manageable set of values so that learning can be performed within a reasonable time-frame. One common feature extracted is the Mel-Frequency Cepstral Coefficient (MFCC) which is a measure of the timbre of a piece of music . Other features may be employed to represent the key , chords , harmonies , melody , main pitch , beats per minute or rhythm in the piece. There are a number of available audio feature extraction tools [ 4 ] Available here",
    "links": [
      "Social tagging",
      "Audio search engine",
      "Doi (identifier)",
      "Beats per minute",
      "Automatic summarization",
      "Mel-frequency cepstral coefficient",
      "Acoustic fingerprint",
      "Similarity metric",
      "Psychology",
      "Musicology",
      "Search by sound",
      "Polyphony and monophony in instruments",
      "Parsons code",
      "Harmonic",
      "Sound recognition",
      "Mp3",
      "Information retrieval",
      "List of music software",
      "Emotion classification",
      "Copyright",
      "Query by humming",
      "3D sound localization",
      "Signal processing",
      "Semantic Web",
      "Speech analytics",
      "Multimodal interaction",
      "Pattern matching",
      "Audio file format",
      "Jazz",
      "Database",
      "Rhythm",
      "Transcription (music)",
      "Computational intelligence",
      "Psychoacoustics",
      "Musical similarity",
      "International Society for Music Information Retrieval",
      "Collaborative software",
      "Music identification",
      "User interface",
      "Multi-agent system",
      "Pandora Radio",
      "Usability",
      "Audio mining",
      "Sheet music",
      "Harmony",
      "MIDI",
      "Computer audition",
      "Speech processing",
      "Digital signal processing",
      "International Conference on Acoustics, Speech, and Signal Processing",
      "Music",
      "Pitch (music)",
      "Mobile app",
      "3D sound reconstruction",
      "Musical composition",
      "Machine learning",
      "OCLC (identifier)",
      "Formal methods",
      "Recommender systems",
      "Ogg",
      "Melody",
      "Music notation",
      "Semantic audio",
      "Sound and music computing",
      "Frequency",
      "Score following",
      "Rock music",
      "Multimedia information retrieval",
      "Automatic generation of music",
      "Intellectual property",
      "Audio signal",
      "Metadata",
      "Digital rights management",
      "Feature extraction",
      "Classical music",
      "Acoustic fingerprinting",
      "Informatics",
      "Speech recognition",
      "Polyphonic",
      "Automatic content recognition",
      "Digital library",
      "Interdisciplinary science",
      "Human–computer interaction",
      "Intelligent agent",
      "ISBN (identifier)",
      "A Dictionary of Musical Themes",
      "Computational auditory scene analysis",
      "WAV",
      "Ethnomusicology",
      "Timbre",
      "Information",
      "Chord (music)",
      "Speaker recognition",
      "Optical music recognition"
    ]
  },
  "Web mining": {
    "url": "https://en.wikipedia.org/wiki/Web_mining",
    "title": "Web mining",
    "content": "Data mining is the process of extracting and finding patterns in massive data sets involving methods at the intersection of machine learning , statistics , and database systems . [ 1 ] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. [ 1 ] [ 2 ] [ 3 ] [ 4 ] Data mining is the analysis step of the \" knowledge discovery in databases \" process, or KDD. [ 5 ] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing , model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization , and online updating . [ 1 ] The term \"data mining\" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction ( mining ) of data itself . [ 6 ] It also is a buzzword [ 7 ] and is frequently applied to any form of large-scale data or information processing ( collection , extraction , warehousing , analysis, and statistics) as well as any application of computer decision support systems , including artificial intelligence (e.g., machine learning) and business intelligence . Often the more general terms ( large scale ) data analysis and analytics —or, when referring to actual methods, artificial intelligence and machine learning —are more appropriate. The actual data mining task is the semi- automatic or automatic analysis of massive quantities of data to extract previously unknown, interesting patterns such as groups of data records ( cluster analysis ), unusual records ( anomaly detection ), and dependencies ( association rule mining , sequential pattern mining ). This usually involves using database techniques such as spatial indices . These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics . For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system . Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps. The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign , regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data. [ 8 ] The related terms data dredging , data fishing , and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations. In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983. [ 9 ] [ 10 ] Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative). The term data mining appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, the phrase \"database mining\"™, was used, but since it was trademarked by HNC, a San Diego –based company, to pitch their Database Mining Workstation; [ 11 ] researchers consequently turned to data mining . Other terms used include data archaeology , information harvesting , information discovery , knowledge extraction , etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) [ 12 ] and this term became more popular in the AI and machine learning communities. However, the term data mining became more popular in the business and press communities. [ 13 ] Currently, the terms data mining and knowledge discovery are used interchangeably. The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). [ 14 ] The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks , cluster analysis , genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns. [ 15 ] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets. The knowledge discovery in databases (KDD) process is commonly defined with the stages: It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases: or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation. Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. [ 16 ] [ 17 ] [ 18 ] [ 19 ] The only other data mining standard named in these polls was SEMMA . However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, [ 20 ] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008. [ 21 ] Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse . Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data . Data mining involves six common classes of tasks: [ 5 ] Data mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be reproduced on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper statistical hypothesis testing . A simple version of this problem in machine learning is known as overfitting , but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening. [ 22 ] The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting . To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves . If the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge. The premier professional body in the field is the Association for Computing Machinery 's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining ( SIGKDD ). [ 23 ] [ 24 ] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, [ 25 ] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\". [ 26 ] Computer science conferences on data mining include: Data mining topics are also present in many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases . There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft. For exchanging the extracted models—in particular for use in predictive analytics —the key standard is the Predictive Model Markup Language (PMML), which is an XML -based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG. [ 27 ] Data mining is used wherever there is digital data available. Notable examples of data mining can be found throughout business, medicine, science, finance, construction, and surveillance. While the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to user behavior (ethical and otherwise). [ 28 ] The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy , legality, and ethics . [ 29 ] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE , has raised privacy concerns. [ 30 ] [ 31 ] Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation . Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). [ 32 ] The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous. [ 33 ] Data may also be modified so as to become anonymous, so that individuals may not readily be identified. [ 32 ] However, even \" anonymized \" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL . [ 34 ] The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial, emotional, or bodily harm to the indicated individual. In one instance of privacy violation , the patrons of Walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies. [ 35 ] Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles , developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden 's global surveillance disclosure , there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency , and attempts to reach an agreement with the United States have failed. [ 36 ] In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places. [ 37 ] In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week , \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approaching a level of incomprehensibility to average individuals.\" [ 38 ] This underscores the necessity for data anonymity in data aggregation and mining practices. U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation. Even if there is no copyright in a dataset, the European Union recognises a Database right , so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive . Under European copyright database laws , the mining of in-copyright works (such as by web mining ) without the permission of the copyright owner is permitted under Articles 3 and 4 of the 2019 Directive on Copyright in the Digital Single Market . A specific TDM exception for scientific research is described in article 3, whereas a more general exception described in article 4 only applies if the copyright holder has not opted out. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. [ 39 ] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013. [ 40 ] On the recommendation of the Hargreaves review , this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception . [ 41 ] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. Since 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020. [ 42 ] US copyright law , and in particular its provision for fair use , upholds the legality of content mining in America, and other fair use countries such as Israel , Taiwan and South Korea . As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining. [ 43 ] The following applications are available under free/open-source licenses. Public access to application source code is also available. The following applications are available under proprietary licenses. For more information about extracting information out of data (as opposed to analyzing data), see:",
    "links": [
      "Very-large-scale integration",
      "Spreadsheets",
      "Video game",
      "Buzzword",
      "Reproducibility",
      "Artificial intelligence",
      "Data integrity",
      "Michael Lovell",
      "Data type",
      "Data scrubbing",
      "Feature engineering",
      "DeepDream",
      "Column-oriented DBMS",
      "Networking hardware",
      "Total Information Awareness",
      "Test set",
      "Multiprocessing",
      "Mean shift",
      "Overfitting",
      "Rendering (computer graphics)",
      "Decision tree learning",
      "Data editing",
      "Python (programming language)",
      "International Conference on Learning Representations",
      "Online algorithm",
      "Subspace clustering",
      "Programming team",
      "Israel",
      "Interaction design",
      "Memtransistor",
      "ECML PKDD",
      "Computational geometry",
      "Industrial process control",
      "Semantic analysis (machine learning)",
      "Data governance",
      "Software framework",
      "Data format management",
      "Fault tolerance",
      "System on a chip",
      "Digital marketing",
      "Printed circuit board",
      "Fact (data warehouse)",
      "Taiwan",
      "Grammar induction",
      "Document management system",
      "Random forest",
      "National security",
      "Diffusion model",
      "Orange (software)",
      "Integrated development environment",
      "Data management",
      "Database right",
      "Wireless sensor network",
      "Distributed computing",
      "Software engineering",
      "E-commerce",
      "Computational biology",
      "DATADVANCE",
      "Microsoft Academic Search",
      "Knowledge representation and reasoning",
      "Data sharing",
      "Early-arriving fact",
      "Autoencoder",
      "Named-entity recognition",
      "Database",
      "Data ecosystem",
      "Human-in-the-loop",
      "Ontology learning",
      "Convolutional neural network",
      "Electronic discovery",
      "Reinforcement learning",
      "Physics-informed neural networks",
      "Star schema",
      "Concurrency (computer science)",
      "Trevor Hastie",
      "RapidMiner",
      "Hardware security",
      "Data augmentation",
      "Information integration",
      "Linear discriminant analysis",
      "Unsupervised learning",
      "Augmented reality",
      "Form factor (design)",
      "Electronic design automation",
      "Network architecture",
      "Open-source",
      "Control theory",
      "Dimension (data warehouse)",
      "Data warehouse automation",
      "Receiver operating characteristic",
      "Cross-industry standard process for data mining",
      "National Security Agency",
      "Carrot2",
      "Bioinformatics",
      "MultiDimensional eXpressions",
      "Data deduplication",
      "Cyber-physical system",
      "Modeling language",
      "Data wrangling",
      "Law enforcement",
      "Java (programming language)",
      "Google Scholar",
      "Formal methods",
      "Privacy violation",
      "Data re-identification",
      "Computer security",
      "Network scheduler",
      "AAAI Conference on Artificial Intelligence",
      "Computational mathematics",
      "Computational learning theory",
      "Coefficient of determination",
      "Computing",
      "Predictive Model Markup Language",
      "Hidden Markov model",
      "Spamming",
      "Data quality",
      "Marketing campaign",
      "Support vector machine",
      "Neural field",
      "Missing data",
      "Algorithm",
      "Policy gradient method",
      "KNIME",
      "Vapnik–Chervonenkis theory",
      "Metadata",
      "Corporate information factory",
      "Misnomer",
      "Mathematical software",
      "South Korea",
      "Snowflake schema",
      "Electronic voting",
      "Structured prediction",
      "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases",
      "Statistical model",
      "Algorithmic efficiency",
      "Data scraping",
      "Data dredging",
      "Bing Liu (computer scientist)",
      "Open-source software",
      "Deep learning",
      "Knowledge extraction",
      "Jiawei Han",
      "US Congress",
      "Online analytical processing",
      "Ralph Kimball",
      "Wayback Machine",
      "Analytics",
      "Kluwer Academic Publishers",
      "Computational complexity",
      "SIGKDD",
      "ISSN (identifier)",
      "Integrated circuit",
      "Bayes' theorem",
      "SIGMOD",
      "BIRCH",
      "Crowdsourcing",
      "Data migration",
      "Human–computer interaction",
      "Quantum computing",
      "ISBN (identifier)",
      "Data loss",
      "Mobile computing",
      "Sixth normal form",
      "Software development process",
      "Requirements analysis",
      "Multi expression programming",
      "Mathematical optimization",
      "Data extraction",
      "Data anonymization",
      "Intrusion detection system",
      "Data transformation",
      "International Conference on Very Large Data Bases",
      "Statistics",
      "LIONsolver",
      "Bayesian network",
      "Neural network (machine learning)",
      "Confusion matrix",
      "Aggregate function",
      "Cryptocurrency",
      "Information system",
      "Regression analysis",
      "Oracle Data Mining",
      "Isolation forest",
      "Slowly changing dimension",
      "Knowledge discovery",
      "Open access",
      "Profiling (information science)",
      "Data vault modeling",
      "NLTK",
      "Automatic summarization",
      "Reservoir computing",
      "Market basket",
      "S2CID (identifier)",
      "Anomaly detection",
      "Sequence mining",
      "Neuro-symbolic AI",
      "Anchor modeling",
      "OpenNN",
      "Surrogate key",
      "Statistical hypothesis testing",
      "T-distributed stochastic neighbor embedding",
      "Programming language theory",
      "Mamba (deep learning architecture)",
      "Database Directive",
      "Domain driven data mining",
      "Data synchronization",
      "Fair use",
      "Software maintenance",
      "Philip S. Yu",
      "Computer animation",
      "International Joint Conference on Artificial Intelligence",
      "Thermodynamic computing",
      "Cryptography",
      "Concurrent computing",
      "Statistical inference",
      "Image compression",
      "Educational technology",
      "Dashboard (business)",
      "Business intelligence",
      "International Journal of Data Warehousing and Mining",
      "Data snooping",
      "Domain-specific language",
      "Java Data Mining",
      "List of computer size categories",
      "Software quality",
      "Electrochemical RAM",
      "Software repository",
      "Decision rules",
      "Data exhaust",
      "Self-organizing map",
      "General Architecture for Text Engineering",
      "Software configuration management",
      "Sparse dictionary learning",
      "Cambridge University Press",
      "Usama Fayyad",
      "Stellar Wind",
      "Operating system",
      "Examples of data mining",
      "Learning to rank",
      "Interpreter (computing)",
      "List of datasets for machine-learning research",
      "AI",
      "Information extraction",
      "Software design",
      "Interdisciplinary",
      "Database management",
      "Social computing",
      "Middleware",
      "Data hub",
      "Microsoft",
      "Visualization (graphics)",
      "Dan Linstedt",
      "Information theory",
      "Copyright law of the European Union",
      "Data corruption",
      "Data ethics",
      "Learning curve (machine learning)",
      "Supervised learning",
      "Control flow",
      "Information security",
      "Machine learning",
      "Conference on Neural Information Processing Systems",
      "Training set",
      "Ethics",
      "Pattern",
      "Numerical analysis",
      "Model of computation",
      "Journal of Machine Learning Research",
      "Single version of the truth",
      "Factor analysis",
      "Real-time computing",
      "Restricted Boltzmann machine",
      "Perceptron",
      "Naive Bayes classifier",
      "Canonical correlation",
      "System deployment",
      "Tanagra (machine learning)",
      "Academic Press",
      "Data reduction",
      "Amazon SageMaker",
      "Graphical model",
      "GNU Project",
      "Enterprise information system",
      "Data mart",
      "Association for Computing Machinery",
      "XML for Analysis",
      "Statistical noise",
      "Reinforcement learning from human feedback",
      "Data",
      "Automated planning and scheduling",
      "Self-play (reinforcement learning technique)",
      "Amazon.com",
      "Hardware acceleration",
      "Multithreading (computer architecture)",
      "Conference on Knowledge Discovery and Data Mining",
      "Empirical risk minimization",
      "Data curation",
      "Neuromorphic engineering",
      "Kernel machines",
      "Analysis of algorithms",
      "ACM Computing Classification System",
      "Data structure",
      "Information privacy",
      "Software deployment",
      "San Diego",
      "Probability",
      "Data erasure",
      "Dependability",
      "Online machine learning",
      "Scikit-learn",
      "OLAP cube",
      "LeNet",
      "Theory of computation",
      "C++",
      "Probably approximately correct learning",
      "Hierarchical clustering",
      "Decision tree",
      "Outline of computer science",
      "Surveillance capitalism",
      "Echo state network",
      "Business intelligence software",
      "Intention mining",
      "Exploratory data analysis",
      "Real-time data",
      "Dimensionality reduction",
      "Transformer (deep learning architecture)",
      "Topological data analysis",
      "Comparison of OLAP servers",
      "Compiler construction",
      "Neural radiance field",
      "Association rule mining",
      "Proper generalized decomposition",
      "Feature learning",
      "Degenerate dimension",
      "Active learning (machine learning)",
      "Virtual reality",
      "Principal component analysis",
      "SPSS",
      "Computational social science",
      "Robert Tibshirani",
      "Hewlett-Packard",
      "Educational data mining",
      "JSTOR (identifier)",
      "Data loading",
      "Data library",
      "Oracle Corporation",
      "Glossary of artificial intelligence",
      "Information retrieval",
      "Data engineering",
      "Data publishing",
      "Psychometrics",
      "Mlpack",
      "Personally identifiable information",
      "Semantics (computer science)",
      "Open data",
      "Expectation–maximization algorithm",
      "Discrete mathematics",
      "Big data",
      "Multi-agent reinforcement learning",
      "Spatial index",
      "Privacy",
      "Lua (programming language)",
      "Word processor",
      "Dependency (computer science)",
      "Data compression",
      "Apprenticeship learning",
      "State–action–reward–state–action",
      "KDD Conference",
      "Dimension table",
      "Quantum machine learning",
      "Bias–variance tradeoff",
      "Self-supervised learning",
      "Random sample consensus",
      "Green computing",
      "Recurrent neural network",
      "Logic in computer science",
      "Non-negative matrix factorization",
      "Computer vision",
      "Q-learning",
      "Stochastic computing",
      "Data remanence",
      "Algorithm design",
      "Geographic information system",
      "Data storage",
      "Bootstrap aggregating",
      "Prentice Hall",
      "SAS Institute",
      "PMID (identifier)",
      "Data processing",
      "Peripheral",
      "U-Net",
      "Multimedia database",
      "Enterprise software",
      "Randomized algorithm",
      "Academic journal",
      "Artificial neural network",
      "ELKI",
      "Database system",
      "Software construction",
      "Reverse star schema",
      "Drug discovery",
      "Operational data store",
      "Electronic publishing",
      "Semi-supervised learning",
      "Data science",
      "Data mesh",
      "List of datasets in computer vision and image processing",
      "OCLC (identifier)",
      "Review of Economic Studies",
      "World Wide Web",
      "Microsoft Analysis Services",
      "SEMMA",
      "Behavior informatics",
      "Data infrastructure",
      "Data aggregation",
      "International Safe Harbor Privacy Principles",
      "Gated recurrent unit",
      "Cluster analysis",
      "Aggregate (data warehouse)",
      "Operations research",
      "Data exploration",
      "Stakeholder (corporate)",
      "Programming tool",
      "Computer science",
      "Data rescue",
      "Photograph manipulation",
      "Boosting (machine learning)",
      "OPTICS algorithm",
      "International Conference on Machine Learning",
      "Social media mining",
      "Virtual machine",
      "User behavior analytics",
      "Data cooperative",
      "Google",
      "Parallel computing",
      "NetOwl",
      "Network security",
      "Text mining",
      "Intellectual property",
      "Extract, transform, load",
      "Mathematical analysis",
      "Morgan Kaufmann",
      "Social software",
      "StatSoft",
      "Data recovery",
      "Multi-task learning",
      "Health Insurance Portability and Accountability Act",
      "Data cleansing",
      "Health informatics",
      "AOL",
      "Communication protocol",
      "Ensemble learning",
      "Computing platform",
      "Computer network",
      "Multilinear subspace learning",
      "Data warehouse",
      "Torch (machine learning)",
      "Temporal difference learning",
      "Computational problem",
      "Ubiquitous computing",
      "Computer accessibility",
      "Google Cloud Platform",
      "Distributed artificial intelligence",
      "Graphics processing unit",
      "Curriculum learning",
      "Predictive analytics",
      "Confidentiality",
      "IBM",
      "HOLAP",
      "Jerome H. Friedman",
      "Learning classifier system",
      "Cross Industry Standard Process for Data Mining",
      "Relevance vector machine",
      "Topological deep learning",
      "Human-centered computing",
      "Data set",
      "Solid modeling",
      "Family Educational Rights and Privacy Act",
      "Doi (identifier)",
      "Statistical classification",
      "Vertica",
      "Data pre-processing",
      "Ian H. Witten",
      "KDnuggets",
      "Outline of machine learning",
      "Multivariate statistics",
      "PSeven",
      "A priori probability",
      "PolyAnalyst",
      "Local outlier factor",
      "Security hacker",
      "Programming paradigm",
      "Computational physics",
      "Logistic regression",
      "SPSS Modeler",
      "Scientific computing",
      "Independent component analysis",
      "Machine Learning (journal)",
      "Data farming",
      "Cross-validation (statistics)",
      "European Commission",
      "Data steward",
      "AlexNet",
      "STATISTICA",
      "Computability theory",
      "InformationWeek",
      "Outlier detection",
      "Processor (computing)",
      "Cyberwarfare",
      "Computational engineering",
      "Fact table",
      "K-nearest neighbors algorithm",
      "ADVISE",
      "Programming language",
      "Extract, load, transform",
      "Applied statistics",
      "Linear regression",
      "Springer Verlag",
      "Measure (data warehouse)",
      "Data Mining Extensions",
      "Generative model",
      "Software development",
      "Network service",
      "Data de-identification",
      "Data redundancy",
      "Computer graphics",
      "Data lineage",
      "Agent mining",
      "Automata theory",
      "Directive on Copyright in the Digital Single Market",
      "Google Book Search Settlement Agreement",
      "Edward Snowden",
      "Natural Language Toolkit",
      "Data validation",
      "Association rule learning",
      "UIMA",
      "Rule-based machine learning",
      "Data analysis",
      "Embedded system",
      "Conditional random field",
      "Statistical",
      "ROLAP",
      "Data integration",
      "Computer hardware",
      "Vision transformer",
      "Dimensional modeling",
      "Computer architecture",
      "Computational chemistry",
      "Library (computing)",
      "Computer data storage",
      "Data degradation",
      "Batch learning",
      "CJEU",
      "Limitations and exceptions to copyright",
      "Application security",
      "Support vector machines",
      "Feedforward neural network",
      "Natural language processing",
      "Data and information visualization",
      "Angoss",
      "Fuzzy clustering",
      "European Union",
      "Decision support system",
      "PSPP",
      "Automated machine learning",
      "Information Society Directive",
      "Formal language",
      "DBSCAN",
      "Generative adversarial network",
      "Bill Inmon",
      "Boltzmann machine",
      "Structured data analysis (statistics)",
      "Data cleaning",
      "List of reporting software",
      "Meta-learning (computer science)",
      "Genetic algorithms",
      "Chemicalize.org",
      "Data philanthropy",
      "Multimodal learning",
      "Statistical learning theory",
      "Data preservation",
      "XML",
      "Data collection",
      "Data security",
      "K-means clustering",
      "US copyright law",
      "Data privacy",
      "Spiking neural network",
      "Mechanistic interpretability",
      "Data retention",
      "Data dictionary",
      "Time series analysis",
      "Qlucore",
      "Occam learning",
      "Enterprise bus matrix",
      "MOLAP",
      "CURE algorithm",
      "MOA (Massive Online Analysis)",
      "Global surveillance disclosure",
      "Data acquisition",
      "Digital library",
      "Data archaeology",
      "Philosophy of artificial intelligence",
      "Gregory Piatetsky-Shapiro",
      "R (programming language)",
      "Security service (telecommunication)",
      "Density estimation",
      "Weka (machine learning)",
      "Digital art",
      "Data fusion",
      "SSRN (identifier)",
      "Sequential pattern mining",
      "Long short-term memory",
      "Network performance",
      "Theoretical computer science",
      "Computational complexity theory",
      "Web scraping"
    ]
  },
  "Hypertext": {
    "url": "https://en.wikipedia.org/wiki/Hypertext",
    "title": "Hypertext",
    "content": "Hypertext is text displayed on a computer display or other electronic devices with references ( hyperlinks ) to other text that the reader can immediately access. [ 1 ] Hypertext documents are interconnected by hyperlinks, which are typically activated by a mouse click, keypress set, or screen touch. Apart from text, the term \"hypertext\" is also used to describe tables, images, and other presentational materials with integrated hyperlinks. Hypertext is one of the key underlying concepts of the World Wide Web , [ 2 ] where Web pages are often written in the Hypertext Markup Language (HTML). As implemented on the Web, hypertext enables the easy-to-use publication of information over the Internet . \"(...)'Hypertext' is a recent coinage. 'Hyper-' is used in the mathematical sense of extension and generality (as in 'hyperspace,' 'hypercube') rather than the medical sense of 'excessive' ('hyperactivity'). There is no implication about size — a hypertext could contain only 500 words or so. 'Hyper-' refers to structure and not size.\" — Theodor H. Nelson , Brief Words on the Hypertext , 23 January 1967 The English prefix \"hyper-\" comes from the Greek prefix \"ὑπερ-\" and means \"over\" or \"beyond\"; it has a common origin with the prefix \"super-\" which comes from Latin. It signifies the overcoming of the previous linear constraints of written text. The term \"hypertext\" is often used where the term \" hypermedia \" might seem appropriate. In 1992, author Ted Nelson – who coined both terms in 1963 [ 3 ] [ 4 ] – wrote: By now the word \"hypertext\" has become generally accepted for branching and responding text, but the corresponding word \"hypermedia\", meaning complexes of branching and responding graphics, movies and sound – as well as text – is much less used. Instead they use the strange term \"interactive multimedia\": this is four syllables longer, and does not express the idea of extending hypertext. — Nelson , Literary Machines , 1992 Hypertext documents can either be static (prepared and stored in advance) or dynamic (continually changing in response to user input, such as dynamic web pages ). Static hypertext can be used to cross-reference collections of data in documents, software applications , or books on CDs . A well-constructed system can also incorporate other user-interface conventions, such as menus and command lines. Links used in a hypertext document usually replace the current piece of hypertext with the destination document. A lesser known feature is StretchText , which expands or contracts the content in place, thereby giving more control to the reader in determining the level of detail of the displayed document. Some implementations support transclusion , where text or other content is included by reference and automatically rendered in place. Hypertext can be used to support very complex and dynamic systems of linking and cross-referencing. The most famous implementation of hypertext is the World Wide Web , written in the final months of 1990 and released on the Internet in 1991. In 1941, Jorge Luis Borges published \" The Garden of Forking Paths \", a short story that is often considered an inspiration for the concept of hypertext. [ 5 ] In 1945, Vannevar Bush wrote an article in The Atlantic Monthly called \" As We May Think \", about a futuristic proto-hypertext device he called a Memex . A Memex would hypothetically store — and record — content on reels of microfilm, using electric photocells to read coded symbols recorded next to individual microfilm frames while the reels spun at high speed, and stopping on command. The coded symbols would enable the Memex to index, search, and link content to create and follow associative trails. Because the Memex was never implemented and could only link content in a relatively crude fashion — by creating chains of entire microfilm frames — the Memex is regarded only as a proto-hypertext device, but it is fundamental to the history of hypertext because it directly inspired the invention of hypertext by Ted Nelson and Douglas Engelbart. In 1965, Ted Nelson coined the terms 'hypertext' and 'hypermedia' as part of a model he developed for creating and using linked content (first published reference 1965). [ 7 ] He later worked with Andries van Dam to develop the Hypertext Editing System (text editing) in 1967 at Brown University . It was implemented using the terminal IBM 2250 with a light pen which was provided as a pointing device . [ 8 ] By 1976, its successor FRESS was used in a poetry class in which students could browse a hyperlinked set of poems and discussion by experts, faculty and other students, in what was arguably the world's first online scholarly community [ 9 ] which van Dam says \"foreshadowed wikis, blogs and communal documents of all kinds\". [ 10 ] Ted Nelson said in the 1960s that he began implementation of a hypertext system he theorized, which was named Project Xanadu , but his first and incomplete public release was finished much later, in 1998. [ 6 ] During this period, Nelson also proposed using Vladimir Nabokov’s 1962 novel Pale Fire as part of a demonstration to IBM, intending to show how hypertext could support complex, non-linear forms of literary analysis. The novel, structured as a long poem with an extensive, self-referential commentary and index, embodied the principles of associative linking and user-directed navigation that Nelson believed defined hypertext. [ 11 ] Its layered design enabled readers to follow multiple interpretive paths through the text, resembling the branching structures later implemented in digital hypertext systems. However, IBM chose a more technically conventional presentation, and the literary demonstration was never realized. [ 12 ] Douglas Engelbart independently began working on his NLS system in 1962 at Stanford Research Institute, although delays in obtaining funding, personnel, and equipment meant that its key features were not completed until 1968. In December of that year, Engelbart demonstrated a 'hypertext' (meaning editing) interface to the public for the first time, in what has come to be known as \" The Mother of All Demos \". In 1971 a system called Scrapbook , produced by David Yates and his team at the UK's National Physical Laboratory , went live. It was an information storage and retrieval system that included what would now be called word processing, e-mail and hypertext. ZOG , an early hypertext system, was developed at Carnegie Mellon University during the 1970s, used for documents on Nimitz class aircraft carriers, and later evolving as KMS (Knowledge Management System). The first hypermedia application is generally considered to be the Aspen Movie Map , implemented in 1978. The Movie Map allowed users to arbitrarily choose which way they wished to drive in a virtual cityscape, in two seasons (from actual photographs) as well as 3-D polygons . In France, the launch of the Minitel system in 1982 provided widespread public access to interactive digital content via telephone lines and videotex terminals. Minitel allowed users to search directories, make purchases, read news, and access databases using a system of on-screen menus and numbered links. Although it was based on videotex rather than the dynamic linking protocols of later hypertext systems, Minitel introduced many users to the practice of navigating non-linear networks of information. Its use of branching menus and user-selected paths anticipated key aspects of hypertext interaction, particularly the idea of browsing through interconnected data by following associative or logical links. As one of the earliest large-scale deployments of an online information service, Minitel helped familiarize the public with interactive computing and laid cultural groundwork for the broader adoption of hypertext and web technologies in the 1990s. [ 13 ] Between 1984 and 1987 Frank Halasz, Randall Trigg, and Thomas Moran developed NoteCards at Xerox PARC. This early hypertext system was designed to support information analysis and idea processing, employing a central metaphor of \"notecards\" which operated as discrete units of information that could contain text or graphics. These notecards could be interconnected through typed, directional links, enabling users to create semantically distinct relationships. A key component of NoteCards was the \"Browser card,\" which provided a graphical overview of the structure of linked notecards, facilitating navigation within complex information networks. [ 14 ] Operating on Xerox Lisp machines, NoteCards' primary impact was within the research community rather than as a commercial product. Its most significant contribution to the field of hypertext is often attributed to the insights gained from its use, Halasz identified critical challenges such as search and query in large hypertexts, composite structures, versioning, and collaborative work. [ 15 ] In 1980, Tim Berners-Lee created ENQUIRE , an early hypertext database system somewhat like a wiki but without hypertext punctuation, which was not invented until 1987. The early 1980s also saw a number of experimental \"hyperediting\" functions in word processors and hypermedia programs, many of whose features and terminology were later analogous to the World Wide Web . Guide , the first significant hypertext system for personal computers , was developed by Peter J. Brown at the University of Kent in 1982. In 1980, Roberto Busa , [ 16 ] an Italian Jesuit priest and one of the pioneers in the usage of computers for linguistic and literary analysis, [ 17 ] published the Index Thomisticus , as a tool for performing text searches within the massive corpus of Aquinas 's works. [ 18 ] Sponsored by the founder of IBM, Thomas J. Watson , [ 19 ] the project lasted about 30 years (1949–1980), and eventually produced the 56 printed volumes of the Index Thomisticus the first important hypertext work about Saint Thomas Aquinas books and of a few related authors. [ 20 ] In 1983, Ben Shneiderman at the University of Maryland Human - Computer Interaction Lab led a group that developed the HyperTies system that was commercialized by Cognetics Corporation . They studied many designs before adopting the blue color for links . Hyperties was used to create the July 1988 issue of the Communications of the ACM as a hypertext document and then the first commercial electronic book Hypertext Hands-On! . In August 1987, Apple Computer released HyperCard for the Macintosh line at the MacWorld convention . Its impact, combined with interest in Peter J. Brown's GUIDE (marketed by OWL and released earlier that year) and Brown University's Intermedia , led to broad interest in and enthusiasm for hypertext, hypermedia, databases, and new media in general. The first ACM Hypertext (hyperediting and databases) academic conference took place in November 1987, in Chapel Hill NC, where many other applications, including the branched literature writing software Storyspace , were also demonstrated. [ 21 ] Meanwhile, Nelson (who had been working on and advocating his Xanadu system for over two decades) convinced Autodesk to invest in his revolutionary ideas. The project continued at Autodesk for four years, but no product was released. In 1989, Tim Berners-Lee, then a scientist at CERN , proposed and later prototyped a new hypertext project in response to a request for a simple, immediate, information-sharing facility, to be used among physicists working at CERN and other academic institutions. He called the project \"WorldWideWeb\". [ 22 ] HyperText is a way to link and access information of various kinds as a web of nodes in which the user can browse at will. Potentially, HyperText provides a single user-interface to many large classes of stored information, such as reports, notes, data-bases, computer documentation and on-line systems help. We propose the implementation of a simple scheme to incorporate several different servers of machine-stored information already available at CERN, including an analysis of the requirements for information access needs by experiments... A program which provides access to the hypertext world we call a browser. ― T. Berners-Lee, R. Cailliau, 12 November 1990, CERN [ 22 ] In 1992, Lynx was born as an early Internet web browser. Its ability to provide hypertext links within documents that could reach into documents anywhere on the Internet began the creation of the Web on the Internet. As new web browsers were released, traffic on the World Wide Web quickly exploded from only 500 known web servers in 1993 to over 10,000 in 1994. As a result, all previous hypertext systems were overshadowed by the success of the Web, even though it lacked many features of those earlier systems, such as integrated browsers/editors (a feature of the original WorldWideWeb browser, which was not carried over into most of the other early Web browsers). Besides the already mentioned Project Xanadu , Hypertext Editing System , NLS , HyperCard , and World Wide Web, there are other noteworthy early implementations of hypertext, with different feature sets: Among the top academic conferences for new research in hypertext is the annual ACM Conference on Hypertext and Social Media . [ 23 ] The Electronic Literature Organization hosts annual conferences discussing hypertext fiction , poetry and other forms of electronic literature . Although not exclusively about hypertext, the World Wide Web series of conferences, organized by IW3C2 , [ 24 ] also include many papers of interest. There is a list on the Web with links to all conferences in the series. [ 25 ] Hypertext writing has developed its own style of fiction, coinciding with the growth and proliferation of hypertext development software and the emergence of electronic networks. Hypertext fiction is one of earliest genres of electronic literature , or literary works that are designed to be read in digital media. Two software programs specifically designed for literary hypertext, Storyspace and Intermedia , became available in the 1990s. Judy Malloy 's Uncle Roger (1986) and Michael Joyce 's afternoon, a story (1987) are generally considered the first works of hypertext fiction. [ 26 ] [ 27 ] An advantage of writing a narrative using hypertext technology is that the meaning of the story can be conveyed through a sense of spatiality and perspective that is arguably unique to digitally networked environments. An author's creative use of nodes, the self-contained units of meaning in a hypertextual narrative, can play with the reader's orientation and add meaning to the text. One of the most successful computer games, Myst , was first written in HyperCard. The game was constructed as a series of Ages, each Age consisting of a separate HyperCard stack. The full stack of the game consists of over 2500 cards. In some ways, Myst redefined interactive fiction, using puzzles and exploration as a replacement for hypertextual narrative. [ 28 ] Critics of hypertext claim that it inhibits the old, linear, reader experience by creating several different tracks to read on. This can also been seen as contributing to a postmodernist fragmentation of worlds. In some cases, hypertext may be detrimental to the development of appealing stories (in the case of hypertext Gamebooks ), where ease of linking fragments may lead to non-cohesive or incomprehensible narratives. [ 29 ] However, they do see value in its ability to present several different views on the same subject in a simple way. [ 30 ] This echoes the arguments of 'medium theorists' like Marshall McLuhan who look at the social and psychological impacts of the media. New media can become so dominant in public culture that they effectively create a \"paradigm shift\" [ 31 ] as people have shifted their perceptions, understanding of the world, and ways of interacting with the world and each other in relation to new technologies and media. So hypertext signifies a change from linear, structured and hierarchical forms of representing and understanding the world into fractured, decentralized and changeable media based on the technological concept of hypertext links. In the 1990s, women and feminist artists took advantage of hypertext and produced dozens of works. Linda Dement 's Cyberflesh Girlmonster a hypertext CD-ROM that incorporates images of women's body parts and remixes them to create new monstrous yet beautiful shapes. Caitlin Fisher's award-winning online hypertext novella These Waves of Girls (2001) is set in three time periods of the protagonist exploring polymorphous perversity enacted in her queer identity through memory. The story is written as a reflection diary of the interconnected memories of childhood, adolescence, and adulthood. It consists of an associated multi-modal collection of nodes includes linked text, still and moving images, manipulable images, animations, and sound clips. Adrienne Eisen (pen name for Penelope Trunk ) wrote hypertexts that were subversive narrative journeys into the mind of a woman whose erotic encounters were charged with a post-feminist satirical edge that cuts deep into the American psyche. There are various forms of hypertext fiction, each of which is structured differently. Below are four:",
    "links": [
      "Issue-based information system",
      "Electronic literature",
      "These Waves of Girls",
      "Internationalized Resource Identifier",
      "Information access",
      "Ecogovernmentality",
      "Guide (hypertext)",
      "FRESS",
      "Doi (identifier)",
      "HCalendar",
      "Thomas J. Watson",
      "Light pen",
      "TriG (syntax)",
      "Daniel Miller (anthropologist)",
      "Douglas Engelbart",
      "Computer display",
      "Science, technology and society",
      "Semantic HTML",
      "Morphological analysis (problem-solving)",
      "Web engineering",
      "Wiki",
      "Olog",
      "S2CID (identifier)",
      "Facebook Platform",
      "JSTOR (identifier)",
      "Hyperlinks",
      "Radial tree",
      "Visual analytics",
      "Amiga",
      "Windows Help",
      "Entity–relationship model",
      "Cross-reference",
      "Gabriella Coleman",
      "As We May Think",
      "Greek language",
      "Office Workstations Ltd",
      "Sadie Plant",
      "Http",
      "Semantic Web Rule Language",
      "Bibliographic Ontology",
      "Microformat",
      "Organizational chart",
      "Tree structure",
      "Cognitive map",
      "Hdl (identifier)",
      "Semantic service-oriented architecture",
      "Graphic communication",
      "Hypertext fiction",
      "HRecipe",
      "Digital anthropology",
      "Ecological anthropology",
      "Semantics (computer science)",
      "ZigZag (software)",
      "Afternoon, a story",
      "COinS",
      "SHACL",
      "Distributed Data Management Architecture",
      "Linda Dement",
      "Cyborg anthropology",
      "E-text",
      "StretchText",
      "GRDDL",
      "Brown University",
      "Anchor text",
      "Concept lattice",
      "Pointing device",
      "Michael Joyce (writer)",
      "Andries van Dam",
      "Lewis H. Morgan",
      "Semantic Web",
      "CD-ROM",
      "Wiki software",
      "Hyperbolic tree",
      "Environmental anthropology",
      "Timeline",
      "Dendrogram",
      "Semantic reasoner",
      "List of concept- and mind-mapping software",
      "Postmodernist",
      "Hypertext Editing System",
      "Literary Machines",
      "Jorge Luis Borges",
      "SPARQL",
      "KMS (hypertext)",
      "Semantic matching",
      "Graph drawing",
      "Knowledge representation and reasoning",
      "Semantically Interlinked Online Communities",
      "Semantic computing",
      "University of Kent",
      "Texinfo",
      "Symbolics Document Examiner",
      "Marcel Mauss",
      "Sociogram",
      "Digital humanities",
      "Database",
      "Diagrammatic reasoning",
      "Treemapping",
      "Mizuko Ito",
      "Mental model",
      "HAtom",
      "Judy Malloy",
      "Rule-based system",
      "Academic conference",
      "GNU",
      "University of Maryland Human - Computer Interaction Lab",
      "HProduct",
      "Roberto Busa",
      "Ontology",
      "Ted Nelson",
      "Society of Jesus",
      "XLink",
      "ACM Conference on Hypertext and Social Media",
      "Semantic analytics",
      "Electronic Document System",
      "RDF Schema",
      "Timeline of hypertext technology",
      "Portable Document Format",
      "Dublin Core",
      "Mind map",
      "Communications of the ACM",
      "Web page",
      "Cognetics Corporation",
      "IBM 2250",
      "Aspen Movie Map",
      "Geotagging",
      "Marshall McLuhan",
      "Donna Haraway",
      "Design rationale",
      "Electronic Literature Organization",
      "IXBRL",
      "Hyperkino",
      "The Atlantic Monthly",
      "Object–role modeling",
      "Wicked problem",
      "Paul Virilio",
      "Gamebook",
      "Linked data",
      "Mark Fisher",
      "Simple Knowledge Organization System",
      "Web Ontology Language",
      "Issue tree",
      "Edmund Snow Carpenter",
      "International World Wide Web Conference Committee",
      "Web 2.0",
      "Computer Decisions",
      "Dataspaces",
      "Mouse (computing)",
      "Amigaguide",
      "Argument map",
      "Reza Negarestani",
      "Conceptual graph",
      "Fanged Noumena",
      "CERN",
      "Lynx (web browser)",
      "Nature–culture divide",
      "Notation3",
      "Data and information visualization",
      "Information Presentation Facility",
      "Nick Land",
      "Cultural ecology",
      "Visual language",
      "The Mother of All Demos",
      "Collective intelligence",
      "World Wide Web",
      "Transclusion",
      "Information architecture",
      "Apple Macintosh",
      "Carnegie Mellon University",
      "Resource Description Framework",
      "Roy Ellen",
      "Cybernetic Culture Research Unit",
      "Aquinas",
      "Metadata Object Description Schema",
      "Index Thomisticus",
      "Hyperwords",
      "Description logic",
      "Myst",
      "Time (magazine)",
      "Topic map",
      "RDF/XML",
      "Semantic mapper",
      "Intermedia (hypertext)",
      "HTTP",
      "Semantic publishing",
      "HyperCard",
      "Hypertext (semiotics)",
      "Macworld Conference & Expo",
      "Infographic",
      "IMDb (identifier)",
      "RDFa",
      "SAWSDL",
      "Rave",
      "BIBFRAME",
      "HCard",
      "Research Resource Identifier",
      "Business decision mapping",
      "La Stampa",
      "Microdata (HTML)",
      "WIRED",
      "Internet",
      "Pathfinder network",
      "Hyperlink",
      "Mike Wesch",
      "Metadata Authority Description Schema",
      "Cybertext",
      "Concept map",
      "Saint Thomas Aquinas",
      "Schema.org",
      "Bruno Latour",
      "Canada",
      "XML",
      "HReview",
      "Tom Boellstorff",
      "Electronic devices",
      "N-Triples",
      "Patchwork Girl (hypertext)",
      "Information design",
      "Uniform Resource Identifier",
      "HTML",
      "Library 2.0",
      "ENQUIRE",
      "Paul Vitanyi",
      "Cladistics",
      "Tim Berners-Lee",
      "Problem structuring methods",
      "Metadata",
      "Knowledge management",
      "JSON-LD",
      "Semantic network",
      "Knowledge visualization",
      "Storyspace",
      "Polygon mesh",
      "Vannevar Bush",
      "Hypertext Markup Language",
      "Social anthropology",
      "Capitalist Realism",
      "Ben Shneiderman",
      "Knowledge extraction",
      "Rule Interchange Format",
      "The Virtual Disappearance of Miriam",
      "Wayback Machine",
      "Semantic search",
      "ZOG (hypertext)",
      "DOAP",
      "Shelley Jackson",
      "Folksonomy",
      "Application software",
      "Semantic triple",
      "TriX (serialization format)",
      "Hari Kunzru",
      "NLS (computer system)",
      "Penelope Trunk",
      "ISSN (identifier)",
      "FOAF",
      "NoteCards",
      "Semantic wiki",
      "Digital library",
      "Memex",
      "The Garden of Forking Paths",
      "ISBN (identifier)",
      "The Interactive Encyclopedia System",
      "Leslie White",
      "History of hypertext",
      "1UP.com",
      "Reference (computer science)",
      "Web Science Trust",
      "Workbench (AmigaOS)",
      "Geovisualization",
      "Benjamin H. Bratton",
      "Autodesk",
      "Solid (web decentralization project)",
      "Decision tree",
      "Layered graph drawing",
      "Schema (psychology)",
      "Ray Brassier",
      "Dynamic web page",
      "Personal computer",
      "Symbolics",
      "Cultural anthropology",
      "Project Xanadu",
      "Hypermedia",
      "André Leroi-Gourhan",
      "Common Logic",
      "Ontology (information science)",
      "Apple Computer",
      "Hyperdata",
      "Turtle (syntax)",
      "Compact disc",
      "Short story",
      "IBM",
      "Political ecology",
      "National Physical Laboratory (United Kingdom)"
    ]
  },
  "Independence (mathematical logic)": {
    "url": "https://en.wikipedia.org/wiki/Independence_(mathematical_logic)",
    "title": "Independence (mathematical logic)",
    "content": "In mathematical logic , independence is the unprovability of some specific sentence from some specific set of other sentences. The sentences in this set are referred to as \"axioms\". A sentence σ is independent of a given first-order theory T if T neither proves nor refutes σ; that is, it is impossible to prove σ from T , and it is also impossible to prove from T that σ is false. Sometimes, σ is said (synonymously) to be undecidable from T . (This concept is unrelated to the idea of \" decidability \" as in a decision problem .) A theory T is independent if no axiom in T is provable from the remaining axioms in T . A theory for which there is an independent set of axioms is independently axiomatizable . Some authors say that σ is independent of T when T simply cannot prove σ, and do not necessarily assert by this that T cannot refute σ. These authors will sometimes say \"σ is independent of and consistent with T \" to indicate that T can neither prove nor refute σ. Many interesting statements in set theory are independent of Zermelo–Fraenkel set theory (ZF). The following statements in set theory are known to be independent of ZF, under the assumption that ZF is consistent: The following statements (none of which have been proved false) cannot be proved in ZFC (the Zermelo–Fraenkel set theory plus the axiom of choice) to be independent of ZFC, under the added hypothesis that ZFC is consistent. The following statements are inconsistent with the axiom of choice, and therefore with ZFC. However they are probably independent of ZF, in a corresponding sense to the above: They cannot be proved in ZF, and few working set theorists expect to find a refutation in ZF. However ZF cannot prove that they are independent of ZF, even with the added hypothesis that ZF is consistent. Since 2000, logical independence has become understood as having crucial significance in the foundations of physics. [ 1 ] [ 2 ]",
    "links": [
      "Kolmogorov complexity",
      "Cantor's theorem",
      "Propositional variable",
      "Algebraic logic",
      "Boolean function",
      "Hilbert's axioms",
      "Doi (identifier)",
      "Model theory",
      "Monadic second-order logic",
      "String (formal languages)",
      "Lambda calculus",
      "Many-valued logic",
      "Three-valued logic",
      "Image (mathematics)",
      "New Foundations",
      "Schröder–Bernstein theorem",
      "Von Neumann–Bernays–Gödel set theory",
      "Predicate (mathematical logic)",
      "Free variables and bound variables",
      "List of Hilbert systems",
      "Universal set",
      "Logicism",
      "Rule of inference",
      "Uniqueness quantification",
      "Proposition",
      "Element (mathematics)",
      "Turing machine",
      "Material conditional",
      "Inhabited set",
      "Atomic formula",
      "Lemma (mathematics)",
      "Tarski's undefinability theorem",
      "Signature (logic)",
      "Minimal axioms for Boolean algebra",
      "Formation rule",
      "Euclid's Elements",
      "Compactness theorem",
      "Negation",
      "Finite-valued logic",
      "Semantics of logic",
      "List of mathematical theories",
      "Timeline of mathematical logic",
      "Springer-Verlag",
      "Equiconsistency",
      "List of set identities and relations",
      "Hereditary set",
      "Type (model theory)",
      "Abstract logic",
      "Logical truth",
      "Axiom",
      "Sentence (mathematical logic)",
      "Intersection (set theory)",
      "Prime model",
      "Automated theorem proving",
      "History of mathematical logic",
      "Logical constant",
      "Formal system",
      "Second-order arithmetic",
      "Non-logical symbol",
      "Computability theory",
      "Universal quantification",
      "Kripke–Platek set theory",
      "Non-Euclidean geometry",
      "Transitive set",
      "Partition of a set",
      "Singleton (mathematics)",
      "Robinson arithmetic",
      "Ground formula",
      "Soundness",
      "Free logic",
      "Model complete theory",
      "Infinite-valued logic",
      "Monadic predicate calculus",
      "Extensionality",
      "Formal grammar",
      "Non-standard model of arithmetic",
      "Bibcode (identifier)",
      "List of statements independent of ZFC",
      "Axiom schema",
      "Reports on Mathematical Physics",
      "List of axioms",
      "Finite set",
      "Truth predicate",
      "Ordinal number",
      "Logical conjunction",
      "Enumeration",
      "Domain of a function",
      "Extension by new constant and function names",
      "Tarski's theory of truth",
      "Self-verifying theories",
      "Elementary function arithmetic",
      "Conservative extension",
      "Uncountable set",
      "Empty set",
      "Term (logic)",
      "Logical equivalence",
      "Classical logic",
      "Axiomatic system",
      "Tarski's axiomatization of the reals",
      "History of logic",
      "Cantor's diagonal argument",
      "Kripke's theory of truth",
      "Axiom of real determinacy",
      "General set theory",
      "Constructive set theory",
      "Automata theory",
      "Decidability (logic)",
      "Continuum hypothesis",
      "Gödel numbering",
      "Church–Turing thesis",
      "Naive set theory",
      "Relation (mathematics)",
      "Variable (mathematics)",
      "Russell's paradox",
      "Category theory",
      "List of first-order theories",
      "Elliott Mendelson",
      "Tautology (logic)",
      "Second-order logic",
      "Tarski–Grothendieck set theory",
      "Syntax (logic)",
      "Gödel's incompleteness theorems",
      "Inaccessible cardinal",
      "Satisfiability",
      "Semantic theory of truth",
      "Function (mathematics)",
      "Peano axioms",
      "Mathematical object",
      "Term logic",
      "Aleph number",
      "Philosophy of mathematics",
      "Logical equality",
      "Logical consequence",
      "Injective function",
      "Theories of truth",
      "Inference",
      "Bijection",
      "Binary operation",
      "Structure (mathematical logic)",
      "Higher-order logic",
      "Strongly inaccessible cardinal",
      "Mathematical logic",
      "ArXiv (identifier)",
      "Information theory",
      "Geometry",
      "Ground expression",
      "Countable set",
      "Ordinal analysis",
      "Cantor's paradox",
      "Addison-Wesley",
      "Boolean algebras canonically defined",
      "Hilbert system",
      "First-order logic",
      "P (complexity)",
      "Atomic sentence",
      "Substructure (mathematics)",
      "Truth value",
      "Large cardinal",
      "Fuzzy set",
      "Skolem arithmetic",
      "Argument",
      "Boolean algebra",
      "Finitary relation",
      "Infinite set",
      "Syllogism",
      "Formal language",
      "Truth table",
      "Saturated model",
      "Codomain",
      "Alphabet (formal languages)",
      "Surjective function",
      "Supertask",
      "Computably enumerable set",
      "Paradoxes of set theory",
      "Logic",
      "Venn diagram",
      "Euclidean geometry",
      "Functional predicate",
      "Complement (set theory)",
      "Decision problem",
      "Quantifier rank",
      "True arithmetic",
      "Löwenheim–Skolem theorem",
      "Halting problem",
      "Operation (mathematics)",
      "Axiomatization of Boolean algebras",
      "Foundations of geometry",
      "New Journal of Physics",
      "Sequent calculus",
      "Parallel postulate",
      "Proof theory",
      "Complete theory",
      "Non-standard model",
      "Logical disjunction",
      "Equivalence relation",
      "Principia Mathematica",
      "List of formal systems",
      "Elementary diagram",
      "Validity (logic)",
      "Axiom of choice",
      "Cardinality",
      "Union (set theory)",
      "Theorem",
      "Square of opposition",
      "Morse–Kelley set theory",
      "Church encoding",
      "Logical connective",
      "Natural deduction",
      "Construction of the real numbers",
      "Chapman & Hall",
      "Diagram (mathematical logic)",
      "Primitive recursive function",
      "Zermelo–Fraenkel set theory",
      "Metalanguage",
      "Strength (mathematical logic)",
      "Recursion",
      "Category of sets",
      "Recursive set",
      "Undecidable problem",
      "Universe (mathematics)",
      "Theory (mathematical logic)",
      "Uninterpreted function",
      "Map (mathematics)",
      "Kurepa tree",
      "Gödel's completeness theorem",
      "Interpretation function",
      "Proof of impossibility",
      "Type theory",
      "Isomorphism",
      "Tarski's axioms",
      "Ultrafilter (set theory)",
      "Reverse mathematics",
      "Power set",
      "Forcing (mathematics)",
      "Constructible universe",
      "P versus NP problem",
      "Axiom of determinacy",
      "Substitution (logic)",
      "Predicate logic",
      "Ultraproduct",
      "Quantifier (logic)",
      "Cartesian product",
      "Logical biconditional",
      "Interpretation (logic)",
      "Spectrum of a theory",
      "Set (mathematics)",
      "Interpretation (model theory)",
      "Well-formed formula",
      "ISBN (identifier)",
      "Urelement",
      "Arity",
      "NP (complexity)",
      "Concrete category",
      "Categorical theory",
      "Spectrum of a sentence",
      "Elementary equivalence",
      "Computable function",
      "Banach–Tarski paradox",
      "Propositional calculus",
      "Consistency",
      "Von Neumann universe",
      "Ackermann set theory",
      "AD+",
      "Predicate variable",
      "Primitive recursive arithmetic",
      "Extension by definitions",
      "Grothendieck universe",
      "Suslin's problem",
      "Deductive system",
      "Formal proof",
      "Class (set theory)",
      "Category (mathematics)",
      "Foundations of mathematics",
      "Transfer principle",
      "Expression (mathematics)",
      "Playfair's axiom",
      "Set theory",
      "Open formula",
      "Symbol (formal)",
      "Fixed-point logic",
      "Computable set",
      "Finite model theory",
      "T-schema",
      "Existential quantification",
      "Lindström's theorem",
      "Propositional formula",
      "Atomic model (mathematical logic)",
      "Formal semantics (logic)"
    ]
  },
  "Information needs": {
    "url": "https://en.wikipedia.org/wiki/Information_needs",
    "title": "Information needs",
    "content": "In information science , library science , and information retrieval , an information need is person's gap in knowledge leading to a description of information they lack. [ 1 ] It is closely related to relevance : if something is relevant for a person in relation to a given task, the person needs the information for that task. [ 2 ] Information needs are related to, but distinct from information requirements . They are studied for: The concept of information needs was coined by Robert S. Taylor , an American information journalist, in a 1962 article \"The Process of Asking Questions\". [ 3 ] In this paper, Taylor attempted to describe how an inquirer obtains an answer from an information system , by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system. According to Taylor, information need has four levels: There are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback). Herbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles: William J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research . In the framework, he places the information users at the centre of ten systems, which are: \"In 2012, the University of Southern California was funded by the Federal Communications Commission to examine a wide range of social sciences from multiple disciplines to propose a set of critical information needs,\" according to Friedland. [ 4 ] He continued, \"USC reached out to a team of scholars collectively identified as the Communications Policy Research Network (CPRN). ... CPRN found that communities need access to eight categories of critical information ...:",
    "links": [
      "Information seeking",
      "ISBN (identifier)",
      "Information system",
      "University of Southern California",
      "Doi (identifier)",
      "Library science",
      "User research",
      "Federal Communications Commission",
      "ISSN (identifier)",
      "Needs",
      "Information science",
      "Robert Saxton Taylor",
      "Information retrieval",
      "Relevance"
    ]
  },
  "Database": {
    "url": "https://en.wikipedia.org/wiki/Database",
    "title": "Database",
    "content": "In computing , a database is an organized collection of data or a type of data store based on the use of a database management system ( DBMS ), the software that interacts with end users , applications , and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system . Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data have become widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file . Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system , while large databases are hosted on computer clusters or cloud storage . The design of databases spans formal techniques and practical considerations, including data modeling , efficient data representation and storage, query languages , security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance . Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables , and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL , because they use different query languages . Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. Because of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it. Outside the world of professional information technology , the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system. [ 1 ] Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups: Both a database and its DBMS conform to the principles of a particular database model . [ 5 ] \"Database system\" refers collectively to the database model, database management system, and database. [ 6 ] Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments . DBMSs are found at the heart of most database applications . DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions. [ citation needed ] Since DBMSs comprise a significant market , computer and storage vendors often take into account DBMS requirements in their own development plans. [ 7 ] Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML ), the type(s) of computer they run on (from a server cluster to a mobile phone ), the query language (s) used to access the database (such as SQL or XQuery ), and their internal engineering, which affects performance, scalability , resilience, and security. The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors , computer memory , computer storage , and computer networks . The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks , which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape . The subsequent development of database technology can be divided into three eras based on data model or structure: navigational , [ 8 ] SQL/ relational , and post-relational. The two main early navigational data models were the hierarchical model and the CODASYL model ( network model ). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another. The relational model , first proposed in 1970 by Edgar F. Codd , departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity . Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018 [update] they remain dominant: IBM Db2 , Oracle , MySQL , and Microsoft SQL Server are the most searched DBMS . [ 9 ] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models. [ citation needed ] Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch , which led to the coining of the term \"post-relational\" and also the development of hybrid object–relational databases . The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases . A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs. The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing . The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense. [ 10 ] As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman , author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL , the group responsible for the creation and standardization of COBOL . In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach , and soon a number of commercial products based on this approach entered the market. The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods: Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API ). However, CODASYL databases were complex and required significant training and effort to produce useful applications. IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360 . IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator . IMS is classified by IBM as a hierarchical database . IDMS and Cincom Systems ' TOTAL databases are classified as network databases. IMS remains in use as of 2014 [update] . [ 11 ] Edgar F. Codd worked at IBM in San Jose, California , in an office primarily involved in the development of hard disk systems. [ 12 ] He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks . [ 13 ] The paper described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of \" tables \", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations ) aimed to ensure that each \"fact\" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated. Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based. The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus ; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit. In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys. For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided. As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic. Codd's paper inspired teams at various universities to research the subject, including one at University of California, Berkeley [ 12 ] led by Eugene Wong and Michael Stonebraker , who started INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access , known as QUEL . Over time, INGRES moved to the emerging SQL standard. IBM itself did one test implementation of the relational model, PRTV , and a production one, Business System 12 , both now discontinued. Honeywell wrote MRDS for Multics , and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs. In 1970, the University of Michigan began development of the MICRO Information Management System [ 14 ] based on D.L. Childs ' Set-Theoretic Data model. [ 15 ] [ 16 ] [ 17 ] The university in 1974 hosted a debate between Codd and Bachman which Bruce Lindsay of IBM later described as \"throwing lightning bolts at each other!\". [ 12 ] MICRO was used to manage very large data sets by the US Department of Labor , the U.S. Environmental Protection Agency , and researchers from the University of Alberta , the University of Michigan , and Wayne State University . It ran on IBM mainframe computers using the Michigan Terminal System . [ 18 ] The system remained in production until 1998. In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38 , the early offering of Teradata , and the Britton Lee, Inc. database machine. Another approach to hardware support for database management was ICL 's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle ( Exadata ). IBM formed a team led by Codd that started working on a prototype system, System R despite opposition from others at the company. [ 12 ] The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL [ citation needed ] – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS , and, later, Database 2 ( IBM Db2 ). Larry Ellison 's Oracle Database (or more simply, Oracle ) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979. [ 19 ] Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL . PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store , as do many large companies and financial institutions). In Sweden, Codd's paper was also read and Mimer SQL was developed in the mid-1970s at Uppsala University . In 1984, this project was consolidated into an independent enterprise. Another data model, the entity–relationship model , emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant. [ citation needed ] Besides IBM and various software companies such as Sybase and Informix Corporation , most large computer hardware vendors by the 1980s had their own database systems such as DEC 's VAX Rdb/VMS . [ 20 ] The decade ushered in the age of desktop computing . The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE . The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff , the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\" [ 21 ] dBASE was one of the top selling software titles in the 1980s and early 1990s. By the start of the decade databases had become a billion-dollar industry in about ten years. [ 20 ] The 1990s, along with a rise in object-oriented programming , saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects . That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields. [ 22 ] The term \" object–relational impedance mismatch \" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem. Database sales grew rapidly during the dotcom bubble and, after its end, the rise of ecommerce . The popularity of open source databases such as MySQL has grown since 2000, to the extent that Ken Jacobs of Oracle said in 2005 that perhaps \"these guys are doing to us what we did to IBM\". [ 20 ] XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records. NoSQL databases are often very fast, [ 23 ] [ 24 ] do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally . In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem , it is impossible for a distributed system to simultaneously provide consistency , availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency. NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system. Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software ). Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems , computerized parts inventory systems , and many content management systems that store websites as collections of webpages in a database. One way to classify databases involves the type of their contents, for example: bibliographic , document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases. Connolly and Begg define database management system (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database.\" [ 28 ] Examples of DBMS's include MySQL , MariaDB , PostgreSQL , Microsoft SQL Server , Oracle Database , and Microsoft Access . The DBMS acronym is sometimes extended to indicate the underlying database model , with RDBMS for the relational , OODBMS for the object (oriented) and ORDBMS for the object–relational model . Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems. The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide: [ 29 ] It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities. [ 30 ] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine . Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount. The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime. [ a ] Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier. [ 32 ] A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email. External interaction with the database will be via an application program that interfaces with the DBMS. [ 33 ] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information. A programmer will code interactions to the database (sometimes referred to as a datasource ) via an application program interface (API) or via a database language . The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET . Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages : Database languages are specific to a particular data model. Notable examples include: A database language may also incorporate features like: Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata , \"data about the data\", and internal data structures ) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database. [ 37 ] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database). Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database. Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases . Often storage redundancy is employed to increase performance. A common example is storing materialized views , which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy. Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated. With data virtualization , the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach. [ 38 ] Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program). Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces. This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases. Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security ), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption ). Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause. [ 39 ] Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash . A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock , etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). The acronym ACID describes some ideal properties of a database transaction: atomicity , consistency , isolation , and durability . A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs. After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.). When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation. After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned ( tuning ) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc. Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state. Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the * Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. [ 40 ] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc. Other DBMS features might include: Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as \" DevOps for database\". [ 41 ] The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model , often with the aid of drawing tools. Another popular approach is the Unified Modeling Language . A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes. Producing the conceptual data model sometimes involves input from business processes , or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data. Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design). The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization . The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency. The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design , and the output is the physical data model . A key goal during this stage is data independence , meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS. Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself. A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format. Common logical data models for databases include: An object–relational database combines the two related structures. Physical data models include: Other models include: Specialized models are optimized for particular types of data: A database management system provides three views of the database data: While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database. The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model. [ 43 ] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance. The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types. Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research ). Research activity includes theory and development of prototypes . Notable research topics have included models , the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID , and more. The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems -TODS, Data and Knowledge Engineering -DKE) and annual conferences (e.g., ACM SIGMOD , ACM PODS , VLDB , IEEE ICDE).",
    "links": [
      "Data Base Task Group",
      "Very-large-scale integration",
      "Spreadsheets",
      "Video game",
      "Mobile phone",
      "Data integrity",
      "Artificial intelligence",
      "Database index",
      "Database object",
      "Column-oriented DBMS",
      "Networking hardware",
      "Web engineering",
      "Facebook Platform",
      "Conformational dynamics data bank",
      "Central processing unit",
      "Entity–relationship model",
      "Cardinality (data modeling)",
      "Rendering (computer graphics)",
      "Multiprocessing",
      "Data (computing)",
      "List of online databases",
      "Website",
      "Database publishing",
      "Microformat",
      "Data control language",
      "Programming team",
      "Semantic service-oriented architecture",
      "Interaction design",
      "Teradata",
      "Computational geometry",
      "Industrial process control",
      "Event store",
      "Data hierarchy",
      "HRecipe",
      "Software framework",
      "Network database model",
      "Fault tolerance",
      "System on a chip",
      "Digital marketing",
      "Printed circuit board",
      "Fact (data warehouse)",
      "Document management system",
      "List of online real estate databases",
      "Symposium on Principles of Database Systems",
      "SQL",
      "Consistency (database systems)",
      "Information technology",
      "Integrated development environment",
      "Wireless sensor network",
      "Foreign key",
      "Multivalue model",
      "Distributed computing",
      "Software engineering",
      "E-commerce",
      "Computational biology",
      "Semantic matching",
      "Object model",
      "Parts inventory system",
      "Knowledge representation and reasoning",
      "Computer software",
      "Early-arriving fact",
      "Hypertext",
      "Logic programming",
      "Graph database",
      "Digital humanities",
      "Candidate key",
      "Database administration",
      "Reinforcement learning",
      "Hierarchical database model",
      "Academic conference",
      "Star schema",
      "List of databases",
      "Concurrency (computer science)",
      "Knowledge",
      "Oracle database",
      "Hardware security",
      "Library",
      "Open source",
      "Wayne State University",
      "Database-as-IPC",
      "Augmented reality",
      "Unsupervised learning",
      "Geotagging",
      "Create, read, update and delete",
      "Cincom Systems",
      "Form factor (design)",
      "American National Standards Institute",
      "Electronic design automation",
      "EJB QL",
      "Partition (database)",
      "Network architecture",
      "Control theory",
      "Datasource",
      "Dimension (data warehouse)",
      "Data warehouse automation",
      "Superkey",
      "Spreadsheet",
      "MultiDimensional eXpressions",
      "Cyber-physical system",
      "Object-oriented programming",
      "Computer terminal",
      "Modeling language",
      "PRTV",
      "Denormalization",
      "IBM System/38",
      "Dotcom bubble",
      "Formal methods",
      "Mobile database",
      "Database application",
      "Computer security",
      "Shard (database architecture)",
      "Database transaction",
      "Network scheduler",
      "Embedded database",
      "Data virtualization",
      "Description logic",
      "Index (database)",
      "Relational model",
      "Object Data Management Group",
      "Computational mathematics",
      "Topic map",
      "RDF/XML",
      "Computing",
      "Referential integrity",
      "JDBC",
      "BIBFRAME",
      "List of relational database management systems",
      "Flat file",
      "SAWSDL",
      "List of academic databases and search engines",
      "Total cost of ownership",
      "Log shipping",
      "Query Rewriting",
      "US Department of Labor",
      "Research Resource Identifier",
      "System Development Corporation",
      "National Trauma Data Bank",
      "Multiprocessor",
      "Metadata Authority Description Schema",
      "Physical security",
      "Codd's 12 rules",
      "Schema.org",
      "Card file",
      "Algorithm",
      "Prototype",
      "Online encyclopedia",
      "Business process modeling",
      "Heterogeneous database system",
      "David L. Childs",
      "Ling Liu (computer scientist)",
      "Database security",
      "ACM Transactions on Database Systems",
      "Metadata",
      "Knowledge management",
      "Electronic voting",
      "Corporate information factory",
      "Mathematical software",
      "Snowflake schema",
      "Relational calculus",
      "Algorithmic efficiency",
      "CAP theorem",
      "Open-source software",
      "Online analytical processing",
      "Knowledge extraction",
      "Ralph Kimball",
      "Comparison of relational database management systems",
      "Wayback Machine",
      "Software",
      "Hazardous Substances Data Bank",
      "Computational complexity",
      "Kernel (operating system)",
      "QUEL query languages",
      "ISSN (identifier)",
      "Data access",
      "Shared-nothing architecture",
      "Real-time database",
      "Integrated circuit",
      "List of column-oriented DBMSes",
      "SIGMOD",
      "Michigan Terminal System",
      "Human–computer interaction",
      "Client–server architecture",
      "Quantum computing",
      "ISBN (identifier)",
      "Mobile computing",
      "Sixth normal form",
      "Software development process",
      "Requirements analysis",
      "Database server",
      "Mathematical optimization",
      "Reference (computer science)",
      "Star Wars Databank",
      "Data extraction",
      "Protein Data Bank",
      "Web Science Trust",
      "Solid (web decentralization project)",
      "Microsoft Access",
      "Intrusion detection system",
      "Data transformation",
      "Database testing",
      "Statistics",
      "Hypermedia",
      "Common Logic",
      "Uppsala University",
      "Inverted index",
      "Database trigger",
      "Hyperdata",
      "Intelligent database",
      "Two-phase locking",
      "Oxford English Dictionary",
      "Information system",
      "Ecommerce",
      "Multi-model database",
      "Slowly changing dimension",
      "Internationalized Resource Identifier",
      "Comparison of object–relational database management systems",
      "Access control",
      "Distributed transaction",
      "Data vault modeling",
      "Database tuning",
      "Database integrity",
      "Data encryption",
      "S2CID (identifier)",
      "University of Alberta",
      "Database administrator",
      "Pointer (computer programming)",
      "MarkLogic",
      "Anchor modeling",
      "Flight reservation system",
      "Bibliographic Ontology",
      "Surrogate key",
      "Object–relational database",
      "Database model",
      "Database engine",
      "Programming language theory",
      "SHACL",
      "Computer memory",
      "Henry F. Korth",
      "Software maintenance",
      "Data query language",
      "Computer animation",
      "Query language",
      "Hierarchical database",
      "Honeywell",
      "Thermodynamic computing",
      "Cryptography",
      "Concurrent computing",
      "Semantic Web",
      "Cursor (databases)",
      "Image compression",
      "Content Addressable File Store",
      "Computer clusters",
      "Educational technology",
      "Dashboard (business)",
      "Stored procedure",
      "Business intelligence",
      "Academia",
      "DBASE",
      "Domain-specific language",
      "Comparison of object database management systems",
      "List of computer size categories",
      "Software quality",
      "Software repository",
      "Key–value database",
      "List of SQL software and tools",
      "DevOps",
      "Logical data model",
      "Negative database",
      "Software configuration management",
      "Data mining",
      "Relational database",
      "Operating system",
      "Semantic data model",
      "Content management system",
      "Interpreter (computing)",
      "Transaction log",
      "Johannes Gehrke",
      "Key–value store",
      "Email",
      "Software design",
      "Terminology-oriented database",
      "Cloud technology",
      "Journal of Database Management",
      "Social computing",
      "Middleware",
      "Data hub",
      "Informix Corporation",
      "Visualization (graphics)",
      "Shared memory architecture",
      "Raghu Ramakrishnan",
      "Linked data",
      "Query plan",
      "Simple Knowledge Organization System",
      "Web Ontology Language",
      "Dataspaces",
      "Database theory",
      "XQuery API for Java",
      "Database preservation",
      "ODBC",
      "Information theory",
      "Dan Linstedt",
      "Control flow",
      "Supervised learning",
      "Information security",
      "Machine learning",
      "Numerical analysis",
      "Cloud storage",
      "Model of computation",
      "Synonym (database)",
      "B-tree",
      "DBOS",
      "Information architecture",
      "Operational database",
      "Magnetic tape",
      "Resource Description Framework",
      "Single version of the truth",
      "RAID",
      "Real-time computing",
      "Parallelization",
      "Time series database",
      "Hash function",
      "Object–relational model",
      "Halloween Problem",
      "HTTP",
      "Conceptual data model",
      "Enhanced entity–relationship model",
      "Index cards",
      "HCard",
      "Spatial database",
      "Enterprise information system",
      "Data store",
      "Data mart",
      "Association for Computing Machinery",
      "Information retrieval query language",
      "XML for Analysis",
      "Netezza",
      "Data",
      "Automated planning and scheduling",
      "IEEE",
      "Hardware acceleration",
      "Multithreading (computer architecture)",
      "HReview",
      "Eugene Wong",
      "Analysis of algorithms",
      "ACM Computing Classification System",
      "N-Triples",
      "Edgar F. Codd",
      "Data structure",
      "Uniform Resource Identifier",
      "ACID",
      "Information Management System",
      "Information privacy",
      "Software deployment",
      "End user",
      "In-memory database",
      "INGRES",
      "Main memory",
      "Probability",
      "Semantic search",
      "Abraham Silberschatz",
      "Application software",
      "Dependability",
      "Database log",
      "Transaction (database)",
      "Unstructured data",
      "OLAP cube",
      "Comparison of database tools",
      "Theory of computation",
      "Entity–attribute–value model",
      "Datalog",
      "Triplestore",
      "Database language",
      "Outline of computer science",
      "Primary key",
      "Scalability",
      "Relation (database)",
      "Storage media",
      "MySQL",
      "Array DBMS",
      "Business intelligence software",
      "User (computing)",
      "Comparison of OLAP servers",
      "Application programming interface",
      "Compiler construction",
      "Computer programming",
      "Object–relational impedance mismatch",
      "Exadata",
      "Database connection",
      "Degenerate dimension",
      "MariaDB",
      "Virtual reality",
      "Database audit",
      "Computational social science",
      "Microsoft SQL Server",
      "Electron Microscopy Data Bank",
      "Data loading",
      "Deductive database",
      "Information retrieval",
      "VLDB conference",
      "Locks with ordered sharing",
      "Semantics (computer science)",
      "COinS",
      "Parallel database",
      "Database forensics",
      "Discrete mathematics",
      "Integrated Data Store",
      "XML database",
      "Data modeling",
      "Word processor",
      "XQuery",
      "Null (SQL)",
      "Dimension table",
      "Semantic reasoner",
      "Oracle Database",
      "Graph (data structure)",
      "Green computing",
      "Logic in computer science",
      "Transaction processing system",
      "Computer vision",
      "Fuzzy logic",
      "Stochastic computing",
      "Semantic computing",
      "Database design",
      "Multidimensional database",
      "Britton Lee, Inc.",
      "NewSQL",
      "Transaction processing",
      "View (SQL)",
      "Rule-based system",
      "Algorithm design",
      "Christopher J. Date",
      "Geographic information system",
      "Atomicity (database systems)",
      "Database caching",
      "User interface",
      "Data storage",
      "Entity",
      "Market (economics)",
      "ACNielsen",
      "Semantic analytics",
      "Dublin Core",
      "Saxon XSLT",
      "Data processing",
      "Peripheral",
      "Physical data model",
      "Enterprise software",
      "Object (computer science)",
      "Workflow",
      "Multimedia database",
      "Randomized algorithm",
      "Academic journal",
      "Column (database)",
      "Software construction",
      "Armstrong's axioms",
      "Reverse star schema",
      "Web 2.0",
      "Memory bank",
      "Electronic publishing",
      "Operational data store",
      "Hierarchical model",
      "List of biodiversity databases",
      "Data mesh",
      "OCLC (identifier)",
      "World Wide Web",
      "First-order predicate calculus",
      "Database management system",
      "Database-centric architecture",
      "Data and Knowledge Engineering",
      "Aggregate (data warehouse)",
      "Operations research",
      "SQL/XML",
      "Bibliographic database",
      "Temporal database",
      "RDFa",
      "Enterprise resource planning",
      "Programming tool",
      "Internet",
      "Computer science",
      "Sublanguage",
      "Data repository",
      "Photograph manipulation",
      "Database activity monitoring",
      "Virtual machine",
      "Distributed system",
      "Unified Modeling Language",
      "Array data structure",
      "Parallel computing",
      "Network security",
      "Data Source Name",
      "Extract, transform, load",
      "Mathematical analysis",
      "JSON-LD",
      "Social software",
      "Multi-task learning",
      "Relational algebra",
      "Knowledge base",
      "Semantic network",
      "Active database",
      "Lists of databases",
      "Computer storage",
      "DEC (company)",
      "Multitier architecture",
      "Crash (computing)",
      "Communication protocol",
      "Health informatics",
      "Database abstraction layer",
      "Dortmund Data Bank",
      "DOAP",
      "Lotus 1-2-3",
      "Semantic triple",
      "IBM System R",
      "TriX (serialization format)",
      "Computing platform",
      "Materialized view",
      "FOAF",
      "Semantic wiki",
      "Computer network",
      "Open API",
      "Data warehouse",
      "Database schema",
      "Customer relationship management",
      "International Organization for Standardization",
      "Computational problem",
      "Server cluster",
      "Ubiquitous computing",
      "Hard disk",
      "Computer accessibility",
      "Unique key",
      "Mimer SQL",
      "Wide-column store",
      "Distributed artificial intelligence",
      "Query optimizer",
      "Universal Product Code",
      "Graphics processing unit",
      "Consistency model",
      "IBM",
      "HOLAP",
      "NoSQL",
      "Human-centered computing",
      "Eventual consistency",
      "Lock (database)",
      "Solid modeling",
      "Michael Stonebraker",
      "Flat-file database",
      "Doi (identifier)",
      "HCalendar",
      "TriG (syntax)",
      "Semantic HTML",
      "Web server",
      "Data bank",
      "Database machine",
      "Security hacker",
      "Computational physics",
      "Programming paradigm",
      "Semantic Web Rule Language",
      "Databank Systems",
      "List of in-memory databases",
      "Application server",
      "Data definition language",
      "Application program interface",
      "File system",
      "University of Michigan",
      "Cross-validation (statistics)",
      "List of biological databases",
      "Indexing software",
      "GRDDL",
      "Comparison of multi-model databases",
      "Computability theory",
      "Object-oriented database",
      "Computer scientists",
      "Apollo program",
      "Processor (computing)",
      "Cyberwarfare",
      "Computational engineering",
      "Fact table",
      "Table (database)",
      "Programming language",
      "Database virtualization",
      "Extract, load, transform",
      "SPARQL",
      "System/360",
      "Data model",
      "Larry Ellison",
      "EXist",
      "Business System 12",
      "Query optimization",
      "Outline of databases",
      "Load file",
      "Semantically Interlinked Online Communities",
      "Measure (data warehouse)",
      "Preprocessor",
      "Data Mining Extensions",
      "Software development",
      "Network model",
      "Hard disk drive",
      "HAtom",
      "Object–relational mapping",
      "Network service",
      "Computer graphics",
      "HProduct",
      "Linked list",
      "Multics Relational Data Store",
      "Automata theory",
      "RDF Schema",
      "Two-phase commit protocol",
      "Java Database Connectivity",
      "COBOL",
      "Web browser",
      "List of online music databases",
      "Embedded system",
      "Database refactoring",
      "ROLAP",
      "INP (database)",
      "Computer multitasking",
      "Turing Award",
      "Computer hardware",
      "IXBRL",
      "Computer architecture",
      "Dimensional modeling",
      "Casio Databank",
      "Computational chemistry",
      "Library (computing)",
      "Computer data storage",
      "Human resources",
      "IBM Db2",
      "Application security",
      "Document-oriented database",
      "Notation3",
      "Natural language processing",
      "Isolation (database systems)",
      "OQL",
      "Collective intelligence",
      "DBMS",
      "Privilege (Computing)",
      "Decision support system",
      "PACELC design principle",
      "MICRO Information Management System",
      "Formal language",
      "Metadata Object Description Schema",
      "Navigational database",
      "Sybase",
      "Cloud database",
      "Database transactions",
      "List of facial expression databases",
      "ADO.NET",
      "Bill Inmon",
      "Object database",
      "Semantic mapper",
      "Semantic publishing",
      "Character encoding",
      "Probabilistic database",
      "List of reporting software",
      "Server (computing)",
      "Charles Bachman",
      "Durability (database systems)",
      "U.S. Environmental Protection Agency",
      "Microdata (HTML)",
      "San Jose, California",
      "Hyperlink",
      "Database normalization",
      "Row (database)",
      "XML",
      "Data security",
      "Concurrency control",
      "Federated database system",
      "Batch processing",
      "Data manipulation language",
      "Library 2.0",
      "Data dictionary",
      "Dataphor",
      "Data independence",
      "Property (programming)",
      "CODASYL",
      "Programmer",
      "Open Database Connectivity",
      "Enterprise bus matrix",
      "Rule Interchange Format",
      "C. Wayne Ratliff",
      "MOLAP",
      "Middleware (distributed applications)",
      "Folksonomy",
      "Desktop computing",
      "University of California, Berkeley",
      "Content store",
      "International Computers Limited",
      "Multics",
      "Digital library",
      "Philosophy of artificial intelligence",
      "PostgreSQL",
      "VAX Rdb/VMS",
      "Security service (telecommunication)",
      "API",
      "Abstract interpretation",
      "Digital art",
      "IBM Research",
      "Distributed database",
      "Ontology (information science)",
      "Network performance",
      "Horizontal scaling",
      "Turtle (syntax)",
      "Backup",
      "Theoretical computer science",
      "Computational complexity theory"
    ]
  },
  "Punched cards": {
    "url": "https://en.wikipedia.org/wiki/Punched_cards",
    "title": "Punched cards",
    "content": "A punched card (also punch card [ 1 ] ) is a stiff paper-based medium used to store digital information via the presence or absence of holes in predefined positions. Developed over the 18th to 20th centuries, punched cards were widely used for data processing , the control of automated machines , and computing . Early applications included controlling weaving looms and recording census data. Punched cards were widely used in the 20th century, where unit record machines , organized into data processing systems , used punched cards for data input , data output, and data storage . [ 2 ] [ 3 ] The IBM 12-row/80-column punched card format came to dominate the industry. Many early digital computers used punched cards as the primary medium for input of both computer programs and data . Punched cards were used for decades before being replaced by magnetic tape data storage . While punched cards are now obsolete as a storage medium, as of 2012, some voting machines still used punched cards to record votes. [ 4 ] Punched cards had a significant cultural impact in the 20th century. Their legacy persists in modern computing, influencing the 80-character line standard still present in some command-line interfaces and programming environments. The idea of control and data storage via punched holes was developed independently on several occasions in the modern period. In most cases there is no evidence that each of the inventors was aware of the earlier work. Basile Bouchon developed the control of a loom by punched holes in paper tape in 1725. The design was improved by his assistant Jean-Baptiste Falcon and by Jacques Vaucanson . [ 5 ] Although these improvements controlled the patterns woven, they still required an assistant to operate the mechanism. In 1804 Joseph Marie Jacquard demonstrated a mechanism to automate loom operation. A number of punched cards were linked into a chain of any length. Each card held the instructions for shedding (raising and lowering the warp ) and selecting the shuttle for a single pass. [ 6 ] Semyon Korsakov was reputedly the first to propose punched cards in informatics for information store and search. Korsakov announced his new method and machines in September 1832. [ 7 ] Charles Babbage proposed the use of \"Number Cards\", \"pierced with certain holes and stand[ing] opposite levers connected with a set of figure wheels ... advanced they push in those levers opposite to which there are no holes on the cards and thus transfer that number together with its sign\" in his description of the Calculating Engine's Store. [ 8 ] There is no evidence that he built a practical example. In 1881, Jules Carpentier developed a method of recording and playing back performances on a harmonium using punched cards. The system was called the Mélographe Répétiteur and \"writes down ordinary music played on the keyboard dans le langage de Jacquard\", [ 9 ] that is as holes punched in a series of cards. By 1887 Carpentier had separated the mechanism into the Melograph which recorded the player's key presses and the Melotrope which played the music. [ 10 ] [ 11 ] At the end of the 1800s Herman Hollerith created a method for recording data on a medium that could then be read by a machine, [ 12 ] [ 13 ] [ 14 ] [ 15 ] developing punched card data processing technology for the 1890 U.S. census . [ 16 ] This was inspired in part by Jacquard loom weaving technology and by railway punch photographs. [ 17 ] Punch photographs were quick ways for conductors to mark a ticket with a description of the ticket buyer (e.g., short or tall, dark or light hair). [ 17 ] They were used to reduce ticket fraud, as conductors could \"read\" the punched holes to get a basic description of the person to whom the ticket was sold. [ 17 ] Hollerith's tabulating machines read and summarized data stored on punched cards and they began use for government and commercial data processing. Initially, these electromechanical machines only counted holes, but by the 1920s they had units for carrying out basic arithmetic operations. [ 18 ] : 124 Hollerith founded the Tabulating Machine Company (1896) which was one of four companies that were amalgamated via stock acquisition to form a fifth company, Computing-Tabulating-Recording Company (CTR) in 1911, later renamed International Business Machines Corporation (IBM) in 1924. Other companies entering the punched card business included The Tabulator Limited (Britain, 1902), Deutsche Hollerith-Maschinen Gesellschaft mbH (Dehomag) (Germany, 1911), Powers Accounting Machine Company (US, 1911), Remington Rand (US, 1927), and H.W. Egli Bull (France, 1931). [ 19 ] These companies, and others, manufactured and marketed a variety of punched cards and unit record machines for creating, sorting, and tabulating punched cards, even after the development of electronic computers in the 1950s. Both IBM and Remington Rand tied punched card purchases to machine leases, a violation of the US 1914 Clayton Antitrust Act . In 1932, the US government took both to court on this issue. Remington Rand settled quickly. IBM viewed its business as providing a service and that the cards were part of the machine. IBM fought all the way to the Supreme Court and lost in 1936; the court ruled that IBM could only set card specifications. [ 20 ] [ 21 ] : 300–301 \"By 1937... IBM had 32 presses at work in Endicott, N.Y., printing, cutting and stacking five to 10 million punched cards every day.\" [ 22 ] Punched cards were even used as legal documents, such as U.S. Government checks [ 23 ] and savings bonds. [ 24 ] During World War II punched card equipment was used by the Allies in some of their efforts to decrypt Axis communications. See, for example, Central Bureau in Australia. At Bletchley Park in England, \"some 2 million punched cards a week were being produced, indicating the sheer scale of this part of the operation\". [ 25 ] In Nazi Germany, punched cards were used for the censuses of various regions and other purposes [ 26 ] [ 27 ] (see IBM and the Holocaust ). Punched card technology developed into a powerful tool for business data-processing. By 1950 punched cards had become ubiquitous in industry and government. \"Do not fold, spindle or mutilate,\" a warning that appeared on some punched cards distributed as documents such as checks and utility bills to be returned for processing, became a motto for the post- World War II era. [ 28 ] [ 29 ] In 1956 [ 30 ] IBM signed a consent decree requiring, amongst other things, that IBM would by 1962 have no more than one-half of the punched card manufacturing capacity in the United States. Tom Watson Jr.'s decision to sign this decree, where IBM saw the punched card provisions as the most significant point, completed the transfer of power to him from Thomas Watson Sr . [ 21 ] The Univac UNITYPER introduced magnetic tape for data entry in the 1950s. During the 1960s, the punched card was gradually replaced as the primary means for data storage by magnetic tape , as better, more capable computers became available. Mohawk Data Sciences introduced a magnetic tape encoder in 1965, a system marketed as a keypunch replacement which was somewhat successful. Punched cards were still commonly used for entering both data and computer programs until the mid-1980s when the combination of lower cost magnetic disk storage , and affordable interactive terminals on less expensive minicomputers made punched cards obsolete for these roles as well. [ 31 ] : 151 However, their influence lives on through many standard conventions and file formats. The terminals that replaced the punched cards, the IBM 3270 for example, displayed 80 columns of text in text mode , for compatibility with existing software. Some programs still operate on the convention of 80 text columns, although fewer and fewer do as newer systems employ graphical user interfaces with variable-width type fonts. The terms punched card , punch card , and punchcard were all commonly used, as were IBM card and Hollerith card (after Herman Hollerith ). [ 1 ] IBM used \"IBM card\" or, later, \"punched card\" at first mention in its documentation and thereafter simply \"card\" or \"cards\". [ 33 ] [ 34 ] Specific formats were often indicated by the number of character positions available, e.g. 80-column card . A sequence of cards that is input to or output from some step in an application's processing is called a card deck or simply deck . The rectangular, round, or oval bits of paper punched out were called chad ( chads ) or chips (in IBM usage). Sequential card columns allocated for a specific use, such as names, addresses, multi-digit numbers, etc., are known as a field . The first card of a group of cards, containing fixed or indicative information for that group, is known as a master card . Cards that are not master cards are detail cards . The Hollerith punched cards used for the 1890 U.S. census were blank. [ 35 ] Following that, cards commonly had printing such that the row and column position of a hole could be easily seen. Printing could include having fields named and marked by vertical lines, logos, and more. [ 36 ] \"General purpose\" layouts (see, for example, the IBM 5081 below) were also available. For applications requiring master cards to be separated from following detail cards, the respective cards had different upper corner diagonal cuts and thus could be separated by a sorter. [ 37 ] Other cards typically had one upper corner diagonal cut so that cards not oriented correctly, or cards with different corner cuts, could be identified. Herman Hollerith was awarded three patents [ 39 ] in 1889 for electromechanical tabulating machines . These patents described both paper tape and rectangular cards as possible recording media. The card shown in U.S. patent 395,781 of January 8 was printed with a template and had hole positions arranged close to the edges so they could be reached by a railroad conductor 's ticket punch , with the center reserved for written descriptions. Hollerith was originally inspired by railroad tickets that let the conductor encode a rough description of the passenger: I was traveling in the West and I had a ticket with what I think was called a punch photograph...the conductor...punched out a description of the individual, as light hair, dark eyes, large nose, etc. So you see, I only made a punch photograph of each person. [ 18 ] : 15 When use of the ticket punch proved tiring and error-prone, Hollerith developed the pantograph \"keyboard punch\". It featured an enlarged diagram of the card, indicating the positions of the holes to be punched. A printed reading board could be placed under a card that was to be read manually. [ 35 ] : 43 Hollerith envisioned a number of card sizes. In an article he wrote describing his proposed system for tabulating the 1890 U.S. census , Hollerith suggested a card 3 by 5 + 1 ⁄ 2 inches (7.6 by 14.0 cm) of Manila stock \"would be sufficient to answer all ordinary purposes.\" [ 40 ] The cards used in the 1890 census had round holes, 12 rows and 24 columns. A reading board for these cards can be seen at the Columbia University Computing History site. [ 41 ] At some point, 3 + 1 ⁄ 4 by 7 + 3 ⁄ 8 inches (83 by 187 mm) became the standard card size. These are the dimensions of the then-current paper currency of 1862–1923. [ 42 ] This size was needed in order to use available banking-type storage for the 60,000,000 punched cards to come nationwide. [ 41 ] Hollerith's original system used an ad hoc coding system for each application, with groups of holes assigned specific meanings, e.g. sex or marital status. His tabulating machine had up to 40 counters, each with a dial divided into 100 divisions, with two indicator hands; one which stepped one unit with each counting pulse, the other which advanced one unit every time the other dial made a complete revolution. This arrangement allowed a count up to 9,999. During a given tabulating run counters were assigned specific holes or, using relay logic , combination of holes. [ 40 ] Later designs led to a card with ten rows, each row assigned a digit value, 0 through 9, and 45 columns. [ 43 ] This card provided for fields to record multi-digit numbers that tabulators could sum, instead of their simply counting cards. Hollerith's 45 column punched cards are illustrated in Comrie 's The application of the Hollerith Tabulating Machine to Brown's Tables of the Moon . [ 44 ] By the late 1920s, customers wanted to store more data on each punched card. In 1927, [ 45 ] Thomas J. Watson Sr. , IBM's head, asked two of his top inventors, Clair D. Lake and J. Royden Pierce , to independently develop ways to increase data capacity without increasing the size of the punched card. [ 46 ] Pierce wanted to keep round holes and 45 columns but to allow each column to store more data; Lake suggested rectangular holes, which could be spaced more tightly, allowing 80 columns per punched card, thereby nearly doubling the capacity of the older format. [ 47 ] Watson picked the latter solution, introduced as The IBM Card , in part because it was compatible with existing tabulator designs and in part because it could be protected by patents and give the company a distinct advantage, [ 48 ] and because \"competitors using mechanical sensing of holes would find it difficult to make the change\". [ 45 ] Introduced in 1928, the IBM card format [ 49 ] had rectangular holes, 80 columns, and 10 rows. [ 50 ] Card size is 7 + 3 ⁄ 8 by 3 + 1 ⁄ 4 inches (187 by 83 mm). The cards are made of smooth stock, 0.007 inches (180 μm) thick. There are about 143 cards to the inch (56/cm). In 1930, the IBM card format had rectangular holes, 80 columns, and 12 rows, with two more rows added to the top of the card for alphabetic coding. [ 45 ] In 1964, IBM changed from square to round corners. [ 51 ] They come typically in boxes of 2,000 cards [ 52 ] or as continuous form cards. Continuous form cards could be both pre-numbered and pre-punched for document control (checks, for example). [ 53 ] Initially designed to record responses to yes–no questions , support for numeric, alphabetic and special characters was added through the use of columns and zones. The top three positions of a column are called zone punching positions , 12 (top), 11, and 0 (0 may be either a zone punch or a digit punch). [ 54 ] For decimal data the lower ten positions are called digit punching positions , 0 (top) through 9. [ 54 ] An arithmetic sign can be specified for a decimal field by overpunching the field's rightmost column with a zone punch: 12 for plus, 11 for minus (CR). For Pound sterling pre-decimalization currency a penny column represents the values zero through eleven; 10 (top), 11, then 0 through 9 as above. An arithmetic sign can be punched in the adjacent shilling column. [ 55 ] : 9 Zone punches had other uses in processing, such as indicating a master card. [ 56 ] Diagram: [ 57 ] Note: The 11 and 12 zones were also called the X and Y zones, respectively. In 1931, IBM began introducing upper-case letters and special characters (Powers-Samas had developed the first commercial alphabetic punched card representation in 1921). [ 58 ] [ 59 ] [ nb 1 ] The 26 letters have two punches (zone [12,11,0] + digit [1–9]). The languages of Germany, Sweden, Denmark, Norway, Spain, Portugal and Finland require up to three additional letters; their punching is not shown here. [ 60 ] : 88–90 Most special characters have two or three punches (zone [12,11,0, or none] + digit [2–7] + 8); a few special characters were exceptions: \"&\" is 12 only, \"-\" is 11 only, and \"/\" is 0 + 1). The Space character has no punches. [ 60 ] : 38 The information represented in a column by a combination of zones [12, 11, 0] and digits [0–9] is dependent on the use of that column. For example, the combination \"12-1\" is the letter \"A\" in an alphabetic column, a plus signed digit \"1\" in a signed numeric column, or an unsigned digit \"1\" in a column where the \"12\" has some other use. The introduction of EBCDIC in 1964 defined columns with as many as six punches (zones [12,11,0,8,9] + digit [1–7]). IBM and other manufacturers used many different 80-column card character encodings . [ 61 ] [ 62 ] A 1969 American National Standard defined the punches for 128 characters and was named the Hollerith Punched Card Code (often referred to simply as Hollerith Card Code ), honoring Hollerith. [ 60 ] : 7 For some computer applications, binary formats were used, where each hole represented a single binary digit (or \" bit \"), every column (or row) is treated as a simple bit field , and every combination of holes is permitted. For example, on the IBM 701 [ 63 ] and IBM 704 , [ 64 ] card data was read, using an IBM 711 , into memory in row binary format. For each of the twelve rows of the card, 72 of the 80 columns, skipping the other eight, would be read into two 36-bit words, requiring 864 bits to store the whole card; a control panel was used to select the 72 columns to be read. Software would translate this data into the desired form. One convention was to use columns 1 through 72 for data, and columns 73 through 80 to sequentially number the cards, as shown in the picture above of a punched card for FORTRAN. Such numbered cards could be sorted by machine so that if a deck was dropped the sorting machine could be used to arrange it back in order. This convention continued to be used in FORTRAN, even in later systems where the data in all 80 columns could be read. The IBM card readers 3504, 3505 and the multifunction unit 3525 used a different encoding scheme for column binary data, also known as card image , where each column, split into two rows of 6 (12–3 and 4–9) was encoded into two 8-bit bytes, holes in each group represented by bits 2 to 7 (MSb numbering , bit 0 and 1 unused ) in successive bytes. This required 160 8-bit bytes, or 1280 bits, to store the whole card. [ 65 ] As an aid to humans who had to deal with the punched cards, the IBM 026 and later 029 and 129 key punch machines could print human-readable text above each of the 80 columns. As a prank, punched cards could be made where every possible punch position had a hole. Such \" lace cards \" lacked structural strength, and would frequently buckle and jam inside the machine. [ 66 ] The IBM 80-column punched card format dominated the industry, becoming known as just IBM cards , even though other companies made cards and equipment to process them. [ 67 ] One of the most common punched card formats is the IBM 5081 card format, a general purpose layout with no field divisions. This format has digits printed on it corresponding to the punch positions of the digits in each of the 80 columns. Other punched card vendors manufactured cards with this same layout and number. Long cards were available with a scored stub on either end which, when torn off, left an 80 column card. The torn off card is called a stub card . 80-column cards were available scored, on either end, creating both a short card and a stub card when torn apart. Short cards can be processed by other IBM machines. [ 53 ] [ 68 ] A common length for stub cards was 51 columns. Stub cards were used in applications requiring tags, labels, or carbon copies. [ 53 ] According to the IBM Archive: IBM's Supplies Division introduced the Port-A-Punch in 1958 as a fast, accurate means of manually punching holes in specially scored IBM punched cards. Designed to fit in the pocket, Port-A-Punch made it possible to create punched card documents anywhere. The product was intended for \"on-the-spot\" recording operations—such as physical inventories, job tickets and statistical surveys—because it eliminated the need for preliminary writing or typing of source documents. [ 69 ] In 1969 IBM introduced a new, smaller, round-hole, 96-column card format along with the IBM System/3 low-end business computer. These cards have tiny, 1 mm diameter circular holes, smaller than those in paper tape . Data is stored in 6-bit BCD , with three rows of 32 characters each, or 8-bit EBCDIC . In this format, each column of the top tiers are combined with two punch rows from the bottom tier to form an 8-bit byte, and the middle tier is combined with two more punch rows, so that each card contains 64 bytes of 8-bit-per-byte binary coded data. [ 70 ] As in the 80 column card, readable text was printed in the top section of the card. There was also a fourth row of 32 characters that could be printed. This format was never widely used; it was IBM-only, but they did not support it on any equipment beyond the System/3, where it was quickly superseded by the 1973 IBM 3740 Data Entry System using 8-inch floppy disks . The format was however recycled in 1978 when IBM re-used the mechanism in its IBM 3624 ATMs as print-only receipt printers. The Powers/Remington Rand card format was initially the same as Hollerith's; 45 columns and round holes. In 1930, Remington Rand leap-frogged IBM's 80 column format from 1928 by coding two characters in each of the 45 columns – producing what is now commonly called the 90-column card. [ 31 ] : 142 There are two sets of six rows across each card. The rows in each set are labeled 0, 1/2, 3/4, 5/6, 7/8 and 9. The even numbers in a pair are formed by combining that punch with a 9 punch. Alphabetic and special characters use three or more punches. [ 71 ] [ 72 ] The British Powers-Samas company used a variety of card formats for their unit record equipment . They began with 45 columns and round holes. Later 36-, 40- and 65-column cards were provided. A 130-column card was also available – formed by dividing the card into two rows, each row with 65 columns and each character space with five punch positions. A 21-column card was comparable to the IBM Stub card. [ 55 ] : 47–51 Mark sense ( electrographic ) cards, developed by Reynold B. Johnson at IBM, [ 73 ] have printed ovals that could be marked with a special electrographic pencil. Cards would typically be punched with some initial information, such as the name and location of an inventory item. Information to be added, such as quantity of the item on hand, would be marked in the ovals. Card punches with an option to detect mark sense cards could then punch the corresponding information into the card. Aperture cards have a cut-out hole on the right side of the punched card. A piece of 35 mm microfilm containing a microform image is mounted in the hole. Aperture cards are used for engineering drawings from all engineering disciplines. Information about the drawing, for example the drawing number, is typically punched and printed on the remainder of the card. IBM's Fred M. Carroll [ 74 ] developed a series of rotary presses that were used to produce punched cards, including a 1921 model that operated at 460 cards per minute (cpm). In 1936 he introduced a completely different press that operated at 850 cpm. [ 22 ] [ 75 ] Carroll's high-speed press, containing a printing cylinder, revolutionized the company's manufacturing of punched cards. [ 76 ] It is estimated that between 1930 and 1950, the Carroll press accounted for as much as 25 percent of the company's profits. [ 21 ] Discarded printing plates from these card presses, each printing plate the size of an IBM card and formed into a cylinder, often found use as desk pen/pencil holders, and even today are collectible IBM artifacts (every card layout [ 77 ] had its own printing plate). In the mid-1930s a box of 1,000 cards cost $1.05 (equivalent to $24 in 2024). [ 78 ] While punched cards have not been widely used for generations, the impact was so great for most of the 20th century that they still appear from time to time in popular culture. For example: metaphor... symbol of the \"system\"—first the registration system and then bureaucratic systems more generally ... a symbol of alienation ... Punched cards were the symbol of information machines, and so they became the symbolic point of attack. Punched cards, used for class registration, were first and foremost a symbol of uniformity. .... A student might feel \"he is one of out of 27,500 IBM cards\" ... The president of the Undergraduate Association criticized the University as \"a machine ... IBM pattern of education.\"... Robert Blaumer explicated the symbolism: he referred to the \"sense of impersonality... symbolized by the IBM technology.\"... A common example of the requests often printed on punched cards which were to be individually handled, especially those intended for the public to use and return is \"Do Not Fold, Spindle or Mutilate\" (in the UK \"Do not bend, spike, fold or mutilate\"). [ 28 ] : 43–55 Coined by Charles A. Phillips, [ 88 ] it became a motto [ 89 ] for the post– World War II era (even though many people had no idea what spindle meant), and was widely mocked and satirized. Some 1960s students at Berkeley wore buttons saying: \"Do not fold, spindle or mutilate. I am a student\". [ 90 ] The motto was also used for a 1970 book by Doris Miles Disney [ 91 ] with a plot based around an early computer dating service and a 1971 made-for-TV movie based on that book, and a similarly titled 1967 Canadian short film, Do Not Fold, Staple, Spindle or Mutilate . Processing of punched cards was handled by a variety of machines, including:",
    "links": [
      "Graphical user interface",
      "Bletchley Park",
      "EBCDIC",
      "Edge-notched card",
      "Fortran",
      "Doi (identifier)",
      "Sperry Rand",
      "Smithsonian Institution",
      "Text mode",
      "Thomas J. Watson",
      "36-bit",
      "Emerson W. Pugh",
      "Papyrus",
      "Raúl Rojas",
      "History of computing hardware",
      "The Minority Report",
      "S2CID (identifier)",
      "John J. Donovan",
      "Manila paper",
      "Eric S. Raymond",
      "Card image",
      "Warp (weaving)",
      "Bit field",
      "Computer dating",
      "Data (computing)",
      "Monthly Notices of the Royal Astronomical Society",
      "Tabulating machine",
      "Railroad conductor",
      "Remington Rand",
      "Thomas Watson Jr.",
      "BCD (character encoding)",
      "University of Miami",
      "Reynold B. Johnson",
      "UNITYPER",
      "Jules Carpentier",
      "IBM 3270",
      "Springer-Verlag",
      "Sir Isaac Pitman & Sons Ltd",
      "Television film",
      "Kimball tag",
      "Index card",
      "Public art",
      "Computer programming in the punched card era",
      "Musée Historique des Tissus",
      "Writing",
      "World War II",
      "Arthur C. Clarke",
      "Leslie John Comrie",
      "UNIVAC",
      "Optical mark recognition",
      "Signed overpunch",
      "OUP Oxford",
      "Francis Joseph Murray",
      "Bibcode (identifier)",
      "Free Speech Movement",
      "Doubleday Science Fiction",
      "Philip K. Dick",
      "Paper tape",
      "Brian De Palma",
      "Relay logic",
      "School of Mines",
      "US GPO",
      "Mark sense",
      "Disk drive",
      "Maya Lin",
      "Lace card",
      "ATM",
      "1890 United States census",
      "Clayton Antitrust Act",
      "Data storage",
      "Columbia University",
      "Do Not Fold, Staple, Spindle or Mutilate",
      "Journal of the Royal Musical Association",
      "Powers-Samas",
      "Punched tape",
      "Rescue Party",
      "Leon E. Truesdell",
      "Data processing",
      "Data processing system",
      "Voting machine",
      "Shed (weaving)",
      "IBM System/3",
      "Baen Books",
      "Characters per line",
      "Penny",
      "IBM 704",
      "Little, Brown & Company",
      "Houghton Mifflin",
      "Addison-Wesley Publishing Company, Inc.",
      "Simon and Schuster",
      "Frederick Phillips Brooks",
      "Doris Miles Disney",
      "Automated machine",
      "LCCN (identifier)",
      "Magnetic tape data storage",
      "Fitzroy Dearborn",
      "Digital information",
      "Computer terminal",
      "IBM 711",
      "Mohawk Data Sciences",
      "Do Not Fold, Spindle or Mutilate",
      "British Tabulating Machine Company",
      "Powers Accounting Machine",
      "IEEE Annals of the History of Computing",
      "Herman Hollerith",
      "This Is a Recording (Lily Tomlin album)",
      "OCLC (identifier)",
      "Tabulating machines",
      "Computing-Tabulating-Recording Company",
      "MIT Museum",
      "Brian Randell",
      "Input (computer science)",
      "Continuous stationery",
      "Basile Bouchon",
      "IBM 701",
      "Wiley (publisher)",
      "FITS",
      "The MIT Press",
      "Pantograph",
      "Massachusetts Institute of Technology",
      "Minicomputer",
      "AT&T Corporation",
      "Leslie Comrie",
      "Lily Tomlin",
      "Barcode",
      "Electrographic",
      "Binary code",
      "Dry cleaning",
      "Doubleday Crime Club",
      "Microform",
      "Computer program",
      "Computing",
      "Electromechanics",
      "Spindle (stationery)",
      "Bit numbering",
      "Character encoding",
      "Semyon Korsakov",
      "Steven Arthur Pinker",
      "Aperture card",
      "Kenneth Eugene Iverson",
      "Charles Babbage",
      "The New York Times",
      "Book music",
      "U.S. Government",
      "IEEE",
      "Library of Congress",
      "Digital computer",
      "University of Kentucky",
      "IBM 3624",
      "Paper data storage",
      "James Essinger",
      "EE Times",
      "Pound sterling",
      "Viking (publisher)",
      "IBM 3505",
      "Unit record equipment",
      "Turnaround document",
      "Jacquard loom",
      "Wiley & Son",
      "Computer storage",
      "Douglas W. Jones",
      "Binary numeral system",
      "International Business Machines",
      "Institute of Electrical and Electronics Engineers Inc.",
      "National Archives and Records Administration",
      "Yes–no question",
      "Loom",
      "Paper",
      "Consolidation (business)",
      "Central Bureau",
      "Engineering and Technology History Wiki",
      "Princeton University Press",
      "Shilling",
      "1950 United States census",
      "Chad (computer)",
      "Wired (magazine)",
      "ISBN (identifier)",
      "Keypunch",
      "IBM and the Holocaust",
      "Bit",
      "Groupe Bull",
      "Dehomag",
      "IBM 3740",
      "Engineering drawing",
      "Two-line element set",
      "Consent decree",
      "Harmonium",
      "Punched card input/output",
      "Railroad Gazette",
      "NBC News",
      "Edwin Black",
      "Card sorter",
      "Jacques Vaucanson",
      "Ticket punch",
      "Ibm.com",
      "Columbia University Press",
      "MIT Press",
      "IBM",
      "Joseph Marie Jacquard"
    ]
  },
  "Word embedding": {
    "url": "https://en.wikipedia.org/wiki/Word_embedding",
    "title": "Word embedding",
    "content": "In natural language processing , a word embedding is a representation of a word. The embedding is used in text analysis . Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning. [ 1 ] Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers . Methods to generate this mapping include neural networks , [ 2 ] dimensionality reduction on the word co-occurrence matrix , [ 3 ] [ 4 ] [ 5 ] probabilistic models, [ 6 ] explainable knowledge base method, [ 7 ] and explicit representation in terms of the context in which words appear. [ 8 ] Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing [ 9 ] and sentiment analysis . [ 10 ] In distributional semantics , a quantitative methodological approach for understanding meaning in observed language, word embeddings or semantic feature space models have been used as a knowledge representation for some time. [ 11 ] Such models aim to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The underlying idea that \"a word is characterized by the company it keeps\" was proposed in a 1957 article by John Rupert Firth , [ 12 ] but also has roots in the contemporaneous work on search systems [ 13 ] and in cognitive psychology. [ 14 ] The notion of a semantic space with lexical items (words or multi-word terms) represented as vectors or embeddings is based on the computational challenges of capturing distributional characteristics and using them for practical application to measure similarity between words, phrases, or entire documents. The first generation of semantic space models is the vector space model for information retrieval. [ 15 ] [ 16 ] [ 17 ] Such vector space models for words and their distributional data implemented in their simplest form results in a very sparse vector space of high dimensionality (cf. curse of dimensionality ). Reducing the number of dimensions using linear algebraic methods such as singular value decomposition then led to the introduction of latent semantic analysis in the late 1980s and the random indexing approach for collecting word co-occurrence contexts. [ 18 ] [ 19 ] [ 20 ] [ 21 ] In 2000, Bengio et al. provided in a series of papers titled \"Neural probabilistic language models\" to reduce the high dimensionality of word representations in contexts by \"learning a distributed representation for words\". [ 22 ] [ 23 ] [ 24 ] A study published in NeurIPS (NIPS) 2002 introduced the use of both word and document embeddings applying the method of kernel CCA to bilingual (and multi-lingual) corpora, also providing an early example of self-supervised learning of word embeddings. [ 25 ] Word embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur; these different styles are studied in Lavelli et al., 2004. [ 26 ] Roweis and Saul published in Science how to use \" locally linear embedding \" (LLE) to discover representations of high dimensional data structures. [ 27 ] Most new word embedding techniques after about 2005 rely on a neural network architecture instead of more probabilistic and algebraic models, after foundational work done by Yoshua Bengio [ 28 ] [ circular reference ] and colleagues. [ 29 ] [ 30 ] The approach has been adopted by many research groups after theoretical advances in 2010 had been made on the quality of vectors and the training speed of the model, as well as after hardware advances allowed for a broader parameter space to be explored profitably. In 2013, a team at Google led by Tomas Mikolov created word2vec , a word embedding toolkit that can train vector space models faster than previous approaches. The word2vec approach has been widely used in experimentation and was instrumental in raising interest for word embeddings as a technology, moving the research strand out of specialised research into broader experimentation and eventually paving the way for practical application. [ 31 ] Historically, one of the main limitations of static word embeddings or word vector space models is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, polysemy and homonymy are not handled properly. For example, in the sentence \"The club I tried yesterday was great!\", it is not clear if the term club is related to the word sense of a club sandwich , clubhouse , golf club , or any other sense that club might have. The necessity to accommodate multiple meanings per word in different vectors (multi-sense embeddings) is the motivation for several contributions in NLP to split single-sense embeddings into multi-sense ones. [ 32 ] [ 33 ] Most approaches that produce multi-sense embeddings can be divided into two main categories for their word sense representation, i.e., unsupervised and knowledge-based. [ 34 ] Based on word2vec skip-gram, Multi-Sense Skip-Gram (MSSG) [ 35 ] performs word-sense discrimination and embedding simultaneously, improving its training time, while assuming a specific number of senses for each word. In the Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) this number can vary depending on each word. Combining the prior knowledge of lexical databases (e.g., WordNet , ConceptNet , BabelNet ), word embeddings and word sense disambiguation , Most Suitable Sense Annotation (MSSA) [ 36 ] labels word-senses through an unsupervised and knowledge-based approach, considering a word's context in a pre-defined sliding window. Once the words are disambiguated, they can be used in a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the disambiguation and annotation process to be performed recurrently in a self-improving manner. [ 37 ] The use of multi-sense embeddings is known to improve performance in several NLP tasks, such as part-of-speech tagging , semantic relation identification, semantic relatedness , named entity recognition and sentiment analysis. [ 38 ] [ 39 ] As of the late 2010s, contextually-meaningful embeddings such as ELMo and BERT have been developed. [ 40 ] Unlike static word embeddings, these embeddings are at the token-level, in that each occurrence of a word has its own embedding. These embeddings better reflect the multi-sense nature of words, because occurrences of a word in similar contexts are situated in similar regions of BERT's embedding space. [ 41 ] [ 42 ] Word embeddings for n- grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. [ 43 ] Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics . The results presented by Asgari and Mofrad [ 43 ] suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns. Word embeddings with applications in game design have been proposed by Rabii and Cook [ 44 ] as a way to discover emergent gameplay using logs of gameplay data. The process requires transcribing actions that occur during a game within a formal language and then using the resulting text to create word embeddings. The results presented by Rabii and Cook [ 44 ] suggest that the resulting vectors can capture expert knowledge about games like chess that are not explicitly stated in the game's rules. The idea has been extended to embeddings of entire sentences or even documents, e.g. in the form of the thought vectors concept. In 2015, some researchers suggested \"skip-thought vectors\" as a means to improve the quality of machine translation . [ 45 ] A more recent and popular approach for representing sentences is Sentence-BERT, or SentenceTransformers, which modifies pre-trained BERT with the use of siamese and triplet network structures. [ 46 ] Software for training and using word embeddings includes Tomáš Mikolov 's Word2vec , Stanford University's GloVe , [ 47 ] GN-GloVe, [ 48 ] Flair embeddings, [ 38 ] AllenNLP's ELMo , [ 49 ] BERT , [ 50 ] fastText , Gensim , [ 51 ] Indra, [ 52 ] and Deeplearning4j . Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters . [ 53 ] For instance, the fastText is also used to calculate word embeddings for text corpora in Sketch Engine that are available online. [ 54 ] Word embeddings may contain the biases and stereotypes contained in the trained dataset, as Bolukbasi et al. points out in the 2016 paper \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\" that a publicly available (and popular) word2vec embedding trained on Google News texts (a commonly used data corpus), which consists of text written by professional journalists, still shows disproportionate word associations reflecting gender and racial biases when extracting word analogies. [ 55 ] For example, one of the analogies generated using the aforementioned word embedding is \"man is to computer programmer as woman is to homemaker\". [ 56 ] [ 57 ] Research done by Jieyu Zhou et al. shows that the applications of these trained word embeddings without careful oversight likely perpetuates existing bias in society, which is introduced through unaltered training data. Furthermore, word embeddings can even amplify these biases. [ 58 ] [ 59 ]",
    "links": [
      "Alex Graves (computer scientist)",
      "Alex Krizhevsky",
      "NeurIPS",
      "Artificial intelligence",
      "John Hopfield",
      "Feature engineering",
      "DeepDream",
      "Word-sense disambiguation",
      "Corpus linguistics",
      "Mean shift",
      "Overfitting",
      "Decision tree learning",
      "List of artificial intelligence companies",
      "International Conference on Learning Representations",
      "GPT-5",
      "Hdl (identifier)",
      "Grammar checker",
      "Memtransistor",
      "ECML PKDD",
      "Sentiment analysis",
      "Semantic analysis (machine learning)",
      "Large language model",
      "GPT-3",
      "Kling AI",
      "Universal Dependencies",
      "Christopher D. Manning",
      "Uncanny valley",
      "Autoregressive model",
      "Grammar induction",
      "Random forest",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Diffusion model",
      "Double descent",
      "Brown clustering",
      "ELMo",
      "Deep learning speech synthesis",
      "Parsing",
      "Andrej Karpathy",
      "Action selection",
      "Chinchilla (language model)",
      "Adversarial machine learning",
      "Bibcode (identifier)",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Autoencoder",
      "Named-entity recognition",
      "Human-in-the-loop",
      "SpaCy",
      "Cliff Shaw",
      "Ontology learning",
      "Convolutional neural network",
      "CiteSeerX (identifier)",
      "Reinforcement learning",
      "Physics-informed neural networks",
      "Midjourney",
      "Reasoning model",
      "Data augmentation",
      "Sketch Engine",
      "Computational linguistics",
      "Collocation extraction",
      "Pachinko allocation",
      "Linear discriminant analysis",
      "Unsupervised learning",
      "Speech synthesis",
      "Semantic relatedness",
      "Imagen (text-to-image model)",
      "AlphaGo",
      "GPT-5.2",
      "Speech segmentation",
      "Receiver operating characteristic",
      "Whisper (speech recognition system)",
      "Bioinformatics",
      "Project Debater",
      "ArXiv (identifier)",
      "Retrieval-augmented generation",
      "Pronunciation assessment",
      "Stemming",
      "UBY",
      "Real numbers",
      "Semantic parsing",
      "Google News",
      "Example-based machine translation",
      "AAAI Conference on Artificial Intelligence",
      "Computational learning theory",
      "Batch normalization",
      "Syntactic parsing",
      "Coefficient of determination",
      "Hidden Markov model",
      "Robot control",
      "Support vector machine",
      "Neural field",
      "Highway network",
      "Andrew Ng",
      "Policy gradient method",
      "Neural network",
      "Semantic similarity",
      "Spell checker",
      "Latent diffusion model",
      "Automatic identification and data capture",
      "Vapnik–Chervonenkis theory",
      "AI alignment",
      "Udio",
      "T5 (language model)",
      "Structured prediction",
      "Linguistic Linked Open Data",
      "Handwriting recognition",
      "Deep learning",
      "Music and artificial intelligence",
      "ISSN (identifier)",
      "Vector (mathematics)",
      "BIRCH",
      "Crowdsourcing",
      "History of artificial intelligence",
      "GPT-4o",
      "ISBN (identifier)",
      "Proteomics",
      "Quoc V. Le",
      "Claude Shannon",
      "Automated essay scoring",
      "PMC (identifier)",
      "Bayesian network",
      "Neural network (machine learning)",
      "Confusion matrix",
      "Ideogram (text-to-image model)",
      "Regression analysis",
      "Machine-readable dictionary",
      "Content analysis",
      "AlphaFold",
      "Isolation forest",
      "J. R. Firth",
      "Stop word",
      "Gemini (chatbot)",
      "Automatic summarization",
      "Interactive fiction",
      "GPT-J",
      "Differentiable neural computer",
      "Reservoir computing",
      "S2CID (identifier)",
      "Gemma (language model)",
      "Anomaly detection",
      "Latent semantic analysis",
      "Bank of English",
      "Sentence extraction",
      "AI safety",
      "Neuro-symbolic AI",
      "GPT-4.5",
      "Frank Rosenblatt",
      "T-distributed stochastic neighbor embedding",
      "Club sandwich",
      "Mamba (deep learning architecture)",
      "Alan Turing",
      "GPT-2",
      "Marvin Minsky",
      "Neural net language model",
      "International Joint Conference on Artificial Intelligence",
      "Game design",
      "Adobe Firefly",
      "AutoGPT",
      "Random indexing",
      "GPT-4.1",
      "Named entity recognition",
      "Stephen Grossberg",
      "Sentence embedding",
      "Co-occurrence matrix",
      "BLOOM (language model)",
      "Nathaniel Rochester (computer scientist)",
      "Singular-value decomposition",
      "Electrochemical RAM",
      "Sigmoid function",
      "IBM Watson",
      "Seppo Linnainmaa",
      "Diffusion process",
      "T-Distributed Stochastic Neighbour Embedding",
      "Self-organizing map",
      "Data mining",
      "Sparse dictionary learning",
      "Learning to rank",
      "List of datasets for machine-learning research",
      "Thesaurus (information retrieval)",
      "Information extraction",
      "Herbert A. Simon",
      "Takeo Kanade",
      "Lexical resource",
      "Language model",
      "Shallow parsing",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Attention (machine learning)",
      "Flux (text-to-image model)",
      "Yann LeCun",
      "Document-term matrix",
      "Small language model",
      "Learning curve (machine learning)",
      "Supervised learning",
      "Facial recognition system",
      "Gemini (language model)",
      "Aidan Gomez",
      "Machine learning",
      "Conference on Neural Information Processing Systems",
      "Journal of Machine Learning Research",
      "Factor analysis",
      "GloVe",
      "GPT-5.1",
      "Shun'ichi Amari",
      "Restricted Boltzmann machine",
      "Perceptron",
      "Distant reading",
      "Multi-document summarization",
      "Naive Bayes classifier",
      "Canonical correlation",
      "Vibe coding",
      "Stable Diffusion",
      "Gradient descent",
      "Text corpus",
      "Graphical model",
      "Softmax function",
      "Reinforcement learning from human feedback",
      "GPT Image",
      "Self-play (reinforcement learning technique)",
      "Empirical risk minimization",
      "Semantic decomposition (natural language processing)",
      "Neuromorphic engineering",
      "Hyperparameter (machine learning)",
      "Kernel machines",
      "Explicit semantic analysis",
      "Deep linguistic processing",
      "PropBank",
      "Textual entailment",
      "Online machine learning",
      "Deeplearning4j",
      "Text processing",
      "LeNet",
      "Intelligent agent",
      "Ashish Vaswani",
      "Gating mechanism",
      "Probably approximately correct learning",
      "Hierarchical clustering",
      "Grok (chatbot)",
      "Chatbot",
      "Echo state network",
      "Dimensionality reduction",
      "Transformer (deep learning architecture)",
      "DBpedia",
      "John Schulman",
      "Neural radiance field",
      "Sora (text-to-video model)",
      "James Goodnight",
      "Proper generalized decomposition",
      "Feature learning",
      "Training, validation, and test data sets",
      "Active learning (machine learning)",
      "Principal component analysis",
      "AI-complete",
      "Syntactic parsing (computational linguistics)",
      "Normalization (machine learning)",
      "Glossary of artificial intelligence",
      "Stochastic gradient descent",
      "Machine translation",
      "IBM Granite",
      "Riffusion",
      "Neural machine translation",
      "Polysemy",
      "Expectation–maximization algorithm",
      "Google Ngram Viewer",
      "Multi-agent reinforcement learning",
      "Apprenticeship learning",
      "State–action–reward–state–action",
      "Quantum machine learning",
      "Bias–variance tradeoff",
      "Self-supervised learning",
      "Random sample consensus",
      "Paul Werbos",
      "Yoshua Bengio",
      "Recurrent neural network",
      "Self-driving car",
      "Non-negative matrix factorization",
      "Computer vision",
      "Q-learning",
      "Geoffrey Hinton",
      "Fei-Fei Li",
      "Predictive text",
      "ChatGPT",
      "Recursive self-improvement",
      "Parallel text",
      "Argument mining",
      "Walter Pitts",
      "John von Neumann",
      "Curse of dimensionality",
      "Parameter space",
      "Reflection (artificial intelligence)",
      "Text-to-video model",
      "MuZero",
      "Bootstrap aggregating",
      "Qwen",
      "GPT-4",
      "PMID (identifier)",
      "U-Net",
      "Wikidata",
      "Humanity's Last Exam",
      "Artificial neural network",
      "Homonym",
      "Jürgen Schmidhuber",
      "Chess",
      "Noam Shazeer",
      "Semi-supervised learning",
      "Rectifier (neural networks)",
      "Rule-based machine translation",
      "List of datasets in computer vision and image processing",
      "OCLC (identifier)",
      "Human image synthesis",
      "Real number",
      "Emergent gameplay",
      "Model Context Protocol",
      "Dream Machine (text-to-video model)",
      "OpenAI o4-mini",
      "Graph neural network",
      "Vector space model",
      "Seymour Papert",
      "Joseph Weizenbaum",
      "Artificial general intelligence",
      "Demis Hassabis",
      "Gated recurrent unit",
      "Cluster analysis",
      "David Silver (computer scientist)",
      "Statistical machine translation",
      "Concordancer",
      "FastText",
      "BabelNet",
      "Text corpora",
      "GloVe (machine learning)",
      "Treebank",
      "Boosting (machine learning)",
      "OPTICS algorithm",
      "International Conference on Machine Learning",
      "Open Mind Common Sense",
      "Google",
      "Oliver Selfridge",
      "Lexical analysis",
      "Text mining",
      "Language resource",
      "Artificial human companion",
      "Semantic network",
      "Word sense disambiguation",
      "Genomics",
      "Transfer-based machine translation",
      "Ensemble learning",
      "Conjugate gradient method",
      "Residual neural network",
      "Speech recognition",
      "Lemmatisation",
      "Prompt engineering",
      "Variational autoencoder",
      "Temporal difference learning",
      "John McCarthy (computer scientist)",
      "Multilayer perceptron",
      "Ilya Sutskever",
      "Tomáš Mikolov",
      "15.ai",
      "Curriculum learning",
      "Principal Component Analysis",
      "Lotfi A. Zadeh",
      "Relevance vector machine",
      "Latent Dirichlet allocation",
      "ElevenLabs",
      "Topological deep learning",
      "Doi (identifier)",
      "Statistical classification",
      "Part-of-speech tagging",
      "Huawei PanGu",
      "Loss functions for classification",
      "LaMDA",
      "Weight initialization",
      "Parameter",
      "Outline of machine learning",
      "Local outlier factor",
      "Topic model",
      "WaveNet",
      "Logistic regression",
      "Optical character recognition",
      "Independent component analysis",
      "Machine Learning (journal)",
      "Timeline of artificial intelligence",
      "GPT-1",
      "Claude (language model)",
      "List of artificial intelligence projects",
      "AlexNet",
      "Formal semantics (natural language)",
      "PaLM",
      "Tomas Mikolov",
      "Thought vector",
      "K-nearest neighbors algorithm",
      "DeepSeek (chatbot)",
      "Bag-of-words model",
      "Quasi-Newton method",
      "Distributional semantics",
      "N-gram",
      "Linear regression",
      "GitHub",
      "Generative model",
      "AlphaZero",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Word2vec",
      "François Chollet",
      "Natural Language Toolkit",
      "Association rule learning",
      "Suno AI",
      "Word-sense induction",
      "Rule-based machine learning",
      "Conditional random field",
      "Golf club",
      "Ian Goodfellow",
      "Vision transformer",
      "Science (journal)",
      "Virtual assistant",
      "Batch learning",
      "Oriol Vinyals",
      "Veo (text-to-video model)",
      "Feedforward neural network",
      "Speech corpus",
      "Natural language processing",
      "DBRX",
      "Hallucination (artificial intelligence)",
      "Fuzzy clustering",
      "OpenAI o1",
      "Kunihiko Fukushima",
      "Generative pre-trained transformer",
      "Automated machine learning",
      "Compound-term processing",
      "Text simplification",
      "Llama (language model)",
      "Bigram",
      "Text segmentation",
      "Formal language",
      "Regularization (mathematics)",
      "DBSCAN",
      "OpenAI Five",
      "OpenAI o3",
      "Generative adversarial network",
      "Boltzmann machine",
      "Meeting house",
      "Trigram",
      "WordNet",
      "Text-to-image model",
      "Natural language understanding",
      "Data cleaning",
      "Aurora (text-to-image model)",
      "Meta-learning (computer science)",
      "Neural Turing machine",
      "Jan Leike",
      "BERT (language model)",
      "Multimodal learning",
      "Statistical learning theory",
      "Distributional–relational database",
      "Backpropagation",
      "K-means clustering",
      "Embedding (machine learning)",
      "Spiking neural network",
      "Mechanistic interpretability",
      "Concept mining",
      "Feature space",
      "Occam learning",
      "Bernard Widrow",
      "Natural language generation",
      "CURE algorithm",
      "Terminology extraction",
      "IBM Watsonx",
      "Daniel Kokotajlo (researcher)",
      "DALL-E",
      "Warren Sturgis McCulloch",
      "Question answering",
      "Convolution",
      "Density estimation",
      "Document classification",
      "Activation function",
      "Allen Newell",
      "Recraft",
      "Gensim",
      "Long short-term memory",
      "Truecasing",
      "Mustafa Suleyman",
      "FrameNet",
      "Imitation learning"
    ]
  },
  "Apache Solr": {
    "url": "https://en.wikipedia.org/wiki/Apache_Solr",
    "title": "Apache Solr",
    "content": "Solr (pronounced \"solar\") is an open-source enterprise-search platform, written in Java . Its major features include full-text search , hit highlighting, faceted search , real-time indexing, dynamic clustering, database integration, NoSQL features [ 2 ] and rich document (e.g., Word, PDF) handling. Providing distributed search and index replication, Solr is designed for scalability and fault tolerance . [ 3 ] Solr is widely used for enterprise search and analytics use cases and has an active development community and regular releases. Solr runs as a standalone full-text search server. It uses the Lucene Java search library at its core for full-text indexing and search, and has REST -like HTTP / XML and JSON APIs that make it usable from most popular programming languages. Solr's external configuration allows it to be tailored to many types of applications without Java coding, and it has a plugin architecture to support more advanced customization. Apache Solr is developed in an open, collaborative manner by the Apache Solr project at the Apache Software Foundation . In 2004, Solr was created by Yonik Seeley at CNET Networks as an in-house project to add search capability for the company website. [ 4 ] In January 2006, CNET Networks decided to openly publish the source code by donating it to the Apache Software Foundation . [ 5 ] Like any new Apache project, it entered an incubation period that helped solve organizational, legal, and financial issues. In January 2007, Solr graduated from incubation status into a standalone top-level project (TLP) and grew steadily with accumulated features, thereby attracting users, contributors, and committers. Although quite new as a public project, it powered several high-traffic websites. [ 6 ] In September 2008, Solr 1.3 was released including distributed search capabilities and performance enhancements among many others. [ 7 ] In January 2009, Yonik Seeley along with Grant Ingersoll and Erik Hatcher joined Lucidworks (formerly Lucid Imagination), the first company providing commercial support and training for Apache Solr search technologies. [ citation needed ] Since then, support offerings around Solr have been abundant. [ 8 ] In November 2009, saw the release of Solr 1.4. This version introduced enhancements in indexing, searching and faceting along with many other improvements such as rich document processing ( PDF , Word , HTML ), Search Results clustering based on Carrot2 and also improved database integration. The release also features many additional plug-ins. [ 9 ] In March 2010, the Lucene and Solr projects merged. [ 10 ] Separate downloads continued, but the products were now jointly developed by a single set of committers. In 2011, the Solr version number scheme was changed in order to match that of Lucene. After Solr 1.4, the next release of Solr was labeled 3.1, in order to keep Solr and Lucene on the same version number. [ 11 ] In October 2012, Solr version 4.0 was released, including the new SolrCloud feature. [ 12 ] 2013 and 2014 saw a number of Solr releases in the 4.x line, steadily growing the feature set and improving reliability. In February 2015, Solr 5.0 was released, [ 13 ] the first release where Solr is packaged as a standalone application, [ 14 ] ending official support for deploying Solr as a war . Solr 5.3 featured a built-in pluggable Authentication and Authorization framework. [ 15 ] In April 2016, Solr 6.0 was released. [ 16 ] Added support for executing Parallel SQL queries across SolrCloud collections. Includes StreamExpression support and a new JDBC Driver for the SQL Interface. In September 2017, Solr 7.0 was released. [ 17 ] This release among other things, added support multiple replica types, auto-scaling, and a Math engine. In March 2019, Solr 8.0 was released including many bugfixes and component updates. [ 18 ] Solr nodes can now listen and serve HTTP/2 requests. Be aware that by default, internal requests are also sent by using HTTP/2. Furthermore, an admin UI login was added with support for BasicAuth and Kerberos. And plotting math expressions in Apache Zeppelin is now possible. In November 2020, Bloomberg donated the Solr Operator to the Lucene/Solr project. The Solr Operator helps deploy and run Solr in Kubernetes . In February 2021, Solr was established as a separate Apache project (TLP), independent from Lucene. In May 2022, Solr 9.0 was released, [ 19 ] as the first release independent from Lucene, requiring Java 11, and with highlights such as KNN \"Neural\" search, better modularization, more security plugins and more. In order to search a document, Apache Solr performs the following operations in sequence: Solr has both individuals and companies who contribute new features and bug fixes. [ 20 ] [ 21 ] [ 22 ] [ 23 ] [ 24 ] Solr is bundled as the built-in search in many applications such as content management systems and enterprise content management systems. Hadoop distributions from Cloudera , [ 25 ] Hortonworks [ 26 ] and MapR all bundle Solr as the search engine for their products marketed for big data . DataStax DSE integrates Solr as a search engine with Cassandra . [ 27 ] Solr is supported as an end point in various data processing frameworks and Enterprise integration frameworks. [ citation needed ] Solr exposes industry standard HTTP REST-like APIs with both XML and JSON support, and will integrate with any system or programming language supporting these standards. For ease of use there are also client libraries available for Java , C# , PHP , Python , Ruby and most other popular programming languages. [ 28 ]",
    "links": [
      "Application programming interface",
      "The Apache Software Foundation",
      "NoSQL",
      "Apache Flume",
      "Apache Cocoon",
      "Apache HBase",
      "Software release life cycle",
      "Apache Tika",
      "Packt",
      "AxKit",
      "Lucidworks",
      "Apache Beam",
      "Apache MINA",
      "Apache Ignite",
      "Apache Ivy",
      "Apache Portable Runtime",
      "Apache Lucene",
      "Apache Superset",
      "Deltacloud",
      "Apache Allura",
      "Apache Software Foundation",
      "Apache CouchDB",
      "WAR (file format)",
      "Apache Apex",
      "Apache ActiveMQ",
      "Python (programming language)",
      "Apache Felix",
      "Apache Groovy",
      "Apache CloudStack",
      "Apache Spark",
      "Apache OFBiz",
      "Apache Axis",
      "Cloudera",
      "CNET Networks",
      "Apache Commons Logging",
      "Apache Helix",
      "Fault tolerance",
      "Apache Marmotta",
      "Big data",
      "Apache Wave",
      "Enterprise search",
      "Apache MXNet",
      "Apache Directory",
      "Apache Batik",
      "Apache Stanbol",
      "Apache Mynewt",
      "Apache RocketMQ",
      "Apache Shale",
      "List of information retrieval libraries",
      "Apache Empire-db",
      "Apache FOP (Formatting Objects Processor)",
      "Apache Struts 1",
      "MapR",
      "Apache cTAKES",
      "Apache Aries",
      "Apache Roller",
      "Apache Tomcat",
      "JSON",
      "DataStax",
      "PHP",
      "Apache OpenOffice",
      "Apache Parquet",
      "Apache Drill",
      "Apache JMeter",
      "Apache Tapestry",
      "Apache Accumulo",
      "Repository (version control)",
      "Hortonworks",
      "Apache Nutch",
      "Apache ZooKeeper",
      "REST",
      "Operating system",
      "Content management system",
      "Apache Airflow",
      "Apache Ant",
      "Apache Iceberg",
      "Apache Avro",
      "UIMA",
      "Bean Scripting Framework",
      "Apache Tuscany",
      "Apache CXF",
      "Apache HTTP Server",
      "Apache SystemDS",
      "Enterprise content management",
      "Apache Oozie",
      "Apache Kudu",
      "Carrot2",
      "Apache XML",
      "Commons Daemon",
      "Apache Taverna",
      "Sqoop",
      "Apache MyFaces",
      "Apache Pivot",
      "Index (search engine)",
      "Apache OpenNLP",
      "Apache Qpid",
      "Apache Beehive",
      "Apache PDFBox",
      "Enterprise integration",
      "Manning Publications",
      "Log4j",
      "Apache CarbonData",
      "Lucene",
      "Apache Guacamole",
      "Java (programming language)",
      "Apache Cassandra",
      "Apache SINGA",
      "Apache Hadoop",
      "Apache Ambari",
      "PDF",
      "Apache Jena",
      "Apache Maven",
      "Apache OpenJPA",
      "HTTP",
      "Apache Derby",
      "Apache Brooklyn",
      "Apache Subversion",
      "Apache Commons",
      "Apache Calcite",
      "Apache Impala",
      "Search algorithm",
      "Open Semantic Framework",
      "Apache iBATIS",
      "Apache Xerces",
      "Apache Thrift",
      "NetBeans",
      "Apache Camel",
      "XML",
      "Apache SpamAssassin",
      "Apache Samza",
      "Full text search",
      "Apache Phoenix",
      "Apache Cordova",
      "Apache Jelly",
      "HTML",
      "Kubernetes",
      "Apache Kylin",
      "Apache Flex",
      "Apache Xalan",
      "Apache Hama",
      "Apache Airavata",
      "Ruby (programming language)",
      "Apache License 2.0",
      "Microsoft Word",
      "Apache Sling",
      "Programmer",
      "Jakarta Project",
      "Open-source software",
      "Apache Shiro",
      "Software license",
      "Apache ORC",
      "Apache OpenEJB",
      "NuttX",
      "Apache Struts",
      "Apache Arrow",
      "Apache Flink",
      "Apache Pinot",
      "Apache Giraph",
      "Apache Harmony",
      "Apache Pig",
      "Apache Storm",
      "Apache Continuum",
      "Apache TinkerPop",
      "Apache Geronimo",
      "Apache Velocity",
      "Apache Trafodion",
      "ISBN (identifier)",
      "Etch (protocol)",
      "Faceted search",
      "API",
      "Cross-platform",
      "Apache Cayenne",
      "Apache Hive",
      "Apache James",
      "Apache Wicket",
      "Apache Click",
      "Apache XMLBeans",
      "Apache ODE",
      "Mod perl",
      "Jakarta Slide",
      "Apache Mahout",
      "Apache NiFi",
      "Hadoop",
      "Apache Kafka",
      "Apache Bloodhound",
      "Apache Jackrabbit",
      "Apache POI",
      "Jini",
      "Apache Traffic Server",
      "FreeMarker",
      "Apache Axis2",
      "C Sharp (programming language)",
      "Apache Druid",
      "Apache License"
    ]
  },
  "SPLADE": {
    "url": "https://en.wikipedia.org/wiki/SPLADE",
    "title": "SPLADE",
    "content": "Learned sparse retrieval (LSR) or sparse neural search is an approach to Information Retrieval which uses a sparse vector representation of queries and documents. [ 1 ] It borrows techniques both from lexical bag-of-words and vector embedding algorithms, and is claimed to perform better than either alone. The best-known sparse neural search systems are SPLADE [ 2 ] and its successor SPLADE v2. [ 3 ] Others include DeepCT, [ 4 ] uniCOIL, [ 5 ] EPIC, [ 6 ] DeepImpact, [ 7 ] TILDE and TILDEv2, [ 8 ] Sparta, [ 9 ] SPLADE-max, and DistilSPLADE-max. [ 3 ] Multimodal Learned Sparse Retrieval . LSR approaches have also been extended to the vision-language domain, where they are applied to multimodal data, such as the combination of text and images. [ 10 ] This expansion enables the retrieval of relevant content across different modalities, such as finding images based on text queries or vice versa. Some implementations of SPLADE have similar latency to Okapi BM25 lexical search while giving as good results as state-of-the-art neural rankers on in-domain data. [ 11 ] The Official SPLADE model weights and training code is released under a Creative Commons NonCommercial license . [ 12 ] But there are other independent implementations of SPLADE++ (a variant of SPLADE models) that are released under permissive licenses. SPRINT is a toolkit for evaluating neural sparse retrieval systems. [ 13 ] SPLADE (Sparse Lexical and Expansion Model) is a neural retrieval model that learns sparse vector representations for queries and documents, combining elements of traditional lexical matching with semantic representations derived from transformer-based architectures. [ 14 ] Unlike dense retrieval models that rely on continuous vector spaces, SPLADE produces sparse outputs that are compatible with inverted index structures commonly used in information retrieval systems. [ 14 ] The original SPLADE model was introduced at the 44th International ACM SIGIR Conference in 2021. [ 14 ] An updated version, SPLADE v2, incorporated modifications to its pooling mechanisms, document expansion strategies, and training objectives using knowledge distillation. Empirical evaluations have shown improvements on benchmarks such as the TREC Deep Learning 2019 dataset and the BEIR benchmark suite. [ 15 ] These models aim to maintain retrieval efficiency comparable to traditional sparse methods while enhancing semantic matching capabilities, offering a balance between effectiveness and computational cost. [ 16 ] This computer science article is a stub . You can help Wikipedia by expanding it . This computational linguistics -related article is a stub . You can help Wikipedia by expanding it .",
    "links": [
      "Splayd",
      "Vector embedding",
      "ArXiv (identifier)",
      "ISBN (identifier)",
      "S2CID (identifier)",
      "Computer science",
      "Computational linguistics",
      "Bag-of-words model",
      "Okapi BM25",
      "Information Retrieval",
      "Doi (identifier)",
      "ISSN (identifier)",
      "Creative Commons NonCommercial license"
    ]
  },
  "Data mining": {
    "url": "https://en.wikipedia.org/wiki/Data_mining",
    "title": "Data mining",
    "content": "Data mining is the process of extracting and finding patterns in massive data sets involving methods at the intersection of machine learning , statistics , and database systems . [ 1 ] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. [ 1 ] [ 2 ] [ 3 ] [ 4 ] Data mining is the analysis step of the \" knowledge discovery in databases \" process, or KDD. [ 5 ] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing , model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization , and online updating . [ 1 ] The term \"data mining\" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction ( mining ) of data itself . [ 6 ] It also is a buzzword [ 7 ] and is frequently applied to any form of large-scale data or information processing ( collection , extraction , warehousing , analysis, and statistics) as well as any application of computer decision support systems , including artificial intelligence (e.g., machine learning) and business intelligence . Often the more general terms ( large scale ) data analysis and analytics —or, when referring to actual methods, artificial intelligence and machine learning —are more appropriate. The actual data mining task is the semi- automatic or automatic analysis of massive quantities of data to extract previously unknown, interesting patterns such as groups of data records ( cluster analysis ), unusual records ( anomaly detection ), and dependencies ( association rule mining , sequential pattern mining ). This usually involves using database techniques such as spatial indices . These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics . For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system . Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps. The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign , regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data. [ 8 ] The related terms data dredging , data fishing , and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations. In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983. [ 9 ] [ 10 ] Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative). The term data mining appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, the phrase \"database mining\"™, was used, but since it was trademarked by HNC, a San Diego –based company, to pitch their Database Mining Workstation; [ 11 ] researchers consequently turned to data mining . Other terms used include data archaeology , information harvesting , information discovery , knowledge extraction , etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) [ 12 ] and this term became more popular in the AI and machine learning communities. However, the term data mining became more popular in the business and press communities. [ 13 ] Currently, the terms data mining and knowledge discovery are used interchangeably. The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). [ 14 ] The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks , cluster analysis , genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns. [ 15 ] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets. The knowledge discovery in databases (KDD) process is commonly defined with the stages: It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases: or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation. Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. [ 16 ] [ 17 ] [ 18 ] [ 19 ] The only other data mining standard named in these polls was SEMMA . However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, [ 20 ] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008. [ 21 ] Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse . Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data . Data mining involves six common classes of tasks: [ 5 ] Data mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be reproduced on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper statistical hypothesis testing . A simple version of this problem in machine learning is known as overfitting , but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening. [ 22 ] The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting . To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves . If the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge. The premier professional body in the field is the Association for Computing Machinery 's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining ( SIGKDD ). [ 23 ] [ 24 ] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, [ 25 ] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\". [ 26 ] Computer science conferences on data mining include: Data mining topics are also present in many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases . There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft. For exchanging the extracted models—in particular for use in predictive analytics —the key standard is the Predictive Model Markup Language (PMML), which is an XML -based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG. [ 27 ] Data mining is used wherever there is digital data available. Notable examples of data mining can be found throughout business, medicine, science, finance, construction, and surveillance. While the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to user behavior (ethical and otherwise). [ 28 ] The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy , legality, and ethics . [ 29 ] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE , has raised privacy concerns. [ 30 ] [ 31 ] Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation . Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). [ 32 ] The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous. [ 33 ] Data may also be modified so as to become anonymous, so that individuals may not readily be identified. [ 32 ] However, even \" anonymized \" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL . [ 34 ] The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial, emotional, or bodily harm to the indicated individual. In one instance of privacy violation , the patrons of Walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies. [ 35 ] Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles , developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden 's global surveillance disclosure , there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency , and attempts to reach an agreement with the United States have failed. [ 36 ] In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places. [ 37 ] In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week , \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approaching a level of incomprehensibility to average individuals.\" [ 38 ] This underscores the necessity for data anonymity in data aggregation and mining practices. U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation. Even if there is no copyright in a dataset, the European Union recognises a Database right , so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive . Under European copyright database laws , the mining of in-copyright works (such as by web mining ) without the permission of the copyright owner is permitted under Articles 3 and 4 of the 2019 Directive on Copyright in the Digital Single Market . A specific TDM exception for scientific research is described in article 3, whereas a more general exception described in article 4 only applies if the copyright holder has not opted out. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. [ 39 ] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013. [ 40 ] On the recommendation of the Hargreaves review , this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception . [ 41 ] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. Since 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020. [ 42 ] US copyright law , and in particular its provision for fair use , upholds the legality of content mining in America, and other fair use countries such as Israel , Taiwan and South Korea . As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining. [ 43 ] The following applications are available under free/open-source licenses. Public access to application source code is also available. The following applications are available under proprietary licenses. For more information about extracting information out of data (as opposed to analyzing data), see:",
    "links": [
      "Very-large-scale integration",
      "Spreadsheets",
      "Video game",
      "Buzzword",
      "Reproducibility",
      "Artificial intelligence",
      "Data integrity",
      "Michael Lovell",
      "Data type",
      "Data scrubbing",
      "Feature engineering",
      "DeepDream",
      "Column-oriented DBMS",
      "Networking hardware",
      "Total Information Awareness",
      "Test set",
      "Multiprocessing",
      "Mean shift",
      "Overfitting",
      "Rendering (computer graphics)",
      "Decision tree learning",
      "Data editing",
      "Python (programming language)",
      "International Conference on Learning Representations",
      "Online algorithm",
      "Subspace clustering",
      "Programming team",
      "Israel",
      "Interaction design",
      "Memtransistor",
      "ECML PKDD",
      "Computational geometry",
      "Industrial process control",
      "Semantic analysis (machine learning)",
      "Data governance",
      "Software framework",
      "Data format management",
      "Fault tolerance",
      "System on a chip",
      "Digital marketing",
      "Printed circuit board",
      "Fact (data warehouse)",
      "Taiwan",
      "Grammar induction",
      "Document management system",
      "Random forest",
      "National security",
      "Diffusion model",
      "Orange (software)",
      "Integrated development environment",
      "Data management",
      "Database right",
      "Wireless sensor network",
      "Distributed computing",
      "Software engineering",
      "E-commerce",
      "Computational biology",
      "DATADVANCE",
      "Microsoft Academic Search",
      "Knowledge representation and reasoning",
      "Data sharing",
      "Early-arriving fact",
      "Autoencoder",
      "Named-entity recognition",
      "Database",
      "Data ecosystem",
      "Human-in-the-loop",
      "Ontology learning",
      "Convolutional neural network",
      "Electronic discovery",
      "Reinforcement learning",
      "Physics-informed neural networks",
      "Star schema",
      "Concurrency (computer science)",
      "Trevor Hastie",
      "RapidMiner",
      "Hardware security",
      "Data augmentation",
      "Information integration",
      "Linear discriminant analysis",
      "Unsupervised learning",
      "Augmented reality",
      "Form factor (design)",
      "Electronic design automation",
      "Network architecture",
      "Open-source",
      "Control theory",
      "Dimension (data warehouse)",
      "Data warehouse automation",
      "Receiver operating characteristic",
      "Cross-industry standard process for data mining",
      "National Security Agency",
      "Carrot2",
      "Bioinformatics",
      "MultiDimensional eXpressions",
      "Data deduplication",
      "Cyber-physical system",
      "Modeling language",
      "Data wrangling",
      "Law enforcement",
      "Java (programming language)",
      "Google Scholar",
      "Formal methods",
      "Privacy violation",
      "Data re-identification",
      "Computer security",
      "Network scheduler",
      "AAAI Conference on Artificial Intelligence",
      "Computational mathematics",
      "Computational learning theory",
      "Coefficient of determination",
      "Computing",
      "Predictive Model Markup Language",
      "Hidden Markov model",
      "Spamming",
      "Data quality",
      "Marketing campaign",
      "Support vector machine",
      "Neural field",
      "Missing data",
      "Algorithm",
      "Policy gradient method",
      "KNIME",
      "Vapnik–Chervonenkis theory",
      "Metadata",
      "Corporate information factory",
      "Misnomer",
      "Mathematical software",
      "South Korea",
      "Snowflake schema",
      "Electronic voting",
      "Structured prediction",
      "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases",
      "Statistical model",
      "Algorithmic efficiency",
      "Data scraping",
      "Data dredging",
      "Bing Liu (computer scientist)",
      "Open-source software",
      "Deep learning",
      "Knowledge extraction",
      "Jiawei Han",
      "US Congress",
      "Online analytical processing",
      "Ralph Kimball",
      "Wayback Machine",
      "Analytics",
      "Kluwer Academic Publishers",
      "Computational complexity",
      "SIGKDD",
      "ISSN (identifier)",
      "Integrated circuit",
      "Bayes' theorem",
      "SIGMOD",
      "BIRCH",
      "Crowdsourcing",
      "Data migration",
      "Human–computer interaction",
      "Quantum computing",
      "ISBN (identifier)",
      "Data loss",
      "Mobile computing",
      "Sixth normal form",
      "Software development process",
      "Requirements analysis",
      "Multi expression programming",
      "Mathematical optimization",
      "Data extraction",
      "Data anonymization",
      "Intrusion detection system",
      "Data transformation",
      "International Conference on Very Large Data Bases",
      "Statistics",
      "LIONsolver",
      "Bayesian network",
      "Neural network (machine learning)",
      "Confusion matrix",
      "Aggregate function",
      "Cryptocurrency",
      "Information system",
      "Regression analysis",
      "Oracle Data Mining",
      "Isolation forest",
      "Slowly changing dimension",
      "Knowledge discovery",
      "Open access",
      "Profiling (information science)",
      "Data vault modeling",
      "NLTK",
      "Automatic summarization",
      "Reservoir computing",
      "Market basket",
      "S2CID (identifier)",
      "Anomaly detection",
      "Sequence mining",
      "Neuro-symbolic AI",
      "Anchor modeling",
      "OpenNN",
      "Surrogate key",
      "Statistical hypothesis testing",
      "T-distributed stochastic neighbor embedding",
      "Programming language theory",
      "Mamba (deep learning architecture)",
      "Database Directive",
      "Domain driven data mining",
      "Data synchronization",
      "Fair use",
      "Software maintenance",
      "Philip S. Yu",
      "Computer animation",
      "International Joint Conference on Artificial Intelligence",
      "Thermodynamic computing",
      "Cryptography",
      "Concurrent computing",
      "Statistical inference",
      "Image compression",
      "Educational technology",
      "Dashboard (business)",
      "Business intelligence",
      "International Journal of Data Warehousing and Mining",
      "Data snooping",
      "Domain-specific language",
      "Java Data Mining",
      "List of computer size categories",
      "Software quality",
      "Electrochemical RAM",
      "Software repository",
      "Decision rules",
      "Data exhaust",
      "Self-organizing map",
      "General Architecture for Text Engineering",
      "Software configuration management",
      "Sparse dictionary learning",
      "Cambridge University Press",
      "Usama Fayyad",
      "Stellar Wind",
      "Operating system",
      "Examples of data mining",
      "Learning to rank",
      "Interpreter (computing)",
      "List of datasets for machine-learning research",
      "AI",
      "Information extraction",
      "Software design",
      "Interdisciplinary",
      "Database management",
      "Social computing",
      "Middleware",
      "Data hub",
      "Microsoft",
      "Visualization (graphics)",
      "Dan Linstedt",
      "Information theory",
      "Copyright law of the European Union",
      "Data corruption",
      "Data ethics",
      "Learning curve (machine learning)",
      "Supervised learning",
      "Control flow",
      "Information security",
      "Machine learning",
      "Conference on Neural Information Processing Systems",
      "Training set",
      "Ethics",
      "Pattern",
      "Numerical analysis",
      "Model of computation",
      "Journal of Machine Learning Research",
      "Single version of the truth",
      "Factor analysis",
      "Real-time computing",
      "Restricted Boltzmann machine",
      "Perceptron",
      "Naive Bayes classifier",
      "Canonical correlation",
      "System deployment",
      "Tanagra (machine learning)",
      "Academic Press",
      "Data reduction",
      "Amazon SageMaker",
      "Graphical model",
      "GNU Project",
      "Enterprise information system",
      "Data mart",
      "Association for Computing Machinery",
      "XML for Analysis",
      "Statistical noise",
      "Reinforcement learning from human feedback",
      "Data",
      "Automated planning and scheduling",
      "Self-play (reinforcement learning technique)",
      "Amazon.com",
      "Hardware acceleration",
      "Multithreading (computer architecture)",
      "Conference on Knowledge Discovery and Data Mining",
      "Empirical risk minimization",
      "Data curation",
      "Neuromorphic engineering",
      "Kernel machines",
      "Analysis of algorithms",
      "ACM Computing Classification System",
      "Data structure",
      "Information privacy",
      "Software deployment",
      "San Diego",
      "Probability",
      "Data erasure",
      "Dependability",
      "Online machine learning",
      "Scikit-learn",
      "OLAP cube",
      "LeNet",
      "Theory of computation",
      "C++",
      "Probably approximately correct learning",
      "Hierarchical clustering",
      "Decision tree",
      "Outline of computer science",
      "Surveillance capitalism",
      "Echo state network",
      "Business intelligence software",
      "Intention mining",
      "Exploratory data analysis",
      "Real-time data",
      "Dimensionality reduction",
      "Transformer (deep learning architecture)",
      "Topological data analysis",
      "Comparison of OLAP servers",
      "Compiler construction",
      "Neural radiance field",
      "Association rule mining",
      "Proper generalized decomposition",
      "Feature learning",
      "Degenerate dimension",
      "Active learning (machine learning)",
      "Virtual reality",
      "Principal component analysis",
      "SPSS",
      "Computational social science",
      "Robert Tibshirani",
      "Hewlett-Packard",
      "Educational data mining",
      "JSTOR (identifier)",
      "Data loading",
      "Data library",
      "Oracle Corporation",
      "Glossary of artificial intelligence",
      "Information retrieval",
      "Data engineering",
      "Data publishing",
      "Psychometrics",
      "Mlpack",
      "Personally identifiable information",
      "Semantics (computer science)",
      "Open data",
      "Expectation–maximization algorithm",
      "Discrete mathematics",
      "Big data",
      "Multi-agent reinforcement learning",
      "Spatial index",
      "Privacy",
      "Lua (programming language)",
      "Word processor",
      "Dependency (computer science)",
      "Data compression",
      "Apprenticeship learning",
      "State–action–reward–state–action",
      "KDD Conference",
      "Dimension table",
      "Quantum machine learning",
      "Bias–variance tradeoff",
      "Self-supervised learning",
      "Random sample consensus",
      "Green computing",
      "Recurrent neural network",
      "Logic in computer science",
      "Non-negative matrix factorization",
      "Computer vision",
      "Q-learning",
      "Stochastic computing",
      "Data remanence",
      "Algorithm design",
      "Geographic information system",
      "Data storage",
      "Bootstrap aggregating",
      "Prentice Hall",
      "SAS Institute",
      "PMID (identifier)",
      "Data processing",
      "Peripheral",
      "U-Net",
      "Multimedia database",
      "Enterprise software",
      "Randomized algorithm",
      "Academic journal",
      "Artificial neural network",
      "ELKI",
      "Database system",
      "Software construction",
      "Reverse star schema",
      "Drug discovery",
      "Operational data store",
      "Electronic publishing",
      "Semi-supervised learning",
      "Data science",
      "Data mesh",
      "List of datasets in computer vision and image processing",
      "OCLC (identifier)",
      "Review of Economic Studies",
      "World Wide Web",
      "Microsoft Analysis Services",
      "SEMMA",
      "Behavior informatics",
      "Data infrastructure",
      "Data aggregation",
      "International Safe Harbor Privacy Principles",
      "Gated recurrent unit",
      "Cluster analysis",
      "Aggregate (data warehouse)",
      "Operations research",
      "Data exploration",
      "Stakeholder (corporate)",
      "Programming tool",
      "Computer science",
      "Data rescue",
      "Photograph manipulation",
      "Boosting (machine learning)",
      "OPTICS algorithm",
      "International Conference on Machine Learning",
      "Social media mining",
      "Virtual machine",
      "User behavior analytics",
      "Data cooperative",
      "Google",
      "Parallel computing",
      "NetOwl",
      "Network security",
      "Text mining",
      "Intellectual property",
      "Extract, transform, load",
      "Mathematical analysis",
      "Morgan Kaufmann",
      "Social software",
      "StatSoft",
      "Data recovery",
      "Multi-task learning",
      "Health Insurance Portability and Accountability Act",
      "Data cleansing",
      "Health informatics",
      "AOL",
      "Communication protocol",
      "Ensemble learning",
      "Computing platform",
      "Computer network",
      "Multilinear subspace learning",
      "Data warehouse",
      "Torch (machine learning)",
      "Temporal difference learning",
      "Computational problem",
      "Ubiquitous computing",
      "Computer accessibility",
      "Google Cloud Platform",
      "Distributed artificial intelligence",
      "Graphics processing unit",
      "Curriculum learning",
      "Predictive analytics",
      "Confidentiality",
      "IBM",
      "HOLAP",
      "Jerome H. Friedman",
      "Learning classifier system",
      "Cross Industry Standard Process for Data Mining",
      "Relevance vector machine",
      "Topological deep learning",
      "Human-centered computing",
      "Data set",
      "Solid modeling",
      "Family Educational Rights and Privacy Act",
      "Doi (identifier)",
      "Statistical classification",
      "Vertica",
      "Data pre-processing",
      "Ian H. Witten",
      "KDnuggets",
      "Outline of machine learning",
      "Multivariate statistics",
      "PSeven",
      "A priori probability",
      "PolyAnalyst",
      "Local outlier factor",
      "Security hacker",
      "Programming paradigm",
      "Computational physics",
      "Logistic regression",
      "SPSS Modeler",
      "Scientific computing",
      "Independent component analysis",
      "Machine Learning (journal)",
      "Data farming",
      "Cross-validation (statistics)",
      "European Commission",
      "Data steward",
      "AlexNet",
      "STATISTICA",
      "Computability theory",
      "InformationWeek",
      "Outlier detection",
      "Processor (computing)",
      "Cyberwarfare",
      "Computational engineering",
      "Fact table",
      "K-nearest neighbors algorithm",
      "ADVISE",
      "Programming language",
      "Extract, load, transform",
      "Applied statistics",
      "Linear regression",
      "Springer Verlag",
      "Web mining",
      "Measure (data warehouse)",
      "Data Mining Extensions",
      "Generative model",
      "Software development",
      "Network service",
      "Data de-identification",
      "Data redundancy",
      "Computer graphics",
      "Data lineage",
      "Agent mining",
      "Automata theory",
      "Directive on Copyright in the Digital Single Market",
      "Google Book Search Settlement Agreement",
      "Edward Snowden",
      "Natural Language Toolkit",
      "Data validation",
      "Association rule learning",
      "UIMA",
      "Rule-based machine learning",
      "Data analysis",
      "Embedded system",
      "Conditional random field",
      "Statistical",
      "ROLAP",
      "Data integration",
      "Computer hardware",
      "Vision transformer",
      "Dimensional modeling",
      "Computer architecture",
      "Computational chemistry",
      "Library (computing)",
      "Computer data storage",
      "Data degradation",
      "Batch learning",
      "CJEU",
      "Limitations and exceptions to copyright",
      "Application security",
      "Support vector machines",
      "Feedforward neural network",
      "Natural language processing",
      "Data and information visualization",
      "Angoss",
      "Fuzzy clustering",
      "European Union",
      "Decision support system",
      "PSPP",
      "Automated machine learning",
      "Information Society Directive",
      "Formal language",
      "DBSCAN",
      "Generative adversarial network",
      "Bill Inmon",
      "Boltzmann machine",
      "Structured data analysis (statistics)",
      "Data cleaning",
      "List of reporting software",
      "Meta-learning (computer science)",
      "Genetic algorithms",
      "Chemicalize.org",
      "Data philanthropy",
      "Multimodal learning",
      "Statistical learning theory",
      "Data preservation",
      "XML",
      "Data collection",
      "Data security",
      "K-means clustering",
      "US copyright law",
      "Data privacy",
      "Spiking neural network",
      "Mechanistic interpretability",
      "Data retention",
      "Data dictionary",
      "Time series analysis",
      "Qlucore",
      "Occam learning",
      "Enterprise bus matrix",
      "MOLAP",
      "CURE algorithm",
      "MOA (Massive Online Analysis)",
      "Global surveillance disclosure",
      "Data acquisition",
      "Digital library",
      "Data archaeology",
      "Philosophy of artificial intelligence",
      "Gregory Piatetsky-Shapiro",
      "R (programming language)",
      "Security service (telecommunication)",
      "Density estimation",
      "Weka (machine learning)",
      "Digital art",
      "Data fusion",
      "SSRN (identifier)",
      "Sequential pattern mining",
      "Long short-term memory",
      "Network performance",
      "Theoretical computer science",
      "Computational complexity theory",
      "Web scraping"
    ]
  },
  "CiteSeerX (identifier)": {
    "url": "https://en.wikipedia.org/wiki/CiteSeerX_(identifier)",
    "title": "CiteSeerX (identifier)",
    "content": "CiteSeer X (formerly called CiteSeer ) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science . CiteSeer's goal is to improve the dissemination and access of academic and scientific literature. As a non-profit service that can be freely used by anyone, it has been considered part of the open access movement that is attempting to change academic and scientific publishing to allow greater access to scientific literature. CiteSeer freely provided Open Archives Initiative metadata of all indexed documents and links indexed documents when possible to other sources of metadata such as DBLP and the ACM Portal . To promote open data , CiteSeer X shares its data for non-commercial purposes under a Creative Commons license . [ 1 ] CiteSeer is considered a predecessor of academic search tools such as Google Scholar and Microsoft Academic Search . [ 2 ] CiteSeer-like engines and archives usually only harvest documents from publicly available websites and do not crawl publisher websites. For this reason, authors whose documents are freely available are more likely to be represented in the index. CiteSeer changed its name to ResearchIndex at one point and then changed it back. [ 3 ] CiteSeer was created by researchers Lee Giles , Kurt Bollacker and Steve Lawrence in 1997 while they were at the NEC Research Institute (now NEC Labs ), Princeton, New Jersey , US. CiteSeer's goal was to actively crawl and harvest academic and scientific documents on the web and use autonomous citation indexing to permit querying by citation or by document, ranking them by citation impact . At one point, it was called ResearchIndex. CiteSeer became public in 1998 and had many new features unavailable in academic search engines at that time. These included: CiteSeer was granted a United States patent # 6289342, titled \" Autonomous citation indexing and literature browsing using citation context \", on September 11, 2001. The patent was filed on May 20, 1998, and has priority to January 5, 1998. A continuation patent (US Patent # 6738780) was filed on May 16, 2001, and granted on May 18, 2004. [ citation needed ] After NEC, in 2004 it was hosted as CiteSeer.IST on the World Wide Web at the College of Information Sciences and Technology, The Pennsylvania State University , and had over 700,000 documents. For enhanced access, performance and research, similar versions of CiteSeer were supported at universities such as the Massachusetts Institute of Technology , University of Zürich and the National University of Singapore . However, these versions of CiteSeer proved difficult to maintain and are no longer available. Because CiteSeer only indexes freely available papers on the web and does not have access to publisher metadata, it returns fewer citation counts than sites, such as Google Scholar , that have publisher metadata. CiteSeer had not been comprehensively updated since 2005 due to limitations in its architecture design. It had a representative sampling of research documents in computer and information science but was limited in coverage because it was limited to papers that are publicly available, usually at an author's homepage, or those submitted by an author. To overcome some of these limitations, a modular and open source architecture for CiteSeer was designed – CiteSeer X . CiteSeer X replaced CiteSeer and all queries to CiteSeer were redirected. CiteSeer X [ 4 ] is a public search engine and digital library and repository for scientific and academic papers, primarily with a focus on computer and information science . [ 4 ] However, recently CiteSeer X has been expanding into other scholarly domains such as economics, physics and others. Released in 2008, it was loosely based on the previous CiteSeer search engine and digital library and is built with a new open source infrastructure, SeerSuite, and new algorithms and their implementations. It was developed by researchers Isaac Councill and C. Lee Giles at the College of Information Sciences and Technology , Pennsylvania State University . It continues to support the goals outlined by CiteSeer to actively crawl and harvest academic and scientific documents on the public web and to use a citation inquiry by citations and ranking of documents by the impact of citations. Currently, Lee Giles, Prasenjit Mitra, Susan Gauch, Min-Yen Kan, Pradeep Teregowda, Juan Pablo Fernández Ramírez, Pucktada Treeratpituk, Jian Wu, Douglas Jordan, Steve Carman, Jack Carroll, Jim Jansen, and Shuyi Zheng are or have been actively involved in its development. Recently, a table search feature was introduced. [ 5 ] It has been funded by the National Science Foundation , NASA , and Microsoft Research . CiteSeer X continues to be rated as one of the world's top repositories, and was rated number 1 in July 2010. [ 6 ] It currently has over 6 million documents with nearly 6 million unique authors and 120 million citations. [ timeframe? ] CiteSeer X also shares its software, data, databases and metadata with other researchers, currently by Amazon S3 and by rsync . [ 7 ] Its new modular open source architecture and software (available previously on SourceForge but now on GitHub ) is built on Apache Solr and other Apache and open source tools, which allows it to be a testbed for new algorithms in document harvesting, ranking, indexing, and information extraction. CiteSeer X caches some PDF files that it has scanned. As such, each page includes a DMCA link which can be used to report copyright violations. [ 8 ] CiteSeer X uses automated information extraction tools, usually built on machine learning methods such ParsCit, to extract scholarly document metadata such as title, authors, abstract, citations, etc. As such, there are sometime errors in authors and titles. Other academic search engines have similar errors. CiteSeer X crawls publicly available scholarly documents primarily from author webpages and other open resources, and does not have access to publisher metadata. As such, citation counts in CiteSeer X are usually less than those in Google Scholar and Microsoft Academic Search who have access to publisher metadata. CiteSeer X has nearly one million users worldwide based on unique IP addresses and has millions of hits daily. Annual downloads of document PDFs were nearly 200 million for 2015. CiteSeer X data is regularly shared under a Creative Commons BY-NC-SA license with researchers worldwide and has been and is used in many experiments and competitions. Thanks to its OAI-PMH endpoint, [ 9 ] CiteSeerX is an open archive and its content is indexed like an institutional repository in academic search engines , for instance BASE and Unpaywall consumers. The CiteSeer model had been extended to cover academic documents in business with SmealSearch and in e-business with eBizSearch . However, these were not maintained by their sponsors. An older version of both of these could be once found at BizSeer.IST but is no longer in service. Other Seer-like search and repository systems have been built for chemistry, Chem X Seer and for archaeology, ArchSeer. Another had been built for robots.txt file search, BotSeer . All of these are built on the open source tool SeerSuite , which uses the open source indexer Lucene .",
    "links": [
      "Collection of Computer Science Bibliographies",
      "Open archive",
      "Microsoft Academic",
      "Academic search engines",
      "Metadata",
      "Search engine (computing)",
      "SourceForge",
      "Microsoft Academic Search",
      "Unpaywall",
      "Doi (identifier)",
      "ACM Portal",
      "Citation impact",
      "ChemXSeer",
      "Lucene",
      "Google Scholar",
      "World Wide Web",
      "Patent",
      "Open-source software",
      "Search engine",
      "Microsoft Research",
      "GitHub",
      "Open access (publishing)",
      "Kurt Bollacker",
      "BotSeer",
      "S2CID (identifier)",
      "SmealSearch",
      "Institutional repository",
      "OAI-PMH",
      "DBLP",
      "NASA",
      "Research Papers in Economics",
      "Massachusetts Institute of Technology",
      "Apache Solr",
      "Apache Software Foundation",
      "BASE (search engine)",
      "Creative Commons license",
      "Digital library",
      "ArXiv",
      "Semantic Scholar",
      "National University of Singapore",
      "Arnetminer",
      "ISBN (identifier)",
      "Academic publishing",
      "Bibliographic database",
      "Open Archives Initiative",
      "Rsync",
      "Information extraction",
      "Disciplinary repository",
      "Open data",
      "University of Zürich",
      "Revenue",
      "List of academic databases and search engines",
      "DMCA",
      "NEC Laboratories America",
      "Penn State College of Information Sciences and Technology",
      "Wikidata",
      "Pennsylvania State University",
      "Princeton, New Jersey",
      "Citation index",
      "Computer science",
      "National Science Foundation",
      "Steve Lawrence (computer scientist)",
      "Amazon S3",
      "Information science",
      "Lee Giles",
      "NEC Research Institute"
    ]
  },
  "Censorship": {
    "url": "https://en.wikipedia.org/wiki/Censorship",
    "title": "Censorship",
    "content": "Censorship is the suppression of speech , public communication, or other information . This may be done on the basis that such material is considered objectionable, harmful, sensitive, or \"inconvenient\". [ 2 ] [ 3 ] [ 4 ] Censorship can be conducted by governments [ 5 ] and private institutions. [ 6 ] When an individual such as an author or other creator engages in censorship of their own works or speech, it is referred to as self-censorship . General censorship occurs in a variety of different media, including speech, books, music, films, and other arts, the press , radio, television, and the Internet for a variety of claimed reasons including national security , to control obscenity , pornography , and hate speech , to protect children or other vulnerable groups, to promote or restrict political or religious views, and to prevent slander and libel . Specific rules and regulations regarding censorship vary between legal jurisdictions and/or private organizations. Socrates , while defying attempts by the Athenian state to censor his philosophical teachings, was brought charges that led to his death. The conviction is recorded by Plato: in 399 BC, Socrates went on trial [ 8 ] and was subsequently found guilty of both corrupting the minds of the youth of Athens and of impiety ( asebeia , [ 9 ] \"not believing in the gods of the state\"), [ 10 ] and was sentenced to hemlock . [ 11 ] [ 12 ] [ 13 ] Socrates' student, Plato , is said to have advocated censorship in his essay on The Republic , which opposed the existence of democracy. In contrast to Plato, Greek playwright Euripides (480–406 BC) defended the true liberty of freeborn men, including the right to speak freely. In 1766, Sweden became the first country to abolish censorship by law . [ 14 ] Censorship has been criticized throughout history for being unfair and hindering progress. [ citation needed ] In a 1997 essay on Internet censorship, social commentator Michael Landier explains that censorship is counterproductive as it prevents the censored topic from being discussed. Landier expands his argument by claiming that those who impose censorship must consider what they censor to be true, as individuals believing themselves to be correct would welcome the opportunity to disprove those with opposing views. [ 15 ] Censorship is often used to impose moral values on society, as in the censorship of material considered obscene. English novelist E. M. Forster was a staunch opponent of censoring material on the grounds that it was obscene or immoral, raising the issue of moral subjectivity and the constant changing of moral values. When the 1928 novel Lady Chatterley's Lover was put on trial in 1960 , Forster wrote: [ 16 ] Lady Chatterley's Lover is a literary work of importance...I do not think that it could be held obscene, but am in a difficulty here, for the reason that I have never been able to follow the legal definition of obscenity. The law tells me that obscenity may deprave and corrupt, but as far as I know, it offers no definition of depravity or corruption. Proponents have sought to justify it using different rationales for various types of information censored: In wartime, explicit censorship is carried out with the intent of preventing the release of information that might be useful to an enemy. Typically it involves keeping times or locations secret, or delaying the release of information (e.g., an operational objective) until it is of no possible use to enemy forces. The moral issues here are often seen as somewhat different, as the proponents of this form of censorship argue that the release of tactical information usually presents a greater risk of casualties among one's own forces and could possibly lead to loss of the overall conflict. [ citation needed ] During World War I letters written by British soldiers would have to go through censorship. This consisted of officers going through letters with a black marker and crossing out anything which might compromise operational secrecy before the letter was sent. [ 22 ] The World War II catchphrase \" Loose lips sink ships \" was used as a common justification to exercise official wartime censorship and encourage individual restraint when sharing potentially sensitive information. [ 23 ] An example of \" sanitization \" policies comes from the USSR under Joseph Stalin , where publicly used photographs were often altered to remove people whom Stalin had condemned to execution. Though past photographs may have been remembered or kept, this deliberate and systematic alteration to all of history in the public mind is seen as one of the central themes of Stalinism and totalitarianism . [ citation needed ] Censorship is occasionally carried out to aid authorities or to protect an individual, as with some kidnappings when attention and media coverage of the victim can sometimes be seen as unhelpful. [ 24 ] Religious censorship is a form of censorship where freedom of expression is controlled or limited using religious authority or on the basis of the teachings of the religion . [ 25 ] This form of censorship has a long history and is practiced in many societies and by many religions. Examples include the Galileo affair , Edict of Compiègne , the Index Librorum Prohibitorum (list of prohibited books) and the condemnation of Salman Rushdie 's novel The Satanic Verses by Iranian leader Ayatollah Ruhollah Khomeini . Images of the Islamic figure Muhammad are also regularly censored. In some secular countries, this is sometimes done to prevent hurting religious sentiments. [ 26 ] The content of school textbooks is often an issue of debate, since their target audiences are young people. The term whitewashing is commonly used to refer to revisionism aimed at glossing over difficult or questionable historical events, or a biased presentation thereof. The reporting of military atrocities in history is extremely controversial, as in the case of the Holocaust (or Holocaust denial ), Bombing of Dresden , the Nanking Massacre , as found with Japanese history textbook controversies , the Armenian genocide , the Tiananmen Square protests of 1989 , and the Winter Soldier Investigation of the Vietnam War . In the context of secondary school education, the way facts and history are presented greatly influences the interpretation of contemporary thought, opinion, and socialization. One argument for censoring the type of information disseminated is based on the inappropriate quality of such material for the younger public. The use of the \"inappropriate\" distinction is in itself controversial, as it has changed heavily. A Ballantine Books version of the book Fahrenheit 451 which is the version used by most school classes [ 27 ] contained approximately 75 separate edits, omissions, and changes from the original Bradbury manuscript. In February 2006, a National Geographic cover was censored by the Nashravaran Journalistic Institute . The offending cover was about the subject of love and a picture of an embracing couple was hidden beneath a white sticker. [ 28 ] Economic induced censorship is a type of censorship enacted by economic markets to favor, and disregard, types of information. Economic induced censorship is also caused by market forces which privatize and establish commodification of certain information that is not accessible by the general public, primarily because of the cost associated with commodified information such as academic journals, industry reports and pay to use repositories. [ 29 ] The concept was illustrated as a censorship pyramid [ 30 ] that was conceptualized by primarily Julian Assange , along with Andy Müller-Maguhn , Jacob Appelbaum and Jérémie Zimmermann , in the Cypherpunks (book) . Self-censorship is the act of censoring or classifying one's own discourse. This is done out of fear of, or deference to, the sensibilities or preferences (actual or perceived) of others and without overt pressure from any specific party or institution of authority. Self-censorship is often practiced by film producers , film directors , publishers , news anchors , journalists , musicians , and other kinds of authors , including individuals who use social media . [ 32 ] According to a Pew Research Center and the Columbia Journalism Review survey, \"About one-quarter of the local and national journalists say they have purposely avoided newsworthy stories, while nearly as many acknowledge they have softened the tone of stories to benefit the interests of their news organizations. Fully four-in-ten (41%) admit they have engaged in either or both of these practices.\" [ 33 ] Threats to media freedom have shown a significant increase in Europe in recent years, according to a study published in April 2017 by the Council of Europe . This results in a fear of physical or psychological violence, and the ultimate result is self-censorship by journalists. [ 34 ] Copy approval is the right to read and amend an article, usually an interview, before publication. Many publications [ which? ] refuse to give copy approval but it is increasingly becoming common practice when dealing with publicity anxious celebrities. [ 35 ] Picture approval is the right given to an individual to choose which photos will be published and which will not. Robert Redford is well known for insisting upon picture approval. Writer approval is when writers are chosen based on whether they will write flattering articles or not. Hollywood publicist Pat Kingsley is known for banning certain writers who wrote undesirably about one of her clients from interviewing any of her other clients. [ 36 ] Flooding the public, often through online social networks , with false or misleading information is sometimes called \"reverse censorship\". American legal scholar Tim Wu has explained that this type of information control, sometimes by state actors , can \"distort or drown out disfavored speech through the creation and dissemination of fake news , the payment of fake commentators, and the deployment of propaganda robots .\" [ 37 ] Soft censorship or indirect censorship is the practice of influencing news coverage for example by applying financial pressure on media companies that are deemed critical of a government or its policies and rewarding media outlets and individual journalists who are seen as friendly to the government. [ 38 ] Financial censorship is when financial institutions and payment intermediaries de-bank accounts or inhibit transactions and influence what kind of speech can exist online. [ 39 ] Examples of financial censorship include: Book censorship can be enacted at the national or sub-national level, and can carry legal penalties for their infraction. Books may also be challenged at a local, community level. As a result, books can be removed from schools or libraries, although these bans do not typically extend outside of that area. Aside from the usual justifications of pornography and obscenity , some films are censored due to changing racial attitudes or political correctness in order to avoid ethnic stereotyping and/or ethnic offense despite its historical or artistic value. One example is the still withdrawn \" Censored Eleven \" series of animated cartoons, which may have been innocent then, but are \"incorrect\" now. [ 40 ] Film censorship is carried out by various countries. Film censorship is achieved by censoring the producer or restricting a state citizen. For example, in China the film industry censors LGBT-related films . Filmmakers must resort to finding funds from international investors such as the \"Ford Foundations\" and or produce through an independent film company. [ 41 ] Music censorship has been implemented by states, religions, educational systems, families, retailers and lobbying groups – and in most cases they violate international conventions of human rights. [ 42 ] Censorship of maps is often employed for military purposes. For example, the technique was used in former East Germany , especially for the areas near the border to West Germany in order to make attempts of defection more difficult. Censorship of maps is also applied by Google Maps , where certain areas are grayed out or blacked or areas are purposely left outdated with old imagery. [ 43 ] Art is loved and feared because of its evocative power. Destroying or oppressing art can potentially justify its meaning even more. [ 44 ] British photographer and visual artist Graham Ovenden 's photos and paintings were ordered to be destroyed by a London's magistrate court in 2015 for being \"indecent\" [ 45 ] and their copies had been removed from the online Tate gallery . [ 46 ] A 1980 Israeli law forbade banned artwork composed of the four colours of the Palestinian flag , [ 47 ] and Palestinians were arrested for displaying such artwork or even for carrying sliced melons with the same pattern. [ 48 ] [ 49 ] [ 50 ] Moath al-Alwi is a Guantanamo Bay prisoner who creates model ships as an expression of art. Alwi does so with the few tools he has at his disposal such as dental floss and shampoo bottles, and he is also allowed to use a small pair of scissors with rounded edges. [ 51 ] A few of Alwi's pieces are on display at John Jay College of Criminal Justice in New York. There are also other artworks on display at the College that were created by other inmates. The artwork that is being displayed might be the only way for some of the inmates to communicate with the outside. Though recently, things have changed. The military has come up with a new policy that will not allow the artwork at Guantanamo Bay Military Prison to leave the prison. The artwork created by Alwi and other prisoners is now government property and can be destroyed or disposed of in whatever way the government choose, making it no longer the artist's property. [ 52 ] Around 300 artists in Cuba are fighting for their artistic freedom due to new censorship rules Cuba's government has in place for artists. In December 2018, following the introduction of new rules that would ban music performances and artwork not authorized by the state, performance artist Tania Bruguera was detained upon arriving to Havana and released after four days. [ 53 ] An example of extreme state censorship was the Nazis' requirements of using art as propaganda. Art was only allowed to be used as a political instrument to control people and failure to act in accordance with the censors was punishable by law, even fatal. The Degenerate Art Exhibition was a historical instance of this, the goal of which was to advertise Nazi values and slander others. [ 54 ] Internet censorship is the control or suppression of the publishing or accessing of information on the Internet . It may be carried out by governments or by private organizations either at the behest of the government or on their own initiative. Individuals and organizations may engage in self-censorship on their own or due to intimidation and fear. The issues associated with Internet censorship are similar to those for offline censorship of more traditional media. One difference is that national borders are more permeable online: residents of a country that bans certain information can find it on websites hosted outside the country. Thus, censors must work to prevent access to information even though they lack physical or legal control over the websites themselves. This in turn requires the use of technical censorship methods that are unique to the Internet, such as site blocking and content filtering. [ 60 ] Furthermore, the Domain Name System (DNS) a critical component of the Internet is dominated by centralized and few entities. The most widely used DNS root is administered by the Internet Corporation for Assigned Names and Numbers (ICANN). [ 61 ] [ 62 ] As an administrator they have rights to shut down and seize domain names when they deem necessary to do so and at most times the direction is from governments. This has been the case with Wikileaks shutdowns [ 63 ] and name seizure events such as the ones executed by the National Intellectual Property Rights Coordination Center (IPR Center) managed by the Homeland Security Investigations (HSI). [ 64 ] This makes it easy for internet censorship by authorities as they have control over what should or should not be on the Internet. Some activists and researchers [ who? ] have started opting for alternative DNS roots , though the Internet Architecture Board [ 65 ] (IAB) does not support these DNS root providers. Unless the censor has total control over all Internet-connected computers, such as in North Korea or Cuba , total censorship of information is very difficult or impossible to achieve due to the underlying distributed technology of the Internet. Pseudonymity and data havens (such as Freenet ) protect free speech using technologies that guarantee material cannot be removed and prevents the identification of authors. Technologically savvy users can often find ways to access blocked content . Nevertheless, blocking remains an effective means of limiting access to sensitive information for most users when censors, such as those in China , are able to devote significant resources to building and maintaining a comprehensive censorship system. [ 60 ] Views about the feasibility and effectiveness of Internet censorship have evolved in parallel with the development of the Internet and censorship technologies: A BBC World Service poll of 27,973 adults in 26 countries, including 14,306 Internet users, [ 69 ] was conducted between 30 November 2009 and 7 February 2010. The head of the polling organization felt, overall, that the poll showed that: The poll found that nearly four in five (78%) Internet users felt that the Internet had brought them greater freedom, that most Internet users (53%) felt that \"the internet should never be regulated by any level of government anywhere\", and almost four in five Internet users and non-users around the world felt that access to the Internet was a fundamental right (50% strongly agreed, 29% somewhat agreed, 9% somewhat disagreed, 6% strongly disagreed, and 6% gave no opinion). [ 71 ] The rising use of social media in many nations [ which? ] has led to the emergence of citizens organizing protests through social media, sometimes called \" Twitter Revolutions \". The most notable of these social media-led protests were the Arab Spring uprisings , starting in 2010. In response to the use of social media in these protests, the Tunisian government began a hack of Tunisian citizens' Facebook accounts, and reports arose of accounts being deleted. [ 72 ] Automated systems can be used to censor social media posts, and therefore limit what citizens can say online. This most notably occurs in China , where social media posts are automatically censored depending on content. In 2013, Harvard political science professor Gary King led a study to determine what caused social media posts to be censored and found that posts mentioning the government were not more or less likely to be deleted if they were supportive or critical of the government. Posts mentioning collective action were more likely to be deleted than those that had not mentioned collective action. [ 73 ] Currently, social media censorship appears primarily as a way to restrict Internet users' ability to organize protests. For the Chinese government, seeing citizens unhappy with local governance is beneficial as state and national leaders can replace unpopular officials. King and his researchers were able to predict when certain officials would be removed based on the number of unfavorable social media posts. [ 74 ] Research has proved that criticism is tolerable on social media sites, therefore it is not censored unless it has a higher chance of collective action. It is not important whether the criticism is supportive or unsupportive of the states' leaders, the main priority of censoring certain social media posts is to make sure that no big actions are being made due to something that was said on the internet. Posts that challenge the Party's political leading role in the Chinese government are more likely to be censored due to the challenges it poses to the Chinese Communist Party. [ 75 ] In December 2022 Elon Musk , owner and CEO of Twitter released internal documents from the social media microblogging site to journalists Matt Taibbi , Michael Shellenberger and Bari Weiss . The analysis of these files on Twitter, collectively called, the Twitter Files , explored the content moderation and visibility filtering carried out in collaboration with the Federal Bureau of Investigation on the Hunter Biden laptop controversy . On the platform TikTok, certain hashtags have been categorized by the platform's code and determines how viewers can or cannot interact with the content or hashtag specifically. Some shadowbanned tags include: #acab, #GayArab, #gej due to their referencing of certain social movements and LGBTQ identity. As TikTok guidelines are becoming more localized around the world, some experts believe [ who? ] that this could result in more censorship than before. [ 76 ] Since the early 1980s, advocates of video games have emphasized their use as an expressive medium , arguing for their protection under the laws governing freedom of speech and also as an educational tool. Detractors argue that video games are harmful and therefore should be subject to legislative oversight and restrictions . Many video games have certain elements removed or edited due to regional rating standards . [ 77 ] [ 78 ] For example, in the Japanese and PAL Versions of No More Heroes , blood splatter and gore is removed from the gameplay. Decapitation scenes are implied, but not shown. Scenes of missing body parts after having been cut off, are replaced with the same scene, but showing the body parts fully intact. [ 79 ] Surveillance and censorship are different. Surveillance can be performed without censorship, but it is harder to engage in censorship without some form of surveillance. [ 80 ] Even when surveillance does not lead directly to censorship, the widespread knowledge or belief that a person, their computer, or their use of the Internet is under surveillance can have a \" chilling effect \" and lead to self-censorship. [ 81 ] The former Soviet Union maintained a particularly extensive program of state-imposed censorship. The main organ for official censorship in the Soviet Union was the Chief Agency for Protection of Military and State Secrets generally known as the Glavlit , its Russian acronym. [ 82 ] The Glavlit handled censorship matters arising from domestic writings of just about any kind – even beer and vodka labels. Glavlit censorship personnel were present in every large Soviet publishing house or newspaper; the agency employed 70,000 censors to review information before it was disseminated by publishing houses, editorial offices, and broadcasting studios. No mass medium escaped Glavlit ' s control. All press agencies and radio and television stations had Glavlit representatives on their editorial staffs. [ 83 ] Sometimes, public knowledge of the existence of a specific document is subtly suppressed, a situation resembling censorship. The authorities taking such action will justify it by declaring the work to be \" subversive \" or \"inconvenient\". An example is Michel Foucault 's 1978 text Sexual Morality and the Law (later republished as The Danger of Child Sexuality ), originally published as La loi de la pudeur [literally, \"the law of decency\"]. This work defends the decriminalization of statutory rape and the abolition of age-of-consent laws . [ citation needed ] When a publisher comes under pressure to suppress a book, but has already entered into a contract with the author, they will sometimes effectively censor the book by deliberately ordering a small print run and making minimal, if any, attempts to publicize it. This practice became known in the early 2000s as privishing ( priv ate publ ishing ). [ 84 ] an OpenNet Initiative (ONI) classifications: [ 85 ] Censorship for individual countries is measured by Freedom House (FH) Freedom of the Press report, [ 86 ] Reporters Without Borders (RWB) Press freedom index [ 87 ] and V-Dem government censorship effort index. Censorship aspects are measured by Freedom on the Net [ 55 ] and OpenNet Initiative (ONI) classifications. [ 85 ] Censorship by country collects information on censorship, internet censorship , press freedom , freedom of speech , and human rights by country and presents it in a sortable table, together with links to articles with more information. In addition to countries, the table includes information on former countries, disputed countries, political sub-units within countries, and regional organizations. In French-speaking Belgium , politicians considered far-right are banned from live media appearances such as interviews or debates. [ 88 ] [ 89 ] Very little is formally censored in Canada, aside from \" obscenity \" (as defined in the landmark criminal case of R v Butler ) which is generally limited to pornography and child pornography depicting and/or advocating non-consensual sex, sexual violence, degradation, or dehumanization, in particular that which causes harm (as in R v Labaye ). Most films are simply subject to classification by the British Columbia Film Classification Office under the non-profit Crown corporation by the name of Consumer Protection BC , whose classifications are officially used by the provinces of British Columbia , Saskatchewan , Ontario , and Manitoba . [ 90 ] Cuban media used to be operated under the supervision of the Communist Party's Department of Revolutionary Orientation , which \"develops and coordinates propaganda strategies\". [ 91 ] Connection to the Internet is restricted and censored. [ 92 ] The People's Republic of China employs sophisticated censorship mechanisms, referred to as the Golden Shield Project , to monitor the internet. Popular search engines such as Baidu also remove politically sensitive search results. [ 93 ] [ 94 ] [ 95 ] Strict censorship existed in the Eastern Bloc. [ 96 ] Throughout the bloc, the various ministries of culture held a tight rein on their writers. [ 97 ] Cultural products there reflected the propaganda needs of the state. [ 97 ] Party-approved censors exercised strict control in the early years. [ 98 ] In the Stalinist period, even the weather forecasts were changed if they suggested that the sun might not shine on May Day . [ 98 ] Under Nicolae Ceauşescu in Romania , weather reports were doctored so that the temperatures were not seen to rise above or fall below the levels which dictated that work must stop. [ 98 ] Possession and use of copying machines was tightly controlled in order to hinder the production and distribution of samizdat , illegal self-published books and magazines. Possession of even a single samizdat manuscript such as a book by Andrei Sinyavsky was a serious crime which might involve a visit from the KGB . Another outlet for works which did not find favor with the authorities was publishing abroad. Amid declining car sales in 2020, France banned a television ad by a Dutch bike company, saying the ad \"unfairly discredited the automobile industry\". [ 99 ] The Constitution of India guarantees freedom of expression , but places certain restrictions on content, with a view towards maintaining communal and religious harmony, given the history of communal tension in the nation. [ 100 ] According to the Information Technology Rules 2011, objectionable content includes anything that \"threatens the unity, integrity, defence, security or sovereignty of India, friendly relations with foreign states or public order\". [ 101 ] Notably many pornographic websites are blocked in India. [ specify ] Iraq under Baathist Saddam Hussein had much the same techniques of press censorship as did Romania under Nicolae Ceauşescu but with greater potential violence. [ 102 ] During the GHQ occupation of Japan after WW2, any criticism of the Allies' pre-war policies, the SCAP, the Far East Military Tribunal, the inquiries against the United States and every direct and indirect references to the role played by the Allied High Command in drafting Japan's new constitution or to censorship of publications, movies, newspapers and magazines was subject to massive censorship, purges , media blackout . [ 103 ] In the four years (September 1945–November 1949) since the CCD was active, 200 million pieces of mail and 136 million telegrams were opened, and telephones were tapped 800,000 times. Since no criticism of the occupying forces for crimes such as the dropping of the atomic bomb, rape and robbery by US soldiers was allowed, a strict check was carried out. Those who got caught were put on a blacklist called the watchlist, and the persons and the organizations to which they belonged were investigated in detail, which made it easier to dismiss or arrest the \"disturbing molecule\". [ 104 ] Under subsection 48(3) and (4) of the Penang Islamic Religious Administration Enactment 2004, non-Muslims in Malaysia are penalized for using the following words, or to write or publish them, in any form, version or translation in any language or for use in any publicity material in any medium: \"Allah\", \"Firman Allah\", \"Ulama\", \"Hadith\", \"Ibadah\", \"Kaabah\", \"Qadhi ' \", \"Illahi\", \"Wahyu\", \"Mubaligh\", \"Syariah\", \"Qiblat\", \"Haji\", \"Mufti\", \"Rasul\", \"Iman\", \"Dakwah\", \"Wali\", \"Fatwa\", \"Imam\", \"Nabi\", \"Sheikh\", \"Khutbah\", \"Tabligh\", \"Akhirat\", \"Azan\", \"Al Quran\", \"As Sunnah\", \"Auliya ' \", \"Karamah\", \"False Moon God\", \"Syahadah\", \"Baitullah\", \"Musolla\", \"Zakat Fitrah\", \"Hajjah\", \"Taqwa\" and \"Soleh\". [ 105 ] [ 106 ] [ 107 ] On 4 March 2022, Russian President Vladimir Putin signed into law a bill introducing prison sentences of up to 15 years for those who publish \"knowingly false information\" about the Russian military and its operations, leading to some media outlets in Russia to stop reporting on Ukraine or shutting their media outlet. [ 108 ] [ 109 ] Although the 1993 Russian Constitution has an article expressly prohibiting censorship , [ 110 ] the Russian censorship apparatus Roskomnadzor ordered the country's media to only use information from Russian state sources or face fines and blocks. [ 111 ] As of December 2022, more than 4,000 people were prosecuted under \"fake news\" laws in connection with the Russian invasion of Ukraine . [ 112 ] Novaya Gazeta 's editor-in-chief Dmitry Muratov was awarded the 2021 Nobel Peace Prize for his \"efforts to safeguard freedom of expression\". In March 2022, Novaya Gazeta suspended its print activities after receiving a second warning from Roskomnadzor . [ 113 ] According to Christian Mihr, executive director of Reporters Without Borders , \"censorship in Serbia is neither direct nor transparent, but is easy to prove.\" [ 114 ] According to Mihr there are numerous examples of censorship and self-censorship in Serbia [ 114 ] According to Mihr, Serbian prime minister Aleksandar Vučić has proved \"very sensitive to criticism, even on critical questions,\" as was the case with Natalija Miletic, a correspondent for Deutsche Welle Radio , who questioned him in Berlin about the media situation in Serbia and about allegations that some ministers in the Serbian government had plagiarized their diplomas, and who later received threats and offensive articles on the Serbian press. [ 114 ] Multiple news outlets have accused Vučić of anti-democratic strongman tendencies. [ 115 ] [ 116 ] [ 117 ] [ 118 ] [ 119 ] In July 2014, journalists associations were concerned about the freedom of the media in Serbia, in which Vučić came under criticism. [ 120 ] [ 121 ] In September 2015 five members of United States Congress (Edie Bernice Johnson, Carlos Curbelo, Scott Perry, Adam Kinzinger, and Zoe Lofgren ) have informed Vice President of the United States Joseph Biden that Aleksandar's brother, Andrej Vučić, is leading a group responsible for deteriorating media freedom in Serbia . [ 122 ] In the Republic of Singapore , Section 33 of the Films Act originally banned the making, distribution and exhibition of \"party political films\", at the pain of a fine not exceeding $100,000 or imprisonment for a term not exceeding two years. [ 123 ] The Act further defines a \"party political film\" as any film or video In 2001, the short documentary called A Vision of Persistence on opposition politician J. B. Jeyaretnam was also banned for being a \"party political film\". The makers of the documentary, all lecturers at the Ngee Ann Polytechnic, later submitted written apologies and withdrew the documentary from being screened at the 2001 Singapore International Film Festival in April, having been told they could be charged in court. [ 124 ] Another short documentary called Singapore Rebel by Martyn See , which documented Singapore Democratic Party leader Dr Chee Soon Juan 's acts of civil disobedience, was banned from the 2005 Singapore International Film Festival on the same grounds and See is being investigated for possible violations of the Films Act. [ 125 ] This law, however, is often disregarded when such political films are made supporting the ruling People's Action Party (PAP). Channel NewsAsia 's five-part documentary series on Singapore's PAP ministers in 2005, for example, was not considered a party political film. [ 126 ] Exceptions are also made when political films are made concerning political parties of other nations. Films such as Michael Moore 's 2004 documentary Fahrenheit 911 are thus allowed to screen regardless of the law. [ 127 ] Since March 2009, the Films Act has been amended to allow party political films as long as they were deemed factual and objective by a consultative committee. Some months later, this committee lifted the ban on Singapore Rebel. [ 128 ] Independent journalism did not exist in the Soviet Union until Mikhail Gorbachev became its leader. Gorbachev adopted glasnost (openness), political reform aimed at reducing censorship; before glasnost all reporting was directed by the Communist Party or related organizations. Pravda , the predominant newspaper in the Soviet Union, had a monopoly. Foreign newspapers were available only if they were published by communist parties sympathetic to the Soviet Union. Online access to all language versions of Wikipedia was blocked in Turkey on 29 April 2017 by Erdoğan 's government. [ 129 ] Article 299 of the Turkish Penal Code deems it illegal to \"Insult the President of Turkey \" . A person who is sentenced for a violation of this article can be sentenced to a prison term between one and four years and if the violation was made in public the verdict can be elevated by a sixth. [ 130 ] Prosecutions often target critics of the government, independent journalists, and political cartoonists. [ 131 ] Between 2014 and 2019, 128,872 investigations were launched for this offense and prosecutors opened 27,717 criminal cases. [ 132 ] From December 1956 until 1974 the Irish republican political party Sinn Féin was banned from participating in elections by the Northern Ireland Government. [ 133 ] From 1988 until 1994 the British government prevented the UK media from broadcasting the voices (but not words) of Sinn Féin and ten Irish republican and Ulster loyalist groups. [ 134 ] In the United States, most forms of censorship are self-imposed rather than enforced by the government. The government does not routinely censor material, although state and local governments often restrict what is provided in libraries and public schools. [ 135 ] In addition, distribution, receipt, and transmission (but not mere private possession ) of obscene material may be prohibited by law. Furthermore, under FCC v. Pacifica Foundation , the FCC has the power to prohibit the transmission of indecent material over broadcast. Additionally, critics of campaign finance reform in the United States say this reform imposes widespread restrictions on political speech. [ 136 ] [ 137 ] On the censorship of critical perspectives of U.S. history in American schools, a study published by Indiana University in 2024 found that \"in 16 Republican-dominated states, policies have been enacted to restrict the teaching of critical perspectives on race, sexuality, and other controversial subjects and to perpetuate a positive view of U.S. history\". [ 138 ] In 2019, Julia Carrie Wong wrote \"today's reactionaries are picking up the mantle of generations of Americans who have fought to ensure that white children are taught a version of America's past that is more hagiographic than historic\", with examples cited including Oregon enacting a law in the 1920s that banned the use of any textbook in schools that \"speaks slightingly of the founders\", to Lynne Cheney , the chair of the National Endowment for the Humanities , launching a campaign in the 1990s against an effort to introduce new standards for teaching U.S. history which she found insufficiently \"celebratory\". [ 139 ] In 1973, a military coup took power in Uruguay, and the State practiced censorship. For example, writer Eduardo Galeano was imprisoned and later was forced to flee. His book Open Veins of Latin America was banned by the right-wing military government, not only in Uruguay, but also in Chile and Argentina. [ 140 ]",
    "links": [
      "Internet Corporation for Assigned Names and Numbers",
      "Film director",
      "Infotainment",
      "Information access",
      "Buzzword",
      "Categorization",
      "Manitoba",
      "Assassination",
      "Herd mentality",
      "Bleep censor",
      "Purge",
      "Censorship in North Korea",
      "Tiananmen Square protests of 1989",
      "Censorship of images in the Soviet Union",
      "Persuasion",
      "Censorship by Twitter",
      "Sanitization (classified information)",
      "Mere-exposure effect",
      "Obedience",
      "UNESCO",
      "Orwellian",
      "Internet censorship in the Netherlands",
      "Propaganda and censorship in Italy during the First World War",
      "Michael Moore",
      "Jacob Appelbaum",
      "Bombing of Dresden",
      "Alternative DNS root",
      "Censorship of LGBTQ issues",
      "Twitter Revolution (disambiguation)",
      "Russian 2022 war censorship laws",
      "Censorship in Nigeria",
      "Dissident",
      "Internet police",
      "Nineteen Eighty-Four",
      "Nicolae Ceauşescu",
      "Moralistic fallacy",
      "Socrates",
      "Portugal",
      "Tabloid journalism",
      "Oxford Dictionaries (website)",
      "Internet censorship in Hong Kong",
      "Internet censorship in Finland",
      "National security",
      "World War II",
      "Homeland Security Investigations",
      "News embargo",
      "Censorship in the Czech Republic",
      "Mikhail Gorbachev",
      "Information technology",
      "Media freedom in Turkey",
      "Censorship in Denmark",
      "Lawn sign",
      "Broadcast delay",
      "R v Penguin Books Ltd",
      "Model ship",
      "Compliance (psychology)",
      "List of websites blocked in Singapore",
      "Supreme Commander for the Allied Powers",
      "Radio advertisement",
      "Firehose of falsehood",
      "Parental controls",
      "Deepfake",
      "False advertising",
      "Joseph Stalin",
      "Military government of Chile (1973–1990)",
      "Channel NewsAsia",
      "Freedom of Speech in Cameroon",
      "Censorship in Spain",
      "Serbia",
      "Obsessive–compulsive personality disorder",
      "Book censorship in China",
      "Wordfilter",
      "Free speech",
      "Internet censorship in Romania",
      "Outlaw",
      "Internet censorship in Pakistan",
      "Emotive conjugation",
      "Fear, uncertainty, and doubt",
      "Advocacy group",
      "2017 block of Wikipedia in Turkey",
      "Lynne Cheney",
      "Ostracism",
      "Psychosocial",
      "Naturalistic fallacy",
      "Censorship in Sweden",
      "Censorship in South Asia",
      "Censorship in the Russian Empire",
      "Censorship in Myanmar",
      "Censorship in the Dutch East Indies",
      "Internet censorship in Australia",
      "Film censorship",
      "Doublespeak",
      "Kids Online Safety Act",
      "Saddam Hussein",
      "Singapore International Film Festival",
      "Collateral censorship",
      "List of regionally censored video games",
      "Censorship in the United States",
      "Blocking of YouTube videos in Germany",
      "Publicity",
      "Deception",
      "Censorship by Facebook",
      "Propaganda model",
      "Communist Party of the Soviet Union",
      "Mass media regulation",
      "Twitter Files",
      "Deviance (sociology)",
      "Internet censorship in Serbia",
      "Chinese censorship abroad",
      "Censorship in Turkey",
      "Protest",
      "Character assassination",
      "Censorship of TikTok",
      "Criminal speech",
      "Publisher",
      "Internet censorship in Uruguay",
      "Putin's Palace",
      "Misinformation",
      "Wikipedia",
      "Internet censorship in China",
      "Racial hoax",
      "Bribery",
      "Online hate speech",
      "Social network",
      "Internet in Nepal",
      "Funding bias",
      "Warsaw Pact invasion of Czechoslovakia",
      "Visa Inc.",
      "Internet censorship in Russia",
      "Counterculture",
      "Censorship in Armenia",
      "Music censorship",
      "Vint Cerf",
      "False dilemma",
      "Cover-up",
      "Steam (service)",
      "Counterfire (group)",
      "Politics",
      "Exile",
      "Book burnings in Chile",
      "Statutory rape",
      "Super-injunctions in English law",
      "Censored Eleven",
      "Blocking of Twitter in Nigeria",
      "Penang",
      "William Pynchon",
      "Heckler",
      "Censorship in Francoist Spain",
      "E. M. Forster",
      "Eduardo Galeano",
      "Literary forgery",
      "Demonizing the enemy",
      "Social influence",
      "Socialization",
      "Internet censorship in Bhutan",
      "Big lie",
      "Philosophy of information",
      "Airborne leaflet propaganda",
      "Shock value",
      "Media bias in the United States",
      "Flag-waving",
      "East Germany",
      "Media blackout",
      "Library and information science",
      "Internet censorship in Iraq",
      "AI alignment",
      "Cherry picking",
      "Social contagion",
      "Censorship of Snapchat",
      "President of Turkey",
      "Tania Bruguera",
      "Winter Soldier Investigation",
      "Vietnam War",
      "Managing the news",
      "Influence of mass media",
      "British Columbia",
      "Wayback Machine",
      "Censorship by copyright",
      "Internet censorship in South Africa",
      "Whataboutism",
      "Content-control software",
      "Internet censorship in Azerbaijan",
      "Trust and safety",
      "Censorship in Brazil",
      "Political dissent",
      "Block of Wikipedia in Turkey",
      "Video game censorship",
      "United States obscenity law",
      "ISSN (identifier)",
      "Columbia Journalism Review",
      "Burning of books and burying of scholars",
      "News broadcasting",
      "Purity spiral",
      "Chilling effect",
      "Internet censorship in Egypt",
      "Censorship in Serbia",
      "Internet censorship in Myanmar",
      "Operant conditioning",
      "Advocacy",
      "ISBN (identifier)",
      "Stalinism",
      "Émigré",
      "Censorship of student media",
      "List of books banned by governments",
      "Censorship in the European Union",
      "Pew Research Center",
      "Agenda-setting theory",
      "Censor bars",
      "Rebellion",
      "Japanese history textbook controversies",
      "Suspicious activity report",
      "Censorship in Uganda",
      "Legal aspects of file sharing",
      "Guerrilla communication",
      "Performance art",
      "Censorship in Honduras",
      "Pluralistic ignorance",
      "Loyalty marketing",
      "Information",
      "Freedom of speech",
      "Bari Weiss",
      "Censorship in Bahrain",
      "Censorship in Nepal",
      "Milieu control",
      "Disinformation",
      "List of banned films",
      "Speech",
      "Internet censorship in India",
      "Obscenity",
      "Malinformation",
      "Censorship in Belarus",
      "Crowd psychology",
      "Google Maps",
      "Press freedom",
      "24-hour news cycle",
      "Human rights in Turkmenistan",
      "Cult of personality",
      "Electronic Frontier Foundation",
      "Censorship in Zimbabwe",
      "Censorship in the Russian Federation",
      "Peer pressure",
      "Domain Name System",
      "Censorship of LGBT issues",
      "Minimisation (psychology)",
      "Non-disclosure agreement",
      "S2CID (identifier)",
      "Censorship in East Germany",
      "Moath al-Alwi",
      "Media freedom in Serbia",
      "Censorship in the Federal Republic of Germany",
      "Bandwagon effect",
      "Request for Comments",
      "Cold calling",
      "Authoritarian leadership style",
      "People's Action Party",
      "Legal Jurisdiction",
      "Shooting and crying",
      "Right-wing authoritarianism",
      "Pornography",
      "Sex in advertising",
      "Ozzie Zehner",
      "Conspiracy of silence (expression)",
      "Book burning",
      "Science and technology studies",
      "Censorship in the Islamic Republic of Iran",
      "Media proprietor",
      "Appeal to fear",
      "Robert Redford",
      "Conformity",
      "Social media and the Arab Spring",
      "Internet censorship in the Republic of Ireland",
      "Holocaust denial",
      "Censorship in Chad",
      "Pixelization",
      "Censorship in Poland",
      "Block of Wikipedia in Russia",
      "Loaded language",
      "Social reality",
      "Preservation (library and archival science)",
      "Internet censorship in Belarus",
      "Deutsche Welle",
      "Communal reinforcement",
      "Protest paradigm",
      "Classified Information",
      "Normative social influence",
      "List of websites blocked in mainland China",
      "Dogma",
      "Canvassing",
      "Censorship in Peru",
      "Censorship of GitHub",
      "Dissenter",
      "Overton window",
      "Hacktivism",
      "Atrocity propaganda",
      "Censorship in Germany",
      "Sales promotion",
      "Censorship of Twitter",
      "Hazing",
      "Nikolay Gretsch",
      "Censorship in Canada",
      "Censorship in Samoa",
      "Attack ad",
      "Shadow banning",
      "Fake news",
      "Censorship in Vietnam",
      "Algorithmic bias",
      "Social construction of gender",
      "Political prisoner",
      "Internet censorship in Switzerland",
      "Understatement",
      "Surveillance",
      "Internet censorship in Italy",
      "Outline of information science",
      "Trial of Socrates",
      "Gag order",
      "Internet censorship in Spain",
      "Anti-authoritarianism",
      "2021 Nobel Peace Prize",
      "Media ethics",
      "Censorship in Venezuela",
      "Media bias",
      "Gish gallop",
      "Eclecticism",
      "Behavioral contagion",
      "Internet censorship in North Korea",
      "Open Veins of Latin America",
      "Memory hole",
      "Constitution of Russia",
      "Roskomnadzor",
      "Age verification system",
      "Josef Buršík",
      "Advertising slogan",
      "Brand",
      "Political correctness",
      "New generation warfare",
      "Internet in Yemen",
      "Media democracy",
      "Telegram in Iran",
      "Censorship in Taiwan",
      "CNET",
      "Internet bot",
      "Propaganda of the deed",
      "Blacklisting",
      "Internet censorship in Malawi",
      "Closure (sociology)",
      "Fig leaf",
      "Vogelfrei",
      "Censorship in Jamaica",
      "Fahrenheit 451",
      "Internet censorship in Germany",
      "Gary King (political scientist)",
      "Persona non grata",
      "Data haven",
      "Information architecture",
      "Censorship in Qatar",
      "Debanking",
      "Civil death",
      "Musician",
      "Censorship of school curricula in the United States",
      "Washington, DC",
      "Matt Taibbi",
      "Dumbing down",
      "University of South Florida",
      "Censorship in the Empire of Japan",
      "False balance",
      "Graham Ovenden",
      "Promotional merchandise",
      "Jérémie Zimmermann",
      "Monumental propaganda",
      "Ideological repression",
      "National Geographic (magazine)",
      "Military tactics",
      "Information behavior",
      "Censorship in Italy",
      "Far-right",
      "KGB",
      "Martyn See",
      "Press Freedom Index",
      "Product marketing",
      "Media manipulation",
      "Fahrenheit 911",
      "Senbu",
      "Internet censorship in Uzbekistan",
      "Teasing",
      "Free speech limitations",
      "Censorship of student media in the United States",
      "Censorship in Sudan",
      "Subversion",
      "Censorship in Romania",
      "French-speaking Belgium",
      "Authoritarianism",
      "Book censorship",
      "Cosmopolitanism",
      "Wrocław",
      "Edict of Compiègne",
      "Door-to-door",
      "Freedom of assembly",
      "Civil Censorship Detachment",
      "News media",
      "Censorship by Yahoo",
      "Normalization of deviance",
      "Censorship in Nazi Germany",
      "Recep Tayyip Erdoğan",
      "Groupthink",
      "Quantum information science",
      "Sinn Féin",
      "Social proof",
      "Memory conformity",
      "Deplatforming",
      "Censorship of music",
      "Radio jamming in China",
      "Fearmongering",
      "Slogan",
      "Council of Europe",
      "Nazi book burnings",
      "Media event",
      "Censorship in Hong Kong",
      "Andrei Sinyavsky",
      "Catch and kill",
      "Public diplomacy",
      "Consensus reality",
      "Historical negationism",
      "Wikimedia censorship in mainland China",
      "Radio jamming",
      "Darnton, Robert",
      "Censorship of YouTube",
      "Media censorship in Singapore",
      "Public enemy",
      "Prior restraint",
      "Negative campaigning",
      "Government",
      "Invented tradition",
      "Censorship in Bolivia",
      "SAGE Publishing",
      "Internet censorship in Malaysia",
      "Constitution of India",
      "Ontario",
      "Political warfare",
      "Criticism of advertising",
      "Censorship in the Philippines",
      "Sound bite",
      "Federal Bureau of Investigation",
      "Censorship in Cambodia",
      "Youth activism",
      "Media circus",
      "Minced oath",
      "Censorship in Iran",
      "Samizdat",
      "Solidarity (Polish trade union)",
      "Hays Code",
      "Devil's advocate",
      "FCC v. Pacifica Foundation",
      "Alternative media",
      "Gaslighting",
      "Exaggeration",
      "List of North Korean websites banned in South Korea",
      "Religious censorship",
      "Andy Müller-Maguhn",
      "Censorship in the Ancien Régime",
      "Human rights",
      "Information warfare",
      "Censorship in the Middle East",
      "Telecommunications in El Salvador",
      "Stanford prison experiment",
      "Freedom of the press in Ukraine",
      "Web blocking in the United Kingdom",
      "Internet censorship in South Korea",
      "Chair of the National Endowment for the Humanities",
      "Censorship in Singapore",
      "Audio deepfake",
      "Marketing",
      "Internet censorship in Denmark",
      "Outcast (person)",
      "Redaction",
      "Self-segregation",
      "National myth",
      "Internet censorship in Syria",
      "JSTOR (identifier)",
      "Internet in Afghanistan",
      "Internet censorship in the United States",
      "Mass media",
      "Election promise",
      "Internet freedom",
      "Communist Party of Cuba",
      "Elon Musk",
      "Shunning",
      "National conservatism",
      "Information retrieval",
      "Corporate censorship",
      "Knowledge organization",
      "Salman Rushdie",
      "Lying press",
      "Nationalism",
      "False consensus effect",
      "Satire",
      "Sensationalism",
      "Hermit",
      "Totalitarianism",
      "Censorship in Ecuador",
      "Freenet",
      "Internet activism",
      "Identification (psychology)",
      "Media ecology",
      "Radio jamming in Korea",
      "Party platform",
      "Mobbing",
      "Censorship of Google",
      "Occupation (protest)",
      "WikiLeaks",
      "Privacy",
      "Baidu",
      "Civil disobedience",
      "Culture jamming",
      "Data modeling",
      "Infomercial",
      "Harmonisation of law",
      "The Holocaust",
      "Brainwashing",
      "Galileo affair",
      "Harm principle",
      "Virus hoax",
      "Myles Burnyeat",
      "Merriam-Webster",
      "Censorship in Portugal",
      "Information society",
      "Islamic religious police",
      "National Council of Educational Research and Training",
      "Block (Internet)",
      "Tyranny of the majority",
      "Daria Impiombato",
      "Blocking of Telegram in Russia",
      "Taxonomy",
      "Reporters Without Borders",
      "Oversimplification",
      "Content moderation",
      "Victoria and Albert Museum",
      "Cypherpunks (book)",
      "Censorship in Mexico",
      "Censorship in Uruguay",
      "News anchor",
      "Censorship in Japan",
      "White propaganda",
      "Concision (media studies)",
      "Ethnic stereotype",
      "Goddess of Democracy",
      "People's Republic of Romania",
      "Psychological warfare",
      "May Day",
      "Fictitious entry",
      "Analysis paralysis",
      "Crime contagion model",
      "Hoax",
      "Corporate propaganda",
      "Blasphemy law",
      "Video manipulation",
      "Lady Chatterley's Lover",
      "Hysterical contagion",
      "The Independent",
      "Censorship in Israel",
      "Film producer",
      "PMID (identifier)",
      "Weasel word",
      "Idiosyncrasy",
      "Internet censorship in Turkey",
      "Censorship in Kenya",
      "Polish People's Republic",
      "Brazilian Congressional Bill No. 2630",
      "Games as art",
      "Billboard",
      "Freedom of expression in India",
      "Fogging (censorship)",
      "Speech code",
      "Ulster loyalism",
      "1973 Chilean coup d'état",
      "Individualism",
      "Michael Shellenberger",
      "Moscow Times",
      "British Columbia Film Classification Office",
      "2016–present purges in Turkey",
      "Alternative facts",
      "Online Safety Act 2023",
      "False accusation",
      "The Republic (Plato)",
      "Rally 'round the flag effect",
      "Forgery",
      "Postal censorship",
      "Freedom of speech by country",
      "Normalization (sociology)",
      "Asebeia",
      "Censorship in Nicaragua",
      "Manipulation (psychology)",
      "Internet censorship in New Zealand",
      "Book censorship in the United States",
      "OCLC (identifier)",
      "Concentration of media ownership",
      "Strategic lawsuit against public participation",
      "Die Tageszeitung",
      "Media franchise",
      "Irish republicanism",
      "Copy machine",
      "Censorship of Wikipedia",
      "Internet censorship in Vietnam",
      "Ad hominem",
      "Netanya Academic College",
      "Defamation",
      "The Internet",
      "Internet censorship in the People's Republic of China",
      "Food libel laws",
      "Streisand effect",
      "Indiana University",
      "Human rights in Yemen",
      "Nashravaran Journalistic Institute",
      "List of hoaxes",
      "Computational propaganda",
      "Armenian genocide",
      "National Intellectual Property Rights Coordination Center",
      "Call-out culture",
      "Propaganda",
      "Advertising",
      "Visual art",
      "Narcotizing dysfunction",
      "Media freedom in Azerbaijan",
      "Breaching experiment",
      "Censorship in Austria",
      "Social rejection",
      "Censorship in Malaysia",
      "De-banking",
      "Political campaign",
      "Asch conformity experiments",
      "Restrictions on TikTok in the United States",
      "Censorship in Rwanda",
      "Pseudonymity",
      "Photograph manipulation",
      "Propaganda techniques",
      "Preference falsification",
      "Victims of Communism Memorial",
      "The Satanic Verses",
      "Nanking Massacre",
      "Censorship in Russia",
      "Euphemism",
      "Activism",
      "Red team",
      "Milgram experiment",
      "Censorship in Tunisia",
      "Emotional contagion",
      "Iran",
      "Telemarketing",
      "Political censorship",
      "Intellectual property",
      "Censorship in Eritrea",
      "Framing (social sciences)",
      "Child pornography",
      "Book censorship in Iran",
      "Enemy of the state",
      "Taboo",
      "Smear campaign",
      "Singapore",
      "Freedom of the press in Djibouti",
      "Michel Foucault",
      "Internet censorship in cuba",
      "Name recognition",
      "Journalist",
      "Censorship in Saudi Arabia",
      "Joseph Biden",
      "R v Butler",
      "Censorship in the United Arab Emirates",
      "List of banned video games by country",
      "Zoe Lofgren",
      "Half-truth",
      "Film censorship in the United States",
      "Passing (sociology)",
      "Wedge issue",
      "Censorship in Islamic societies",
      "Hush money",
      "Cartographic censorship",
      "Market research",
      "List of banned political parties",
      "Censorship in Pakistan",
      "Left-wing nationalism",
      "Soviet Union",
      "Product demonstration",
      "Pirate Party",
      "Military censorship",
      "Spin (propaganda)",
      "Sales",
      "Censorship in Cuba",
      "Self-censorship",
      "Newspaper",
      "Internet in Kazakhstan",
      "Internet censorship in Indonesia",
      "Influence-for-hire",
      "John Gilmore (activist)",
      "Bibliometrics",
      "Censorship in Bangladesh",
      "Internet censorship in Singapore",
      "Communist parties",
      "Enemy of the people",
      "Internet censorship in Iran",
      "Hunter Biden laptop controversy",
      "Petition",
      "V-Dem Democracy indices",
      "Social media",
      "Dmitry Muratov",
      "Untouchability",
      "Crowd manipulation",
      "Information science",
      "Fear of missing out",
      "Expurgation",
      "Fifth column",
      "Proscription",
      "World War I",
      "Ronald Hamowy",
      "Social media age verification laws by country",
      "David (Michelangelo)",
      "Euripides",
      "Censorship in China",
      "Internet censorship in France",
      "Queen Victoria",
      "Doi (identifier)",
      "Cato Institute",
      "Children's Online Privacy Protection Act",
      "Spiral of silence",
      "Social integration",
      "Boycott",
      "Newspeak",
      "Block of Wikipedia in Venezuela",
      "List of websites blocked in Russia",
      "Book censorship in India",
      "Campaign advertising",
      "Opinion corridor",
      "Age-of-consent reform",
      "Love",
      "Censorship in Indonesia",
      "Film censorship in Malaysia",
      "Author",
      "Eastern Bloc media and propaganda",
      "Censorship in the Maldives",
      "Scapegoating",
      "Baathist",
      "Information seeking",
      "YouTube",
      "Promotional model",
      "Grassroots",
      "Mass media in Albania",
      "Censorship by Apple",
      "University of Missouri–Kansas City School of Law",
      "Memory",
      "Censorship in Greece",
      "Black propaganda",
      "Censorship in the Republic of Ireland",
      "Oxford University Press",
      "Mass surveillance",
      "Internet filter",
      "West Germany",
      "Censorship by country",
      "Censorship by proxy",
      "Article 299 (Turkish Penal Code)",
      "Hallin's spheres",
      "Censorship in Iraq",
      "Internet censorship in Morocco",
      "Censorship in Communist Romania",
      "Censorship in Thailand",
      "Academic bias",
      "Pravda",
      "Censorship in India",
      "Censorship in the United Kingdom",
      "Obfuscation",
      "Anticonformity (psychology)",
      "Min (god)",
      "Censorship in Finland",
      "J. B. Jeyaretnam",
      "Newspaper theft",
      "Historical revisionism (negationism)",
      "Native advertising",
      "Testimonial",
      "Palestinian flag",
      "Obscurantism",
      "Self-publishing",
      "Turkey",
      "Whitewashing (censorship)",
      "OpenNet Initiative",
      "Indoctrination",
      "Countries blocking access to The Pirate Bay",
      "Dissent",
      "Internalization (sociology)",
      "Loose lips sink ships",
      "Push poll",
      "Vladimir Putin",
      "The American Heritage Dictionary of the English Language",
      "Oxford Internet Institute",
      "Internet censorship in Japan",
      "Ayatollah Ruhollah Khomeini",
      "Bucha massacre",
      "Annoyance factor",
      "Ideograph (rhetoric)",
      "Digital Services Act",
      "Glasnost",
      "Control freak",
      "Internet censorship in the United Kingdom",
      "Censorship in Guatemala",
      "Book censorship in Canada",
      "Defection",
      "Censorship in South Korea",
      "Copycat suicide",
      "Authoritarian personality",
      "Censorship in New Zealand",
      "Russia",
      "Fake news website",
      "Heckler's veto",
      "Censorship in Paraguay",
      "Internet censorship in Thailand",
      "Eccentricity (behavior)",
      "Insubordination",
      "Censorship in France",
      "Astroturfing",
      "Twitter",
      "State actor",
      "Novaya Gazeta",
      "Singapore Democratic Party",
      "Pricing",
      "Julian Assange",
      "Spaving",
      "Freedom of thought",
      "Moral police",
      "Blood libel",
      "Mastercard",
      "Video game content rating system",
      "Chee Soon Juan",
      "Public relations",
      "Broadcast law",
      "General Directorate for the Protection of State Secrets in the Press",
      "Computer data storage",
      "Internet censorship in Cuba",
      "Hagiographic",
      "Cordon sanitaire médiatique",
      "Anti-social behaviour",
      "Library classification",
      "Imprisonment",
      "Blocking of Twitter in Brazil",
      "Product placement",
      "Kibibyte",
      "Tim Wu",
      "Casting",
      "Censorship in Sri Lanka",
      "Censorship in Algeria",
      "Conium maculatum",
      "Russian fake news laws",
      "Herd behavior",
      "Degenerate Art Exhibition",
      "Censorship in the Soviet Union",
      "Urban legends and myths",
      "Internet censorship in Paraguay",
      "Fakelore",
      "Plain folks",
      "Echo chamber (media)",
      "Internet censorship circumvention",
      "Media activism",
      "Censorship of the iTunes Store",
      "Behavioral addiction",
      "Subversion (politics)",
      "Puritan",
      "Cartographic propaganda",
      "Creeping normality",
      "Time (magazine)",
      "R v Labaye",
      "Culture shock",
      "Censorship by Google",
      "Accusation in a mirror",
      "Domain name",
      "Transfer (propaganda)",
      "Internet censorship in Canada",
      "Non-apology apology",
      "No More Heroes (video game)",
      "Russian invasion of Ukraine",
      "List of websites blocked in Belgium",
      "Censorship in Communist Poland",
      "Damnatio memoriae",
      "Censorship in Somalia",
      "False flag",
      "Internet censorship and surveillance by country",
      "Reputation management",
      "Television censorship",
      "April Fools' Day",
      "Sedition",
      "Censorship in Australia",
      "Television advertisement",
      "Freedom of the press",
      "Appeal to emotion",
      "Homo sacer",
      "Computer and network surveillance",
      "Cultural studies",
      "Political party",
      "Glittering generality",
      "Countersignaling",
      "Mobile marketing",
      "Censorship of Facebook",
      "Media scrum",
      "Internet censorship in Tunisia",
      "Internet safety",
      "Pueblo clown",
      "Film censorship in China",
      "Malaysia",
      "Plato",
      "Human Rights Day",
      "Military intelligence",
      "Informatics",
      "Internet censorship in Saudi Arabia",
      "Crown corporation",
      "Censorship in the People's Republic of China",
      "Dog whistle (politics)",
      "Social Credit System",
      "Aleksandar Vučić",
      "Judith Butler",
      "Freedom of expression",
      "Campaign finance reform in the United States",
      "Word-of-mouth marketing",
      "Hate speech laws by country",
      "Blood censorship in China",
      "Patriotism",
      "Toxic positivity",
      "Intellectual freedom",
      "Julia Carrie Wong",
      "List of websites blocked in the United Kingdom",
      "Censorship of Roblox",
      "Political demonstration",
      "Censorship in Bhutan",
      "Hate speech",
      "Film censorship in the United Kingdom",
      "Saskatchewan",
      "Advertorial",
      "Bydgoszcz events",
      "Religion",
      "Internet censorship",
      "Cancel culture",
      "Censorship by TikTok",
      "National intranet",
      "Ontology (information science)",
      "Index Librorum Prohibitorum",
      "Ilya Yashin",
      "Censorship in South Sudan",
      "Internet censorship in Iceland",
      "Censorship of Telegram",
      "Lawfare",
      "Gatekeeping (communication)",
      "Systemic bias",
      "Information management",
      "Impiety",
      "Tate Britain"
    ]
  },
  "Outline of information science": {
    "url": "https://en.wikipedia.org/wiki/Outline_of_information_science",
    "title": "Outline of information science",
    "content": "The following outline is provided as an overview of and topical guide to information science: Information science – interdisciplinary field primarily concerned with the analysis, collection, classification , manipulation, storage, retrieval and dissemination of information . [ 1 ] Practitioners within the field study the application and usage of knowledge in organizations , along with the interaction between people, organizations and any existing information systems , with the aim of creating, replacing, improving or understanding information systems. Information science can be described as all of the following: As an interdisciplinary field, information science draws upon and incorporates concepts and methodologies from: There are many fields which claim to be \"sciences\" or \"disciplines\" which are difficult to distinguish from each other and from information science. Some of them are:",
    "links": [
      "Interdisciplinarity",
      "Peer review",
      "Information access",
      "Open access",
      "Media studies",
      "Categorization",
      "Action research",
      "Seymour Lubetzky",
      "Informatics (academic field)",
      "Gottfried Leibniz",
      "Document Engineering",
      "Personal information management",
      "Organization",
      "George Boole",
      "Information retrieval",
      "Knowledge organization",
      "Information literacy",
      "Information seeking",
      "Groupware",
      "Association of Information Technology Professionals",
      "Academic discipline",
      "Memory institution",
      "Science and technology studies",
      "Steganography",
      "Memory",
      "Scientometrics",
      "African Journal of Library, Archives and Information Science",
      "Internet studies",
      "Internet search engines and libraries",
      "Usability engineering",
      "Interviews",
      "Archival research",
      "Privacy",
      "Data modeling",
      "Journal of Information Science",
      "Journal of the Association for Information Science and Technology",
      "Canadian Journal of Information and Library Science",
      "Applied science",
      "Information policy",
      "Longitudinal study",
      "Semantic Web",
      "Warren Weaver",
      "Preservation (library and archival science)",
      "Information technology",
      "Mathematics",
      "Knowledge engineering",
      "Chartered Institute of Library and Information Professionals",
      "Alexander Ivanovich Mikhailov",
      "American Society for Information Science and Technology",
      "Institute of Museum and Library Services",
      "Gerald Salton",
      "Information society",
      "Communication studies",
      "Case study",
      "Human factors",
      "J.W. Graham Medal",
      "Information systems",
      "Design philosophy",
      "Henri La Fontaine",
      "Law",
      "Participant observation",
      "Censorship",
      "Claude E. Shannon Award",
      "Information seeking behavior",
      "Document management",
      "Melville Dewey",
      "Management",
      "Philosophy",
      "Informative modelling",
      "Luciano Floridi",
      "Social sciences",
      "Public policy",
      "S. R. Ranganathan",
      "List of Information Schools",
      "Communications",
      "International Federation for Information and Documentation",
      "Eugene Garfield",
      "IEEE Richard W. Hamming Medal",
      "International Journal of Geographical Information Science",
      "Science",
      "Information architecture",
      "User-centered design",
      "O'Moore Medal",
      "Scientific classification",
      "Suzanne Briet",
      "Information history",
      "Journal of Librarianship and Information Science",
      "National Commission on Libraries and Information Science",
      "Documentation science",
      "Archival science",
      "TripleC",
      "Computer science",
      "Information and Computer Science",
      "AltaVista",
      "Philosophy of information",
      "Frederick Kilgour",
      "Records management",
      "Google",
      "Frederick Wilfrid Lancaster",
      "Wilhelm Ostwald",
      "Commerce",
      "Library and information science",
      "Information ethics",
      "IEEE Reynold B. Johnson Information Storage Systems Award",
      "Intellectual property",
      "Cultural studies",
      "Tim Berners-Lee",
      "Knowledge management",
      "Museology",
      "Life history (sociology)",
      "Outline (list)",
      "Computer storage",
      "Vannevar Bush",
      "Information school",
      "Wayback Machine",
      "Information system",
      "Information Research",
      "Jesse Shera",
      "Information Sciences (journal)",
      "Cognitive science",
      "Scientific communication",
      "Scholarly communication",
      "Historical method",
      "Academic publishing",
      "Claude Shannon",
      "Intellectual freedom",
      "Library science",
      "Observation",
      "Bibliometrics",
      "Michael Buckland",
      "International Federation for Information Processing",
      "Knowledge transfer",
      "John Shaw Billings",
      "Browsing",
      "Ontology (information science)",
      "Information, Communication & Society",
      "Paul Otlet",
      "Information",
      "Dialog (online database)",
      "Information explosion",
      "Human-computer interaction",
      "Information management",
      "Information science",
      "Content analysis",
      "Discourse analysis"
    ]
  },
  "Data retrieval": {
    "url": "https://en.wikipedia.org/wiki/Data_retrieval",
    "title": "Data retrieval",
    "content": "Data retrieval means obtaining data from a database management system (DBMS), like for example an object-oriented database (ODBMS). In this case, it is considered that data is represented in a structured way, and there is no ambiguity in data. In order to retrieve the desired data the user presents a set of criteria by a query . Then the database management system selects the demanded data from the database. The retrieved data may be stored in a file, printed, or viewed on the screen. A query language , like for example Structured Query Language (SQL), is used to prepare the queries. SQL is an American National Standards Institute (ANSI) standardized query language developed specifically to write database queries. Each database management system may have its own language, but most are relational. [ clarification needed ] Reports and queries are the two primary forms of the retrieved data from a database. There are some overlaps between them, but queries generally select a relatively small portion of the database, while reports show larger amounts of data. Queries also present the data in a standard format and usually display it on the monitor; whereas reports allow formatting of the output however you like and is normally printed. Reports are designed using a report generator built into the database management system.",
    "links": [
      "Data structure",
      "Report generator",
      "Data maintenance",
      "ISBN (identifier)",
      "Database model",
      "Object database",
      "Ambiguity",
      "Database management system",
      "Studentlitteratur",
      "Query by Example",
      "Relational database",
      "American National Standards Institute",
      "Structured Query Language",
      "Information retrieval",
      "Financial reporting",
      "Query language"
    ]
  },
  "Don Swanson": {
    "url": "https://en.wikipedia.org/wiki/Don_Swanson",
    "title": "Don Swanson",
    "content": "Don R. Swanson (October 10, 1924 – November 18, 2012) was an American information scientist, most known for his work in literature-based discovery in the biomedical domain. His particular method has been used as a model for further work, and is often referred to as Swanson linking . He was an investigator in the Arrowsmith System project, [ 1 ] which seeks to determine meaningful links between Medline articles to identify previously undiscovered public knowledge. He had been professor emeritus of the University of Chicago since 1996, and remained active in a post-retirement appointment until his health began to decline in 2009. Swanson was born in Los Angeles on October 10, 1924, the son of Harry Windfield and Grace Clara (Sandstrom) Swanson. He served with the United States Navy Reserve from 1943 to 1946, [ 2 ] and received his B.S. in Physics at Caltech , Pasadena, California in 1945. He gained his M.A from Rice Institute , Houston, Texas , two years later, and then a PhD in Theoretical Physics from the University of California at Berkeley in 1952. [ 3 ] From 1952 to 1954 Swanson worked as a computer systems analyst at Hughes Aircraft Company Research and Development Laboratories in Culver City, California . In 1955 had joined Ramo-Wooldridge Corporation . [ 2 ] Initially working as a research scientist, [ 4 ] by 1959 he was manager of the Synthetic Intelligence Dept. at Ramo-Wooldridge. There he led a project contracted to the Council on Library Resources , with Noam Chomsky and Paul L. Garvin as linguistic advisors, to investigate machine indexing of 'a small experimental library of scientific text (ca. 300,000 words)'. [ 5 ] Swanson collaborated further with Garvin on Russian-English machine translation , considering problems of polysemy , in work funded by the Rome Air Development Center . In December 1959 he attended the annual meeting of the American Anthropological Association , speaking on 'Engineering Aspects' in a symposium on the uses of data processing equipment in anthropology. [ 6 ] By 1961 he was a member of the National Science Foundation 's Science Information Council. [ 7 ] In 1963 Swanson joined the University of Chicago as a professor in the Graduate School of Library Science . He also served as dean of the graduate school from 1963 to 1972, from 1977 to 1979 and again from 1987 to 1989. From 1972 to 1976 he was a research fellow at the Chicago Institute for Psychoanalysis . [ 2 ] In the 1980s Swanson pioneered literature-based discovery in the biomedical domain, building the Arrowsmith System around a discovery method that has since become known as Swanson linking . [ 8 ] He hypothesized that the combination of two separately published results indicating an A-B relationship and a B-C relationship are evidence of an unknown or unexplored A-C relationship. He used this to propose fish oil as a treatment for Raynaud syndrome , due to their shared relationship with blood viscosity . [ 9 ] From 1992 to 1996 Swanson was professor of the biosciences collection division and the humanities division at Chicago. In 1996 he became professor emeritus. [ 2 ] In 2000, Swanson was awarded the Award of Merit - Association for Information Science and Technology , the highest honor of the society, for his \"lifetime achievements in research and scholarship.\" [ 10 ] [ 11 ]",
    "links": [
      "American Society for Information Science and Technology",
      "Los Angeles",
      "Hughes Aircraft Company",
      "Rice University",
      "Doi (identifier)",
      "University of Chicago",
      "Bibcode (identifier)",
      "Arrowsmith System",
      "Wayback Machine",
      "Physics",
      "Swanson linking",
      "University of California at Berkeley",
      "Houston, Texas",
      "S2CID (identifier)",
      "University of California, Berkeley",
      "Rome Air Development Center",
      "Chicago Institute for Psychoanalysis",
      "ISSN (identifier)",
      "Literature-based discovery",
      "Council on Library Resources",
      "Raynaud syndrome",
      "Noam Chomsky",
      "Pasadena, California",
      "University of Chicago Graduate Library School",
      "Machine translation",
      "California Institute of Technology",
      "Blood viscosity",
      "ISBN (identifier)",
      "Machine indexing",
      "Cicely Popplewell",
      "Fish oil",
      "Polysemy",
      "American Anthropological Association",
      "PMID (identifier)",
      "Ramo-Wooldridge Corporation",
      "Award of Merit - Association for Information Science and Technology",
      "PMC (identifier)",
      "National Science Foundation",
      "Culver City, California",
      "United States Navy Reserve",
      "Bioinformatics",
      "MEDLINE"
    ]
  },
  "Relevance feedback": {
    "url": "https://en.wikipedia.org/wiki/Relevance_feedback",
    "title": "Relevance feedback",
    "content": "Relevance feedback is a feature of some information retrieval and recommender systems. The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback , and to use information about whether or not those results are relevant to perform a new query. We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or \"pseudo\" feedback. Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as relevance judgments. Users may indicate relevance explicitly using a binary or graded relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as \"not relevant\", \"somewhat relevant\", \"relevant\", or \"very relevant\"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance. An example of this would be the SearchWiki feature implemented by Google on their search website. The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known Rocchio algorithm . A performance metric which became popular around 2005 to measure the usefulness of a ranking algorithm based on the explicit relevance feedback is normalized discounted cumulative gain . Other measures include precision at k and mean average precision . Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions. [ 1 ] [ 2 ] There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response. [ 3 ] [ 4 ] The key differences of implicit relevance feedback from that of explicit include: [ 5 ] An example of this is dwell time , which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results. Pseudo relevance feedback, also known as blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top \"k\" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is: Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments. This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis. [ 6 ] Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task [ citation needed ] . But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents. [ 7 ] Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required. Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query. Relevance feedback is often implemented using the Rocchio algorithm .",
    "links": [
      "Rocchio algorithm",
      "Figure of merit",
      "Precision (information retrieval)",
      "Query expansion",
      "ISBN (identifier)",
      "Wayback Machine",
      "Dwell time (information retrieval)",
      "Recommender system",
      "Relevance (information retrieval)",
      "Algorithm",
      "Doi (identifier)",
      "Feedback",
      "Google",
      "Information retrieval",
      "SearchWiki",
      "Tf-idf"
    ]
  },
  "Learning to rank": {
    "url": "https://en.wikipedia.org/wiki/Learning_to_rank",
    "title": "Learning to rank",
    "content": "Learning to rank [ 1 ] ( LTR ) or machine-learned ranking ( MLR ) is the application of machine learning , often supervised , semi-supervised or reinforcement learning , in the construction of ranking models for information retrieval and recommender systems. [ 2 ] Training data may, for example, consist of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data. Ranking is a central part of many information retrieval problems, such as document retrieval , collaborative filtering , sentiment analysis , and online advertising . A possible architecture of a machine-learned search engine is shown in the accompanying figure. Training data consists of queries and documents matching them together with the relevance degree of each match. It may be prepared manually by human assessors (or raters , as Google calls them), who check results for some queries and determine relevance of each result. It is not feasible to check the relevance of all documents, and so typically a technique called pooling is used — only the top few documents, retrieved by some existing ranking models are checked. This technique may introduce selection bias. Alternatively, training data may be derived automatically by analyzing clickthrough logs (i.e. search results which got clicks from users), [ 3 ] query chains , [ 4 ] or such search engines' features as Google's (since-replaced) SearchWiki . Clickthrough logs can be biased by the tendency of users to click on the top search results on the assumption that they are already well-ranked. Training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries. Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used. [ 5 ] First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the vector space model , Boolean model , weighted AND, [ 6 ] or BM25 . This phase is called top- k {\\displaystyle k} document retrieval and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes. [ 7 ] In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents. Learning to rank algorithms have been applied in areas other than information retrieval: For the convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called feature vectors . Such an approach is sometimes called bag of features and is analogous to the bag of words model and vector space model used in information retrieval for representation of documents. Components of such vectors are called features , factors or ranking signals . They may be divided into three groups (features from document retrieval are shown as examples): Some examples of features, which were used in the well-known LETOR dataset: Selecting and designing good features is an important area in machine learning, which is called feature engineering . There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics. Examples of ranking quality measures: DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used. [ 11 ] Other metrics such as MAP, MRR and precision, are defined only for binary judgments. Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric: Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document. Learning to Rank approaches are often categorized using one of three approaches: pointwise (where individual documents are ranked), pairwise (where pairs of documents are ranked into a relative order), and listwise (where an entire list of documents are ordered). Tie-Yan Liu of Microsoft Research Asia has analyzed existing algorithms for learning to rank problems in his book Learning to Rank for Information Retrieval . [ 1 ] He categorized them into three groups by their input spaces, output spaces, hypothesis spaces (the core function of the model) and loss functions : the pointwise, pairwise, and listwise approach. In practice, listwise approaches often outperform pairwise approaches and pointwise approaches. This statement was further supported by a large scale experiment on the performance of different learning-to-rank methods on a large collection of benchmark data sets. [ 14 ] In this section, without further notice, x {\\displaystyle x} denotes an object to be evaluated, for example, a document or an image, f ( x ) {\\displaystyle f(x)} denotes a single-value hypothesis, h ( ⋅ ) {\\displaystyle h(\\cdot )} denotes a bi-variate or multi-variate function and L ( ⋅ ) {\\displaystyle L(\\cdot )} denotes the loss function. In this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score. Formally speaking, the pointwise approach aims at learning a function f ( x ) {\\displaystyle f(x)} predicting the real-value or ordinal score of a document x {\\displaystyle x} using the loss function L ( f ; x j , y j ) {\\displaystyle L(f;x_{j},y_{j})} . A number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values. In this case, the learning-to-rank problem is approximated by a classification problem — learning a binary classifier h ( x u , x v ) {\\displaystyle h(x_{u},x_{v})} that can tell which document is better in a given pair of documents. The classifier shall take two documents as its input and the goal is to minimize a loss function L ( h ; x u , x v , y u , v ) {\\displaystyle L(h;x_{u},x_{v},y_{u,v})} . The loss function typically reflects the number and magnitude of inversions in the induced ranking. In many cases, the binary classifier h ( x u , x v ) {\\displaystyle h(x_{u},x_{v})} is implemented with a scoring function f ( x ) {\\displaystyle f(x)} . As an example, RankNet [ 15 ] adapts a probability model and defines h ( x u , x v ) {\\displaystyle h(x_{u},x_{v})} as the estimated probability of the document x u {\\displaystyle x_{u}} has higher quality than x v {\\displaystyle x_{v}} : where CDF ( ⋅ ) {\\displaystyle {\\text{CDF}}(\\cdot )} is a cumulative distribution function , for example, the standard logistic CDF , i.e. These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is often difficult in practice because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used. For example the SoftRank algorithm. [ 16 ] LambdaMART is a pairwise algorithm which has been empirically shown to approximate listwise objective functions. [ 17 ] A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method: Regularized least-squares based ranking. The work is extended in [ 26 ] to learning to rank from general preference graphs. Note: as most supervised learning-to-rank algorithms can be applied to pointwise, pairwise and listwise case, only those methods which are specifically designed with ranking in mind are shown above. Norbert Fuhr introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation; [ 49 ] a specific variant of this approach (using polynomial regression ) had been published by him three years earlier. [ 18 ] Bill Cooper proposed logistic regression for the same purpose in 1992 [ 19 ] and used it with his Berkeley research group to train a successful ranking function for TREC . Manning et al. [ 50 ] suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques. Several conferences, such as NeurIPS , SIGIR and ICML have had workshops devoted to the learning-to-rank problem since the mid-2000s (decade). Commercial web search engines began using machine-learned ranking systems since the 2000s (decade). One of the first search engines to start using it was AltaVista (later its technology was acquired by Overture , and then Yahoo ), which launched a gradient boosting -trained ranking function in April 2003. [ 51 ] [ 52 ] Bing 's search is said to be powered by RankNet algorithm, [ 53 ] [ when? ] which was invented at Microsoft Research in 2005. In November 2009 a Russian search engine Yandex announced [ 54 ] that it had significantly increased its search quality due to deployment of a new proprietary MatrixNet algorithm, a variant of gradient boosting method which uses oblivious decision trees. [ 55 ] Recently they have also sponsored a machine-learned ranking competition \"Internet Mathematics 2009\" [ 56 ] based on their own search engine's production data. Yahoo has announced a similar competition in 2010. [ 57 ] As of 2008, Google 's Peter Norvig denied that their search engine exclusively relies on machine-learned ranking. [ 58 ] Cuil 's CEO, Tom Costello, suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models \"learn what people say they like, not what people actually like\". [ 59 ] In January 2017, the technology was included in the open source search engine Apache Solr . [ 60 ] It is also available in the open source OpenSearch and Elasticsearch . [ 61 ] [ 62 ] These implementations make learning to rank widely accessible for enterprise search. Similar to recognition applications in computer vision , recent neural network based ranking algorithms are also found to be susceptible to covert adversarial attacks , both on the candidates and the queries. [ 63 ] With small perturbations imperceptible to human beings, ranking order could be arbitrarily altered. In addition, model-agnostic transferable adversarial examples are found to be possible, which enables black-box adversarial attacks on deep ranking systems without requiring access to their underlying implementations. [ 63 ] [ 64 ] Conversely, the robustness of such ranking systems can be improved via adversarial defenses such as the Madry defense. [ 65 ]",
    "links": [
      "Discounted cumulative gain",
      "Isolation forest",
      "Relevance vector machine",
      "Topological deep learning",
      "Neural radiance field",
      "Normalized discounted cumulative gain",
      "Doi (identifier)",
      "Statistical classification",
      "Proper generalized decomposition",
      "Feature learning",
      "Aya Soffer",
      "Active learning (machine learning)",
      "Feature engineering",
      "DeepDream",
      "Microsoft Research",
      "Principal component analysis",
      "Reservoir computing",
      "Document retrieval",
      "Text Retrieval Conference",
      "S2CID (identifier)",
      "JSTOR (identifier)",
      "Outline of machine learning",
      "Inverse document frequency",
      "Anomaly detection",
      "Local outlier factor",
      "Mean shift",
      "Logistic regression",
      "Neuro-symbolic AI",
      "Decision tree learning",
      "Glossary of artificial intelligence",
      "Stochastic gradient descent",
      "International Conference on Learning Representations",
      "Information retrieval",
      "Machine translation",
      "Memtransistor",
      "ECML PKDD",
      "Sentiment analysis",
      "T-distributed stochastic neighbor embedding",
      "Semantic analysis (machine learning)",
      "Mamba (deep learning architecture)",
      "Google SearchWiki",
      "Okapi BM25",
      "Independent component analysis",
      "Ranking function",
      "Machine Learning (journal)",
      "Language modeling",
      "Expectation–maximization algorithm",
      "Elo rating system",
      "Cuil",
      "AlexNet",
      "International Joint Conference on Artificial Intelligence",
      "Yahoo",
      "Multi-agent reinforcement learning",
      "Grammar induction",
      "Random forest",
      "Triplet loss",
      "Norbert Fuhr",
      "Recommender system",
      "Apprenticeship learning",
      "Diffusion model",
      "State–action–reward–state–action",
      "Marc Najork",
      "Quantum machine learning",
      "Bias–variance tradeoff",
      "K-nearest neighbors algorithm",
      "Self-supervised learning",
      "MatrixNet",
      "Anand Rajaraman",
      "Random sample consensus",
      "HITS algorithm",
      "Computational biology",
      "Recurrent neural network",
      "Peter Norvig",
      "Bradley–Terry model",
      "Non-negative matrix factorization",
      "Computer vision",
      "Q-learning",
      "Bibcode (identifier)",
      "Electrochemical RAM",
      "Tie-Yan Liu",
      "Linear regression",
      "Feature (machine learning)",
      "Autoencoder",
      "Generative model",
      "Self-organizing map",
      "Human-in-the-loop",
      "Ontology learning",
      "Convolutional neural network",
      "Collaborative filtering",
      "Data mining",
      "Reinforcement learning",
      "Sparse dictionary learning",
      "Physics-informed neural networks",
      "TF-IDF",
      "Apache Solr",
      "Mean reciprocal rank",
      "Ordinal regression",
      "Spearman's rank correlation coefficient",
      "Bootstrap aggregating",
      "List of datasets for machine-learning research",
      "Content-based image retrieval",
      "Association rule learning",
      "Linear discriminant analysis",
      "Unsupervised learning",
      "Rule-based machine learning",
      "U-Net",
      "Conditional random field",
      "World Wide Web Conference",
      "Andrei Broder",
      "Classification (machine learning)",
      "Artificial neural network",
      "Vision transformer",
      "Feature vector",
      "Receiver operating characteristic",
      "Batch learning",
      "Training data",
      "Semi-supervised learning",
      "Precision (information retrieval)",
      "ArXiv (identifier)",
      "Feedforward neural network",
      "Learning curve (machine learning)",
      "Supervised learning",
      "Relevance (information retrieval)",
      "Scoring function",
      "Machine learning",
      "Fuzzy clustering",
      "Conference on Neural Information Processing Systems",
      "List of datasets in computer vision and image processing",
      "Journal of Machine Learning Research",
      "Elasticsearch",
      "Automated machine learning",
      "Factor analysis",
      "Restricted Boltzmann machine",
      "Vector space model",
      "Mehryar Mohri",
      "Perceptron",
      "DBSCAN",
      "Gated recurrent unit",
      "Cluster analysis",
      "Naive Bayes classifier",
      "Canonical correlation",
      "AAAI Conference on Artificial Intelligence",
      "Computational learning theory",
      "Generative adversarial network",
      "Loss function",
      "Boltzmann machine",
      "Polynomial regression",
      "Coefficient of determination",
      "Bag of words",
      "Hidden Markov model",
      "Data cleaning",
      "Graphical model",
      "Meta-learning (computer science)",
      "Online advertising",
      "Yandex",
      "Overture Services, Inc.",
      "PageRank",
      "Reinforcement learning from human feedback",
      "Query-level feature",
      "Support vector machine",
      "Neural field",
      "Multimodal learning",
      "Self-play (reinforcement learning technique)",
      "Statistical learning theory",
      "AltaVista",
      "Boosting (machine learning)",
      "OPTICS algorithm",
      "Empirical risk minimization",
      "International Conference on Machine Learning",
      "Policy gradient method",
      "Neuromorphic engineering",
      "Partial order",
      "K-means clustering",
      "Kernel machines",
      "Google",
      "Multimedia information retrieval",
      "Spiking neural network",
      "Binary classifier",
      "Neural Information Processing Systems",
      "Mechanistic interpretability",
      "OpenSearch (software)",
      "Vapnik–Chervonenkis theory",
      "Structured prediction",
      "Occam learning",
      "Open-source software",
      "Deep learning",
      "Kendall's tau",
      "Special Interest Group on Information Retrieval",
      "Ensemble learning",
      "University of California at Berkeley",
      "Cumulative distribution function",
      "Wayback Machine",
      "CURE algorithm",
      "Plackett–Luce model",
      "Online machine learning",
      "ISSN (identifier)",
      "Gradient boosting",
      "BIRCH",
      "Transformer (deep learning architecture)",
      "Crowdsourcing",
      "Bing (search engine)",
      "LeNet",
      "Image retrieval",
      "Jianchang Mao",
      "Standard Boolean model",
      "ISBN (identifier)",
      "Temporal difference learning",
      "Web search engine",
      "Density estimation",
      "Probably approximately correct learning",
      "Hierarchical clustering",
      "Mean Average Precision",
      "Bayesian network",
      "Neural network (machine learning)",
      "Long short-term memory",
      "Confusion matrix",
      "Echo state network",
      "Curriculum learning",
      "Dimensionality reduction",
      "Regression analysis"
    ]
  },
  "Information science": {
    "url": "https://en.wikipedia.org/wiki/Information_science",
    "title": "Information science",
    "content": "Information science [ 1 ] [ 2 ] [ 3 ] (abbreviated as infosci ) is an academic field which is primarily concerned with the analysis , collection, classification , manipulation, storage, retrieval , movement, dissemination, and protection of information . [ 4 ] Practitioners within and outside the field engage in the study of knowledge application and usage in organizations . They also examine the interaction between people, organizations, and any existing information systems . The objective of this study is to create, replace, improve, or understand the information systems. Historically, information science has evolved as a transdisciplinary field, both drawing from and contributing to diverse domains. [ 5 ] Information science methodologies are applied across numerous domains, reflecting the discipline's versatility and relevance. Key application areas include: The interdisciplinary nature of information science continues to expand as new technological developments and social practices emerge, creating innovative research frontiers that bridge traditional disciplinary boundaries. Information science focuses on understanding problems from the perspective of the stakeholders involved and then applying information and other technologies as needed. In other words, it tackles systemic problems first rather than individual pieces of technology within that system. In this respect, one can see information science as a response to technological determinism , the belief that technology \"develops by its own laws, that it realizes its own potential, limited only by the material resources available and the creativity of its developers. It must therefore be regarded as an autonomous system controlling and ultimately permeating all other subsystems of society.\" [ 7 ] Many universities have entire colleges, departments or schools devoted to the study of information science, while numerous information-science scholars work in disciplines such as communication , healthcare , computer science , law , and sociology . Several institutions have formed an I-School Caucus (see List of I-Schools ), but numerous others besides these also have comprehensive information specializations. Within information science, current issues as of 2013 [update] include: The first known usage of the term \"information science\" was in 1955. [ 8 ] An early definition of Information science (going back to 1968, the year when the American Documentation Institute renamed itself as the American Society for Information Science and Technology ) states: Some authors use informatics as a synonym for information science . This is especially true when related to the concept developed by A. I. Mikhailov and other Soviet authors in the mid-1960s. The Mikhailov school saw informatics as a discipline related to the study of scientific information. [ 10 ] Informatics is difficult to precisely define because of the rapidly evolving and interdisciplinary nature of the field. Definitions reliant on the nature of the tools used for deriving meaningful information from data are emerging in Informatics academic programs. [ 11 ] Regional differences and international terminology complicate the problem. Some people [ which? ] note that much of what is called \"Informatics\" today was once called \"Information Science\" – at least in fields such as Medical Informatics . For example, when library scientists also began to use the phrase \"Information Science\" to refer to their work, the term \"informatics\" emerged: Another term discussed as a synonym for \"information studies\" is \" information systems \". Brian Campbell Vickery 's Information Systems (1973) placed information systems within IS. [ 12 ] Ellis, Allen & Wilson (1999) , on the other hand, provided a bibliometric investigation describing the relation between two different fields: \"information science\" and \"information systems\". [ 13 ] Philosophy of information studies conceptual issues arising at the intersection of psychology , computer science , information technology , and philosophy . It includes the investigation of the conceptual nature and basic principles of information , including its dynamics, utilisation and sciences, as well as the elaboration and application of information-theoretic and computational methodologies to its philosophical problems. [ 14 ] Robert Hammarberg pointed out that there is no coherent distinction between information and data: \"an Information Processing System (IPS) cannot process data except in terms of whatever representational language is inherent to it, [so] data could not even be apprehended by an IPS without becoming representational in nature, and thus losing their status of being raw, brute, facts.\" [ 15 ] In science and information science, an ontology formally represents knowledge as a set of concepts within a domain , and the relationships between those concepts. It can be used to reason about the entities within that domain and may be used to describe the domain. More specifically, an ontology is a model for describing the world that consists of a set of types, properties, and relationship types. Exactly what is provided around these varies, but they are the essentials of an ontology. There is also generally an expectation that there be a close resemblance between the real world and the features of the model in an ontology. [ 16 ] In theory, an ontology is a \"formal, explicit specification of a shared conceptualisation\". [ 17 ] An ontology renders shared vocabulary and taxonomy which models a domain with the definition of objects and/or concepts and their properties and relations. [ 18 ] Ontologies are the structural frameworks for organizing information and are used in artificial intelligence , the Semantic Web , systems engineering , software engineering , biomedical informatics , library science , enterprise bookmarking , and information architecture as a form of knowledge representation about the world or some part of it. The creation of domain ontologies is also essential to the definition and use of an enterprise architecture framework . Authors such as Ingwersen [ 3 ] argue that informatology has problems defining its own boundaries with other disciplines. According to Popper \"Information science operates busily on an ocean of commonsense practical applications, which increasingly involve the computer ... and on commonsense views of language, of communication, of knowledge and Information, computer science is in little better state\". [ 19 ] Other authors, such as Furner, deny that information science is a true science. [ 20 ] An information scientist is an individual, usually with a relevant subject degree or high level of subject knowledge, who provides focused information to scientific and technical research staff in industry or to subject faculty and students in academia. The industry *information specialist/scientist* and the academic information subject specialist/librarian have, in general, similar subject background training, but the academic position holder will be required to hold a second advanced degree—e.g. Master of Library Science (MLS), Military Intelligence (MI), Master of Arts (MA)—in information and library studies in addition to a subject master's. The title also applies to an individual carrying out research in information science. A systems analyst works on creating, designing, and improving information systems for a specific need. Often systems analysts work with one or more businesses to evaluate and implement organizational processes and techniques for accessing information in order to improve efficiency and productivity within the organization(s). An information professional is an individual who preserves, organizes, and disseminates information. Information professionals are skilled in the organization and retrieval of recorded knowledge. Traditionally, their work has been with print materials, but these skills are being increasingly used with electronic, visual, audio, and digital materials. Information professionals work in a variety of public, private, non-profit, and academic institutions. Information professionals can also be found within organisational and industrial contexts, [ 21 ] and are performing roles that include system design and development and system analysis. Information science, in studying the collection, classification , manipulation, storage, retrieval and dissemination of information has origins in the common stock of human knowledge. Information analysis has been carried out by scholars at least as early as the time of the Assyrian Empire with the emergence of cultural depositories, what is today known as libraries and archives. [ 22 ] Institutionally, information science emerged in the 19th century along with many other social science disciplines. As a science, however, it finds its institutional roots in the history of science , beginning with publication of the first issues of Philosophical Transactions , generally considered the first scientific journal, in 1665 by the Royal Society. The institutionalization of science occurred throughout the 18th century. In 1731, Benjamin Franklin established the Library Company of Philadelphia , the first library owned by a group of public citizens, which quickly expanded beyond the realm of books and became a center of scientific experimentation , and which hosted public exhibitions of scientific experiments. [ 23 ] Benjamin Franklin invested a town in Massachusetts with a collection of books that the town voted to make available to all free of charge, forming the first public library of the United States . [ 24 ] Academie de Chirurgia ( Paris ) published Memoires pour les Chirurgiens , generally considered to be the first medical journal , in 1736. The American Philosophical Society , patterned on the Royal Society ( London ), was founded in Philadelphia in 1743. As numerous other scientific journals and societies were founded, Alois Senefelder developed the concept of lithography for use in mass printing work in Germany in 1796. By the 19th century the first signs of information science emerged as separate and distinct from other sciences and social sciences but in conjunction with communication and computation. In 1801, Joseph Marie Jacquard invented a punched card system to control operations of the cloth weaving loom in France. It was the first use of \"memory storage of patterns\" system. [ 25 ] As chemistry journals emerged throughout the 1820s and 1830s, [ 26 ] Charles Babbage developed his \"difference engine\", the first step towards the modern computer, in 1822 and his \"analytical engine\" by 1834. By 1843 Richard Hoe developed the rotary press, and in 1844 Samuel Morse sent the first public telegraph message. By 1848 William F. Poole begins the Index to Periodical Literature, the first general periodical literature index in the US. In 1854 George Boole published An Investigation into Laws of Thought..., which lays the foundations for Boolean algebra , which is later used in information retrieval . [ 27 ] In 1860 a congress was held at Karlsruhe Technische Hochschule to discuss the feasibility of establishing a systematic and rational nomenclature for chemistry. The congress did not reach any conclusive results, but several key participants returned home with Stanislao Cannizzaro 's outline (1858), which ultimately convinces them of the validity of his scheme for calculating atomic weights. [ 28 ] By 1865, the Smithsonian Institution began a catalog of current scientific papers, which became the International Catalogue of Scientific Papers in 1902. [ 29 ] The following year the Royal Society began publication of its Catalogue of Papers in London. In 1868, Christopher Sholes, Carlos Glidden, and S. W. Soule produced the first practical typewriter . By 1872 Lord Kelvin devised an analogue computer to predict the tides, and by 1875 Frank Stephen Baldwin was granted the first US patent for a practical calculating machine that performs four arithmetic functions. [ 26 ] Alexander Graham Bell and Thomas Edison invented the telephone and phonograph in 1876 and 1877 respectively, and the American Library Association was founded in Philadelphia. In 1879 Index Medicus was first issued by the Library of the Surgeon General, U.S. Army, with John Shaw Billings as librarian, and later the library issues Index Catalogue, which achieved an international reputation as the most complete catalog of medical literature. [ 30 ] The discipline of documentation science , which marks the earliest theoretical foundations of modern information science, emerged in the late part of the 19th century in Europe together with several more scientific indexes whose purpose was to organize scholarly literature. Many information science historians cite Paul Otlet and Henri La Fontaine as the fathers of information science with the founding of the International Institute of Bibliography (IIB) in 1895. [ 31 ] A second generation of European Documentalists emerged after the Second World War , most notably Suzanne Briet . [ 32 ] However, \"information science\" as a term is not popularly used in academia until sometime in the latter part of the 20th century. [ 33 ] Documentalists emphasized the utilitarian integration of technology and technique toward specific social goals. According to Ronald Day, \"As an organized system of techniques and technologies, documentation was understood as a player in the historical development of global organization in modernity – indeed, a major player inasmuch as that organization was dependent on the organization and transmission of information.\" [ 33 ] Otlet and Lafontaine (who won the Nobel Prize in 1913) not only envisioned later technical innovations but also projected a global vision for information and information technologies that speaks directly to postwar visions of a global \"information society\". Otlet and Lafontaine established numerous organizations dedicated to standardization, bibliography, international associations, and consequently, international cooperation. These organizations were fundamental for ensuring international production in commerce, information, communication and modern economic development, and they later found their global form in such institutions as the League of Nations and the United Nations . Otlet designed the Universal Decimal Classification , based on Melville Dewey 's decimal classification system. [ 33 ] Although he lived decades before computers and networks emerged, what he discussed prefigured what ultimately became the World Wide Web . His vision of a great network of knowledge focused on documents and included the notions of hyperlinks , search engines , remote access, and social networks . Otlet not only imagined that all the world's knowledge should be interlinked and made available remotely to anyone, but he also proceeded to build a structured document collection. This collection involved standardized paper sheets and cards filed in custom-designed cabinets according to a hierarchical index (which culled information worldwide from diverse sources) and a commercial information retrieval service (which answered written requests by copying relevant information from index cards). Users of this service were even warned if their query was likely to produce more than 50 results per search. [ 33 ] By 1937 documentation had formally been institutionalized, as evidenced by the founding of the American Documentation Institute (ADI), later called the American Society for Information Science and Technology . With the 1950s came increasing awareness of the potential of automatic devices for literature searching and information storage and retrieval. As these concepts grew in magnitude and potential, so did the variety of information science interests. By the 1960s and 70s, there was a move from batch processing to online modes, from mainframe to mini and microcomputers. Additionally, traditional boundaries among disciplines began to fade and many information science scholars joined with other programs. They further made themselves multidisciplinary by incorporating disciplines in the sciences, humanities and social sciences, as well as other professional programs, such as law and medicine in their curriculum. Among the individuals who had distinct opportunities to facilitate interdisciplinary activity targeted at scientific communication was Foster E. Mohrhardt , director of the National Agricultural Library from 1954 to 1968. [ 34 ] By the 1980s, large databases, such as Grateful Med at the National Library of Medicine , and user-oriented services such as Dialog and Compuserve , were for the first time accessible by individuals from their personal computers. The 1980s also saw the emergence of numerous special interest groups to respond to the changes. By the end of the decade, special interest groups were available involving non-print media, social sciences, energy and the environment, and community information systems. Today, information science largely examines technical bases, social consequences, and theoretical understanding of online databases, widespread use of databases in government, industry, and education, and the development of the Internet and World Wide Web. [ 35 ] Dissemination has historically been interpreted as unilateral communication of information. With the advent of the internet , and the explosion in popularity of online communities , social media has changed the information landscape in many respects, and creates both new modes of communication and new types of information\", [ 36 ] changing the interpretation of the definition of dissemination. The nature of social networks allows for faster diffusion of information than through organizational sources. [ 37 ] The internet has changed the way we view, use, create, and store information; now it is time to re-evaluate the way we share and spread it. Social media networks provide an open information environment for the mass of people who have limited time or access to traditional outlets of information diffusion, [ 37 ] this is an \"increasingly mobile and social world [that] demands...new types of information skills\". [ 36 ] Social media integration as an access point is a very useful and mutually beneficial tool for users and providers. All major news providers have visibility and an access point through networks such as Facebook and Twitter maximizing their breadth of audience. Through social media people are directed to, or provided with, information by people they know. The ability to \"share, like, and comment on...content\" [ 38 ] increases the reach farther and wider than traditional methods. People like to interact with information, they enjoy including the people they know in their circle of knowledge. Sharing through social media has become so influential that publishers must \"play nice\" if they desire to succeed. Although, it is often mutually beneficial for publishers and Facebook to \"share, promote and uncover new content\" [ 38 ] to improve both user base experiences. The impact of popular opinion can spread in unimaginable ways. Social media allows interaction through simple to learn and access tools; The Wall Street Journal offers an app through Facebook, and The Washington Post goes a step further and offers an independent social app that was downloaded by 19.5 million users in six months, [ 38 ] proving how interested people are in the new way of being provided information. The connections and networks sustained through social media help information providers learn what is important to people. The connections people have throughout the world enable the exchange of information at an unprecedented rate. It is for this reason that these networks have been realized for the potential they provide. \"Most news media monitor Twitter for breaking news\", [ 37 ] as well as news anchors frequently request the audience to tweet pictures of events. [ 38 ] The users and viewers of the shared information have earned \"opinion-making and agenda-setting power\". [ 37 ] This channel has been recognized for the usefulness of providing targeted information based on public demand. The following areas are some of those that information science investigates and develops. Information access is an area of research at the intersection of Informatics , Information Science, Information Security , Language Technology , and Computer Science . The objectives of information access research are to automate the processing of large and unwieldy amounts of information and to simplify users' access to it. What about assigning privileges and restricting access to unauthorized users? The extent of access should be defined in the level of clearance granted for the information. Applicable technologies include information retrieval , text mining , text editing , machine translation , and text categorisation . In discussion, information access is often defined as concerning the insurance of free and closed or public access to information and is brought up in discussions on copyright , patent law , and public domain . Public libraries need resources to provide knowledge of information assurance. Information architecture (IA) is the art and science of organizing and labelling websites , intranets , online communities and software to support usability. [ 39 ] It is an emerging discipline and community of practice focused on bringing together principles of design and architecture to the digital landscape . [ 40 ] Typically it involves a model or concept of information which is used and applied to activities that require explicit details of complex information systems . These activities include library systems and database development. Information management (IM) is the collection and management of information from one or more sources and the distribution of that information to one or more audiences. This sometimes involves those who have a stake in, or a right to that information. Management means the organization of and control over the structure, processing and delivery of information. Throughout the 1970s this was largely limited to files, file maintenance, and the life cycle management of paper-based files, other media and records. With the proliferation of information technology starting in the 1970s, the job of information management took on a new light and also began to include the field of data maintenance. Information retrieval (IR) is the area of study concerned with searching for documents, for information within documents, and for metadata about documents, as well as that of searching structured storage , relational databases , and the World Wide Web . Automated information retrieval systems are used to reduce what has been called \" information overload \". Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications . An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs , for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy . An object is an entity that is represented by information in a database . User queries are matched against the database information. Depending on the application the data objects may be, for example, text documents, images, [ 41 ] audio, [ 42 ] mind maps [ 43 ] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata. Most IR systems compute a numeric score on how well each object in the database match the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. [ 44 ] Information seeking is the process or activity of attempting to obtain information in both human and technological contexts. Information seeking is related to, but different from, information retrieval (IR). Much library and information science (LIS) research has focused on the information-seeking practices of practitioners within various fields of professional work. Studies have been carried out into the information-seeking behaviors of librarians, [ 45 ] academics, [ 46 ] medical professionals, [ 47 ] engineers [ 48 ] and lawyers [ 49 ] (among others). Much of this research has drawn on the work done by Leckie, Pettigrew (now Fisher) and Sylvain, who in 1996 conducted an extensive review of the LIS literature (as well as the literature of other academic fields) on professionals' information seeking. The authors proposed an analytic model of professionals' information seeking behaviour, intended to be generalizable across the professions, thus providing a platform for future research in the area. The model was intended to \"prompt new insights... and give rise to more refined and applicable theories of information seeking\" ( Leckie, Pettigrew & Sylvain 1996 , p. 188). The model has been adapted by Wilkinson (2001) who proposes a model of the information seeking of lawyers. Recent studies in this topic address the concept of information-gathering that \"provides a broader perspective that adheres better to professionals' work-related reality and desired skills.\" [ 50 ] ( Solomon & Bronstein 2021 ). An information society is a society where the creation, distribution, diffusion, uses, integration and manipulation of information is a significant economic, political, and cultural activity. The aim of an information society is to gain competitive advantage internationally, through using IT in a creative and productive way. The knowledge economy is its economic counterpart, whereby wealth is created through the economic exploitation of understanding. People who have the means to partake in this form of society are sometimes called digital citizens . Basically, an information society is the means of getting information from one place to another ( Wark 1997 , p. 22). As technology has become more advanced over time so too has the way we have adapted in sharing this information with each other. Information society theory discusses the role of information and information technology in society, the question of which key concepts should be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology. Knowledge representation (KR) is an area of artificial intelligence research aimed at representing knowledge in symbols to facilitate inferencing from those knowledge elements, creating new elements of knowledge. The KR can be made to be independent of the underlying knowledge model or knowledge base system (KBS) such as a semantic network . [ 51 ] Knowledge Representation (KR) research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain. A symbol vocabulary and a system of logic are combined to enable inferences about elements in the KR to create new KR sentences. Logic is used to supply formal semantics of how reasoning functions should be applied to the symbols in the KR system. Logic is also used to define how operators can process and reshape the knowledge. Examples of operators and operations include, negation, conjunction, adverbs, adjectives, quantifiers and modal operators. The logic is interpretation theory. These elements—symbols, operators, and interpretation theory—are what give sequences of symbols meaning within a KR.",
    "links": [
      "Information access",
      "The Wall Street Journal",
      "Categorization",
      "Artificial intelligence",
      "Information engineering",
      "Lawrence J. Rosenblum",
      "List of I-Schools",
      "Howard G. Funkhouser",
      "Demography",
      "Special library",
      "Visual analytics",
      "David McCandless",
      "Iterative design",
      "Cataloging (library science)",
      "Foster E. Mohrhardt",
      "Website",
      "Private law",
      "History of the social sciences",
      "Hdl (identifier)",
      "Otto Neurath",
      "Bruce H. McCormick",
      "Legal informatics",
      "Information policy",
      "Crime mapping",
      "Enterprise bookmarking",
      "Organization studies",
      "Information technology",
      "Moritz Stefaner",
      "Data management",
      "Oliver Byrne (mathematician)",
      "Software engineering",
      "George Furnas",
      "Ejnar Hertzsprung",
      "Communication studies",
      "Richard Hoe",
      "Dissemination",
      "Scientific modelling",
      "Graph drawing",
      "Theodicy",
      "Knowledge representation and reasoning",
      "Hanspeter Pfister",
      "Academic library",
      "Fritz Kahn",
      "Digital humanities",
      "Database",
      "Public library",
      "Law library",
      "Manuel Lima",
      "Tom Gruber",
      "Knowledge",
      "Social work",
      "Claudio Silva (computer scientist)",
      "Spatial analysis",
      "Library",
      "Computational linguistics",
      "Behavioural sciences",
      "Management",
      "Henry Gantt",
      "Miriah Meyer",
      "Annual Review of Information Science and Technology",
      "Bioinformatics",
      "Social history",
      "Business",
      "London",
      "Tamara Munzner",
      "Online communities",
      "Physical anthropology",
      "Nobel Prize",
      "Schematic",
      "Wikipedia",
      "Knowledge economy",
      "Area studies",
      "Medicine",
      "Metaphysics",
      "Information history",
      "Social network",
      "Glossary of library and information science",
      "Compuserve",
      "Quantum social science",
      "Information overload",
      "Public domain",
      "United Nations",
      "Philosophy of social science",
      "History",
      "John Snow",
      "David Goodsell",
      "Preservation (library and archive)",
      "American Library Association",
      "Science studies",
      "Charles-René de Fourcroy",
      "Philosophy of information",
      "Jacques Bertin",
      "Library and information science",
      "Institutionalization",
      "Cynthia Brewer",
      "Information ethics",
      "League of Nations",
      "Metadata",
      "John Tukey",
      "Knowledge management",
      "Social visualization",
      "Martin M. Wattenberg",
      "Rural sociology",
      "Lithography",
      "Public health",
      "Vannevar Bush",
      "Cartography",
      "Wayback Machine",
      "Philosophy and economics",
      "Digital citizens",
      "Analytics",
      "Germany",
      "Intranet",
      "Francis Amasa Walker",
      "ISSN (identifier)",
      "Brian Campbell Vickery",
      "Human–computer interaction",
      "ISBN (identifier)",
      "Healthcare",
      "Universal Decimal Classification",
      "Library science",
      "Virtual unfolding",
      "Classification",
      "Enterprise architecture framework",
      "Cultural anthropology",
      "Howard Wainer",
      "Statistics",
      "Kwan-Liu Ma",
      "Intelligence analysis",
      "Information",
      "Dialog (online database)",
      "Political ecology",
      "Information system",
      "Plot (graphics)",
      "Jessica Hullman",
      "Edmond Halley",
      "Media studies",
      "Physical geography",
      "Smithsonian Institution",
      "Patent law",
      "Statistical graphics",
      "Nigel Holmes",
      "Psychology",
      "Jeffrey Heer",
      "George Boole",
      "S2CID (identifier)",
      "Human science",
      "Max O. Lorenz",
      "Misleading graph",
      "Environmental studies",
      "Vocabulary",
      "World history (field)",
      "Global studies",
      "Data visualization",
      "Borden Dent",
      "Mary Eleanor Spear",
      "Software visualization",
      "Science and technology studies",
      "Semantics",
      "Medical imaging",
      "Sholes and Glidden typewriter",
      "Special interest groups",
      "Information Security",
      "Efficiency",
      "Semantic Web",
      "Second World War",
      "Social science",
      "Preservation (library and archival science)",
      "Outline of information technology",
      "Technology",
      "Mathematics",
      "Educational technology",
      "Alexander Ivanovich Mikhailov",
      "Critical animal studies",
      "Private library",
      "Business studies",
      "School library",
      "Arthur H. Robinson",
      "Edgar Anderson",
      "Henri La Fontaine",
      "Philosophical Transactions",
      "Censorship",
      "Relational database",
      "Outline of information science",
      "Index of sociology articles",
      "Political sociology",
      "Ben Fry",
      "Geography",
      "Interdisciplinary",
      "André-Michel Guerry",
      "Visualization (graphics)",
      "Gottfried Wilhelm Leibniz",
      "Inference",
      "Adolphe Quetelet",
      "International relations",
      "Charles Booth (social reformer)",
      "Knowledge representation",
      "Information theory",
      "Public law",
      "Problem solving",
      "Charles Joseph Minard",
      "Information architecture",
      "Historical sociology",
      "Clifford A. Pickover",
      "Imaging science",
      "Thomas Edison",
      "Florence Nightingale",
      "Molecular graphics",
      "Environmental informatics",
      "Information behavior",
      "Biomedical informatics",
      "Stuart Card",
      "Vegan studies",
      "Journal of the American Society for Information Science",
      "Documentation science",
      "John Venn",
      "Humanities",
      "Technical illustration",
      "Military history",
      "W. E. B. Du Bois",
      "Data",
      "Francis Galton",
      "Quantum information science",
      "Ade Olufeko",
      "Graphic organizer",
      "Facebook",
      "Linguistics",
      "Social anthropology",
      "Information art",
      "Cultural history",
      "Joseph Priestley",
      "Network science",
      "Catherine Plaisant",
      "Education",
      "Abnormal psychology",
      "Charles Dupin",
      "Legal history",
      "Gender studies",
      "National Library of Medicine",
      "Query string",
      "Information visualization",
      "Bang Wong",
      "Geovisualization",
      "Language Technology",
      "Development studies",
      "Polymath",
      "Henry Norris Russell",
      "Daniel A. Keim",
      "Domain of discourse",
      "Computer graphics (computer science)",
      "Biological data visualization",
      "John Shaw Billings",
      "Sociology",
      "Paul Otlet",
      "Medical journal",
      "Information professional",
      "Jock D. Mackinlay",
      "Political economy",
      "Economic history",
      "Computational social science",
      "Color coding in data visualization",
      "Archaeology",
      "Benjamin Franklin",
      "Samuel Morse",
      "Concept",
      "Information retrieval",
      "Knowledge organization",
      "Rudolf Modley",
      "Machine translation",
      "Macroeconomics",
      "Geographic information science",
      "Business administration",
      "Society",
      "Criminology",
      "Social media analytics",
      "Discrete mathematics",
      "Reason",
      "Privacy",
      "Data modeling",
      "Inferencing",
      "Computer Science",
      "Cognitive psychology",
      "American Society for Information Science and Technology",
      "History of science",
      "Information society",
      "Taxonomy",
      "Sheelagh Carpendale",
      "Project Muse",
      "Information need",
      "Auxiliary sciences of history",
      "Mathematical diagram",
      "Analysis",
      "Law",
      "Anthrozoology",
      "Chart",
      "Food studies",
      "George G. Robertson",
      "Stanislao Cannizzaro",
      "IT",
      "Developmental psychology",
      "Alexander Graham Bell",
      "PMID (identifier)",
      "Matthew Henry Phineas Riall Sankey",
      "Urban sociology",
      "Communication",
      "Archives management",
      "Text editing",
      "Data science",
      "Political history",
      "Skeletal formula",
      "John Maeda",
      "Alois Senefelder",
      "World Wide Web",
      "Economics",
      "Photograph",
      "Neuroimaging",
      "Suzanne Briet",
      "Boolean algebra",
      "Ideogram",
      "Hadley Wickham",
      "Semiotics",
      "Urban planning",
      "Integrated geography",
      "Geisteswissenschaft",
      "Edward Walter Maunder",
      "Paris",
      "Medical Informatics",
      "Infographic",
      "Epistemology",
      "Charles Babbage",
      "Internet",
      "Computer science",
      "Philosophy of history",
      "Patent drawing",
      "Regional planning",
      "Cybersecurity",
      "Mathematical economics",
      "Chemical imaging",
      "Visual culture",
      "Pictogram",
      "United States",
      "Intellectual property",
      "Text mining",
      "Search engine (computing)",
      "Semantic network",
      "Health informatics",
      "Anthropology",
      "Leland Wilkinson",
      "User interface design",
      "Technological determinism",
      "Information Sciences (journal)",
      "Medical library",
      "Cognitive science",
      "List of social science journals",
      "Jurisprudence",
      "Bibliometrics",
      "Assyrian Empire",
      "Relevancy",
      "Michael Maltz",
      "Social media",
      "Human-computer interaction",
      "Human ecology",
      "Hans Rosling",
      "History of libraries",
      "IT law",
      "Philosophy of science",
      "Doi (identifier)",
      "Mauro Martino",
      "Grateful Med",
      "Technical drawing",
      "Organization",
      "Erwin Raisz",
      "National Agricultural Library",
      "Computer and information science",
      "Arthur Lyon Bowley",
      "Collections management",
      "Technical geography",
      "Aaron Koblin",
      "Information seeking",
      "Copyright",
      "Groupware",
      "Environmental social science",
      "Visual perception",
      "Information retrieval applications",
      "Graph of a function",
      "Memory",
      "American Philosophical Society",
      "Document",
      "Business analytics",
      "Map",
      "Fernanda Viégas",
      "Political science",
      "Massachusetts",
      "Cultural artifact",
      "Scientific experimentation",
      "Regional science",
      "CPK coloring",
      "History of technology",
      "Information systems",
      "Architecture",
      "Social informatics",
      "Systems analyst",
      "Harry Beck",
      "Community studies",
      "Computer graphics",
      "Alfred Inselberg",
      "Frank Stephen Baldwin",
      "Information technologies",
      "Archive",
      "Melville Dewey",
      "Diagram",
      "Information scientist",
      "Sociology of the Internet",
      "Social psychology",
      "Twitter",
      "Gaspard Monge",
      "Knowledge Acquisition",
      "Philosophy",
      "Outline of social science",
      "Human geography",
      "Public policy",
      "Philosophy of psychology",
      "Michael Friendly",
      "Computer data storage",
      "Library management",
      "Library classification",
      "Design",
      "Social epistemology",
      "Thomas A. DeFanti",
      "Comparative politics",
      "Mind maps",
      "Political philosophy",
      "Decision support system",
      "Sankey diagram",
      "Public libraries",
      "Public administration",
      "Alan MacEachren",
      "Outline of library and information science",
      "Pat Hanrahan",
      "Structured storage",
      "Toussaint Loua",
      "Archival science",
      "Productivity",
      "Systems engineering",
      "Hyperlink",
      "Personality psychology",
      "Microeconomics",
      "Volume rendering",
      "Value sensitive design",
      "Royal Society",
      "Shared decision-making",
      "William S. Cleveland",
      "Table (information)",
      "Text categorisation",
      "Cultural studies",
      "Edward Tufte",
      "Ben Shneiderman",
      "Flow visualization",
      "Informatics",
      "Econometrics",
      "Learning analytics",
      "List of national legal systems",
      "Information economics",
      "The Washington Post",
      "Web search engine",
      "Scientific visualization",
      "William Playfair",
      "Intellectual freedom",
      "Howard T. Fisher",
      "Land-use planning",
      "Engineering drawing",
      "John B. Peddle",
      "Library Company of Philadelphia",
      "Graphic design",
      "Karl Wilhelm Pohlke",
      "Ontology (information science)",
      "Chartjunk",
      "August Kekulé",
      "Christopher R. Johnson",
      "Information management",
      "Gordon Kindlmann",
      "Joseph Marie Jacquard"
    ]
  },
  "Ill-posed": {
    "url": "https://en.wikipedia.org/wiki/Ill-posed",
    "title": "Ill-posed",
    "content": "In mathematics , a well-posed problem is one for which the following properties hold: [ a ] Examples of archetypal well-posed problems include the Dirichlet problem for Laplace's equation , and the heat equation with specified initial conditions. These might be regarded as 'natural' problems in that there are physical processes modelled by these problems. Problems that are not well-posed in the sense above are termed ill-posed . A simple example is a global optimization problem, because the location of the optima is generally not a continuous function of the parameters specifying the objective, even when the objective itself is a smooth function of those parameters. Inverse problems are often ill-posed; for example, the inverse heat equation, deducing a previous distribution of temperature from final data, is not well-posed in that the solution is highly sensitive to changes in the final data. Continuum models must often be discretized in order to obtain a numerical solution. While solutions may be continuous with respect to the initial conditions, they may suffer from numerical instability when solved with finite precision , or with errors in the data. Even if a problem is well-posed, it may still be ill-conditioned , meaning that a small error in the initial data can result in much larger errors in the answers. Problems in nonlinear complex systems (so-called chaotic systems) provide well-known examples of instability. An ill-conditioned problem is indicated by a large condition number . If the problem is well-posed, then it stands a good chance of solution on a computer using a stable algorithm . If it is not well-posed, it needs to be re-formulated for numerical treatment. Typically this involves including additional assumptions, such as smoothness of solution. This process is known as regularization . [ 1 ] Tikhonov regularization is one of the most commonly used for regularization of linear ill-posed problems. The existence of local solutions is often an important part of the well-posedness problem, and it is the foundation of many estimate methods, for example the energy method below. There are many results on this topic. For example, the Cauchy–Kowalevski theorem for Cauchy initial value problems essentially states that if the terms in a partial differential equation are all made up of analytic functions and a certain transversality condition is satisfied (the hyperplane or more generally hypersurface where the initial data are posed must be non- characteristic with respect to the partial differential operator), then on certain regions, there necessarily exist solutions which are as well analytic functions. This is a fundamental result in the study of analytic partial differential equations. Surprisingly, the theorem does not hold in the setting of smooth functions; an example discovered by Hans Lewy in 1957 consists of a linear partial differential equation whose coefficients are smooth (i.e., have derivatives of all orders) but not analytic for which no solution exists. So the Cauchy–Kowalevski theorem is necessarily limited in its scope to analytic functions. The energy method is useful for establishing both uniqueness and continuity with respect to initial conditions (i.e. it does not establish existence). The method is based upon deriving an upper bound of an energy-like functional for a given problem. Example : Consider the diffusion equation on the unit interval with homogeneous Dirichlet boundary conditions and suitable initial data f ( x ) {\\displaystyle f(x)} (e.g. for which f ( 0 ) = f ( 1 ) = 0 {\\displaystyle f(0)=f(1)=0} ). u t = D u x x , 0 < x < 1 , t > 0 , D > 0 , u ( x , 0 ) = f ( x ) , u ( 0 , t ) = 0 , u ( 1 , t ) = 0 , {\\displaystyle {\\begin{aligned}u_{t}&=Du_{xx},&&0<x<1,\\,t>0,\\,D>0,\\\\u(x,0)&=f(x),\\\\u(0,t)&=0,\\\\u(1,t)&=0,\\\\\\end{aligned}}} Multiply the equation u t = D u x x {\\displaystyle u_{t}=Du_{xx}} by u {\\displaystyle u} and integrate in space over the unit interval to obtain ∫ 0 1 u u t d x = D ∫ 0 1 u u x x d x ⟹ ∫ 0 1 1 2 ∂ t u 2 d x = D u u x | 0 1 − D ∫ 0 1 ( u x ) 2 d x ⟹ 1 2 ∂ t ‖ u ‖ 2 2 = 0 − D ∫ 0 1 ( u x ) 2 d x ≤ 0 {\\displaystyle {\\begin{aligned}&&\\int _{0}^{1}uu_{t}dx&=D\\int _{0}^{1}uu_{xx}dx\\\\\\Longrightarrow &&\\int _{0}^{1}{\\frac {1}{2}}\\partial _{t}u^{2}dx&=Duu_{x}{\\Big |}_{0}^{1}-D\\int _{0}^{1}(u_{x})^{2}dx\\\\\\Longrightarrow &&{\\frac {1}{2}}\\partial _{t}\\|u\\|_{2}^{2}&=0-D\\int _{0}^{1}(u_{x})^{2}dx\\leq 0\\end{aligned}}} This tells us that ‖ u ‖ 2 {\\displaystyle \\|u\\|_{2}} ( p-norm ) cannot grow in time. By multiplying by two and integrating in time, from 0 {\\displaystyle 0} up to t {\\displaystyle t} , one finds ‖ u ( ⋅ , t ) ‖ 2 2 ≤ ‖ f ( ⋅ ) ‖ 2 2 {\\displaystyle \\|u(\\cdot ,t)\\|_{2}^{2}\\leq \\|f(\\cdot )\\|_{2}^{2}} This result is the energy estimate for this problem. To show uniqueness of solutions, assume there are two distinct solutions to the problem, call them u {\\displaystyle u} and v {\\displaystyle v} , each satisfying the same initial data. Upon defining w = u − v {\\displaystyle w=u-v} then, via the linearity of the equations, one finds that w {\\displaystyle w} satisfies w t = D w x x , 0 < x < 1 , t > 0 , D > 0 , w ( x , 0 ) = 0 , w ( 0 , t ) = 0 , w ( 1 , t ) = 0 , {\\displaystyle {\\begin{aligned}w_{t}&=Dw_{xx},&&0<x<1,\\,t>0,\\,D>0,\\\\w(x,0)&=0,\\\\w(0,t)&=0,\\\\w(1,t)&=0,\\\\\\end{aligned}}} Applying the energy estimate tells us ‖ w ( ⋅ , t ) ‖ 2 2 ≤ 0 {\\displaystyle \\|w(\\cdot ,t)\\|_{2}^{2}\\leq 0} which implies u = v {\\displaystyle u=v} ( almost everywhere ). Similarly, to show continuity with respect to initial conditions, assume that u {\\displaystyle u} and v {\\displaystyle v} are solutions corresponding to different initial data u ( x , 0 ) = f ( x ) {\\displaystyle u(x,0)=f(x)} and v ( x , 0 ) = g ( x ) {\\displaystyle v(x,0)=g(x)} . Considering w = u − v {\\displaystyle w=u-v} once more, one finds that w {\\displaystyle w} satisfies the same equations as above but with w ( x , 0 ) = f ( x ) − g ( x ) {\\displaystyle w(x,0)=f(x)-g(x)} . This leads to the energy estimate ‖ w ( ⋅ , t ) ‖ 2 2 ≤ D ‖ f ( ⋅ ) − g ( ⋅ ) ‖ 2 2 {\\displaystyle \\|w(\\cdot ,t)\\|_{2}^{2}\\leq D\\|f(\\cdot )-g(\\cdot )\\|_{2}^{2}} which establishes continuity (i.e. as f {\\displaystyle f} and g {\\displaystyle g} become closer, as measured by the L 2 {\\displaystyle L^{2}} norm of their difference, then ‖ w ( ⋅ , t ) ‖ 2 → 0 {\\displaystyle \\|w(\\cdot ,t)\\|_{2}\\to 0} ). The maximum principle is an alternative approach to establish uniqueness and continuity of solutions with respect to initial conditions for this example. The existence of solutions to this problem can be established using Fourier series . If it is possible to denote the solution to a Cauchy problem ∂ u ∂ t = A u , u ( 0 ) = u 0 (1) {\\displaystyle {\\frac {\\partial u}{\\partial t}}=Au,u(0)=u_{0}{\\text{ (1)}}} , where A is a linear operator mapping a dense linear subspace D(A) of X into X (see discontinuous linear map ), with u ( t ) = S ( t ) u 0 {\\displaystyle u(t)=S(t)u_{0}} , where { S ( t ) ; t ≥ 0 } {\\displaystyle \\{S(t);t\\geq 0\\}} is a family of linear operators on X , satisfying then (1) is well-posed. Hille–Yosida theorem states the criteria on A for such a { S ( t ) ; t ≥ 0 } {\\displaystyle \\{S(t);t\\geq 0\\}} to exist.",
    "links": [
      "Mathematics",
      "Numerical stability",
      "ArXiv (identifier)",
      "Physical phenomena",
      "C0-semigroup",
      "Numerical instability",
      "Doi (identifier)",
      "Hans Lewy",
      "Bibcode (identifier)",
      "Archetypal",
      "Heat equation",
      "Discretization",
      "Total absorption spectroscopy",
      "Analytic function",
      "Maximum principle",
      "Accuracy and precision",
      "Partial differential equation",
      "P-norm",
      "Jacques Hadamard",
      "Lewy's example",
      "Initial condition",
      "Uniqueness quantification",
      "Regularization (mathematics)",
      "Error",
      "Mathematical model",
      "Tikhonov regularization",
      "ISBN (identifier)",
      "Discontinuous linear map",
      "Dense subspace",
      "Expectation–maximization algorithm",
      "PMID (identifier)",
      "Method of characteristics",
      "Continuous function",
      "Hyperplane",
      "PMC (identifier)",
      "Global optimization",
      "Inverse problem",
      "Condition number",
      "Lawrence C. Evans",
      "Chaos theory",
      "Cauchy–Kowalevski theorem",
      "Hille–Yosida theorem",
      "Almost everywhere",
      "Hypersurface",
      "Ill-conditioned",
      "Dirichlet boundary conditions",
      "Complex systems"
    ]
  },
  "Computational linguistics": {
    "url": "https://en.wikipedia.org/wiki/Computational_linguistics",
    "title": "Computational linguistics",
    "content": "Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language , as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics , computer science , artificial intelligence , mathematics , logic , philosophy , cognitive science , cognitive psychology , psycholinguistics , anthropology and neuroscience , among others. Computational linguistics is closely related to mathematical linguistics . The field overlapped with artificial intelligence since the efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English. [ 1 ] Since rule-based approaches were able to make arithmetic (systematic) calculations much faster and more accurately than humans, it was expected that lexicon , morphology , syntax and semantics can be learned using explicit rules, as well. After the failure of rule-based approaches , David Hays [ 2 ] coined the term in order to distinguish the field from AI and co-founded both the Association for Computational Linguistics (ACL) and the International Committee on Computational Linguistics (ICCL) in the 1970s and 1980s. What started as an effort to translate between languages evolved into a much wider field of natural language processing . [ 3 ] [ 4 ] In order to be able to meticulously study the English language , an annotated text corpus was much needed. The Penn Treebank [ 5 ] was one of the most used corpora. It consisted of IBM computer manuals, transcribed telephone conversations, and other texts, together containing over 4.5 million words of American English, annotated using both part-of-speech tagging and syntactic bracketing. [ 6 ] Japanese sentence corpora were analyzed and a pattern of log-normality was found in relation to sentence length. [ 7 ] The fact that during language acquisition , children are largely only exposed to positive evidence, [ 8 ] meaning that the only evidence for what is a correct form is provided, and no evidence for what is not correct, [ 9 ] was a limitation for the models at the time because the now available deep learning models were not available in late 1980s. [ 10 ] It has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span, [ 11 ] which explained the long period of language acquisition in human infants and children. [ 11 ] Robots have been used to test linguistic theories. [ 12 ] Enabled to learn as children might, models were created based on an affordance model in which mappings between actions, perceptions, and effects were created and linked to spoken words. Crucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure. Using the Price equation and Pólya urn dynamics, researchers have created a system which not only predicts future linguistic evolution but also gives insight into the evolutionary history of modern-day languages. [ 13 ] Noam Chomsky 's theories have influenced computational linguistics, particularly in understanding how infants learn complex grammatical structures, such as those described in Chomsky normal form . [ 14 ] Attempts have been made to determine how an infant learns a \"non-normal grammar\" as theorized by Chomsky normal form. [ 9 ] Research in this area combines structural approaches with computational models to analyze large linguistic corpora like the Penn Treebank , helping to uncover patterns in language acquisition. [ 15 ]",
    "links": [
      "Compiler construction",
      "Sociolinguistics",
      "Interdisciplinarity",
      "Very-large-scale integration",
      "Human speechome project",
      "Human-centered computing",
      "Video game",
      "Artificial intelligence",
      "Solid modeling",
      "Doi (identifier)",
      "Pólya urn",
      "Second-language acquisition",
      "Terminology",
      "Arithmetic",
      "Virtual reality",
      "Networking hardware",
      "Computational social science",
      "Foma (software)",
      "S2CID (identifier)",
      "Semantometrics",
      "International Committee on Computational Linguistics",
      "Multimedia database",
      "Corpus linguistics",
      "Security hacker",
      "Enterprise software",
      "Rendering (computer graphics)",
      "Programming paradigm",
      "Multiprocessing",
      "Computational physics",
      "Phrase structure grammar",
      "Morphology (linguistics)",
      "Information retrieval",
      "Language acquisition",
      "Programming team",
      "Orthography",
      "Interaction design",
      "Computational geometry",
      "Programming language theory",
      "Industrial process control",
      "Semantics (computer science)",
      "Semantics",
      "Linguistic typology",
      "Syntax",
      "Linguistic distance",
      "Software framework",
      "Software maintenance",
      "Fault tolerance",
      "Discrete mathematics",
      "System on a chip",
      "Digital marketing",
      "Printed circuit board",
      "Cross-validation (statistics)",
      "Computer animation",
      "Document management system",
      "Grammar induction",
      "Word processor",
      "Computability theory",
      "Cryptography",
      "Thermodynamic computing",
      "Concurrent computing",
      "Grammatical Framework",
      "Computer-assisted reviewing",
      "Generative grammar",
      "Mathematical linguistics",
      "Processor (computing)",
      "Cyberwarfare",
      "Image compression",
      "Computational engineering",
      "Integrated development environment",
      "Mathematics",
      "Wireless sensor network",
      "Programming language",
      "Cognitive psychology",
      "Educational technology",
      "Formalism (linguistics)",
      "Software engineering",
      "Green computing",
      "Distributed computing",
      "E-commerce",
      "Logic in computer science",
      "Computational biology",
      "Domain-specific language",
      "Computer vision",
      "Philosophy of language",
      "Stochastic computing",
      "Applied linguistics",
      "Grammaticalization",
      "Price equation",
      "Bibcode (identifier)",
      "List of computer size categories",
      "Association for Computational Linguistics",
      "Software quality",
      "Software repository",
      "Knowledge representation and reasoning",
      "Database",
      "Glossematics",
      "Software development",
      "SpaCy",
      "Software configuration management",
      "Affordance",
      "Interactional linguistics",
      "CiteSeerX (identifier)",
      "Network service",
      "Algorithm design",
      "Data mining",
      "Reinforcement learning",
      "Concurrency (computer science)",
      "Anthropological linguistics",
      "Geographic information system",
      "Computer graphics",
      "Noam Chomsky",
      "Operating system",
      "Computational modelling",
      "Hardware security",
      "NooJ",
      "Automata theory",
      "Origin of language",
      "Interpreter (computing)",
      "Lexicostatistics",
      "Autonomy of syntax",
      "Software design",
      "Augmented reality",
      "Writing system",
      "PMID (identifier)",
      "Peripheral",
      "Semantic relatedness",
      "Unsupervised learning",
      "Form factor (design)",
      "Embedded system",
      "Glottochronology",
      "Lexicography",
      "Psycholinguistics",
      "Philosophy",
      "Electronic design automation",
      "Middleware",
      "Iconicity",
      "Randomized algorithm",
      "Computer hardware",
      "Social computing",
      "Network architecture",
      "Visualization (graphics)",
      "Computer architecture",
      "AI winter",
      "Phonology",
      "Chomsky normal form",
      "Software construction",
      "Control theory",
      "Translation studies",
      "Computational chemistry",
      "Library (computing)",
      "Computer data storage",
      "Electronic publishing",
      "Natural language",
      "Functional discourse grammar",
      "Prague linguistic circle",
      "Natural language user interface",
      "Application security",
      "Translation memory",
      "ArXiv (identifier)",
      "Information theory",
      "Cyber-physical system",
      "Natural language processing",
      "English language",
      "Modeling language",
      "Control flow",
      "Supervised learning",
      "Information security",
      "Machine learning",
      "Part-of-speech",
      "Computational Linguistics (journal)",
      "Numerical analysis",
      "Model of computation",
      "Pragmatics",
      "World Wide Web",
      "Decision support system",
      "Formal methods",
      "Computer security",
      "Etymology",
      "Functional linguistics",
      "Orismology",
      "GloVe",
      "Computational models of language acquisition",
      "Real-time computing",
      "Network scheduler",
      "Systemic functional linguistics",
      "Formal language",
      "Internet linguistics",
      "Interlinguistics",
      "Cognitive grammar",
      "Computational mathematics",
      "Logic",
      "Operations research",
      "Computing",
      "WordNet",
      "Determinism",
      "Dependency grammar",
      "Text linguistics",
      "Enterprise information system",
      "Computational semiotics",
      "Index of linguistics articles",
      "Philology",
      "Ethnography of communication",
      "Programming tool",
      "Automated planning and scheduling",
      "Computer science",
      "Treebank",
      "Dialog systems",
      "Historical linguistics",
      "Photograph manipulation",
      "Hardware acceleration",
      "Log-normality",
      "Language interpretation",
      "Multithreading (computer architecture)",
      "Algorithm",
      "Usage-based models of language",
      "Phonetics",
      "History of linguistics",
      "Virtual machine",
      "David G. Hays",
      "Analysis of algorithms",
      "Linguistic description",
      "ACM Computing Classification System",
      "Parallel computing",
      "Network security",
      "Mathematical analysis",
      "Construction grammar",
      "Artificial intelligence in fiction",
      "Software deployment",
      "Social software",
      "Electronic voting",
      "Mathematical software",
      "Multi-task learning",
      "Linguistics",
      "Distributionalism",
      "Algorithmic efficiency",
      "Health informatics",
      "Anthropology",
      "Neurolinguistics",
      "Open-source software",
      "Deep learning",
      "Communication protocol",
      "Syntax–semantics interface",
      "Probability",
      "Wayback Machine",
      "Computational semantics",
      "Dependability",
      "Principle of compositionality",
      "Collostructional analysis",
      "Computational complexity",
      "Computing platform",
      "Cognitive science",
      "Integrated circuit",
      "Ethnomethodology",
      "Computer network",
      "Digital library",
      "Forensic linguistics",
      "Human–computer interaction",
      "Philosophy of artificial intelligence",
      "Theory of computation",
      "Quantum computing",
      "ISBN (identifier)",
      "Philosophy of linguistics",
      "Mobile computing",
      "Security service (telecommunication)",
      "Quantitative linguistics",
      "Software development process",
      "Requirements analysis",
      "Mathematical optimization",
      "Language documentation",
      "Lexicon",
      "Computational problem",
      "Ubiquitous computing",
      "Digital art",
      "LGBTQ linguistics",
      "Intrusion detection system",
      "Theory of language",
      "Computer accessibility",
      "Outline of computer science",
      "PMC (identifier)",
      "Linguistic prescription",
      "Statistics",
      "Computational lexicology",
      "Neuroscience",
      "Distributed artificial intelligence",
      "Network performance",
      "Conversation analysis",
      "Outline of linguistics",
      "Conservative and innovative language",
      "Universal Networking Language",
      "Graphics processing unit",
      "Information system",
      "Structural linguistics",
      "Theoretical computer science",
      "Computational complexity theory",
      "Discourse analysis"
    ]
  },
  "Controlled vocabulary": {
    "url": "https://en.wikipedia.org/wiki/Controlled_vocabulary",
    "title": "Controlled vocabulary",
    "content": "A controlled vocabulary provides a way to organize knowledge for subsequent retrieval. Controlled vocabularies are used in subject indexing schemes, subject headings , thesauri , [ 1 ] [ 2 ] taxonomies and other knowledge organization systems . Controlled vocabulary schemes mandate the use of predefined, preferred terms that have been preselected by the designers of the schemes, in contrast to natural language vocabularies, which have no such restriction. [ 3 ] In library and information science , controlled vocabulary is a carefully selected list of words and phrases , which are used to tag units of information (document or work) so that they may be more easily retrieved by a search. [ 4 ] [ 5 ] Controlled vocabularies solve the problems of homographs , synonyms and polysemes by a bijection between concepts and preferred terms. In short, controlled vocabularies reduce unwanted ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency. [ 3 ] For example, in the Library of Congress Subject Headings [ 6 ] (a subject heading system that uses a controlled vocabulary), preferred terms—subject headings in this case—have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms ( cockroach versus Periplaneta americana ), and choices between synonyms ( automobile versus car ), among other difficult issues. Choices of preferred terms are based on the principles of user warrant (what terms users are likely to use), literary warrant (what terms are generally used in the literature and documents), and structural warrant (terms chosen by considering the structure, scope of the controlled vocabulary). Controlled vocabularies also typically handle the problem of homographs with qualifiers. For example, the term pool has to be qualified to refer to either swimming pool or the game pool to ensure that each preferred term or heading refers to only one concept. [ 7 ] There are two main kinds of controlled vocabulary tools used in libraries: subject headings [ 8 ] and thesauri . While the differences between the two are diminishing, there are still some minor differences: The terms are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the Library of Congress system , Medical Subject Headings (MeSH) created by the United States National Library of Medicine , and Sears . Well known thesauri include the Art and Architecture Thesaurus and the ERIC Thesaurus. When selecting terms for a controlled vocabulary, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-coordination (in which case the degree of enumeration versus synthesis becomes an issue) and post-coordination in the system is another important issue. Controlled vocabulary elements (terms/phrases) employed as tags , to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as metadata . There are three main types of indexing languages. When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document. In recent years free text search as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is indexed ). These methods have been compared in some studies, such as the 2007 article, \"A Comparative Evaluation of Full-text, Concept-based, and Context-sensitive Search\". [ 9 ] Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce irrelevant items in the retrieval list. These irrelevant items ( false positives ) are often caused by the inherent ambiguity of natural language . Take the English word football for example. Football is the name given to a number of different team sports . Worldwide the most popular of these team sports is association football , which also happens to be called soccer in several countries. The word football is also applied to rugby football ( rugby union and rugby league ), American football , Australian rules football , Gaelic football , and Canadian football . A search for football therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by tagging the documents in such a way that the ambiguities are eliminated. Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually relevant to the search topic). In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct preferred term is searched, there is no need to search for other terms that might be synonyms of that term. A controlled vocabulary search may lead to unsatisfactory recall , in that it will fail to retrieve some documents that are actually relevant to the search question. This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with that of the indexer. Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with \"football\" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless. On the other hand, free text searches have high exhaustivity (every word is searched) so although it has much lower precision, it has potential for high recall as long as the searcher overcome the problem of synonyms by entering every combination. Controlled vocabularies may become outdated rapidly in fast developing fields of knowledge, unless the preferred terms are updated regularly. Even in an ideal scenario, a controlled vocabulary is often less specific than the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while this precise problem is not a factor in a free text, as it uses the author's own words. The use of controlled vocabularies can be costly compared to free text searches because human experts or expensive automated systems are necessary to index each entry. Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision. Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including faceted classification , which enables a given data record or document to be described in multiple ways. Word choice in chosen vocabularies is not neutral, and the indexer must carefully consider the ethics of their word choices. For example, traditionally colonialist terms have often been the preferred terms in chosen vocabularies when discussing First Nations issues, which has caused controversy. [ 10 ] Controlled vocabularies, such as the Library of Congress Subject Headings , are an essential component of bibliography , the study and classification of books. They were initially developed in library and information science . In the 1950s, government agencies began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the Medical Subject Headings (MeSH) developed by the U.S. National Library of Medicine . Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup X.25 networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first full text databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library. In large organizations, controlled vocabularies may be introduced to improve technical communication . The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing. This consistency of terms is one of the most important concepts in technical writing and knowledge management , where effort is expended to use the same word throughout a document or organization instead of slightly different ones to refer to the same thing. Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a Semantic Web , in which the content of Web pages is described using a machine-readable metadata scheme. One of the first proposals for such a scheme is the Dublin Core Initiative. An example of a controlled vocabulary which is usable for indexing web pages is PSH . It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web. [ 11 ] To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The eXchangeable Faceted Metadata Language (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on faceted classification principles. [ 12 ] [ non-primary source needed ] Controlled vocabularies of the Semantic Web define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of \"Person\", such as the Friend of a Friend ( FOAF ) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of Schema.org . [ 13 ] Similarly, a book can be described using the Book vocabulary of Schema.org [ 14 ] and general publication terms from the Dublin Core vocabulary, [ 15 ] an event with the Event vocabulary of Schema.org , [ 16 ] and so on. To use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa, HTML5 Microdata , or JSON-LD in the markup, or RDF serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files.",
    "links": [
      "Full text",
      "Doi (identifier)",
      "Terminology",
      "X.25",
      "Organization",
      "S2CID (identifier)",
      "Library of Congress Subject Heading",
      "Ontology (computer science)",
      "Free text search",
      "Rugby football",
      "Document",
      "Relevance",
      "Bibliography",
      "Semantic Web",
      "Gaelic football",
      "Art and Architecture Thesaurus",
      "Recall (information retrieval)",
      "False positives",
      "Named-entity recognition",
      "Knowledge organization system",
      "IMS VDEX",
      "Word (linguistics)",
      "Thesaurus (information retrieval)",
      "Dublin Core",
      "Defining vocabulary",
      "Soccer",
      "PMID (identifier)",
      "Rugby league",
      "Homographs",
      "Hyponym",
      "Bijection",
      "Library catalog",
      "Natural language",
      "Thesauri",
      "Canadian football",
      "OCLC (identifier)",
      "Relevance (Information Retrieval)",
      "Controlled natural language",
      "Resource Description Framework",
      "Hypernym",
      "United States National Library of Medicine",
      "Football (word)",
      "Library of Congress Subject Headings",
      "Microdata (HTML)",
      "Polythematic Structured Subject Heading System",
      "Subject indexing",
      "Schema.org",
      "Phrase",
      "Synonyms",
      "Tag (metadata)",
      "Polyseme",
      "Library and information science",
      "JSON-LD",
      "Metadata",
      "Knowledge management",
      "Universal Data Element Framework",
      "Authority control",
      "Technical communication",
      "Nomenclature",
      "Wayback Machine",
      "Web indexing",
      "Team sport",
      "ISSN (identifier)",
      "FOAF",
      "Football (soccer)",
      "American football",
      "ISBN (identifier)",
      "Education Resources Information Center",
      "Rugby union",
      "Sears Subject Headings",
      "Australian rules football",
      "Technical writing",
      "Taxonomy (general)",
      "PMC (identifier)",
      "Medical Subject Headings",
      "Faceted classification",
      "Vocabulary-based transformation",
      "Subject heading"
    ]
  },
  "Vertical search": {
    "url": "https://en.wikipedia.org/wiki/Vertical_search",
    "title": "Vertical search",
    "content": "A vertical search engine is distinct from a general web search engine , in that it focuses on a specific segment of online content. They are also called specialty or topical search engines. The vertical content area may be based on topicality, media type, or genre of content. Common verticals include shopping, the automotive industry, legal information, medical information, scholarly literature, job search and travel. Examples of vertical search engines include the Library of Congress , Mocavo , Nuroa , Trulia , and Yelp . In contrast to general web search engines, which attempt to index large portions of the World Wide Web using a web crawler , vertical search engines typically use a focused crawler which attempts to index only relevant web pages to a pre-defined topic or set of topics. Some vertical search sites focus on individual verticals, while other sites include multiple vertical searches within one search engine. Vertical search offers several potential benefits over general search engines: Vertical search can be viewed as similar to enterprise search where the domain of focus is the enterprise, such as a company, government or other organization. In 2013, consumer price comparison websites with integrated vertical search engines such as FindTheBest drew large rounds of venture capital funding, indicating a growth trend for these applications of vertical search technology. [ 1 ] [ 2 ] Domain-specific verticals focus on a specific topic. John Battelle describes this in his book The Search (2005): Domain-specific search solutions focus on one area of knowledge, creating customized search experiences, that because of the domain's limited corpus and clear relationships between concepts, provide extremely relevant results for searchers. [ 3 ] Any general search engine would be indexing all the pages and searches in a breadth-first manner to collect documents. The spidering in domain-specific search engines more efficiently searches a small subset of documents by focusing on a particular set. Spidering accomplished with a reinforcement-learning framework has been found to be three times more efficient than breadth-first search . [ 4 ] In early 2014, the Defense Advanced Research Projects Agency ( DARPA ) released a statement on their website outlining the preliminary details of the \"Memex program\", which aims at developing new search technologies overcoming some limitations of text-based search. [ 5 ] DARPA wants the Memex technology developed in this research to be usable for search engines that can search for information on the Deep Web – the part of the Internet that is largely unreachable by commercial search engines like Google or Yahoo . DARPA's website describes that \"The goal is to invent better methods for interacting with and sharing information, so users can quickly and thoroughly organize and search subsets of information relevant to their individual interests\". [ 6 ] As reported in a 2015 Wired article, the search technology being developed in the Memex program \"aims to shine a light on the dark web and uncover patterns and relationships in online data to help law enforcement and others track illegal activity\". [ 7 ] DARPA intends for the program to replace the centralized procedures used by commercial search engines, stating that the \"creation of a new domain-specific indexing and search paradigm will provide mechanisms for improved content discovery, information extraction, information retrieval, user collaboration, and extension of current search capabilities to the deep web, the dark web, and nontraditional (e.g. multimedia) content\". [ 8 ] In their description of the program, DARPA explains the program's name as a tribute to Bush's original Memex invention, which served as an inspiration. [ 5 ] In April 2015, it was announced parts of Memex would be open sourced. [ 9 ] Modules were available for download. [ 8 ]",
    "links": [
      "Dark web",
      "Yelp, Inc.",
      "Taxonomy for search engines",
      "Forbes",
      "World Wide Web",
      "John Battelle",
      "CiteSeerX (identifier)",
      "Mocavo.com",
      "Trulia",
      "Breadth-first search",
      "Wired (magazine)",
      "Nuroa",
      "Focused crawler",
      "Web search engine",
      "FindTheBest",
      "Deep Web (search indexing)",
      "DARPA",
      "Yahoo",
      "Enterprise search",
      "Search engine indexing",
      "Ontologies",
      "Google",
      "Web crawler"
    ]
  },
  "Information extraction": {
    "url": "https://en.wikipedia.org/wiki/Information_extraction",
    "title": "Information extraction",
    "content": "Information extraction ( IE ) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. Typically, this involves processing human language texts by means of natural language processing (NLP). [ 1 ] Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction. Recent advances in NLP techniques have allowed for significantly improved performance compared to previous years. [ 2 ] An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: from an online news sentence such as: A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow automated reasoning about the logical form of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context . Information extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) [ 3 ] has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to \"understand\" an attack article only enough to find data corresponding to the slots in this template. Information extraction dates back to the late 1970s in the early days of NLP. [ 4 ] An early commercial system from the mid-1980s was JASPER built for Reuters by the Carnegie Group Inc with the aim of providing real-time financial news to financial traders. [ 5 ] Beginning in 1987, IE was spurred by a series of Message Understanding Conferences . MUC is a competition-based conference [ 6 ] that focused on the following domains: Considerable support came from the U.S. Defense Advanced Research Projects Agency ( DARPA ), who wished to automate mundane tasks performed by government analysts, such as scanning newspapers for possible links to terrorism. [ citation needed ] The present significance of IE pertains to the growing amount of information available in unstructured form. Tim Berners-Lee , inventor of the World Wide Web , refers to the existing Internet as the web of documents [ 7 ] and advocates that more of the content be made available as a web of data . [ 8 ] Until this transpires, the web largely consists of unstructured documents lacking semantic metadata . Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into relational form , or by marking-up with XML tags. An intelligent agent monitoring a news data feed requires IE to transform unstructured data into something that can be reasoned with. A typical application of IE is to scan a set of documents written in a natural language and populate a database with the information extracted. [ 9 ] Applying information extraction to text is linked to the problem of text simplification in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences. Typical IE tasks and subtasks include: Note that this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. Machine learning, statistical analysis and/or natural language processing are often used in IE. IE on non-text documents is becoming an increasingly interesting topic [ when? ] in research, and information extracted from multimedia documents can now [ when? ] be expressed in a high level structure as it is done on text. This naturally leads to the fusion of extracted information from multiple kinds of documents and sources. IE has been the focus of the MUC conferences. The proliferation of the Web , however, intensified the need for developing IE systems that help people to cope with the enormous amount of data that are available online. Systems that perform IE from online text should meet the requirements of low cost, flexibility in development and easy adaptation to new domains. MUC systems fail to meet those criteria. Moreover, linguistic analysis performed for unstructured text does not exploit the HTML/ XML tags and the layout formats that are available in online texts. As a result, less linguistically intensive approaches have been developed for IE on the Web using wrappers , which are sets of highly accurate rules that extract a particular page's content. Manually developing wrappers has proved to be a time-consuming task, requiring a high level of expertise. Machine learning techniques, either supervised or unsupervised , have been used to induce such rules automatically. Wrappers typically handle highly structured collections of web pages, such as product catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web. Recent effort on adaptive information extraction motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail- including mixed types. Such systems can exploit shallow natural language knowledge and thus can be also applied to less structured texts. A recent [ when? ] development is Visual Information Extraction, [ 16 ] [ 17 ] that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code. The following standard approaches are now widely accepted: Numerous other approaches exist for IE including hybrid approaches that combine some of the standard approaches previously listed.",
    "links": [
      "Latent Dirichlet allocation",
      "Stop word",
      "Michelle Obama",
      "Doi (identifier)",
      "Relationship extraction",
      "Part-of-speech tagging",
      "Automatic summarization",
      "Interactive fiction",
      "Logical form",
      "Knowledge Base",
      "AI-complete",
      "Word-sense disambiguation",
      "S2CID (identifier)",
      "Karin Verspoor",
      "Keyword extraction",
      "Syntactic parsing (computational linguistics)",
      "Corpus linguistics",
      "Latent semantic analysis",
      "Bank of English",
      "Sentence extraction",
      "Topic model",
      "Optical character recognition",
      "Information retrieval",
      "Machine translation",
      "Reuters",
      "Grammar checker",
      "Name resolution (semantics and text extraction)",
      "Table extraction",
      "Sentiment analysis",
      "Semantic analysis (machine learning)",
      "Large language model",
      "Message Understanding Conference",
      "Neural machine translation",
      "Universal Dependencies",
      "Google Ngram Viewer",
      "DARPA",
      "Formal semantics (natural language)",
      "Enterprise search",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Named entity recognition",
      "Multinomial logistic regression",
      "Recurrent neural network",
      "Bag-of-words model",
      "Parsing",
      "Predictive text",
      "Bibcode (identifier)",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "Named-entity recognition",
      "Machine-readable data",
      "General Architecture for Text Engineering",
      "Word embedding",
      "Parallel text",
      "Argument mining",
      "SpaCy",
      "Automated reasoning",
      "Ontology learning",
      "CiteSeerX (identifier)",
      "Apache Nutch",
      "Ontology extraction",
      "Outline of artificial intelligence",
      "Relational database",
      "DBpedia",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Word2vec",
      "Computational linguistics",
      "Thesaurus (information retrieval)",
      "Open information extraction",
      "Collocation extraction",
      "Pachinko allocation",
      "Natural Language Toolkit",
      "Lexical resource",
      "Unsupervised learning",
      "Word-sense induction",
      "PMID (identifier)",
      "Language model",
      "Speech synthesis",
      "Wikidata",
      "Conditional random field",
      "Shallow parsing",
      "DBpedia Spotlight",
      "Speech segmentation",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Document-term matrix",
      "Mallet (software project)",
      "Natural language",
      "ArXiv (identifier)",
      "Applications of artificial intelligence",
      "DARPA TIPSTER Program",
      "Small language model",
      "Natural language processing",
      "Supervised learning",
      "Speech corpus",
      "Pronunciation assessment",
      "Hallucination (artificial intelligence)",
      "Rule-based machine translation",
      "Textmining",
      "Machine learning",
      "Wrapper (data mining)",
      "World Wide Web",
      "Stemming",
      "UBY",
      "Compound-term processing",
      "Text simplification",
      "GloVe",
      "Data deluge",
      "Bigram",
      "Semantic parsing",
      "Text segmentation",
      "Example-based machine translation",
      "Coreference",
      "Distant reading",
      "List of emerging technologies",
      "Multi-document summarization",
      "Statistical machine translation",
      "Semantic web",
      "Trigram",
      "Concordancer",
      "Text corpus",
      "WordNet",
      "Hidden Markov model",
      "Anaphora (linguistics)",
      "Natural language understanding",
      "FastText",
      "BabelNet",
      "Internet",
      "Treebank",
      "BERT (language model)",
      "Barack Obama",
      "Maximum-entropy Markov model",
      "XML",
      "Semantic decomposition (natural language processing)",
      "Semantic similarity",
      "Spell checker",
      "Thomson Reuters",
      "Automatic identification and data capture",
      "Joint venture",
      "Explicit semantic analysis",
      "Lexical analysis",
      "Text mining",
      "Tim Berners-Lee",
      "Concept mining",
      "Metadata",
      "Language resource",
      "ClearForest",
      "Multimedia",
      "Semantic network",
      "Linguistic Linked Open Data",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Transfer-based machine translation",
      "Knowledge extraction",
      "Wayback Machine",
      "Textual entailment",
      "Speech recognition",
      "Terminology extraction",
      "Context (language use)",
      "OpenNLP",
      "Unstructured data",
      "Lemmatisation",
      "Text processing",
      "Rohini Kesavan Srihari",
      "ISBN (identifier)",
      "Faceted search",
      "Question answering",
      "Data extraction",
      "Chatbot",
      "Document classification",
      "Automated essay scoring",
      "PMC (identifier)",
      "Naïve Bayes classifier",
      "Semantic translation",
      "Truecasing",
      "Long short-term memory",
      "FrameNet",
      "Named entity",
      "Real-time data",
      "Transformer (deep learning architecture)",
      "Machine-readable dictionary",
      "Web scraping"
    ]
  },
  "Communications of the ACM": {
    "url": "https://en.wikipedia.org/wiki/Communications_of_the_ACM",
    "title": "Communications of the ACM",
    "content": "Communications of the ACM ( CACM ) is the monthly journal of the Association for Computing Machinery (ACM). It was established in 1958, [ 2 ] with Saul Rosen as its first managing editor. It is sent to all ACM members. [ 3 ] [ 4 ] Articles are intended for readers with backgrounds in all areas of computer science and information systems . The focus is on the practical implications of advances in information technology and associated management issues; ACM also publishes a variety of more theoretical journals. The magazine straddles the boundary of a science magazine , trade magazine , and a scientific journal . While the content is subject to peer review , the articles published are often summaries of research that may also be published elsewhere. Material published must be accessible and relevant to a broad readership. [ 5 ] From 1960 onward, CACM also published algorithms , expressed in ALGOL . The collection of algorithms later became known as the Collected Algorithms of the ACM. [ 6 ] CACM announced a transition to entirely open access in February 2024, [ 1 ] as part of ACM's commitment to make all articles open access. According to the Journal Citation Reports , the journal has a 2023 impact factor of 11.1. [ 7 ]",
    "links": [
      "Peer review",
      "ALGOL",
      "International Symposium on Microarchitecture",
      "Open access",
      "Doi (identifier)",
      "International Symposium on Computer Architecture",
      "ACM-AAAI Allen Newell Award",
      "International Symposium on Physical Design",
      "S2CID (identifier)",
      "ACM SIGHPC",
      "Grace Hopper Celebration of Women in Computing",
      "ACM SIGGRAPH",
      "SIGCSE Technical Symposium on Computer Science Education",
      "Symposium on Discrete Algorithms",
      "Symposium on Operating Systems Principles",
      "SIGCHI",
      "Conference on Human Factors in Computing Systems",
      "ACM Symposium on User Interface Software and Technology",
      "Special Interest Group on Design Automation",
      "Ken Kennedy Award",
      "ACM Prize in Computing",
      "Upsilon Pi Epsilon",
      "Symposium on Principles of Database Systems",
      "Knuth Prize",
      "ACM/IEEE Virtual Reality International Conference",
      "Steven A. Coons Award",
      "Web of Science",
      "Alonzo Church Award",
      "Bibcode (identifier)",
      "Richard Tapia Celebration of Diversity in Computing",
      "ACM Fellow",
      "SIGMM",
      "Information systems",
      "Saul Rosen",
      "ACM SIGLOG",
      "List of science magazines",
      "International Conference on Functional Programming",
      "International Conference on Mobile Computing and Networking",
      "ACM Digital Library",
      "Clarivate",
      "SIGSOFT",
      "International Conference on Architectural Support for Programming Languages and Operating Systems",
      "Symposium on Principles of Distributed Computing",
      "Andrew A. Chien",
      "Gordon Bell Prize",
      "ACM SIGOPS",
      "Symposium on Computational Geometry",
      "ACM Eugene L. Lawler Award",
      "Turing Award",
      "SIGSIM",
      "International Symposium on Symbolic and Algebraic Computation",
      "SIGDOC",
      "ACM SIGARCH",
      "ArXiv (identifier)",
      "SIGCOMM",
      "Distributed Event-Based Systems",
      "Design Automation Conference",
      "SPLASH (conference)",
      "Journal Citation Reports",
      "ACM SIGACT",
      "Grace Murray Hopper Award",
      "ACM-W",
      "Eckert–Mauchly Award",
      "Foundations of Genetic Algorithms",
      "SIGCSE",
      "History of Programming Languages (conference)",
      "SIGGRAPH",
      "RISKS Digest",
      "Symposium on Theory of Computing",
      "SIGAI",
      "Symposium on Principles of Programming Languages",
      "Joint Conference on Digital Libraries",
      "Federated Computing Research Conference",
      "ACM Conference on Fairness, Accountability, and Transparency",
      "Gödel Prize",
      "Association for Computing Machinery",
      "Genetic and Evolutionary Computation Conference",
      "Computer science",
      "Symposium on Parallelism in Algorithms and Architectures",
      "Computers in Entertainment",
      "ACM Software System Award",
      "ACM/IEEE Supercomputing Conference",
      "Algorithm",
      "ACM Student Research Competition",
      "ISO 4 (infobox)",
      "SIGUCCS",
      "SIGACCESS",
      "Special Interest Group on Information Retrieval",
      "Scientific journal",
      "ACM Conference on Recommender Systems",
      "SIGKDD",
      "ISSN (identifier)",
      "XRDS (magazine)",
      "SIGMOBILE",
      "SIGMOD",
      "ACM Interactions",
      "AAAI/ACM Conference on AI, Ethics, and Society",
      "Trade magazine",
      "Paris Kanellakis Award",
      "Journal of the ACM",
      "SIGMETRICS",
      "Programming Language Design and Implementation",
      "International Collegiate Programming Contest",
      "ACM Doctoral Dissertation Award",
      "ACM SIGEVO",
      "ACM Computing Surveys",
      "ACM SIGWEB",
      "CHI Academy",
      "Impact factor",
      "ACM Queue",
      "SIGPLAN",
      "SIAM/ACM Prize in Computational Science and Engineering",
      "ACM Multimedia",
      "SIGSAM"
    ]
  },
  "Language model": {
    "url": "https://en.wikipedia.org/wiki/Language_model",
    "title": "Language model",
    "content": "A language model is a model of the human brain's ability to produce natural language . [ 1 ] [ 2 ] Language models are useful for a variety of tasks, including speech recognition , [ 3 ] machine translation , [ 4 ] natural language generation (generating more human-like text), optical character recognition , route optimization , [ 5 ] handwriting recognition , [ 6 ] grammar induction , [ 7 ] and information retrieval . [ 8 ] [ 9 ] Large language models (LLMs), currently their most advanced form as of 2019, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet ). They have superseded recurrent neural network -based models, which had previously superseded the purely statistical models, such as the word n -gram language model . Noam Chomsky did pioneering work on language models in the 1950s by developing a theory of formal grammars . [ 10 ] In 1980, statistical approaches were explored and found to be more useful for many purposes than rule-based formal grammars. Discrete representations like word n -gram language models , with probabilities for discrete combinations of words, made significant advances. In the 2000s, continuous representations for words, such as word embeddings , began to replace discrete representations. [ 11 ] Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning, and common relationships between pairs of words like plurality or gender. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ' Shannon -style' experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [ 12 ] A word n -gram language model is a statistical model of language which calculates the probability of the next word in a sequence from a fixed size window of previous words. If one previous word is considered, it is a bigram model; if two words, a trigram model; if n − 1 words, an n -gram model. [ 13 ] Special tokens are introduced to denote the start and end of a sentence ⟨ s ⟩ {\\displaystyle \\langle s\\rangle } and ⟨ / s ⟩ {\\displaystyle \\langle /s\\rangle } . To prevent a zero probability being assigned to unseen words, the probability of each seen word is slightly lowered to make room for the unseen words in a given corpus . To achieve this, various smoothing methods are used, from simple \"add-one\" smoothing (assigning a count of 1 to unseen n -grams, as an uninformative prior ) to more sophisticated techniques, such as Good–Turing discounting or back-off models . Maximum entropy language models encode the relationship between a word and the n -gram history using feature functions. The equation is P ( w m ∣ w 1 , … , w m − 1 ) = 1 Z ( w 1 , … , w m − 1 ) exp ⁡ ( a T f ( w 1 , … , w m ) ) {\\displaystyle P(w_{m}\\mid w_{1},\\ldots ,w_{m-1})={\\frac {1}{Z(w_{1},\\ldots ,w_{m-1})}}\\exp(a^{T}f(w_{1},\\ldots ,w_{m}))} where Z ( w 1 , … , w m − 1 ) {\\displaystyle Z(w_{1},\\ldots ,w_{m-1})} is the partition function , a {\\displaystyle a} is the parameter vector, and f ( w 1 , … , w m ) {\\displaystyle f(w_{1},\\ldots ,w_{m})} is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain n -gram. It is helpful to use a prior on a {\\displaystyle a} or some form of regularization . The log-bilinear model is another example of an exponential language model. Skip-gram language model is an attempt at overcoming the data sparsity problem that the preceding model (i.e. word n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are skipped over (thus the name \"skip-gram\"). [ 15 ] Formally, a k -skip- n -gram is a length- n subsequence where the components occur at distance at most k from each other. For example, in the input text: the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences In skip-gram model, semantic relations between words are represented by linear combinations , capturing a form of compositionality . For example, in some such models, if v is the function that maps a word w to its n -d vector representation, then v ( k i n g ) − v ( m a l e ) + v ( f e m a l e ) ≈ v ( q u e e n ) {\\displaystyle v(\\mathrm {king} )-v(\\mathrm {male} )+v(\\mathrm {female} )\\approx v(\\mathrm {queen} )} Continuous representations or embeddings of words are produced in recurrent neural network -based language models (known also as continuous space language models ). [ 18 ] Such continuous space embeddings help to alleviate the curse of dimensionality , which is the consequence of the number of possible sequences of words increasing exponentially with the size of the vocabulary, further causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [ 19 ] A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation . [ 20 ] [ 21 ] The largest and most capable LLMs are generative pre-trained transformers ( GPTs ) and provide the core capabilities of modern chatbots . LLMs can be fine-tuned for specific tasks or guided by prompt engineering . [ 22 ] These models acquire predictive power regarding syntax , semantics , and ontologies [ 23 ] inherent in human language corpora , but they also inherit inaccuracies and biases present in the data they are trained on. [ 24 ] They consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents , code generation , knowledge retrieval , and automated reasoning that previously required bespoke systems. [ 25 ] LLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture , introduced in 2017, replaced recurrence with self-attention , allowing efficient parallelization , longer context handling, and scalable training on unprecedented data volumes. [ 26 ] This innovation enabled models like GPT , BERT , and their successors, which demonstrated emergent behaviors at scale, such as few-shot learning and compositional reasoning. [ 27 ] Reinforcement learning , particularly policy gradient algorithms , has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. [ 28 ] Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM's output distribution, against reward signals derived from human or automated preference judgments. [ 29 ] This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance. Although sometimes matching human performance, it is not clear whether they are plausible cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [ 33 ] Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [ 34 ] Various data sets have been developed for use in evaluating language processing systems. [ 35 ] These include:",
    "links": [
      "Alex Graves (computer scientist)",
      "Alex Krizhevsky",
      "Artificial intelligence",
      "John Hopfield",
      "Uninformative prior",
      "Feature engineering",
      "DeepDream",
      "Word-sense disambiguation",
      "Corpus linguistics",
      "Mean shift",
      "Overfitting",
      "Decision tree learning",
      "List of artificial intelligence companies",
      "International Conference on Learning Representations",
      "GPT-5",
      "Grammar checker",
      "Memtransistor",
      "ECML PKDD",
      "Sentiment analysis",
      "Large language model",
      "Semantic analysis (machine learning)",
      "GPT-3",
      "Kling AI",
      "Universal Dependencies",
      "Automatic programming",
      "Christopher D. Manning",
      "Uncanny valley",
      "Autoregressive model",
      "Grammar induction",
      "Random forest",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Diffusion model",
      "Double descent",
      "Word n-gram language model",
      "Deep learning speech synthesis",
      "Parsing",
      "Andrej Karpathy",
      "Action selection",
      "Chinchilla (language model)",
      "Adversarial machine learning",
      "Bibcode (identifier)",
      "Smoothing",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Autoencoder",
      "Named-entity recognition",
      "Word embedding",
      "Human-in-the-loop",
      "SpaCy",
      "Cliff Shaw",
      "Ontology learning",
      "Convolutional neural network",
      "CiteSeerX (identifier)",
      "Reinforcement learning",
      "Physics-informed neural networks",
      "Midjourney",
      "Noam Chomsky",
      "Reasoning model",
      "Data augmentation",
      "Computational linguistics",
      "Collocation extraction",
      "Pachinko allocation",
      "Generative artificial intelligence",
      "Linear discriminant analysis",
      "Emma Brunskill",
      "Unsupervised learning",
      "Speech synthesis",
      "Imagen (text-to-image model)",
      "AlphaGo",
      "GPT-5.2",
      "Speech segmentation",
      "Receiver operating characteristic",
      "Whisper (speech recognition system)",
      "Project Debater",
      "ArXiv (identifier)",
      "Retrieval-augmented generation",
      "Pronunciation assessment",
      "Transformer architecture",
      "Stemming",
      "UBY",
      "Semantic parsing",
      "Example-based machine translation",
      "AAAI Conference on Artificial Intelligence",
      "Computational learning theory",
      "Batch normalization",
      "Coefficient of determination",
      "Hidden Markov model",
      "Robot control",
      "Fine-tuning (deep learning)",
      "Support vector machine",
      "Neural field",
      "Highway network",
      "Andrew Ng",
      "Policy gradient method",
      "Semantic similarity",
      "Spell checker",
      "Latent diffusion model",
      "Automatic identification and data capture",
      "Vapnik–Chervonenkis theory",
      "AI alignment",
      "Udio",
      "T5 (language model)",
      "Structured prediction",
      "Linguistic Linked Open Data",
      "Handwriting recognition",
      "Statistical model",
      "Deep learning",
      "Wayback Machine",
      "Music and artificial intelligence",
      "ISSN (identifier)",
      "BIRCH",
      "Crowdsourcing",
      "History of artificial intelligence",
      "GPT-4o",
      "ISBN (identifier)",
      "Quoc V. Le",
      "Claude Shannon",
      "Automated essay scoring",
      "PMC (identifier)",
      "Bayesian network",
      "Neural network (machine learning)",
      "Confusion matrix",
      "Ideogram (text-to-image model)",
      "Regression analysis",
      "Machine-readable dictionary",
      "Compositionality",
      "AlphaFold",
      "Isolation forest",
      "Stop word",
      "Gemini (chatbot)",
      "Automatic summarization",
      "Interactive fiction",
      "GPT-J",
      "Differentiable neural computer",
      "Reservoir computing",
      "S2CID (identifier)",
      "Gemma (language model)",
      "Anomaly detection",
      "Latent semantic analysis",
      "Bank of English",
      "Sentence extraction",
      "AI safety",
      "Neuro-symbolic AI",
      "GPT-4.5",
      "Frank Rosenblatt",
      "T-distributed stochastic neighbor embedding",
      "Mamba (deep learning architecture)",
      "Alan Turing",
      "Semantics",
      "Syntax",
      "GPT-2",
      "Marvin Minsky",
      "International Joint Conference on Artificial Intelligence",
      "Adobe Firefly",
      "AutoGPT",
      "GPT-4.1",
      "Stephen Grossberg",
      "BLOOM (language model)",
      "Nathaniel Rochester (computer scientist)",
      "Electrochemical RAM",
      "Sigmoid function",
      "IBM Watson",
      "Seppo Linnainmaa",
      "Algorithmic bias",
      "Diffusion process",
      "Self-organizing map",
      "Data mining",
      "Sparse dictionary learning",
      "Learning to rank",
      "List of datasets for machine-learning research",
      "Thesaurus (information retrieval)",
      "Information extraction",
      "Herbert A. Simon",
      "Takeo Kanade",
      "Lexical resource",
      "Shallow parsing",
      "Language technology",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Attention (machine learning)",
      "Flux (text-to-image model)",
      "Yann LeCun",
      "Document-term matrix",
      "Natural language",
      "Small language model",
      "Learning curve (machine learning)",
      "Supervised learning",
      "Facial recognition system",
      "Gemini (language model)",
      "Aidan Gomez",
      "Machine learning",
      "Conference on Neural Information Processing Systems",
      "Journal of Machine Learning Research",
      "Linear combination",
      "Factor analysis",
      "GloVe",
      "GPT-5.1",
      "Shun'ichi Amari",
      "Restricted Boltzmann machine",
      "Perceptron",
      "Distant reading",
      "Multi-document summarization",
      "Naive Bayes classifier",
      "Canonical correlation",
      "Vibe coding",
      "Stable Diffusion",
      "Gradient descent",
      "Text corpus",
      "Graphical model",
      "Softmax function",
      "Reinforcement learning from human feedback",
      "GPT Image",
      "Self-play (reinforcement learning technique)",
      "Cognitive model",
      "Empirical risk minimization",
      "Factored language model",
      "Semantic decomposition (natural language processing)",
      "Neuromorphic engineering",
      "Hyperparameter (machine learning)",
      "GPT (language model)",
      "Kernel machines",
      "Explicit semantic analysis",
      "Deep linguistic processing",
      "PropBank",
      "Textual entailment",
      "Online machine learning",
      "Cache language model",
      "Text processing",
      "LeNet",
      "Intelligent agent",
      "Ashish Vaswani",
      "Conversational agent",
      "Gating mechanism",
      "Probably approximately correct learning",
      "Hierarchical clustering",
      "Grok (chatbot)",
      "Chatbot",
      "Route optimization",
      "Echo state network",
      "Dimensionality reduction",
      "Transformer (deep learning architecture)",
      "DBpedia",
      "John Schulman",
      "Neural radiance field",
      "Sora (text-to-video model)",
      "James Goodnight",
      "Proper generalized decomposition",
      "Language model benchmark",
      "Feature learning",
      "Training, validation, and test data sets",
      "Active learning (machine learning)",
      "Principal component analysis",
      "AI-complete",
      "Syntactic parsing (computational linguistics)",
      "Normalization (machine learning)",
      "Glossary of artificial intelligence",
      "Stochastic gradient descent",
      "Predictive learning",
      "Information retrieval",
      "Machine translation",
      "Advances in Neural Information Processing Systems",
      "IBM Granite",
      "Riffusion",
      "Scholarpedia",
      "Neural machine translation",
      "Expectation–maximization algorithm",
      "Google Ngram Viewer",
      "Multi-agent reinforcement learning",
      "Apprenticeship learning",
      "State–action–reward–state–action",
      "Quantum machine learning",
      "Bias–variance tradeoff",
      "Self-supervised learning",
      "Random sample consensus",
      "Paul Werbos",
      "Yoshua Bengio",
      "Recurrent neural network",
      "Self-driving car",
      "Non-negative matrix factorization",
      "Formal grammar",
      "Q-learning",
      "Computer vision",
      "Geoffrey Hinton",
      "Predictive text",
      "Fei-Fei Li",
      "ChatGPT",
      "Recursive self-improvement",
      "Accuracy and precision",
      "Parallel text",
      "Argument mining",
      "Automated reasoning",
      "Walter Pitts",
      "John von Neumann",
      "Curse of dimensionality",
      "Reflection (artificial intelligence)",
      "Text-to-video model",
      "MuZero",
      "Bootstrap aggregating",
      "Qwen",
      "GPT-4",
      "Katz's back-off model",
      "PMID (identifier)",
      "U-Net",
      "Wikidata",
      "Exponential growth",
      "Humanity's Last Exam",
      "Artificial neural network",
      "Ethics of artificial intelligence",
      "Jürgen Schmidhuber",
      "Noam Shazeer",
      "Semi-supervised learning",
      "Rectifier (neural networks)",
      "Rule-based machine translation",
      "List of datasets in computer vision and image processing",
      "Human image synthesis",
      "Real number",
      "Model Context Protocol",
      "Dream Machine (text-to-video model)",
      "OpenAI o4-mini",
      "Graph neural network",
      "Seymour Papert",
      "Joseph Weizenbaum",
      "Nearest neighbor search",
      "Artificial general intelligence",
      "Demis Hassabis",
      "Gated recurrent unit",
      "Cluster analysis",
      "David Silver (computer scientist)",
      "Statistical machine translation",
      "Concordancer",
      "FastText",
      "BabelNet",
      "Internet",
      "Treebank",
      "Boosting (machine learning)",
      "OPTICS algorithm",
      "International Conference on Machine Learning",
      "Parallel computing",
      "Oliver Selfridge",
      "Hill climbing",
      "Text mining",
      "Lexical analysis",
      "Language resource",
      "Artificial human companion",
      "Multi-task learning",
      "Semantic network",
      "Transfer-based machine translation",
      "Ensemble learning",
      "Conjugate gradient method",
      "Residual neural network",
      "Speech recognition",
      "Lemmatisation",
      "Prompt engineering",
      "Variational autoencoder",
      "Temporal difference learning",
      "John McCarthy (computer scientist)",
      "Multilayer perceptron",
      "Ilya Sutskever",
      "15.ai",
      "Curriculum learning",
      "Lotfi A. Zadeh",
      "Relevance vector machine",
      "Latent Dirichlet allocation",
      "ElevenLabs",
      "Topological deep learning",
      "Doi (identifier)",
      "Transformer (machine learning)",
      "Statistical classification",
      "Part-of-speech tagging",
      "Principle of maximum entropy",
      "Huawei PanGu",
      "Loss functions for classification",
      "LaMDA",
      "Weight initialization",
      "Parameter",
      "Emergence",
      "Outline of machine learning",
      "Local outlier factor",
      "Topic model",
      "WaveNet",
      "Logistic regression",
      "Optical character recognition",
      "Independent component analysis",
      "Machine Learning (journal)",
      "Timeline of artificial intelligence",
      "GPT-1",
      "Claude (language model)",
      "List of artificial intelligence projects",
      "AlexNet",
      "Artificial intelligence and elections",
      "Formal semantics (natural language)",
      "PaLM",
      "K-nearest neighbors algorithm",
      "Good–Turing discounting",
      "DeepSeek (chatbot)",
      "Bag-of-words model",
      "Quasi-Newton method",
      "Distributional semantics",
      "N-gram",
      "Linear regression",
      "Generative model",
      "AlphaZero",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Word2vec",
      "François Chollet",
      "Natural Language Toolkit",
      "Association rule learning",
      "Suno AI",
      "Word-sense induction",
      "Rule-based machine learning",
      "Conditional random field",
      "Ian Goodfellow",
      "Vision transformer",
      "Virtual assistant",
      "Batch learning",
      "Oriol Vinyals",
      "Veo (text-to-video model)",
      "Feedforward neural network",
      "Speech corpus",
      "Natural language processing",
      "DBRX",
      "Hallucination (artificial intelligence)",
      "Fuzzy clustering",
      "OpenAI o1",
      "Kunihiko Fukushima",
      "Generative pre-trained transformer",
      "Automated machine learning",
      "Compound-term processing",
      "Text simplification",
      "Llama (language model)",
      "Bigram",
      "Text segmentation",
      "Regularization (mathematics)",
      "DBSCAN",
      "OpenAI Five",
      "OpenAI o3",
      "Generative adversarial network",
      "Boltzmann machine",
      "Trigram",
      "Text-to-image model",
      "WordNet",
      "Natural language understanding",
      "Data cleaning",
      "Aurora (text-to-image model)",
      "Meta-learning (computer science)",
      "Neural Turing machine",
      "Jan Leike",
      "BERT (language model)",
      "Multimodal learning",
      "Statistical learning theory",
      "Semantic similarity network",
      "Backpropagation",
      "K-means clustering",
      "Spiking neural network",
      "Mechanistic interpretability",
      "Concept mining",
      "Occam learning",
      "Bernard Widrow",
      "Natural language generation",
      "CURE algorithm",
      "Terminology extraction",
      "Generalization (machine learning)",
      "IBM Watsonx",
      "Daniel Kokotajlo (researcher)",
      "DALL-E",
      "Partition function (mathematics)",
      "Warren Sturgis McCulloch",
      "Question answering",
      "Convolution",
      "Density estimation",
      "Massive Multitask Language Understanding",
      "Document classification",
      "Activation function",
      "Allen Newell",
      "Recraft",
      "Ontology (information science)",
      "Long short-term memory",
      "Truecasing",
      "Mustafa Suleyman",
      "FrameNet",
      "Imitation learning",
      "Web scraping"
    ]
  },
  "PMID (identifier)": {
    "url": "https://en.wikipedia.org/wiki/PMID_(identifier)",
    "title": "PMID (identifier)",
    "content": "PubMed is an openly accessible, free database which includes primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval . [ 1 ] From 1971 to 1997, online access to the MEDLINE database was provided via computer, using phone lines primarily through institutional facilities, such as university libraries . [ 2 ] PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching. [ 3 ] It was released alongside with \"Internet Grateful Med\" (web-version of Grateful Med ). In 2001 Grateful Med was deleted and entirely replaced by PubMed. The PubMed system was offered free to the public starting in June 1997. [ 2 ] In addition to MEDLINE, PubMed provides access to: Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central [ 5 ] and local mirrors, such as Europe PubMed Central . [ 6 ] Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog. [ 7 ] As of 23 May 2023 [update] , PubMed has more than 35 million citations and abstracts dating back to 1966, selectively to the year 1865, and very selectively to 1809. As of the same date [update] , 24.6 million of PubMed's records are listed with their abstracts, and 26.8 million records have links to full-text versions (of which 10.9 million articles are available, full-text for free). [ 8 ] Over the last 10 years (ending 31 December 2019), an average of nearly one million new records were added each year. [ citation needed ] In 2016, NLM changed the indexing system so that publishers are able to directly correct typos and errors in PubMed indexed articles. [ 9 ] PubMed has been reported to include some articles published in predatory journals. MEDLINE and PubMed policies for the selection of journals for database inclusion are slightly different. Weaknesses in the criteria and procedures for indexing journals in PubMed Central may allow publications from predatory journals to leak into PubMed. [ 10 ] On March 4, 2025, there was a temporary global outage of the PubMed database. The outage lasted about a day and ‘stoked fears about the database’s future’ because researchers worldwide depend on PubMed to access peer-reviewed scientific literature. The disruption was apparently not deliberate, and services were restored, but the event highlighted concerns about the reliability and contingency planning for essential research infrastructure [ 11 ] A new PubMed interface was launched in October 2009 and encouraged the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches. [ 12 ] By default the results are sorted by Most Recent, but this can be changed to Best Match, Publication Date, First Author, Last Author, Journal, or Title. [ 13 ] The PubMed website design and domain was updated in January 2020 and became default on 15 May 2020, with the updated and new features. [ 14 ] There was a critical reaction from many researchers who frequently use the site. [ 15 ] PubMed/MEDLINE can be accessed via handheld devices, using for instance the \"PICO\" option (for focused clinical questions) created by the NLM. [ 16 ] A \"PubMed Mobile\" option, providing access to a mobile friendly, simplified PubMed version, is also available. [ 17 ] Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window. PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms. For optimal searches in PubMed, it is necessary to understand its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features; reference librarians and search specialists offer search services. [ 18 ] [ 19 ] The search into PubMed's search window is only recommended for the search of unequivocal topics or new interventions that do not yet have a MeSH heading created, as well as for the search for commercial brands of medicines and proper nouns. It is also useful when there is no suitable heading or the descriptor represents a partial aspect. The search using the thesaurus MeSH is more accurate and will give fewer irrelevant results. In addition, it saves the disadvantage of the free text search in which the spelling, singular/plural or abbreviated differences have to be taken into consideration. On the other side, articles more recently incorporated into the database to which descriptors have not yet been assigned will not be found. Therefore, to guarantee an exhaustive search, a combination of controlled language headings and free text terms must be used. [ 20 ] When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., \"Clinical Trial\"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date). Publication type parameter allows searching by the type of publication , including reports of various kinds of clinical research. [ 21 ] Since July 2005, the MEDLINE article indexing process extracts identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier). [ 22 ] A reference which is judged particularly relevant can be marked and \"related articles\" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the 'Find related data' option. The related articles are then listed in order of \"relatedness\". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm. [ 23 ] The 'related articles' function has been judged to be so precise that the authors of a paper suggested it can be used instead of a full search. [ 24 ] PubMed automatically links to MeSH terms and subheadings. Examples would be: \"bad breath\" links to (and includes in the search) \"halitosis\" and \"heart attack\" to \"myocardial infarction\". Where appropriate, these MeSH terms are automatically \"expanded\", that is, include more specific terms. Terms like \"nursing\" are automatically linked to \"Nursing [MeSH]\" or \"Nursing [Subheading]\". This feature is called Auto Term Mapping and is enacted, by default, in free text searching but not exact phrase searching (i.e. enclosing the search query with double quotes). [ 25 ] This feature makes PubMed searches more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology. [ 25 ] PubMed does not apply automatic mapping of the term in the following circumstances: by writing the quoted phrase (e.g., \"kidney allograft\"), when truncated on the asterisk (e.g., kidney allograft*), and when looking with field labels (e.g., Cancer [ti]). [ 20 ] The PubMed optional facility \"My NCBI\" (with free registration) provides tools for and a wide range of other options. [ 26 ] The \"My NCBI\" area can be accessed from any computer with web-access. An earlier version of \"My NCBI\" was called \"PubMed Cubby\". [ 27 ] LinkOut is an NLM facility to link and make available full-text local journal holdings. [ 28 ] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March 2010 [update] ), from Aalborg University in Denmark to ZymoGenetics in Seattle. [ 29 ] Users at these institutions see their institution's logo within the PubMed search result (if the journal is held at that institution) and can access the full-text. Link out is being consolidated with Outside Tool as of the major platform update coming in the Summer of 2019. [ 30 ] In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016. [ 31 ] In February 2018, PubMed Commons was discontinued due to the fact that \"usage has remained minimal\". [ 32 ] [ 33 ] askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds. [ 34 ] A PMID (PubMed identifier or PubMed unique identifier) [ 35 ] is a unique integer value , starting at 1 , assigned to each PubMed record. A PMID is not the same as a PMCID (PubMed Central identifier) which is the identifier for all works published in the free-to-access PubMed Central . [ 36 ] The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor , editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID. [ citation needed ] Each number that is entered in the PubMed search window is treated by default as if it were a PMID. Therefore, any reference in PubMed can be located using the PMID. [ citation needed ] The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Embase , Ovid , Dialog , EBSCO , Knowledge Finder and many other commercial, non-commercial, and academic providers. [ 37 ] As of October 2008 [update] , more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range [ 38 ] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option. Lu identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories: [ 38 ] As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term \"PubMed derivatives\" has been suggested. [ 38 ] Without the need to store about 90 GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in \"The E-utilities In-Depth: Parameters, Syntax and More\", by Eric Sayers, PhD. [ 49 ] Various citation format generators, taking PMID numbers as input, are examples of web applications making use of the eutils-application program interface. Sample web pages include Citation Generator – Mick Schroeder , Pubmed Citation Generator – Ultrasound of the Week , and Cite this for me . In 2025, the ZBMed , the German National Library of Medicine, announced plans to develop an internationally supported open source version of PubMed. [ 50 ] [ 51 ] Alternative methods to mine the data in PubMed use programming environments such as Matlab , Python or R . In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. [ citation needed ] Code can be automated to systematically query with different keywords such as disease, year, organs, etc. For bulk processing, the full PubMed database is available as XML which can be downloaded from an FTP server. The annual baseline is released in December, followed by daily update files. [ 52 ] In addition to its traditional role as a biomedical database, PubMed has become common resource for training biomedical language models . [ 53 ] The data accessible by PubMed can be mirrored locally using an unofficial tool such as MEDOC. [ 54 ] Millions of PubMed records augment various open data datasets about open access , like Unpaywall . Data analysis tools like Unpaywall Journals are used by libraries to assist with big deal cancellations: libraries can avoid subscriptions for materials already served by instant open access via open archives like PubMed Central. [ 55 ]",
    "links": [
      "Scientific writing",
      "Telepsychiatry",
      "Public health journal",
      "Poster session",
      "ERIH PLUS",
      "Working paper",
      "Peer review",
      "Open access",
      "Conference proceedings",
      "Doi (identifier)",
      "Imprint (trade name)",
      "Grateful Med",
      "Monograph",
      "Decision aids",
      "ORCID",
      "Open access citation advantage",
      "Big deal (subscription model)",
      "EHealth",
      "Annual report",
      "Version of record",
      "S2CID (identifier)",
      "Kenneth H. Wolfe",
      "Documentary editing",
      "Citation cartel",
      "Rankings of academic publishers",
      "Matlab",
      "ICanHazPDF",
      "BASE (search engine)",
      "Python (programming language)",
      "Information retrieval",
      "SCImago Journal Rank",
      "Biological patent",
      "Annals of Surgery",
      "Telecommunication",
      "Teledentistry",
      "Virtual patient",
      "Book",
      "Tele-audiology",
      "Teledermatology",
      "Least publishable unit",
      "Open data",
      "BMJ",
      "Scientometrics",
      "H-index",
      "Acknowledgment index",
      "Copyright policies of academic publishers",
      "Subspecialty",
      "Remote surgery",
      "OpenAlex",
      "Sherpa Romeo",
      "Personal health record",
      "Telepathology",
      "Abstract (summary)",
      "Grey literature",
      "Postprint",
      "Open-notebook science",
      "Chapter (books)",
      "Academic journal publishing reform",
      "University at Buffalo",
      "Composite index (metrics)",
      "E-patient",
      "Sci-Hub",
      "Embase",
      "Open scientific data",
      "Web of Science",
      "Unpaywall",
      "Article-level metrics",
      "Bibcode (identifier)",
      "Index Copernicus",
      "Technical report",
      "Academic library",
      "List of scientific journals",
      "GitHub",
      "List of open-source health software",
      "Position paper",
      "G-index",
      "List of academic publishers by preprint policy",
      "Scientific literature",
      "JournalReview.org",
      "Admission note",
      "Electronic health record",
      "Literature review",
      "Medical record",
      "Eigenfactor",
      "Remote therapy",
      "ETBLAST",
      "OpenAIRE",
      "Aalborg University",
      "ZymoGenetics",
      "Review article",
      "Language model",
      "Correction (newspaper)",
      "Letter to the editor",
      "Wikidata",
      "Pamphlet",
      "White paper",
      "Citation index",
      "Telepharmacy",
      "Life sciences",
      "Academic journal",
      "Retraction in academic publishing",
      "Science (journal)",
      "Research participant",
      "Index Medicus",
      "Telerehabilitation",
      "Science-wide author databases of standardized citation indicators",
      "In absentia health care",
      "National Institutes of Health",
      "Electronic publishing",
      "Remote patient monitoring",
      "Open archive",
      "ArXiv (identifier)",
      "Festschrift",
      "Treatise",
      "Entrez",
      "PICO process",
      "List of university presses",
      "Google Scholar",
      "PubMed Central",
      "Patient opinion leader",
      "PubMed Central Canada",
      "Edited volume",
      "Open research",
      "Research center",
      "Semantic Scholar",
      "United States National Library of Medicine",
      "Europe PubMed Central",
      "Tele-epidemiology",
      "Telehealth",
      "Bibliographic database",
      "Participative decision-making in organizations",
      "Blue Button",
      "Unique identifier",
      "Op-ed",
      "List of academic databases and search engines",
      "Publish or perish",
      "Health education",
      "Altmetrics",
      "Telenursing",
      "Chemical patent",
      "Health information on the Internet",
      "Preprint",
      "AMiner (database)",
      "Patient Activation Measure",
      "Ovid Technologies",
      "Unpaywall Journals",
      "Doctor–patient relationship",
      "Use of technology in treatment of mental disorders",
      "Text publication society",
      "Shared decision-making",
      "Anne O'Tate",
      "Health information on Wikipedia",
      "De-identification",
      "MEDLINE",
      "Retractions in academic publishing",
      "Thesis",
      "Essay",
      "German National Library of Medicine",
      "Knowledge translation",
      "MHealth",
      "Erratum",
      "Citation impact",
      "Health Insurance Portability and Accountability Act",
      "Health informatics",
      "Biomedicine",
      "Patent",
      "Lists of academic journals",
      "Teleophthalmology",
      "EBSCO Publishing",
      "List of open-access journals",
      "Health 2.0",
      "ISSN (identifier)",
      "Serials crisis",
      "Scholarly communication",
      "Ingelfinger rule",
      "Author-level metrics",
      "Collection of articles",
      "R (programming language)",
      "Academic publishing",
      "Highly Cited Researchers",
      "Online patient education",
      "Bibliometrics",
      "Teleradiology",
      "CORE (research service)",
      "PMC (identifier)",
      "Patient participation",
      "PMCID",
      "Impact factor",
      "Dialog (online database)",
      "Journal ranking",
      "Scopus",
      "Medical Subject Headings",
      "Paperity",
      "Learned society"
    ]
  },
  "Temporal information retrieval": {
    "url": "https://en.wikipedia.org/wiki/Temporal_information_retrieval",
    "title": "Temporal information retrieval",
    "content": "Temporal information retrieval ( T-IR ) is an emerging area of research related to the field of information retrieval (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs. According to information theory science (Metzger, 2007), [ 1 ] timeliness or currency is one of the key five aspects that determine a document's credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company's earnings or information on already-happened or invalid predictions. T-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding , query disambiguation, query classification, result diversification and so on. This article contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.",
    "links": [
      "Query understanding",
      "Information theory",
      "Svetlana Lazebnik",
      "Wayback Machine",
      "Krysta Svore",
      "Doi (identifier)",
      "List of Web archiving initiatives",
      "Information retrieval"
    ]
  },
  "Nicholas J. Belkin": {
    "url": "https://en.wikipedia.org/wiki/Nicholas_J._Belkin",
    "title": "Nicholas J. Belkin",
    "content": "Nicholas J. Belkin is a professor at the School of Communication and Information at Rutgers University . Among the main themes of his research are digital libraries ; information-seeking behaviors; and interaction between humans and information retrieval systems . Belkin is best known for his work on human-centered Information Retrieval and the hypothesis of Anomalous State of Knowledge (ASK). Belkin realized that in many cases, users of search systems are unable to precisely formulate what they need. They miss some vital knowledge to formulate their queries. In such cases it is more suitable to attempt to describe a user's anomalous state of knowledge than to ask the user to specify her/his need as a request to the system. [ 1 ] [ 2 ] Belkin was the chair of SIGIR in 1995-99, and the president of American Society for Information Science and Technology in 2005. [ 3 ] In 2015, Belkin received the Gerard Salton Award . [ 4 ] Nicholas Belkin studied Slavic Philology at the University of Washington , graduating in 1968. He graduated from the same college in Library Science 2 years later (1970), and read his doctoral thesis in 1977 in the University of London . He worked in the Information Science department of this university from 1975 to 1985. That year, he signed for the Faculty of Communication and Information at Rutgers University (USA). He has been a visiting professor at Western Ontario University (Canada), Dhirubhai Ambani Institute of Information and Communication Technology (India) and Free University of Berlin . He has been a visiting researcher at the National University of Singapore in 1996. He has given more than 200 lectures around the world. He has been president of Association for Computing Machinery SIGIR (Special Interest Group on Information Retrieval) during the period 1995-1999, and president of the American Society of Information Science and Technology (ASIST) in 2005. Nicholas Belkin has served on numerous editorial boards of numerous scientific journals. Among the most prestigious are \"Information Processing and Management\" and \"Information Retrieval\". Nicholas Belkin has approached information retrieval from the so-called cognitive models , that is, those focused on users who access document systems. Belkin approached his research from 3 basic lines: In 1977, Belkin read his thesis where he developed a new theory of the concept documentary information . This would be a structure that would allow the user to transform his anomalous state of knowledge (Anomalous State of Knowledge or ASK), when the need for information is satisfied, producing an adequate connection between the two ends of the documentary process: the producer and the receiver or user. For Belkin, the purpose towards which Documentation works is to make this effective communication possible, which would imply the study of documentary information in human and cognitive communication systems, the connection between this information and its producer, the connection between information and user, gives the idea of the requested information and the effectiveness between information and document and its transmission process. Belkin concludes that the concept of documentary information is the combination of a cognitive communication system, a structural representation of knowledge, the implementation of the project via user when he recognizes the need for information (ASK9, the meaning of the text ( message ) and the interest in solving the problem of information science. This theory has also been developed by Oddy and Brooks. Nicholas Belkin proposed a novel cognitive model of information retrieval, referred to as 'episodic' . In this, Belkin defines a set of interactions that occur between the user and the system during the consultation to \"conceptualize, label and transcribe the need for information, as well as make relevant judgments about one or more documents.\" The components would be the same as those used in the traditional model: navigation ( browsing ), query (querying), display , indexing , representation and matching. This model pays very little attention to the structure of documents and their retrieval, because it focuses on the anomalous state of knowledge of the individual, how to represent it, how to retrieve it, so it is based on the storage, retrieval and interaction of the search strategy. Nicholas Belkin has been awarded numerous times, obtaining in 2003 the Award of Merit , and the Gerard Salton Award in 2015. Belkin has published numerous articles in the most prestigious magazines in the field of Information and Documentation, some awarded by the ASIST. He is also the author of the book: Interaction in Information Systems: A Review of Research from Document Retrieval to Knowledge-Based Systems (1985) co-authored with Alina Vickery .",
    "links": [
      "American Society for Information Science and Technology",
      "Digital libraries",
      "Database index",
      "Doi (identifier)",
      "Special Interest Group on Information Retrieval",
      "S2CID (identifier)",
      "Western Ontario University",
      "Information retrieval",
      "National University of Singapore",
      "User interface",
      "University of Washington",
      "University of London",
      "Theory",
      "Dhirubhai Ambani Institute of Information and Communication Technology",
      "Display device",
      "Free University of Berlin",
      "Association for Computing Machinery",
      "Rutgers University",
      "Philology",
      "Document",
      "School of Communication and Information (Rutgers University)",
      "Award of Merit - Association for Information Science and Technology",
      "Browsing",
      "Cognitive model",
      "Gerard Salton Award",
      "Communication",
      "Library Science",
      "Message"
    ]
  },
  "Citation index": {
    "url": "https://en.wikipedia.org/wiki/Citation_index",
    "title": "Citation index",
    "content": "A citation index is a kind of bibliographic index , an index of citations between publications, allowing the user to easily establish which later documents cite which earlier documents. [ 1 ] A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by citators such as Shepard's Citations (1873). In 1961, Eugene Garfield 's Institute for Scientific Information (ISI) introduced the first citation index for papers published in academic journals , first the Science Citation Index (SCI), and later the Social Sciences Citation Index (SSCI) and the Arts and Humanities Citation Index (AHCI). American Chemical Society converted its printed Chemical Abstract Service (established in 1907) into internet-accessible SciFinder in 2008. The first automated citation indexing [ 2 ] was done by CiteSeer in 1997 and was patented. [ 3 ] Other sources for such data include Google Scholar , Microsoft Academic , Elsevier's Scopus , and the National Institutes of Health 's iCite (for scientific sources) [ 4 ] and Think Tank Alert (for measuring backlinks across policy-oriented think tanks ). The earliest known citation index is an index of biblical citations in rabbinic literature , the Mafteah ha-Derashot , attributed to Maimonides and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study. The Talmudic citation index En Mishpat (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century Shepard's Citations . [ 5 ] [ 6 ] Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed. In English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with Raymond's Reports (1743) and followed by Douglas's Reports (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision. [ 7 ] These early tables of legal citations (\"citators\") were followed by a more complete, book length index, Labatt's Table of Cases...California... (1860) and in 1872 by Wait's Table of Cases...New York... . The most important and best-known citation index for legal cases was released in 1873 with the publication of Shepard's Citations . [ 7 ] William Adair, a former president of Shepard's Citations , suggested in 1920 that citation indexes could serve as a tool for tracking science and engineering literature. [ 8 ] After learning that Eugene Garfield held a similar opinion, Adair corresponded with Garfield in 1953. [ 9 ] The correspondence prompted Garfield to examine Shepard's Citations index as a model that could be extended to the sciences. Two years later Garfield published \"Citation indexes for science\" in the journal Science . [ 10 ] In 1959, Garfield started a consulting business, the Institute for Scientific Information (ISI), in Philadelphia and began a correspondence with Joshua Lederberg about the idea. [ 8 ] In 1961 Garfield received a grant from the U.S. National Institutes of Health to compile a citation index for Genetics. To do so, Garfield's team gathered 1.4 million citations from 613 journals. [ 9 ] From this work, Garfield and the ISI produced the first version of the Science Citation Index , published as a book in 1963. [ 11 ] General-purpose, subscription-based academic citation indexes include: Each of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They are not open-access and differ widely in cost: Web of Science and Scopus are available by subscription (generally to libraries). CiteSeer and Google Scholar are freely available online. Several open-access, subject-specific citation indexing services also exist, such as: Clarivate Analytics ' Web of Science (WoS) and Elsevier's Scopus databases are synonymous with data on international research, and considered as the two most trusted or authoritative sources of bibliometric data for peer-reviewed global research knowledge across disciplines. [ 13 ] [ 14 ] [ 15 ] [ 16 ] [ 17 ] [ 18 ] They are both also used widely for the purposes of researcher evaluation and promotion, institutional impact (for example the role of WoS in the UK Research Excellence Framework 2021 [ note 1 ] ), and international league tables (Bibliographic data from Scopus represents more than 36% of assessment criteria in the THE rankings [ note 2 ] ). But while these databases are generally agreed to contain rigorously-assessed, high quality research, they do not represent the sum of current global research knowledge. [ 19 ] It is often mentioned in popular science articles that the research output of countries in South America, Asia, and Africa are disappointingly low. Sub-Saharan Africa is cited as an example for having \"13.5% of the global population but less than 1% of global research output\". [ note 3 ] This fact is based on data from a World Bank/Elsevier report from 2012 which relies on data from Scopus. [ note 4 ] Research outputs in this context refers to papers specifically published in peer-reviewed journals that are indexed in Scopus. Similarly, many others have analysed putatively global or international collaborations and mobility using the even more selective WoS database. [ 20 ] [ 21 ] [ 22 ] Research outputs in this context refers to papers specifically published in peer-reviewed journals that are indexed either in Scopus or WoS. Both WoS and Scopus are considered highly selective. Both are commercial enterprises, whose standards and assessment criteria are mostly controlled by panels in North America and Western Europe. The same is true for more comprehensive databases such as Ulrich's Web which lists as many as 70,000 journals, [ 23 ] while Scopus has fewer than 50% of these, and WoS has fewer than 25%. [ 13 ] While Scopus is larger and geographically broader than WoS, it still only covers a fraction of journal publishing outside North America and Europe. For example, it reports a coverage of over 2,000 journals in Asia (\"230% more than the nearest competitor\"), [ note 5 ] which may seem impressive until you consider that in Indonesia alone there are more than 7,000 journals listed on the government's Garuda portal [ note 6 ] (of which more than 1,300 are currently listed on DOAJ); [ note 7 ] whilst at least 2,500 Japanese journals listed on the J-Stage platform. [ note 8 ] Similarly, Scopus claims to have about 700 journals listed from Latin America, in comparison with SciELO's 1,285 active journal count; [ note 9 ] but that is just the tip of the iceberg judging by the 1,300+ DOAJ-listed journals in Brazil alone. [ note 10 ] Furthermore, the editorial boards of the journals contained in Wos and Scopus databases are integrated by researchers from western Europe and North America. For example, in the journal Human Geography , 41% of editorial board members are from the United States, and 37.8% from the UK. [ 24 ] Similarly, [ 25 ] ) studied ten leading marketing journals in WoS and Scopus databases, and concluded that 85.3% of their editorial board members are based in the United States. It comes as no surprise that the research that gets published in these journals is the one that fits the editorial boards' world view. [ 25 ] Comparison with subject-specific indexes has further revealed the geographical and topic bias – for example Ciarli [ 26 ] found that by comparing the coverage of rice research in CAB Abstracts (an agriculture and global health database) with WoS and Scopus, the latter \"may strongly under-represent the scientific production by developing countries, and over-represent that by industrialised countries\", and this is likely to apply to other fields of agriculture. This under-representation of applied research in Africa, Asia, and South America may have an additional negative effect on framing research strategies and policy development in these countries. [ 27 ] The overpromotion of these databases diminishes the important role of \"local\" and \"regional\" journals for researchers who want to publish and read locally-relevant content. Some researchers deliberately bypass \"high impact\" journals when they want to publish locally useful or important research in favour of outlets that will reach their key audience quicker, and in other cases to be able to publish in their native language. [ 28 ] [ 29 ] [ 30 ] Furthermore, the odds are stacked against researchers for whom English is a foreign language. 95% of WoS journals are English [ 31 ] [ 32 ] consider the use of English language a hegemonic and unreflective linguistic practice. The consequences include that non-native speakers spend part of their budget on translation and correction and invest a significant amount of time and effort on subsequent corrections, making publishing in English a burden. [ 33 ] [ 34 ] A far-reaching consequence of the use of English as the lingua franca of science is in knowledge production, because its use benefits \"worldviews, social, cultural, and political interests of the English-speaking center\" ( [ 32 ] p. 123). The small proportion of research from South East Asia, Africa, and Latin America which makes it into WoS and Scopus journals is not attributable to a lack of effort or quality of research; but due to hidden and invisible epistemic and structural barriers (Chan 2019 [ note 11 ] ). These are a reflection of \"deeper historical and structural power that had positioned former colonial masters as the centers of knowledge production, while relegating former colonies to peripheral roles\" (Chan 2018 [ note 12 ] ). Many North American and European journals demonstrate conscious and unconscious bias against researchers from other parts of the world. [ note 13 ] Many of these journals call themselves \"international\" but represent interests, authors, and even references only in their own languages. [ note 14 ] [ 35 ] Therefore, researchers in non-European or North American countries commonly get rejected because their research is said to be \"not internationally significant\" or only of \"local interest\" (the wrong \"local\"). This reflects the current concept of \"international\" as limited to a Euro/Anglophone-centric way of knowledge production. [ 36 ] [ 31 ] In other words, \"the ongoing internationalisation has not meant academic interaction and exchange of knowledge, but the dominance of the leading Anglophone journals in which international debates occurs and gains recognition\". [ 37 ] Clarivate Analytics have made some positive steps to broaden the scope of WoS, integrating the SciELO citation index – a move not without criticism [ note 15 ] – and through the creation of the Emerging Sources Index (ESI), which has allowed database access to many more international titles. However, there is still a lot of work to be done to recognise and amplify the growing body of research literature generated by those outside North America and Europe. The Royal Society have previously identified that \"traditional metrics do not fully capture the dynamics of the emerging global science landscape\", and that academia needs to develop more sophisticated data and impact measures to provide a richer understanding of the global scientific knowledge that is available to us. [ 38 ] Academia has not yet built digital infrastructures which are equal, comprehensive, multi-lingual and allows fair participation in knowledge creation. [ 39 ] One way to bridge this gap is with discipline- and region-specific preprint repositories such as AfricArXiv and InarXiv . Open access advocates recommend to remain critical of those \"global\" research databases that have been built in Europe or Northern America and be wary of those who celebrate these products act as a representation of the global sum of human scholarly knowledge. Finally, let us also be aware of the geopolitical impact that such systematic discrimination has on knowledge production, and the inclusion and representation of marginalised research demographics within the global research landscape. [ 19 ]",
    "links": [
      "Cassidy Sugimoto",
      "AfricArXiv",
      "Doi (identifier)",
      "Clarivate Analytics",
      "Philadelphia",
      "S2CID (identifier)",
      "Citation cartel",
      "Citator",
      "Maimonides",
      "SCImago Journal Rank",
      "Hdl (identifier)",
      "Arts and Humanities Citation Index",
      "Scientometrics",
      "H-index",
      "Acknowledgment index",
      "Microsoft Academic",
      "Academic journal publishing reform",
      "Composite index (metrics)",
      "Elsevier",
      "The Lens",
      "Microsoft Academic Search",
      "Web of Science",
      "Article-level metrics",
      "Index Copernicus",
      "Bibcode (identifier)",
      "Korea Citation Index",
      "Redalyc",
      "G-index",
      "Eigenfactor",
      "Emerging Sources Citation Index (ESCI)",
      "Co-citation Proximity Analysis",
      "PMID (identifier)",
      "Rabbinic literature",
      "Academic journal",
      "Social sciences",
      "Citation dynamics",
      "Science (journal)",
      "Science Citation Index Expanded",
      "San Francisco Declaration on Research Assessment",
      "Science-wide author databases of standardized citation indicators",
      "National Institutes of Health",
      "ArXiv (identifier)",
      "Eugene Garfield",
      "Journal Citation Reports",
      "SciFinder",
      "Google Scholar",
      "American Chemical Society",
      "Serbian Citation Index",
      "Initiative for Open Citations",
      "Semantic Scholar",
      "Bibliographic index",
      "Science Citation Index",
      "Think tanks",
      "Altmetrics",
      "Chemical Abstract Service",
      "Erdős number",
      "Social Sciences Citation Index",
      "Citation analysis",
      "CiteSeerX",
      "Shepard's Citations",
      "Indian Citation Index",
      "TheGuardian.com",
      "Co-citation",
      "Thomson Reuters",
      "Citation impact",
      "Astrophysics Data System",
      "Scientific journal",
      "Coercive citation",
      "Chinese Science Citation Database",
      "Indexing and abstracting service",
      "Citation graph",
      "ISSN (identifier)",
      "Author-level metrics",
      "SciELO",
      "CiteScore",
      "ISBN (identifier)",
      "INSPIRE-HEP",
      "Leiden Manifesto",
      "Bibliometrics",
      "SSRN (identifier)",
      "PMC (identifier)",
      "Institute for Scientific Information",
      "CiteSeer",
      "Impact factor",
      "Citation",
      "Russian Science Citation Index",
      "Journal ranking",
      "Bibliographic coupling",
      "Scopus",
      "Kardashian index",
      "Joshua Lederberg",
      "PubMed"
    ]
  },
  "Microsoft": {
    "url": "https://en.wikipedia.org/wiki/Microsoft",
    "title": "Microsoft",
    "content": "Microsoft Corporation is an American multinational technology conglomerate headquartered in Redmond, Washington . Founded in 1975, the company became influential in the rise of personal computers through software like Windows , and has since expanded to Internet services, cloud computing , artificial intelligence , video gaming, and other fields. Often described as a Big Tech company, Microsoft is the largest software company by revenue , one of the most valuable public companies , [ a ] and one of the most valuable brands globally. Founded by Bill Gates and Paul Allen to market BASIC interpreters for the Altair 8800 , Microsoft rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Windows. During the 41 years from 1980 to 2021 Microsoft released 9 versions of MS-DOS with a median frequency of 2 years, and 13 versions of Windows with a median frequency of 3 years. The company's 1986 initial public offering (IPO) and subsequent rise in its share price created three billionaires and an estimated 12,000 millionaires among Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market. Steve Ballmer replaced Gates as CEO in 2000. He oversaw the then-largest of Microsoft's corporate acquisitions in Skype Technologies in 2011, [ 2 ] an increased focus on hardware [ 3 ] [ 4 ] that led to its first in-house PC line, the Surface , in 2012, and the formation of Microsoft Mobile through Nokia . Since Satya Nadella took over as CEO in 2014, the company has changed focus towards cloud computing, [ 5 ] [ 6 ] as well as its acquisition of LinkedIn for $26.2 billion in 2016. [ 7 ] Under Nadella's direction, the company has expanded its video gaming business to support the Xbox brand, establishing the Microsoft Gaming division in 2022 and the acquisition of Activision Blizzard for $68.7 billion in 2023. [ 8 ] Microsoft has been dominant in the IBM PC–compatible operating system market and the office software suite market since the 1990s. Its best-known software products are the Windows line of operating systems and the Microsoft Office and Microsoft 365 suite of productivity applications, which most notably include the Word word processor, Excel spreadsheet editor, and the PowerPoint presentation program. Its flagship hardware products are the Surface lineup of personal computers and Xbox video game consoles, the latter of which includes the Xbox network ; the company also provides a range of consumer Internet services such as Bing web search, the MSN web portal, the Outlook.com (Hotmail) email service and the Microsoft Store . In the enterprise and development fields, Microsoft most notably provides the Azure cloud computing platform, Microsoft SQL Server database software, and Visual Studio . [ citation needed ] In April 2019, Microsoft became the third public U.S. company to be valued at over $1 trillion . It has been criticized for its monopolistic practices , and the company's software received criticism for problems with ease of use , robustness , and security . More recently, it has been criticized for its role in providing services to Israel during the Gaza war . Childhood friends Bill Gates and Paul Allen sought to make a business using their skills in computer programming . [ 10 ] In 1972, they founded Traf-O-Data , which sold a rudimentary computer to track and analyze automobile traffic data. Gates enrolled at Harvard University while Allen pursued a degree in computer science at Washington State University , though he later dropped out to work at Honeywell . [ 11 ] The January 1975 issue of Popular Electronics featured Micro Instrumentation and Telemetry Systems 's (MITS) Altair 8800 microcomputer, [ 12 ] which inspired Allen to suggest that they could program a BASIC interpreter for the device. Gates called MITS and claimed that he had a working interpreter, and MITS requested a demonstration. Allen worked on a simulator for the Altair while Gates developed the interpreter, and it worked flawlessly when they demonstrated it to MITS in March 1975 in Albuquerque, New Mexico . MITS agreed to distribute it, marketing it as Altair BASIC . [ 9 ] : 108, 112–114 Gates and Allen established Microsoft on April 4, 1975, with Gates as CEO, [ 13 ] and Allen suggested the name \"Micro-Soft\", short for micro-computer software. [ 14 ] [ 15 ] In August 1977, the company formed an agreement with ASCII Magazine in Japan, resulting in its first international office of ASCII Microsoft . [ 16 ] Microsoft moved its headquarters to Bellevue, Washington , in January 1979. [ 13 ] Microsoft entered the operating system (OS) business in 1980 with its own version of Unix , licensed from AT&T Corporation a year before, called Xenix , [ 17 ] but it was MS-DOS that solidified the company's dominance. IBM awarded a contract to Microsoft in November 1980 to provide a version of the CP/M OS to be used in the IBM Personal Computer (IBM PC). [ 18 ] For this deal, Microsoft purchased a CP/M clone called 86-DOS from Seattle Computer Products which it branded as MS-DOS, although IBM rebranded it to IBM PC DOS . Microsoft retained ownership of MS-DOS following the release of the IBM PC in August 1981. IBM had copyrighted the IBM PC BIOS , so other companies had to reverse engineer it for non-IBM hardware to run as IBM PC compatibles , but no such restriction applied to the operating systems. The company expanded into new markets with the release of the Microsoft Mouse in 1983, as well as with a publishing division named Microsoft Press . [ 9 ] : 232 Paul Allen resigned from Microsoft in 1983 after developing Hodgkin's lymphoma . [ 19 ] Microsoft released Windows 1.0 on November 20, 1985, as a graphical extension for MS-DOS , [ 9 ] : 242–243, 246 despite having begun jointly developing OS/2 with IBM that August. [ 20 ] Microsoft moved its headquarters from Bellevue to Redmond, Washington , on February 26, 1986, and went public with an initial public offering (IPO) at the NASDAQ exchange on March 13, [ 21 ] with the resulting rise in stock making an estimated four billionaires and 12,000 millionaires from Microsoft employees. [ 22 ] Microsoft released its version of OS/2 to original equipment manufacturers (OEMs) on April 2, 1987. [ 9 ] In 1990, the Federal Trade Commission examined Microsoft for possible collusion due to the partnership with IBM, marking the beginning of more than a decade of legal clashes with the government. [ 23 ] : 243–244 Meanwhile, the company was at work on Microsoft Windows NT , which was heavily based on their copy of the OS/2 code. It shipped on July 21, 1993, with a new modular kernel and the 32-bit Win32 application programming interface (API), making it easier to port from 16-bit (MS-DOS-based) Windows. Microsoft informed IBM of Windows NT, and the OS/2 partnership deteriorated. [ 24 ] In 1990, Microsoft introduced the Microsoft Office suite which bundled separate applications such as Microsoft Word and Microsoft Excel . [ 9 ] : 301 On May 22, Microsoft launched Windows 3.0 , featuring streamlined user interface graphics and improved protected mode capability for the Intel 386 processor, [ 25 ] and both Office and Windows became dominant in their respective areas. [ 26 ] [ 27 ] On July 27, 1994, the Department of Justice's Antitrust Division filed a competitive impact statement that said: \"Beginning in 1988 and continuing until July 15, 1994, Microsoft induced many OEMs to execute anti-competitive per processor licenses. Under a per-processor license, an OEM pays Microsoft a royalty for each computer it sells containing a particular microprocessor, whether the OEM sells the computer with a Microsoft operating system or a non-Microsoft operating system. In effect, the royalty payment to Microsoft when no Microsoft product is being used acts as a penalty, or tax, on the OEM's use of a competing PC operating system. Since 1988, Microsoft's use of per processor licenses has increased.\" [ 28 ] Following Bill Gates's internal \"Internet Tidal Wave memo\" on May 26, 1995, Microsoft began to redefine its offerings and expand its product line into computer networking and the World Wide Web . [ 29 ] With a few exceptions of new companies, like Netscape , Microsoft was the only major and established company that acted fast enough to be a part of the World Wide Web practically from the start. Other companies like Borland , WordPerfect , Novell , IBM and Lotus , being much slower to adapt to the new situation, would give Microsoft market dominance. [ 30 ] The company released Windows 95 on August 24, 1995, featuring pre-emptive multitasking , a completely new user interface with a novel start button , and 32-bit compatibility; similar to NT, it provided the Win32 API. [ 31 ] [ 32 ] : 20 Windows 95 came bundled with the online service MSN , which was intended to be a competitor to services such as CompuServe and AOL . The web browser Internet Explorer was not bundled with the retail release of Windows 95, and was instead included in the later Microsoft Plus! pack, as well as OEM releases of Windows 95. [ 33 ] Backed by a high-profile marketing campaign [ 34 ] and what The New York Times called \"the splashiest, most frenzied, most expensive introduction of a computer product in the industry's history,\" [ 35 ] Windows 95 quickly became a success. [ 36 ] Branching out into new markets in 1996, Microsoft and General Electric 's NBC unit created a new 24/7 cable news channel, MSNBC . [ 37 ] Microsoft created Windows CE 1.0 , a new OS designed for devices with low memory and other constraints, such as personal digital assistants . [ 38 ] In October 1997, the Justice Department filed a motion in the Federal District Court , stating that Microsoft violated an agreement signed in 1994 and asked the court to stop the bundling of Internet Explorer with Windows. [ 9 ] : 323–324 On January 13, 2000, Bill Gates handed over the CEO position to Steve Ballmer , an old college friend of Gates and employee of the company since 1980, while creating a new position for himself as Chief Software Architect . [ 9 ] : 111, 228 [ 13 ] On October 25, 2001, Microsoft released Windows XP , unifying the mainstream and NT lines of OS under the NT codebase. [ 39 ] The company released the Xbox later that year, entering the video game console market dominated by Sony and Nintendo . [ 40 ] In March 2004 the European Union brought antitrust legal action against the company , citing it abused its dominance with the Windows OS, resulting in a judgment of €497 million ($613 million) and requiring Microsoft to produce new versions of Windows XP without Windows Media Player : Windows XP Home Edition N and Windows XP Professional N. [ 41 ] [ 42 ] In November 2005, the company's second video game console, the Xbox 360 , was released. There were two versions, a basic version for $299.99 and a deluxe version for $399.99. [ 43 ] Increasingly present in the hardware business following Xbox, Microsoft 2006 released the Zune series of digital media players, a successor of its previous software platform Portable Media Center . Released in January 2007, the next version of Windows, Vista , focused on features, security and a redesigned user interface dubbed Aero . [ 45 ] [ 46 ] Microsoft Office 2007 , released at the same time, featured a \" Ribbon \" user interface which was a significant departure from its predecessors. Relatively strong sales of both products helped to produce a record profit in 2007. [ 47 ] The European Union imposed another fine of €899 million ($1.4 billion) for Microsoft's lack of compliance with the March 2004 judgment on February 27, 2008, saying that the company charged rivals unreasonable prices for key information about its workgroup and backoffice servers. [ 48 ] Microsoft stated that it was in compliance and that \"these fines are about the past issues that have been resolved \". [ 49 ] Gates retired from his role as Chief Software Architect on June 27, 2008, a decision announced in June 2006, while retaining other positions related to the company in addition to being an advisor for the company on key projects. [ 50 ] [ 51 ] Azure Services Platform , the company's entry into the cloud computing market for Windows, launched on October 27, 2008. [ 52 ] On February 12, 2009, Microsoft announced its intent to open a chain of Microsoft-branded retail stores, and on October 22, 2009, the first retail Microsoft Store opened in Scottsdale, Arizona ; the same day Windows 7 was officially released to the public. As the smartphone industry boomed in the late 2000s, Microsoft had struggled to keep up with its rivals in providing a modern smartphone operating system, falling behind Apple and Google -sponsored Android in the United States. [ 53 ] As a result, in 2010 Microsoft revamped its aging flagship mobile operating system, Windows Mobile , replacing it with the new Windows Phone OS that was released in October that year. [ 54 ] [ 55 ] It used a new user interface design language, codenamed \"Metro\", which prominently used simple shapes, typography, and iconography, utilizing the concept of minimalism. Microsoft implemented a new strategy for the software industry, providing a consistent user experience across all smartphones using the Windows Phone OS. It launched an alliance with Nokia in 2011 and Microsoft worked closely with the company to co-develop Windows Phone, [ 56 ] but remained partners with long-time Windows Mobile OEM HTC . [ 57 ] Microsoft is a founding member of the Open Networking Foundation started on March 23, 2011. Fellow founders were Google , HPE Networking , Yahoo! , Verizon Communications , Deutsche Telekom and 17 other companies. This nonprofit organization is focused on providing support for a cloud computing initiative called Software-Defined Networking. [ 58 ] The initiative is meant to speed innovation through simple software changes in telecommunications networks, wireless networks, data centers, and other networking areas. [ 59 ] Following the release of Windows Phone , Microsoft undertook a gradual rebranding of its product range throughout 2011 and 2012, with the corporation's logos, products, services, and websites adopting the principles and concepts of the Metro design language . [ 60 ] Microsoft unveiled Windows 8 , an operating system designed to power both personal computers and tablet computers , in Taipei in June 2011. [ 61 ] A developer preview was released on September 13, which was subsequently replaced by a consumer preview on February 29, 2012, and released to the public in May. [ 62 ] The Surface was unveiled on June 18, becoming the first computer in the company's history to have its hardware made by Microsoft. [ 63 ] [ 64 ] On June 25, Microsoft paid US$1.2 billion to buy the social network Yammer . [ 65 ] On July 31, it launched the Outlook.com webmail service to compete with Gmail . [ 66 ] On September 4, 2012, Microsoft released Windows Server 2012 . [ 67 ] In July 2012, Microsoft sold its 50% stake in MSNBC, which it had run as a joint venture with NBC since 1996. [ 68 ] On October 26, 2012, Microsoft launched Windows 8 and the Microsoft Surface . [ 64 ] [ 69 ] Three days later, Windows Phone 8 was launched. [ 70 ] To cope with the potential for an increase in demand for products and services, Microsoft opened a number of \"holiday stores\" across the U.S. to complement the increasing number of \"bricks-and-mortar\" Microsoft Stores that opened in 2012. [ 71 ] On March 29, 2013, Microsoft launched a Patent Tracker. [ 72 ] In August 2012, the New York City Police Department announced a partnership with Microsoft for the development of the Domain Awareness System which is used for police surveillance in New York City . [ 73 ] The Kinect , a motion-sensing input device made by Microsoft and designed as a video game controller , first introduced in November 2010, was upgraded for the 2013 release of the Xbox One video game console. Kinect's capabilities were revealed in May 2013: an ultra-wide 1080p camera, function in the dark due to an infrared sensor, higher-end processing power and new software, the ability to distinguish between fine movements (such as a thumb movement), and determining a user's heart rate by looking at their face. [ 74 ] Microsoft filed a patent application in 2011 that suggests that the corporation may use the Kinect camera system to monitor the behavior of television viewers as part of a plan to make the viewing experience more interactive. [ 75 ] On July 19, 2013, Microsoft stocks suffered its biggest one-day percentage sell-off since the year 2000, after its fourth-quarter report raised concerns among investors on the poor showings of both Windows 8 and the Surface tablet. Microsoft suffered a loss of more than US$32 billion. [ 76 ] In line with the maturing PC business, in July 2013, Microsoft announced that it would reorganize into four new business divisions: Operating Systems, Apps, Cloud, and Devices. All previous divisions were dissolved into new divisions without any workforce cuts. [ 77 ] On September 3, 2013, Microsoft agreed to buy Nokia 's mobile unit for $7 billion, [ 78 ] following Amy Hood taking the role of CFO. [ 79 ] On February 4, 2014, Steve Ballmer stepped down as CEO of Microsoft and was succeeded by Satya Nadella , who previously led Microsoft's Cloud and Enterprise division. [ 80 ] On the same day, John W. Thompson took on the role of chairman, in place of Bill Gates, who continued to participate as a technology advisor. [ 81 ] Thompson became the second chairman in Microsoft's history. [ 82 ] On April 25, 2014, Microsoft acquired Nokia Devices and Services for $7.2 billion. [ 83 ] This new subsidiary was renamed Microsoft Mobile Oy. [ 84 ] On September 15, 2014, Microsoft acquired the video game development company Mojang , best known for Minecraft , for $2.5 billion. [ 85 ] On June 8, 2017, Microsoft acquired Hexadite , an Israeli security firm, for $100 million. [ 86 ] [ 87 ] On January 21, 2015, Microsoft announced the release of its first interactive whiteboard , named Surface Hub . [ 88 ] On July 29, 2015, Windows 10 was released, [ 89 ] with its server sibling, Windows Server 2016 , released in September 2016. Microsoft's share of the U.S. smartphone market in January 2016 was 2.7%. [ 90 ] During the summer of 2015 the company lost $7.6 billion related to its mobile-phone business, firing 7,800 employees. [ 91 ] In 2015, the construction of a data center in Mecklenburg County, Virginia , led to the destruction of a historic African American cemetery despite archeological recommendations for preservation. [ 92 ] On March 1, 2016, Microsoft announced the merger of its PC and Xbox divisions, with Phil Spencer announcing that Universal Windows Platform (UWP) apps would be the focus for Microsoft's gaming in the future. [ 93 ] On January 24, 2017, Microsoft showcased Intune for Education at the BETT 2017 education technology conference in London . [ 94 ] Intune for Education is a new cloud-based application and device management service for the education sector. [ 95 ] In May 2016, the company announced it was laying off 1,850 workers, and taking an impairment and restructuring charge of $950 million. [ 91 ] In June 2016, Microsoft announced a project named Microsoft Azure Information Protection. It aims to help enterprises protect their data as it moves between servers and devices. [ 96 ] In November 2016, Microsoft joined the Linux Foundation as a Platinum member during Microsoft's Connect(); developer event in New York. [ 97 ] The cost of each Platinum membership is US$500,000 per year. [ 98 ] Some analysts had deemed this unthinkable ten years prior, however, as in 2001 then-CEO Steve Ballmer called Linux \"cancer\". [ 99 ] Microsoft planned to launch a preview of Intune for Education \"in the coming weeks\", with general availability scheduled for spring 2017, priced at $30 per device, or through volume licensing agreements. [ 100 ] In January 2018, Microsoft patched Windows 10 to account for CPU problems related to Intel's Meltdown security breach . The patch led to issues with the Microsoft Azure virtual machines reliant on Intel's CPU architecture. On January 12, Microsoft released PowerShell Core 6.0 for the macOS and Linux operating systems. [ 101 ] In February 2018, Microsoft ceased notification support for its Windows Phone devices which effectively ended firmware updates for the discontinued devices. [ 101 ] In March 2018, Microsoft recalled Windows 10 S to change it to a mode for the Windows operating system rather than a separate and unique operating system. In March the company also established guidelines that censor users of Office 365 from using profanity in private documents. [ 101 ] In April 2018, Microsoft released the source code for Windows File Manager under the MIT License to celebrate the program's 20th anniversary. In April the company further expressed willingness to embrace open source initiatives by announcing Azure Sphere as its own derivative of the Linux operating system. [ 101 ] In May 2018, Microsoft partnered with 17 American intelligence agencies to develop cloud computing products. The project is dubbed \"Azure Government\" and has ties to the Joint Enterprise Defense Infrastructure (JEDI) surveillance program. [ 101 ] On June 4, 2018, Microsoft officially announced the acquisition of GitHub for $7.5 billion, a deal that closed on October 26, 2018. [ 102 ] [ 103 ] On July 10, 2018, Microsoft revealed the Surface Go platform to the public. Later in the month, it converted Microsoft Teams to gratis . [ 101 ] In August 2018, Microsoft released two projects called Microsoft AccountGuard and Defending Democracy. It also unveiled Snapdragon 850 compatibility for Windows 10 on the ARM architecture . [ 104 ] [ 105 ] [ 101 ] In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for Internet of things (IoT) technologies related to water management. Developed in part by researchers from Kindai University , the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt , analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide. The specific computer programs used in the process fall under the Azure Machine Learning and the Azure IoT Hub platforms. [ 106 ] In September 2018, Microsoft discontinued Skype Classic . [ 101 ] On October 10, 2018, Microsoft joined the Open Invention Network community despite holding more than 60,000 patents. [ 107 ] In November 2018, Microsoft agreed to supply 100,000 Microsoft HoloLens headsets to the United States military in order to \"increase lethality by enhancing the ability to detect, decide and engage before the enemy.\" [ 108 ] In November 2018, Microsoft introduced Azure Multi-Factor Authentication for Microsoft Azure. [ 109 ] In December 2018, Microsoft announced Project Mu , an open source release of the Unified Extensible Firmware Interface (UEFI) core used in Microsoft Surface and Hyper-V products. The project promotes the idea of Firmware as a Service . [ 110 ] In the same month, Microsoft announced the open source implementation of Windows Forms and the Windows Presentation Foundation (WPF) which will allow for further movement of the company toward the transparent release of key frameworks used in developing Windows desktop applications and software. December also saw the company discontinue the Microsoft Edge [Legacy] browser project in favor of the \"New Edge\" browser project, featuring a Chromium based backend. [ 109 ] On February 20, 2019, Microsoft said it would offer its cyber security service AccountGuard to 12 new markets in Europe, including Germany, France and Spain, to close security gaps and protect customers in the political space from hacking. [ 111 ] In February 2019, hundreds of Microsoft employees protested the company's war profiteering from a $480 million contract to develop virtual reality headsets for the United States Army . [ 112 ] On August 5, 2020, Microsoft stopped its xCloud game streaming test for iOS devices. According to Microsoft, the future of xCloud on iOS remains unclear and potentially out of Microsoft's hands. Apple has imposed a strict limit on \" remote desktop clients \" which means applications are only allowed to connect to a user-owned host device or gaming console owned by the user. [ 113 ] On September 21, 2020, Microsoft announced its intent to acquire video game company ZeniMax Media , the parent company of Bethesda Softworks , for about $7.5 billion, with the deal expected to occur in the second half of 2021 fiscal year. [ 114 ] On March 9, 2021, the acquisition was finalized and ZeniMax Media became part of Microsoft's Xbox Game Studios division. [ 115 ] The total price of the deal was $8.1 billion. [ 116 ] On November 10, 2020, Microsoft released the Xbox Series X and Xbox Series S video game consoles. [ 117 ] In February 2021, Microsoft released Azure Quantum for public preview. [ 118 ] The public cloud computing platform provides access to quantum software and quantum hardware including trapped ion , neutral atom , and superconducting systems. [ 119 ] [ 120 ] [ 121 ] [ 122 ] In April 2021, Microsoft announced it would buy Nuance Communications for approximately $16 billion. [ 123 ] The acquisition of Nuance was completed in March 2022. [ 124 ] In 2021, in part due to the strong quarterly earnings spurred by the COVID-19 pandemic , Microsoft's valuation came to nearly $2 trillion. The increased necessity for remote work and distance education drove demand for cloud computing and grew the company's gaming sales. [ 125 ] [ 126 ] [ 127 ] On June 24, 2021, Microsoft announced Windows 11 during a livestreamed event. The announcement came with confusion after Microsoft announced Windows 10 would be the last version of the operating system. [ 128 ] It was released to the general public on October 5, 2021. [ 129 ] In September 2021, it was announced that the company had acquired Takelessons, an online platform that connects students and tutors in numerous subjects. The acquisition positioned Microsoft to grow its presence in the market of providing online education to large numbers of people. [ 130 ] In the same month, Microsoft acquired Australia-based video editing software company Clipchamp . [ 131 ] In October 2021, Microsoft announced that it began rolling out end-to-end encryption (E2EE) support for Microsoft Teams calls in order to secure business communication while using video conferencing software. Users can ensure that their calls are encrypted and can utilize a security code that both parties on a call must verify on their respective ends. [ 132 ] On October 7, Microsoft acquired Ally.io, a software service that measures companies' progress against OKRs . Microsoft plans to incorporate Ally.io into its Viva family of employee experience products. [ 133 ] On January 18, 2022, Microsoft announced the acquisition of American video game developer and holding company Activision Blizzard in an all-cash deal worth $68.7 billion. [ 134 ] Microsoft also named Phil Spencer , head of the Xbox brand since 2014, the inaugural CEO of the newly established Microsoft Gaming division, which now houses the Xbox operations team and the three publishers in the company's portfolio (Xbox Game Studios, ZeniMax Media, Activision Blizzard). Microsoft has not released statements regarding Activision's recent legal controversies regarding employee abuse, but reports have alleged that Activision CEO Bobby Kotick , a major target of the controversy, will leave the company after the acquisition is finalized. [ 135 ] The deal was closed on October 13, 2023. [ 136 ] In December 2022, Microsoft announced a new 10-year deal with the London Stock Exchange Group for products including Microsoft Azure; Microsoft acquired around 4% of LSEG as part of the deal. [ 137 ] In January 2023, CEO Satya Nadella announced Microsoft would lay off some 10,000 employees. [ 138 ] The announcement came a day after hosting a Sting concert for 50 people, including Microsoft executives, in Davos , Switzerland. [ 139 ] On January 23, 2023, Microsoft announced a new multi-year, multi-billion dollar investment deal with ChatGPT developer OpenAI . [ 140 ] In June 2023, Microsoft released Azure Quantum Elements to run molecular simulations and calculations in computational chemistry and materials science using a combination of AI, high-performance computing and quantum computing . [ 141 ] The service includes Copilot, a GPT-4 based large language model tool to query and visualize data, write code, initiate simulations, and educate researchers. [ 141 ] At a November 2023 developer conference, Microsoft announced two new custom-designed computing chips: The Maia chip, designed to run large language models, and Cobalt CPU, designed to power general cloud services on Azure. [ 142 ] [ 143 ] On November 20, 2023, Satya Nadella announced that Sam Altman , who had been ousted as CEO of OpenAI just days earlier, and Greg Brockman , who had resigned as president, would join Microsoft to lead a new advanced AI research team. [ 144 ] [ 145 ] However, the plan was short-lived, as Altman was subsequently reinstated as OpenAI's CEO and Brockman rejoined the company amid pressure from OpenAI's employees and investors on its board. [ 146 ] In March 2024, Inflection AI 's cofounders Mustafa Suleyman and Karen Simonyan announced their departure from the company in order to start Microsoft AI, with Microsoft acqui-hiring nearly the entirety of its 70-person workforce. As part of the deal, Microsoft paid Inflection $650 million to license its technology. [ 147 ] [ 148 ] In January 2024, Microsoft became the most valued publicly traded company. Meanwhile, that month, the company announced a subscription offering of artificial intelligence for small businesses via Copilot Pro. [ 149 ] [ 150 ] In June 2024, Microsoft announced it would be laying off 1,000 employees from the company's mixed reality and Azure cloud computing divisions. [ 151 ] [ 152 ] In June 2024, Microsoft announced that it was building a \"hyperscale data centre\" in South East Leeds. [ 153 ] In July 2024, it was reported that the company was laying off its diversity, equity, and inclusion (DEI) team. [ 154 ] [ 155 ] On July 19, 2024, a global IT outage impacted Microsoft services, affecting businesses, airlines, and financial institutions worldwide. The outage was traced back to a flawed update of CrowdStrike 's cybersecurity software, which resulted in Microsoft systems crashing and causing disruptions across various sectors. Despite CrowdStrike's CEO George Kurtz clarifying that the issue was not a cyberattack, the incident had widespread consequences, leading to delays in air travel, financial transactions, and medical services globally. Microsoft stated that the underlying cause had been fixed but acknowledged ongoing residual impacts on some Microsoft 365 apps and services. [ 156 ] [ 157 ] In September 2024, BlackRock and Microsoft announced a $30 billion fund, the Global AI Infrastructure Investment Partnership, to invest in AI infrastructure such as data centers and energy projects. The fund has the potential to reach $100 billion with debt financing, and partners include Abu Dhabi -backed MGX and Nvidia , which will provide AI expertise. Investments will primarily focus on the U.S., with some in partner countries. [ 158 ] Microsoft also announced relaunch of its controversial tool, Recall, in November 2024 after addressing privacy concerns. Initially criticized for taking regular screenshots without user consent, Recall was changed to an opt-in feature instead of being default on. The UK's Information Commissioner's Office monitored the situation and noted the adjustments, which included enhanced security measures like encryption and biometric access. While experts regarded these changes as improvements, they advised caution, with some recommending further testing before users opted in. [ 159 ] On February 28, 2025, Microsoft announced that Skype would be shutting down on May 5, 2025, to streamline its focus on Microsoft Teams. The company stated there would be no job cuts due to the shutdown. [ 160 ] On April 4, 2025, Microsoft celebrated its 50th anniversary . [ 161 ] On May 30, 2025, it was reported that Microsoft's Russian division would be preparing to file for bankruptcy, days after President Vladimir Putin stated that foreign services providers should be throttled in Russia to make way for domestic software, which included Microsoft. The company previously restructured operations in Russia in June 2022 after being significantly impacted by the Russian invasion of Ukraine , but stated those restructuring efforts have failed. [ 162 ] On May 23, 2025, it was reported that Europol's European Cybercrime Centre has worked with Microsoft to disrupt Lumma Stealer, a significant infostealer threat. The joint operation targeted a sophisticated ecosystem that allowed criminals to exploit stolen information on a massive scale. [ 163 ] On July 2, 2025, Microsoft announced it would cut nearly 4% of its workforce, around 9,000 jobs, to control costs amid heavy AI infrastructure spending, while also restructuring management and streamlining operations. [ 164 ] Microsoft is ranked No. 14 in the 2022 Fortune 500 rankings of the largest United States corporations by total revenue ; [ 165 ] and it was the world's largest software maker by revenue in 2022 according to Forbes Global 2000 . In 2018, Microsoft became the most valuable publicly traded company in the world, [ 166 ] a position it has repeatedly traded with Apple in the years since. [ 167 ] In April 2019, Microsoft became the third U.S. public company to be valued at over $1 trillion . [ b ] As of 2024 [update] , Microsoft has the third-highest global brand valuation . Microsoft is one of only two U.S.-based companies that have a prime credit rating of AAA. [ 168 ] The company is run by a board of directors made up of mostly company outsiders, as is customary for publicly traded companies. Members of the board of directors as of December 2023 are Satya Nadella , Reid Hoffman , Hugh Johnston, Teri List, Sandi Peterson , Penny Pritzker , Carlos Rodriguez, Charles Scharf , John W. Stanton , John W. Thompson , Emma Walmsley and Padmasree Warrior . [ 169 ] Board members are elected every year at the annual shareholders' meeting using a majority vote system. There are four committees within the board that oversee more specific matters. These committees include the Audit Committee, which handles accounting issues with the company including auditing and reporting; the Compensation Committee, which approves compensation for the CEO and other employees of the company; the Governance and Nominating Committee, which handles various corporate matters including the nomination of the board; and the Regulatory and Public Policy Committee, which includes legal/antitrust matters, along with privacy, trade, digital safety, artificial intelligence, and environmental sustainability. [ 170 ] On March 13, 2020, Gates announced that he is leaving the board of directors of Microsoft and Berkshire Hathaway to focus more on his philanthropic efforts. According to Aaron Tilley of The Wall Street Journal this is \"marking the biggest boardroom departure in the tech industry since the death of longtime rival and Apple Inc. co-founder Steve Jobs .\" [ 171 ] On January 13, 2022, The Wall Street Journal reported that Microsoft's board of directors plans to hire an external law firm to review its sexual harassment and gender discrimination policies, and to release a summary of how the company handled past allegations of misconduct against Bill Gates and other corporate executives. [ 172 ] When Microsoft went public and launched its initial public offering (IPO) in 1986, the opening stock price was $21; after the trading day , the price closed at $27.75. As of July 2010, with the company's nine stock splits , any IPO shares would be multiplied by 288; if one were to buy the IPO today, given the splits and other factors, it would cost about 9 cents. [ 9 ] : 235–236 [ 174 ] [ 175 ] The stock price peaked in 1999 at around $119 ($60.928, adjusting for splits). [ 176 ] The company began to offer a dividend on January 16, 2003, starting at eight cents per share for the fiscal year followed by a dividend of sixteen cents per share the subsequent year, switching from yearly to quarterly dividends in 2005 with eight cents a share per quarter and a special one-time payout of three dollars per share for the second quarter of the fiscal year. [ 176 ] [ 177 ] Though the company had subsequent increases in dividend payouts, the price of Microsoft's stock remained steady for years. [ 177 ] [ 178 ] Standard & Poor's and Moody's Investors Service have both given a AAA rating to Microsoft, whose assets were valued at $41 billion as compared to only $8.5 billion in unsecured debt. Consequently, in February 2011 Microsoft released a corporate bond amounting to $2.25 billion with relatively low borrowing rates compared to government bonds . [ 179 ] For the first time in 20 years Apple Inc. surpassed Microsoft in Q1 2011 quarterly profits and revenues due to a slowdown in PC sales and continuing huge losses in Microsoft's Online Services Division (which contains its search engine Bing ). Microsoft profits were $5.2 billion, while Apple Inc. profits were $6 billion, on revenues of $14.5 billion and $24.7 billion respectively. [ 180 ] Microsoft's Online Services Division has been continuously loss-making since 2006 and in Q1 2011 it lost $726 million. This follows a loss of $2.5 billion for the year 2010. [ 181 ] On July 20, 2012, Microsoft posted its first quarterly loss ever, despite earning record revenues for the quarter and fiscal year, with a net loss of $492 million due to a writedown related to the advertising company aQuantive , which had been acquired for $6.2 billion back in 2007. [ 183 ] As of January 2014, Microsoft's market capitalization stood at $314B, [ 184 ] making it the 8th-largest company in the world by market capitalization. [ 185 ] On November 14, 2014, Microsoft overtook ExxonMobil to become the second most-valuable company by market capitalization, behind only Apple Inc. Its total market value was over $410B—with the stock price hitting $50.04 a share, the highest since early 2000. [ 186 ] In 2015, Reuters reported that Microsoft had earnings abroad of $76.4 billion which were untaxed by the Internal Revenue Service . Under U.S. law, corporations do not pay income tax on overseas profits until the profits are brought into the United States. [ 187 ] The key trends of Microsoft are (as at the financial year ending June 30): [ 188 ] [ 189 ] In November 2018, the company won a $480 million military contract with the U.S. government to bring augmented reality (AR) headset technology into the weapon repertoires of American soldiers. The two-year contract may result in follow-on orders of more than 100,000 headsets, according to documentation describing the bidding process. One of the contract's tag lines for the augmented reality technology seems to be its ability to enable \"25 bloodless battles before the 1st battle\", suggesting that actual combat training is going to be an essential aspect of the augmented reality headset capabilities. [ 192 ] Microsoft is an international business. As such, it needs subsidiaries present in whatever national markets it chooses to harvest. An example is Microsoft Canada, which it established in 1985. [ 193 ] Other countries have similar installations, to funnel profits back up to Redmond and to distribute the dividends to the holders of MSFT stock. The 10 largest shareholders of Microsoft in early 2024 were: [ 182 ] [ 194 ] In 2004, Microsoft commissioned research firms to do independent studies comparing the total cost of ownership (TCO) of Windows Server 2003 to Linux ; the firms concluded that companies found Windows easier to administrate than Linux, thus those using Windows would administrate faster resulting in lower costs for their company (i.e. lower TCO). [ 195 ] This spurred a wave of related studies; a study by the Yankee Group concluded that upgrading from one version of Windows Server to another costs a fraction of the switching costs from Windows Server to Linux, although companies surveyed noted the increased security and reliability of Linux servers and concern about being locked into using Microsoft products. [ 196 ] Another study, released by the Open Source Development Labs , claimed that the Microsoft studies were \"simply outdated and one-sided\" and their survey concluded that the TCO of Linux was lower due to Linux administrators managing more servers on average and other reasons. [ 197 ] In July 2014, Microsoft announced plans to lay off 18,000 employees. Microsoft employed 127,104 people as of June 5, 2014, making this about a 14 percent reduction of its workforce as the biggest Microsoft layoff ever. This included 12,500 professional and factory personnel. Previously, Microsoft had eliminated 5,800 jobs in 2009 in line with the Great Recession of 2008–2017. [ 198 ] [ 199 ] In September 2014, Microsoft laid off 2,100 people, including 747 people in the Seattle–Redmond area, where the company is headquartered. The firings came as a second wave of the layoffs that were previously announced. This brought the total number to over 15,000 out of the 18,000 expected cuts. [ 200 ] In October 2014, Microsoft revealed that it was almost done with eliminating 18,000 employees, which was its largest-ever layoff sweep. [ 201 ] In July 2015, Microsoft announced another 7,800 job cuts in the next several months. [ 202 ] In May 2016, Microsoft announced another 1,850 job cuts mostly in its Nokia mobile phone division. As a result, the company will record an impairment and restructuring charge of approximately $950 million, of which approximately $200 million will relate to severance payments. [ 203 ] Microsoft laid off 1,900 employees in its gaming division in January 2024. The layoffs primarily affected Activision Blizzard employees, but some Xbox and ZeniMax employees were also affected. [ 204 ] Blizzard president Mike Ybarra and chief design officer Allen Adham also resigned. [ 205 ] In May 2025, Microsoft announced that it is laying off more than 6,000 employees, around three percent of the company's entire workforce. [ 206 ] In July 2025, Microsoft announced another round of layoffs, cutting approximately 9,000 employees in its largest workforce reduction in over two years. The cuts affected multiple divisions, including Xbox , with 830 positions eliminated at its Redmond, Washington headquarters. [ 207 ] [ 208 ] Microsoft recognizes seven trade unions [ c ] representing 1,750 workers in the United States at its video game subsidiaries Activision Blizzard and ZeniMax Media . [ 209 ] U.S. workers have been vocal in opposing military and law-enforcement contracts with Microsoft. [ 210 ] Bethesda Game Studios is unionized in Canada . [ 211 ] Microsoft South Korea has recognized its union since 2017. [ 212 ] [ 213 ] German employees have elected works councils since 1998. [ 214 ] Microsoft provides information about reported bugs in its software to intelligence agencies of the United States government, prior to the public release of the fix. A Microsoft spokesperson stated that the corporation runs several programs that facilitate the sharing of such information with the U.S. government. [ 215 ] Following media reports about PRISM , NSA's massive electronic surveillance program , in May 2013, several technology companies were identified as participants, including Microsoft. [ 216 ] According to leaks of said program, Microsoft joined the PRISM program in 2007. [ 217 ] However, in June 2013, an official statement from Microsoft flatly denied its participation in the program: \"We provide customer data only when we receive a legally binding order or subpoena to do so, and never on a voluntary basis. In addition, we only ever comply with orders for requests about specific accounts or identifiers. If the government has a broader voluntary national security program to gather customer data, we don't participate in it.\" [ 218 ] During the first six months of 2013, Microsoft received requests that affected between 15,000 and 15,999 accounts. [ 219 ] In December 2013, the company made a statement to further emphasize that it takes its customers' privacy and data protection very seriously, saying that \"government snooping potentially now constitutes an ' advanced persistent threat ,' alongside sophisticated malware and cyber attacks\". [ 220 ] The statement also marked the beginning of three-part program to enhance Microsoft's encryption and transparency efforts. On July 1, 2014, as part of this program, it opened the first (of many) Microsoft Transparency Center, which provides \"participating governments with the ability to review source code for our key products, assure themselves of their software integrity, and confirm there are no \" back doors .\" [ 221 ] Microsoft has also argued that the United States Congress should enact strong privacy regulations to protect consumer data. [ 222 ] In April 2016, the company sued the U.S. government , argued that secrecy orders were preventing the company from disclosing warrants to customers in violation of the company's and customers' rights. Microsoft argued that it was unconstitutional for the government to indefinitely ban Microsoft from informing its users that the government was requesting their emails and other documents and that the Fourth Amendment made it so people or businesses had the right to know if the government searches or seizes their property. On October 23, 2017, Microsoft said it would drop the lawsuit as a result of a policy change by the United States Department of Justice (DoJ). The DoJ had \"changed data request rules on alerting the Internet users about agencies accessing their information.\" In 2022 Microsoft shared a $9 billion contract from the United States Department of Defense for cloud computing with Amazon, Google, and Oracle. [ 223 ] On a Friday afternoon in January 2024, Microsoft disclosed that a Russian state-sponsored group hacked into its corporate systems. The group, accessed \"a very small percentage\" of Microsoft corporate email accounts, which also included members of its senior leadership team and employees in its cybersecurity and legal teams. [ 224 ] Microsoft noted in a blog post that the attack might have been prevented if the accounts in question had enabled multi-factor authentication , a defensive measure which is widely recommended in the industry, including by Microsoft itself. [ 225 ] [T]he Microsoft method. Understand the market, and the customers, and then go pedal to the metal, with release after release focused on what the customers need, incorporating their feedback. That puts the competition into reaction mode. And of course it helps if they also make a strategic error because they are under so much pressure. — Chris Pratley of Microsoft, 2004 [ 226 ] Technical references for developers and articles for various Microsoft magazines such as Microsoft Systems Journal ( MSJ ) are available through the Microsoft Developer Network (MSDN). MSDN also offers subscriptions for companies and individuals, and the more expensive subscriptions usually offer access to pre-release beta versions of Microsoft software. [ 227 ] [ 228 ] In April 2004, Microsoft launched a community site for developers and users, titled Channel 9 , that provides a wiki and an Internet forum . [ 229 ] Another community site that provides daily videocasts and other services, On10.net, launched on March 3, 2006. [ 230 ] Free technical support is traditionally provided through online Usenet newsgroups, and CompuServe in the past, monitored by Microsoft employees; there can be several newsgroups for a single product. Helpful people can be elected by peers or Microsoft employees for Microsoft Most Valuable Professional (MVP) status, which entitles them to a sort of special social status and possibilities for awards and other benefits. [ 231 ] Noted for its internal lexicon , the expression \" eating your own dog food \" is used to describe the policy of using pre-release and beta versions of products inside Microsoft to test them in \"real-world\" situations. [ 232 ] This is usually shortened to just \"dog food\" and is used as a noun, verb, and adjective. Another bit of jargon , FYIFV or FYIV (\" Fuck You, I'm [Fully] Vested\"), is used by an employee to indicate they are financially independent and can avoid work anytime they wish. [ 233 ] Microsoft is an outspoken opponent of the cap on H-1B visas , which allows companies in the U.S. to employ certain foreign workers. Bill Gates claims the cap on H1B visas makes it difficult to hire employees for the company, stating \"I'd certainly get rid of the H1B cap\" in 2005. [ 234 ] Critics of H1B visas argue that relaxing the limits would result in increased unemployment for U.S. citizens due to H1B workers working for lower salaries. [ 235 ] The Human Rights Campaign Corporate Equality Index, a report of how progressive the organization deems company policies towards LGBT employees, rated Microsoft as 87% from 2002 to 2004 and as 100% from 2005 to 2010 after it allowed gender expression. [ 236 ] In August 2018, Microsoft implemented a policy for all companies providing subcontractors to require 12 weeks of paid parental leave to each employee. This expands on the former requirement from 2015 requiring 15 days of paid vacation and sick leave each year. [ 237 ] In 2015, Microsoft established its own parental leave policy to allow 12 weeks off for parental leave with an additional 8 weeks for the parent who gave birth. [ 238 ] In 2011, Greenpeace released a report rating the top ten big brands in cloud computing on the sources of electricity for their data centers . At the time, data centers consumed up to 2% of all global electricity, and this amount was projected to increase. Phil Radford of Greenpeace said, \"We are concerned that this new explosion in electricity use could lock us into old, polluting energy sources instead of the clean energy available today\", [ 239 ] and called on \"Amazon, Microsoft and other leaders of the information-technology industry must embrace clean energy to power their cloud-based data centers\". [ 240 ] In 2013, Microsoft agreed to buy power generated by a Texas wind project to power one of its data centers. [ 241 ] Microsoft is ranked on the 17th place in Greenpeace's Guide to Greener Electronics (16th Edition) that ranks 18 electronics manufacturers according to its policies on toxic chemicals, recycling, and climate change. [ 242 ] Microsoft's timeline for phasing out brominated flame retardant (BFRs) and phthalates in all products was 2012 but its commitment to phasing out PVC is not clear. As of January 2011, [update] it has no products that are completely free from PVC and BFRs. [ 243 ] [ needs update ] Microsoft's main U.S. campus received a silver certification from the Leadership in Energy and Environmental Design (LEED) program in 2008, and it installed over 2,000 solar panels on top of its buildings at its Silicon Valley campus, generating approximately 15 percent of the total energy needed by the facilities in April 2005. [ 244 ] Microsoft makes use of alternative forms of transit. It created one of the world's largest private bus systems, the \"Connector\", to transport people from outside the company; for on-campus transportation, the \"Shuttle Connect\" uses a large fleet of hybrid cars to save fuel. The \"Connector\" does not compete with the public bus system and works with it to provide a cohesive transportation network not just for its employees but also for the public. [ 245 ] Microsoft also subsidizes regional public transport , provided by Sound Transit and King County Metro , as an incentive. [ 244 ] [ 246 ] In February 2010, however, Microsoft took a stance against adding additional public transport and high-occupancy vehicle (HOV) lanes to the State Route 520 and its floating bridge connecting Redmond to Seattle; the company did not want to delay the construction any further. [ 247 ] Microsoft was ranked number 1 in the list of the World's Best Multinational Workplaces by the Great Place to Work Institute in 2011. [ 248 ] In January 2020, the company announced a strategy to take the company carbon negative by 2030 and to remove all carbon that it has emitted since its foundation in 1975. [ 249 ] [ 250 ] [ 251 ] On October 9, 2020, Microsoft permanently allowed remote work . [ 252 ] In January 2021, the company announced on Twitter to join the Climate Neutral Data Centre Pact , which engages the cloud infrastructure and data centers industries to reach carbon neutrality in Europe by 2030, and also disclosed an investment in Climeworks , a direct air capture company partnered with Carbfix for carbon sequestration . [ list 1 ] In the same year, it was awarded the EPA's Green Power Leadership Award, citing the company's all-renewable energy use since 2014. [ 258 ] In September 2023, Microsoft announced that it purchased $200 million in carbon credits to offset 315,000 metric tons of carbon dioxide over 10 years from Heirloom Carbon, a carbon removal company that mixes calcium oxide from heated crushed limestone with water to form carbon hydroxide to absorb carbon dioxide from the atmosphere to mineralize back into limestone while the released carbon dioxide is stored underground or injected into concrete . [ 259 ] [ 260 ] Despite spending spent more than $760 million through its Climate Innovation Fund by June 2024 on sustainability projects—including purchases of more than 5 million metric tonnes of carbon dioxide removal with carbon offsets and more than 34 megawatts of renewable energy—Microsoft's Scope 3 emissions had increased by 31% from the company's 2020 baseline, which caused the company's total emissions to rise by 29% in 2023. [ 261 ] In 2023 Microsoft consumed 24 TWh of electricity, more than countries such as Iceland, Ghana, the Dominican Republic, or Tunisia. [ 262 ] The corporate headquarters, informally known as the Microsoft Redmond campus , is located at One Microsoft Way in Redmond, Washington. [ 263 ] Microsoft initially moved onto the grounds of the campus on February 26, 1986, weeks before the company went public on March 13. The headquarters has since experienced multiple expansions since its establishment. It is estimated to encompass over 8 million ft 2 (750,000 m 2 ) of office space and 30,000–40,000 employees. [ 264 ] Additional offices are located in Bellevue and Issaquah, Washington (90,000 employees worldwide). The company is planning to upgrade its Mountain View, California, campus on a grand scale. The company has occupied this campus since 1981. In 2016, the company bought the 32-acre (13 ha) campus, with plans to renovate and expand it by 25%. [ 265 ] Microsoft operates an East Coast headquarters in Charlotte, North Carolina . [ 266 ] In April 2024, it was announced that Microsoft would be opening a state-of-the-art artificial intelligence 'hub' around Paddington in London, England. It was announced that the division would be led by Jordan Hoffman, who previously worked for Deepmind and Inflection . [ 267 ] On October 26, 2015, the company opened its retail location on Fifth Avenue in New York City. The location features a five-story glass storefront and is 22,270 square feet. [ 268 ] As per company executives, Microsoft had been on the lookout for a flagship location since 2009. [ 269 ] The company's retail locations are part of a greater strategy to help build a connection with its consumers. The opening of the store coincided with the launch of the Surface Book and Surface Pro 4. [ 270 ] On November 12, 2015, Microsoft opened a second flagship store, located in Sydney's Pitt Street Mall. [ 271 ] Microsoft adopted the so-called \" Pac-Man Logo\", designed by Scott Baker, on February 26, 1987, with the concept being similar to InFocus Corporation logo that was adapted a year earlier in 1986. Baker stated \"The new logo, in Helvetica italic typeface, has a slash between the o and s to emphasize the \"soft\" part of the name and convey motion and speed\". [ 272 ] Dave Norris ran an internal joke campaign to save the old logo, which was green, in all uppercase, and featured a fanciful letter O , nicknamed the blibbet , but it was discarded. [ 273 ] Microsoft's logo with the tagline \"Your potential. Our passion.\"—below the main corporate name—is based on a slogan Microsoft used in 2008. In 2002, the company started using the logo in the United States and eventually started a television campaign with the slogan, changed from the previous tagline of \" Where do you want to go today? \" [ 274 ] [ 275 ] [ 276 ] During the private MGX (Microsoft Global Exchange) conference in 2010, Microsoft unveiled the company's next tagline, \"Be What's Next.\" [ 277 ] It also had a slogan/tagline \"Making it all make sense.\" [ 278 ] The Microsoft Pac-Man logo was used for 25 years, 5 months, and 28 days until August 23, 2012, being the longest enduring logo to be used by the company. On August 23, 2012, Microsoft unveiled a new corporate logo at the opening of its 23rd Microsoft store in Boston, indicating the company's shift of focus from the classic style to the tile-centric modern interface, which it uses/will use on the Windows Phone platform, Xbox 360, Windows 8 and the upcoming Office Suites. [ 279 ] The new logo also includes four squares with the colors of the then-current Windows logo which have been used to represent Microsoft's four major products: Windows (blue), Office (orange), Xbox (green) and Bing (yellow). [ 280 ] The logo also resembles the opening of one of the commercials for Windows 95 . [ 281 ] [ 282 ] The company was the official jersey sponsor of Finland's national basketball team at EuroBasket 2015 , [ 284 ] a major sponsor of the Toyota Gazoo Racing WRT ( 2017 – 2020 ) and a sponsor of the Renault F1 Team ( 2016 – 2020 ). In 2025, Microsoft was one of the donors who funded the demolition of the East Wing of the White House and planned building of a ballroom. [ 285 ] In 2015, Microsoft Philanthropies, an internal charitable organization, was established to bring the benefits of technology and the digital revolution to areas and groups that lack them. The organisation's key areas of focus are: donating cloud computing resources to university researchers and nonprofit groups; supporting the expansion of broadband access worldwide; funding international computer science education through YouthSpark; supporting tech education in the U.S. from kindergarten to high school; and donating to global child and refugee relief organizations. [ 286 ] [ 287 ] During the COVID-19 pandemic , Microsoft's president, Brad Smith , announced that it had donated an initial batch of supplies, including 15,000 protection goggles, infrared thermometers , medical caps, and protective suits, to healthcare workers in Seattle, with further aid to come. [ 288 ] During the Russian invasion of Ukraine Microsoft started monitoring cyberattacks originating from the Government of Russia and Russia-backed hackers. In June 2022, Microsoft published the report on Russian cyber attacks and concluded that state-backed Russian hackers \"have engaged in \"strategic espionage\" against governments, think tanks , businesses and aid groups \" in 42 countries supporting Kyiv . [ 289 ] [ 290 ] Microsoft also supports initiatives through its AI for Accessibility grant program, providing funding to various global organizations that create technologies to enhance accessibility for individuals with disabilities. Among grant recipients from the Asia-Pacific region are the Sri Lankan IT company Fortude, the Thailand-based Vulcan Coalition, and the Indonesian organization Kerjabilitas. [ 291 ] Criticism of Microsoft has followed various aspects of its products and business practices. Frequently criticized are the ease of use , robustness , and security of the company's software. It has also been criticized for the use of permatemp employees (employees employed for years as \"temporary\", and therefore without medical benefits), the use of forced retention tactics, which means that employees would be sued if they tried to leave. [ 293 ] Historically, Microsoft has also been accused of overworking employees, in many cases, leading to burnout within just a few years of joining the company. The company is often referred to as a \"Velvet Sweatshop\", a term which originated in a 1989 Seattle Times article, [ 294 ] and later became used to describe the company by some of Microsoft's own employees. [ 295 ] This characterization is derived from the perception that Microsoft provides nearly everything for its employees in a convenient place, but in turn overworks them to a point where it would be bad for their (possibly long-term) health. As reported by several news outlets, [ 296 ] [ 297 ] an Irish subsidiary of Microsoft based in the Republic of Ireland declared £220 bn in profits but paid no corporation tax for the year 2020. This is due to the company being tax resident in Bermuda as mentioned in the accounts for 'Microsoft Round Island One, a subsidiary that collects license fees from the use of Microsoft software worldwide. Dame Margaret Hodge , a Labour MP in the UK said, \"It is unsurprising – yet still shocking – that massively wealthy global corporations openly, unashamedly and blatantly refuse to pay tax on the profits they make in the countries where they undertake business\". [ 297 ] In 2020, ProPublica reported that the company had diverted more than $39 billion in U.S. profits to Puerto Rico using a mechanism structured to make it seem as if the company was unprofitable on paper. As a result, the company paid a tax rate on those profits of \"nearly 0%\". When the Internal Revenue Service audited these transactions, ProPublica reported that Microsoft aggressively fought back, including successfully lobbying Congress to change the law to make it harder for the agency to conduct audits of large corporations. [ 298 ] [ 299 ] In 2023, Microsoft reported in a securities filing that the U.S. Internal Revenue Service was alleging that the company owed the U.S. $28.9 billion in past taxes, plus penalties related to mis-allocation of corporate profits over a decade. [ 300 ] \"Embrace, extend, and extinguish\" (EEE), [ 301 ] also known as \"embrace, extend, and exterminate,\" [ 302 ] is a phrase that the U.S. Department of Justice found [ 303 ] that was used internally by Microsoft [ 304 ] to describe its strategy for entering product categories involving widely used standards, extending those standards with proprietary capabilities, and then using those differences to strongly disadvantage competitors. Microsoft is frequently accused of using anticompetitive tactics and abusing its monopolistic power. People who use its products and services often end up becoming dependent on them, a process known as vendor lock-in . Microsoft was the first company to participate in the PRISM surveillance program , according to leaked NSA documents obtained by The Guardian [ 305 ] and The Washington Post [ 306 ] in June 2013, and acknowledged by government officials following the leak. [ 307 ] The program authorizes the government to secretly access data of non-US citizens hosted by American companies without a warrant. Microsoft has denied participation in such a program. [ 308 ] Jesse Jackson believes Microsoft should hire more minorities and women. In 2015, he praised Microsoft for appointing two women to its board of directors. [ 309 ] In 2020, Salesforce , the manufacturer of the Slack platform, complained to European regulators about Microsoft due to the integration of the Teams service into Office 365. Negotiations with the European Commission continued until the summer of 2023, but reached an impasse that led to Microsoft facing an antitrust investigation from the European Union. [ 310 ] In June 2024, Microsoft faced a potential EU fine after regulators accused it of abusing market power by bundling its Teams video-conferencing app with its Office 365 and Microsoft 365 software. The European Commission issued a statement of objections, alleging Microsoft's practice since 2019 gave Teams an unfair market advantage and limited interoperability with competing software. Despite Microsoft's efforts to avoid deeper scrutiny, including unbundling Teams, regulators remained unconvinced. This action followed a 2019 complaint from Slack, which was later acquired by Salesforce. Microsoft's Teams usage soared during the pandemic, growing from 2 million daily users in 2017 to 300 million in 2023. The company has a history of antitrust battles in the U.S. and Europe, with over €2 billion in EU fines previously imposed for similar abuses. [ 311 ] In October 2024, Microsoft fired two employees, software engineers Ibtihal Aboussad and Vaniya Agrawal, [ 312 ] who organized an unauthorized vigil at its Redmond headquarters to honor Palestinians killed in the Gaza war . The employees, part of the group No Azure for Apartheid, sought to address the company's involvement in the Israeli government's use of its technology. [ 313 ] In February 2025, the Associated Press reported that the Israeli military was utilizing Microsoft-developed artificial intelligence tools in its military and intelligence operations against the people of Gaza. In May 2025, Microsoft issued an unsigned statement confirming that these services had been made available to Israel, while denying that these tools were employed during the massacre of the people of Gaza . [ 314 ] On March 20, 2025, before an event at Seattle's Great Hall with Brad Smith and Steve Ballmer, protestors projected \"Microsoft powers genocide \" on the wall. Subsequently, two employees interrupted AI executive Mustafa Suleyman at a speaking event on April 4, 2025, in protest at the company's support of Israel. [ 315 ] After the disruptions at these events, Microsoft contacted the FBI in search of assistance in surveilling its pro-Palestinian employees and their allies. [ 316 ] The Boycott, Divestment and Sanctions movement added Microsoft to its list of targets for partnering \"with the apartheid regime of Israel and its prison system\". [ 317 ] In August 2025 it was reported that Microsoft provides storage for mass-surveilled Palestinian phone calls that have been used to identify bombing targets in Gaza. [ 318 ] On August 20, 20 Microsoft employees and their allies were arrested after refusing to disperse from a protest on Microsoft's Redmond, Washington campus. [ 316 ] In November 2024, the Federal Trade Commission (FTC) launched an investigation into Microsoft, focusing on potential antitrust violations related to its cloud computing, AI, and cybersecurity businesses. The probe scrutinized Microsoft's bundling of cloud services with products like Office and security tools, as well as its growing AI presence through its partnership with OpenAI. This inquiry is part of broader efforts by the U.S. government to curb the power of major tech companies, especially under FTC chair Lina Khan. Concerns were raised about Microsoft's licensing practices potentially locking customers into its services and its AI investments possibly sidestepping regulatory oversight. [ 319 ] In June 2025 Microsoft helped suspend the email account of an International Criminal Court (ICC) prosecutor in the Netherlands who was investigating Israel for war crimes in order to comply with a Trump executive order. [ 320 ] [ 321 ] In June 2025, a UN expert's report named Microsoft as being \"central to Israel's surveillance apparatus and the ongoing Gaza destruction.\" [ 322 ] In September 2025, Microsoft cut off some services to a unit of Israel's Ministry of Defence after an investigation found its technology had been used to conduct mass surveillance on people in Gaza. [ 323 ] [ 324 ] In late 2025, a non-profit organization filed a complaint within the European Union, raising concerns about Microsoft’s handling of certain data related to Israeli military surveillance. According to the complaint and media reports, Microsoft’s cloud services may have been used to store or process surveillance-related data. The organization requested that European data protection authorities investigate whether this data processing complies with EU law. [ 325 ] [ 326 ] Bundled references 47°38′33″N 122°07′56″W ﻿ / ﻿ 47.64250°N 122.13222°W ﻿ / 47.64250; -122.13222",
    "links": [
      "Meltdown (security vulnerability)",
      "PowerShell",
      "The Wall Street Journal",
      "Surface Neo",
      "Xerox",
      "Tablet computer",
      "IBM PC compatible",
      "Artificial intelligence",
      "Kindai University",
      "SeaTac, Washington",
      "Microsoft Store (retail)",
      "Fujitsu",
      "Trip.com",
      "Baker Hughes",
      "Financial independence",
      "Kevin Scott (computer scientist)",
      "Glenn Greenwald",
      "Surface Laptop Go",
      "Azure Kinect",
      "Flux (graphics software)",
      "Xbox system software",
      "Minecraft",
      "UEFI",
      "Penny Pritzker",
      "Workday, Inc.",
      "Backdoor (computing)",
      "Synaptics",
      "Microsoft Learn",
      "ProClarity",
      "Windows 8",
      "Windows Server 2003",
      "MSNBC",
      "Reuters",
      "Microsoft Power Fx",
      "Microsoft Amalga",
      "VoloMetrix",
      "CDW",
      "Safeco",
      "Achronix",
      "Apple Computer, Inc. v. Microsoft Corp.",
      "Westinghouse Electronics",
      "Microsoft Redmond campus",
      "Berkshire Hathaway",
      "Wunderlist",
      "Seattle Times",
      "Microsoft Defender Antivirus",
      "Great Plains Software",
      "I'm a PC",
      "Alcatel-Lucent v. Microsoft Corp.",
      "Solar panel",
      "Windows Aero",
      "Natali Morris",
      "Xbox Series X and Series S",
      "Obsidian Entertainment",
      "Zscaler",
      "Bill Gates' flower fly",
      "FOX Business",
      "Cray",
      "NSA",
      "PC Magazine",
      "TypeScript",
      "Verisk Analytics",
      "Nokia",
      "Silicon Image",
      "Information technology",
      "Yahoo! Tech",
      "Microsoft Gaming",
      "Microsoft Translator",
      "Home Depot",
      "ICloud",
      "Simplygon",
      "Xilinx",
      "Satya Nadella",
      "Climate Neutral Data Centre Pact",
      "Analog Devices",
      "Emma Walmsley",
      "Caterpillar Inc.",
      "Bibcode (identifier)",
      "Puget Sound region",
      "Charlotte, North Carolina",
      "Microsoft Edge Legacy",
      "Fairchild Semiconductor",
      "Catherine MacGregor",
      "Vertex Pharmaceuticals",
      "WordPerfect",
      "Hexadite",
      "Charles Scharf",
      "Standard & Poor's",
      "Yankee Group",
      "NXP Semiconductors",
      "Bluesky",
      "Compaq",
      "Cloud computing",
      "FYIFV",
      "LifeCam",
      "Adobe Inc.",
      "Maxim Integrated",
      "ByteDance",
      "NBCUniversal",
      "EuroBasket 2015",
      "Akamai Technologies",
      "Merck & Co.",
      "Seattle metropolitan area",
      "Silicon Graphics",
      "Xbox 360",
      "Tokyo",
      "Nokia Lumia 1320",
      "Open source",
      "List of most valuable brands",
      "Nvidia",
      "JD.com",
      "Xamarin",
      "List of Microsoft hardware",
      "Bloomberg L.P.",
      "GlobalFoundries",
      "Augmented reality",
      "Paychex",
      "Microsoft Enterprise Agreement",
      "Spotify",
      "Steve Ballmer",
      "Versions of Windows",
      "Microsoft Office",
      "How to Prevent the Next Pandemic",
      "Apple Inc.",
      "King County Metro",
      "Visual Basic (classic)",
      "Microsoft Corp. v. Lindows.com, Inc.",
      "Microsoft AI",
      "Sam Altman",
      "Nasdaq-100",
      "MacOS",
      "Windows Server 2012",
      "Ross Stores",
      "Dell EMC",
      "Double Fine",
      "Alibaba Cloud",
      "Greg Brockman",
      "Microsoft Outlook",
      "London",
      "JPMorgan Chase",
      "The NPD Group",
      "Mark Mason (executive)",
      "NEC",
      "HTC",
      "Jargon",
      "Mineralization (soil science)",
      "Carlos A. Rodriguez",
      "Channel 9 (Microsoft)",
      "MIT License",
      "Computer security",
      "Internet censorship in China",
      "Microsoft Mouse",
      "Kimball International",
      "Panasonic",
      "Paul Allen",
      "Bethesda Game Studios",
      "Redmond, Washington",
      "Corporate venture capital",
      "2020 Formula One World Championship",
      "Office Assistant",
      "Undead Labs",
      "Google News",
      "Connectix",
      "Windows Phone",
      "Pando Networks",
      "Geode Capital Management",
      "Visa Inc.",
      "Windows Phone 8",
      "Amadeus IT Group",
      "United States Congress",
      "Carbfix",
      "Yupi",
      "Brand valuation",
      "Microsemi",
      "Darigold",
      "InFocus",
      "Idexx Laboratories",
      "ASCII Corporation",
      "Total cost of ownership",
      "Direct air capture",
      "Phil Radford",
      "Fourth Amendment to the United States Constitution",
      "Coupang",
      "The New York Times",
      "ZeniMax Media",
      "The Vanguard Group",
      "Jellyfish.com",
      "Warner Bros. Discovery",
      "VBScript",
      "Altair 8800",
      "Engadget",
      "Personal digital assistant",
      "Consumers Software",
      "Cirque Corporation",
      "Credit rating",
      "Microsoft SwiftKey",
      "Weyerhaeuser",
      "PlaceWare",
      "Fastenal",
      "Emerson Radio",
      "Bethesda Softworks",
      "Surface Duo",
      "Thomson Reuters",
      "Palo Alto Networks",
      "Virtual reality headset",
      "List of Microsoft software",
      "Penton (company)",
      "Game Workers Alliance",
      "Cadence Design Systems",
      "Amazon Web Services",
      "Element Electronics",
      "Law firm",
      "Carbon sequestration",
      "Danger, Inc.",
      "Windows NT",
      "TakeLessons",
      "Microsoft Word",
      "T. Rowe Price",
      "IBM Cloud",
      "LinkedIn",
      "Wayback Machine",
      "Advanced persistent threat",
      "Rare (company)",
      "Software",
      "Removal of Sam Altman from OpenAI",
      "Atmel",
      "2-in-1 laptop",
      "Kernel (operating system)",
      "Ampere Computing",
      "ISSN (identifier)",
      "Consumer electronics",
      "Microsoft Office 2007",
      "USA Today",
      "Clandestine operation",
      "Switching barriers",
      "Carbon neutrality",
      "Windows Vista",
      "Visio Corporation",
      "MileIQ",
      "Quantum computing",
      "ISBN (identifier)",
      "Dividend",
      "Mobile device",
      "Microsoft Surface Go",
      "Permatemp",
      "International Data Group",
      "Azure Quantum",
      "The Guardian",
      "Press Play (company)",
      "Exelon",
      "Disney Streaming",
      "Autodesk",
      "Acquisition of Activision Blizzard by Microsoft",
      "Adviser",
      "Conveyor belt",
      "Forbes Global 2000",
      "Boycott, Divestment and Sanctions",
      "Vermeer Technologies",
      "Trapped ion quantum computer",
      "NetShow",
      "AltspaceVR",
      "Inspur",
      "BENlabs",
      "Autonomous System Number",
      "United States Armed Forces",
      "Bellevue, Washington",
      "Ribbon (computing)",
      "Microsoft Build",
      "Dynabook Inc.",
      "Xcel Energy",
      "Chief executive officer",
      "Signetics",
      "Activision Blizzard",
      "Multi-factor authentication",
      "Outlook.com",
      "Scroogled",
      "Compulsion Games",
      "Asus",
      "Corsair Gaming",
      "Salesforce",
      "Tencent",
      "Neutral atom quantum computer",
      "Microsoft Start",
      "Cognizant",
      "S2CID (identifier)",
      "Quest Software",
      "Microsoft Copilot",
      "Microsoft TechNet",
      "Microchip Technology",
      "ViewSonic",
      "Fortinet",
      "Carbon offsets and credits",
      "Perceptive Pixel",
      "Moody's Investors Service",
      "Surface Go",
      "Zenith Electronics",
      "Windows Hardware Engineering Conference",
      "Expeditors International",
      "Matt Pietrek",
      "Axon Enterprise",
      "Take-Two Interactive",
      "Kingston Technology",
      "Arm Holdings",
      "Acqui-hiring",
      "Phil Spencer (business executive)",
      "Microsoft Windows",
      "Visual Basic for Applications",
      "Suning.com",
      "Johnston Press",
      "Honeywell",
      "MSN TV",
      "General Electric",
      "Source Code (memoir)",
      "Davos",
      "Microsoft Corp. v. Shah",
      "Shopify",
      "Tay (chatbot)",
      "Maxwell Technologies",
      "Plantronics",
      "FIS (company)",
      "Bungie",
      "Memorex",
      "List of computer hardware manufacturers",
      "Microsoft Garage",
      "Flex Ltd.",
      "IDG",
      "Microsoft Visual Programming Language",
      "List of public corporations by market capitalization",
      "Internet of things",
      "Nuance Communications",
      "Seattle",
      "Bisnow Media",
      "Video Games Chronicle",
      "Criticism of Microsoft Windows",
      "3M",
      "Family of Bill Gates",
      "FASA Studio",
      "CBS Interactive",
      "Sherwin-Williams",
      "Xandr",
      "Phthalate",
      "InXile Entertainment",
      "Operating system",
      "Bill Gates",
      "Windows Presentation Foundation",
      "Big Tech",
      "Nokia Lumia 530",
      "Synopsys",
      "President (corporate title)",
      "Marriott International",
      "Asset",
      "HP Inc.",
      "Clipchamp",
      "Razer Inc.",
      "Windows XP",
      "Business @ the Speed of Thought",
      "Office 365",
      "Renault F1 Team",
      "Xbox (console)",
      "Government bond",
      "Logitech",
      "List of mergers and acquisitions by Microsoft",
      "Lam Research",
      "Micro Instrumentation and Telemetry Systems",
      "Microsoft Developer Network",
      "Fish farming",
      "List of companies based in Seattle",
      "Abu Dhabi",
      "The Register",
      "Paccar",
      "Q Sharp",
      "CNET",
      "Lattice Semiconductor",
      "Surface Hub",
      "Zalando",
      "Broadcom",
      "Sharp Corporation",
      "Superconducting quantum computing",
      "Where do you want to go today?",
      "Turtle Beach Corporation",
      "GreenButton",
      "United States v. Microsoft Corp.",
      "Microsoft Surface",
      "Criticism of Windows Vista",
      "Mecklenburg County, Virginia",
      "Forced retention",
      "Nimbus Data",
      "George Kurtz",
      "Viva Engage",
      "Mojang",
      "Trading day",
      "Microsoft Lumia 535",
      "Cypress Semiconductor",
      "ExxonMobil",
      "United States Department of Justice",
      "Aid agency",
      "Division (business)",
      "Microsoft and open source",
      "Microsoft India",
      "Police surveillance in New York City",
      "Microsoft Edge",
      "Old Dominion Freight Line",
      "Albuquerque, New Mexico",
      "SAP",
      "Harvard University",
      "AppLovin",
      "QFC",
      "Xbox network",
      "OneDrive",
      "Dynamics 365",
      "Gaza war",
      "Diversity, equity, and inclusion",
      "Start menu",
      "Borland",
      "Infrared thermometer",
      "Computerworld",
      "Mondelez International",
      "Yandex",
      "Gates Ventures",
      "Microsoft Campus",
      "Costco",
      "Eating your own dog food",
      "Visual Basic (.NET)",
      "KLA Corporation",
      "LGBT",
      "Expedia Group",
      "Apollo 11",
      "Cintas",
      "International Securities Identification Number",
      "HP 300LX",
      "JP Morgan",
      "Carbon footprint",
      "Microsoft Azure",
      "Carbon capture and storage",
      "Altamira Software",
      "Kenmore (brand)",
      "Coca-Cola Europacific Partners",
      "Net income",
      "Magnavox",
      "Game controller",
      "Facebook",
      "Paramount Streaming",
      "Internet forum",
      "Zilog",
      "Linde plc",
      "List of the largest software companies",
      "ProPublica",
      "Remote work",
      "Information privacy",
      "Brominated flame retardant",
      "McDonald's",
      "National Semiconductor",
      "Fortune (magazine)",
      "Havok (company)",
      "BuzzFeed",
      "Seiki Digital",
      "How to Avoid a Climate Disaster",
      "Carolina Dybeck Happe",
      "Regeneron Pharmaceuticals",
      "Microsoft Excel",
      "Leadership in Energy and Environmental Design",
      "Silicon Valley",
      "Visual Studio",
      "Diamondback Energy",
      "Forethought, Inc.",
      "Electronics industry in the United States",
      "Codex Leicester",
      "Limestone",
      "Amgen",
      "Works council",
      "Internet Explorer",
      "London Stock Exchange Group",
      "American Electric Power",
      "Calcium oxide",
      "Jensen Electronics",
      "Qualcomm",
      "Sandisk",
      "Micron Technology",
      "TerraPower",
      "Vanity Fair (magazine)",
      "Windows 7",
      "Altair BASIC",
      "Chief design officer",
      ".NET Foundation",
      "Lexmark",
      "List of Microsoft operating systems",
      "Wayfair",
      "Access Software",
      "Bloomberg.com",
      "OpenAI",
      "Monster Beverage",
      "Personal computer",
      "Video game industry",
      "Trillion-dollar company",
      "Vendor lock-in",
      "NetApp",
      "Water",
      "Bermuda",
      "Biogen",
      "Remote Desktop Services",
      "International Criminal Court",
      "Communications Workers of America",
      "16-bit computing",
      "The Daily Telegraph",
      "OpenSecrets",
      "S&P 100",
      "Kinect",
      "John W. Thompson",
      "Colloquis",
      "Commodore International",
      "Pinduoduo",
      "Bose Corporation",
      "Public transport",
      "Inflection AI",
      "H-1B visa",
      "Business Line",
      "Apple silicon",
      "Computer programming",
      "List of largest companies in the United States by revenue",
      "ASML Holding",
      "Planar Systems",
      "Mellanox Technologies",
      "Traf-O-Data",
      "Finland national basketball team",
      "Breakthrough Energy",
      "Microsoft HoloLens",
      "Palantir Technologies",
      "Visual J++",
      "Share (finance)",
      "DoorDash",
      "Windows Server 2016",
      "Gratis versus libre",
      "2016 Formula One World Championship",
      "Cascade Investment",
      "Intel",
      "Palm, Inc.",
      "Chevron Corporation",
      "Scottsdale, Arizona",
      "Wiki",
      "Evergreen Point Floating Bridge",
      "Microsoft SQL Server",
      "OKR",
      "Visual J Sharp",
      "Playground Games",
      "Dell",
      "Robustness (computer science)",
      "Motorola Mobility",
      "Lotus Software",
      "2017 WRC",
      "Windows Mobile",
      "Microsoft Store (digital)",
      "Polyvinyl chloride",
      "Oracle Corporation",
      "United States Department of Justice Antitrust Division",
      "Professional Developers Conference",
      "IBM Personal Computer",
      "Open Source Development Labs",
      "Sysinternals",
      "Flipkart",
      "Video game console",
      "I386",
      "Windows Update",
      "Gates Foundation",
      "Meta Platforms",
      "RCA Corporation",
      "AQuantive",
      "San Francisco Chronicle",
      "Unix",
      "Ticker symbol",
      "Procter & Gamble",
      "Alphabet Inc.",
      "Chairman",
      "John Heilemann",
      "Joe Belfiore",
      "Baidu",
      "F Sharp (programming language)",
      "Keurig Dr Pepper",
      "Gaza genocide",
      "Copart",
      "César Cernuda",
      "Sound Transit",
      "Microsoft Systems Journal",
      "Special dividend",
      "Kuaishou",
      "Match for Africa",
      "Xbox",
      "Carbon dioxide removal",
      "Citrix Systems",
      "Occupational burnout",
      "CP/M",
      "Jesse Jackson",
      "BASIC interpreter",
      "Booking Holdings",
      "ChatGPT",
      "Xenix",
      "IBM PC DOS",
      "RCA (trademark)",
      "Minorities",
      "Microsoft Development Center Norway",
      "American Express",
      "IBM PC–compatible",
      "PayPal",
      "Westinghouse Electric Company",
      "Software architect",
      "Climeworks",
      "Stock split",
      "Israeli war crimes in the Gaza war",
      "Xbox Game Studios",
      "Powerset (company)",
      "32-bit computing",
      "Buzz Aldrin",
      "Cyberattack",
      "Helvetica",
      "MicroStrategy",
      "Microsoft Bob",
      "AI-assisted targeting in the Gaza Strip",
      "COVID-19 pandemic",
      "Expedia",
      "Starbucks",
      "OS/2",
      "Wall Street Journal",
      "Lululemon",
      "Ninja Theory",
      "High-occupancy vehicle lane",
      "Vox Media",
      "Western Digital",
      "SAS Institute",
      "Acompli",
      "Timeline of Microsoft",
      "Novell",
      "Mission Innovation",
      "100% renewable energy",
      "Microsoft v. United States (2016)",
      "Criticism of Microsoft",
      "C/AL",
      "Tesla, Inc.",
      "Microsoft litigation",
      "Windows Forms",
      "Airbnb",
      "Microsoft campus",
      "Greenpeace",
      "Kraft Heinz",
      "UnitedHealth Group",
      "Hyper-V",
      "PRISM (surveillance program)",
      "Fidelity Investments",
      "UNI Global Union",
      "BASIC",
      "Microsoft Japan",
      "Koss Corporation",
      "Hodgkin's lymphoma",
      "Earnings before interest and taxes",
      "Microsoft Bing",
      "Anti-competitive practices",
      "Sony Corporation of America",
      "Sony",
      "Labour Party (UK)",
      "Issaquah, Washington",
      "Microsoft 365",
      "World Wide Web",
      "Sanmina Corporation",
      "Ina Fried",
      "The Verge",
      "LG Electronics",
      "Outline of Microsoft",
      "MSN",
      "Akihabara",
      "CompuServe",
      "86-DOS",
      "Cisco",
      "Lost on the Grand Banks",
      "Orthocarbonic acid",
      "Microsoft Ignite",
      "ZDNet",
      "Joshua Topolsky",
      "Windows 95",
      "PepsiCo",
      "AVX Corporation",
      "Linux Foundation",
      "CSX Corporation",
      "Reid Hoffman",
      "Toyota Yaris WRC",
      "AppNexus",
      "Bill Gates's house",
      "Micro-Star International",
      "Equity (finance)",
      "Steve Jobs",
      "Internet",
      "Nintendo",
      "Maluuba",
      "Sun Microsystems",
      "ServiceNow",
      "United States Army",
      "RiskIQ",
      "Microsoft hardware",
      "Alibaba Group",
      "Interlink Electronics",
      "Google",
      "Amy Hood",
      "Walmart",
      "Sexual harassment",
      "Embrace, extend, and extinguish",
      "United States district court",
      "Mercado Libre",
      "Fuck",
      "Bobby Kotick",
      "African Americans",
      "Source code",
      "The Walt Disney Company",
      "Gilead Sciences",
      "Champagne (advertisement)",
      "Charter Communications",
      "AOL",
      "LinkExchange",
      "Monster Cable",
      "Boeing",
      "Skullcandy",
      "AMD",
      "FTC v. Microsoft",
      "Constellation Energy",
      "Fortune 1000",
      "Mashable",
      "List of largest information technology companies by revenue",
      "Data center",
      "Rebranding",
      "HPE Networking",
      "Computer network",
      "Npm",
      "Zune",
      "Bing (search engine)",
      "Human Rights Campaign",
      "IntelliMouse",
      "Nike, Inc.",
      "Global surveillance",
      "Datadog",
      "Dexcom",
      "Kyiv",
      "O'Reilly Auto Parts",
      "Lexicon",
      "ADP (company)",
      "Gmail",
      "NetEase",
      "The Road Ahead (Gates book)",
      "Microsoft and unions",
      "Atlassian",
      "Raven Software",
      "Toyota Gazoo Racing WRT",
      "Google Cloud Platform",
      "Windows Media Player",
      "Republic of Ireland",
      "BBC",
      "Windows 3.0",
      "IBM",
      "Rakuten",
      "Linux",
      "Metaswitch",
      "Proprietary software",
      "Groove Networks",
      "Microsoft Inspire",
      "List of largest Internet companies",
      "Atari Corporation",
      "Doi (identifier)",
      "Portable Media Center",
      "Naver Corporation",
      "Open Networking Foundation",
      "File Manager (Windows)",
      "Surface Studio",
      "Slack (software)",
      "Microsoft Research",
      "International Business Times",
      "LinkedIn Learning",
      "EBay",
      "Sting (musician)",
      "Deutsche Telekom",
      "Transact-SQL",
      "Goldman Sachs",
      "Toyota Tsusho",
      "Comcast",
      "Philco",
      "Sage Group",
      "Intuitive Surgical",
      "State Street Corporation",
      "Netflix, Inc.",
      "Hewlett Packard Enterprise",
      "Harman International",
      "T-Mobile US",
      "Sunrise Calendar",
      "Subsidiary",
      "2024 CrowdStrike-related IT outages",
      "Eaton Vance",
      "Windows",
      "Puget Sound Energy",
      "European Commission",
      "BETT",
      "Internal Revenue Service",
      "Mass surveillance",
      "The Trade Desk",
      "Diodes Incorporated",
      "Kathleen Hogan",
      "Think tank",
      "Associated Press",
      "Actel",
      "Skype Technologies",
      "InformationWeek",
      "Profanity",
      "Padmasree Warrior",
      "Surface Pro",
      "Pac-Man",
      "Mainframe computer",
      "Microsoft Press",
      "2020 World Rally Championship",
      "Netscape",
      "Webmail",
      "Onsemi",
      "Xbox Game Pass",
      "Joint Enterprise Defense Infrastructure",
      "Global LGBTQIA+ Employee & Allies at Microsoft",
      "AstraZeneca",
      "Richard Rashid",
      "Applied Materials",
      "50th anniversary",
      "Gender discrimination",
      "LSI Corporation",
      "Vladimir Putin",
      "Big Fish Games",
      "Yahoo!",
      "Meituan",
      "GitHub",
      "Qualcomm Snapdragon",
      "Altera",
      "Popular Electronics",
      "War profiteering",
      "Microsoft Store",
      "Software development",
      "NBCNews.com",
      "The Travelers Companies",
      "Verizon",
      "PC World",
      "Microsoft Corp. v. United States",
      "Texas Instruments",
      "Microsoft Dynamics 365",
      "Vizio",
      "Infor",
      "Acer Inc.",
      "NSAKEY",
      "Port Washington, New York",
      "Semiconductor device",
      "Tabula, Inc.",
      "Kakao",
      "Lenovo",
      "Microsoft Most Valuable Professional",
      "The Coca-Cola Company",
      "Firefly (website)",
      "Surface Pro 3",
      "Amazon (company)",
      "Twisted Pixel Games",
      "Margaret Hodge",
      "Bundling of Microsoft Windows",
      "Electronic component",
      "MS-DOS",
      "Massive Incorporated",
      "Public company",
      "Web browser",
      "Android (operating system)",
      "Twitter",
      "Windows 1.0",
      "Member of Parliament (United Kingdom)",
      "United States Environmental Protection Agency",
      "GE HealthCare",
      "Online service provider",
      "Computer hardware",
      "Freescale Semiconductor",
      "OER Project",
      "Stock",
      "BLU Products",
      "Skype",
      "Electronic Arts",
      "Computational chemistry",
      "History of Microsoft",
      "Microsoft Mobile",
      "ARM architecture",
      "Amphenol",
      "Carbon accounting",
      "Roper Technologies",
      "Washington State University",
      "List of largest technology companies by revenue",
      "Havok (software)",
      "HGST",
      "An Open Letter to Hobbyists",
      "European Union",
      "Huawei",
      "Marvell Technology",
      "Uber",
      "Agence France-Presse",
      "Metro (design language)",
      "Microsoft Corp. v. Commission",
      "Domain Awareness System",
      "Seagate Technology",
      "Microsoft BackOffice Server",
      "Mojave Experiment",
      "Lionhead Studios",
      "AT&T Corporation",
      "Shopee",
      "High Heat Major League Baseball",
      "Azure Sphere",
      "Intuit",
      "Computer program",
      "Interactive whiteboard",
      "Government of Russia",
      "Alaska Airlines",
      "Home appliance",
      "Chromium (web browser)",
      "List of United States defense contractors",
      "BlackRock",
      "Russian invasion of Ukraine",
      "Microsoft Power Platform",
      "Server (computing)",
      "IOS",
      "Criticism of Windows 10",
      "Jabil",
      "The Blue Ribbon SoundWorks",
      "Ease of use",
      "Johnson & Johnson",
      "Ziff Davis",
      "Vishay Intertechnology",
      "Jones Soda",
      "CrowdStrike",
      "IAC (company)",
      "Trade union",
      "Revolution Analytics",
      "Marantz",
      "Allen Adham",
      "Toronto",
      "Visual Studio Code",
      "REI",
      "MIX (Microsoft)",
      "Xbox One",
      "Mojang Studios",
      "Criticism of Windows XP",
      "Microsoft Teams",
      "Google DeepMind",
      "Lazada Group",
      "Mixer (service)",
      "New York City Police Department",
      "Microsoft engineering groups",
      "Nordstrom",
      "KEMET Corporation",
      "Vice-Chairman",
      "The Giving Pledge",
      "Holding company",
      "Outercurve Foundation",
      "Microsoft v. MikeRoweSoft",
      "Samsung Electronics",
      "Harry Shum",
      "Seattle Computer Products",
      "Solectron",
      "Versions of MS-DOS",
      "Nature (journal)",
      "Microsoft PowerPoint",
      "S&P 500",
      "BIOS",
      "NBC",
      "Vaio",
      "Great Recession",
      "U.S. Securities and Exchange Commission",
      "Windows 10",
      "Eddie Bauer",
      "The Washington Post",
      "Protocol (news)",
      "Nasdaq",
      "Wired (magazine)",
      "Fortune 500",
      "Usenet",
      "Open Invention Network",
      "Original equipment manufacturer",
      "John W. Stanton",
      "Windows 11",
      "Initial public offering",
      "Julia Carrie Wong",
      "Microsoft Digital Crimes Unit",
      "United States Department of Defense",
      "Groupon",
      "Brad Smith (American lawyer)",
      "Scott Guthrie",
      "Tellme Networks",
      "CoStar Group",
      "Surface Laptop",
      "Sandi Peterson",
      "List of Microsoft video games",
      "Farecast",
      "Social networking service",
      "Microsoft Servers",
      "Mustafa Suleyman",
      "Washington State Route 520",
      "Distance education",
      "Board of directors",
      "C Sharp (programming language)",
      "Dow Jones Industrial Average",
      "24/7 service"
    ]
  },
  "Legal information retrieval": {
    "url": "https://en.wikipedia.org/wiki/Legal_information_retrieval",
    "title": "Legal information retrieval",
    "content": "Legal information retrieval is the science of information retrieval applied to legal text, including legislation , case law , and scholarly works. [ 1 ] Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means. [ 2 ] Legal information retrieval is a part of the growing field of legal informatics . In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used boolean search methods (exact matches of specified terms) on full text legal documents have been shown to have an average recall rate as low as 20 percent, [ 3 ] meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents. [ 3 ] This may result in failing to retrieve important or precedential cases. In some jurisdictions this may be especially problematic, as legal professionals are ethically obligated to be reasonably informed as to relevant legal documents. [ 4 ] Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high recall rate ) and reducing the number of irrelevant documents (a high precision rate ). This is a difficult task, as the legal field is prone to jargon , [ 5 ] polysemes [ 6 ] (words that have different meanings when used in a legal context), and constant change. Techniques used to achieve these goals generally fall into three categories: boolean retrieval, manual classification of legal text, and natural language processing of legal text. Application of standard information retrieval techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent taxonomy . [ 7 ] Instead, the law is generally filled with open-ended terms, which may change over time. [ 7 ] This can be especially true in common law countries, where each decided case can subtly change the meaning of a certain word or phrase. [ 8 ] Legal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term \"worker\" has four different meanings: [ 9 ] It also has the common meaning: Though the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results. Even if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case. [ 10 ] Case decisions from senior or superior courts may be more relevant than those from lower courts , even where the lower court's decision contains more discussion of the relevant facts. [ 10 ] The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case). [ 10 ] An information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority. Additionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not. [ 10 ] He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions. [ 10 ] Overcoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day [ 2 ] ), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data. [ 2 ] [ 11 ] Boolean searches , where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented but overcome few of the problems discussed above. The recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's recall rate to be roughly 20%, and its precision rate to be roughly 79%. [ 3 ] Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals. [ 12 ] In order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an ontology to classify the texts, based on the way a legal professional might think about them. [ 13 ] These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as Westlaw 's “Natural Language” [ 14 ] or LexisNexis ' Headnote [ 15 ] searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers [ 14 ] or Lexis' Headnotes. [ 15 ] Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted). [ 13 ] These systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text. [ 16 ] In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals. [ 17 ] The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction. [ 18 ] The major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts. [ 16 ] [ 19 ] As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable. [ 20 ] In order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries. [ 2 ] [ 21 ] [ 22 ] Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ Natural Language Processing (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal ontology . Though multiple systems have been postulated, [ 2 ] [ 21 ] [ 22 ] few have reported results. One system, “SMILE,” which attempted to automatically extract classifications from case texts, resulted in an f-measure (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0). [ 23 ] This is probably much lower than an acceptable rate for general usage. [ 23 ] [ 24 ] Despite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems. [ 25 ] [ 26 ] In the mid-90s the Room 5 case law retrieval project used citation mining for summaries and ranked its search results based on citation type and count. This slightly pre-dated the PageRank algorithm at Stanford which was also a citation-based ranking. Ranking of results was based as much on jurisdiction as on number of references. [ 27 ]",
    "links": [
      "Natural language processing",
      "Legislation",
      "Jargon",
      "Doi (identifier)",
      "Directive 89/391/EEC",
      "S2CID (identifier)",
      "F-measure",
      "Natural Language Processing",
      "CiteSeerX (identifier)",
      "Recall rate",
      "Common law",
      "Information retrieval",
      "Ontology",
      "Hdl (identifier)",
      "ISBN (identifier)",
      "Legal ethics",
      "Lower court",
      "Superior court",
      "LexisNexis",
      "Computer-assisted legal research",
      "Taxonomy (general)",
      "PageRank",
      "Boolean search",
      "Legal informatics",
      "Westlaw",
      "Precision rate",
      "Precedential",
      "Case law",
      "Polysemes"
    ]
  },
  "Precision and recall": {
    "url": "https://en.wikipedia.org/wiki/Precision_and_recall",
    "title": "Precision and recall",
    "content": "In pattern recognition, information retrieval , object detection and classification (machine learning) , precision and recall are performance metrics that apply to data retrieved from a collection , corpus or sample space . Precision (also called positive predictive value ) is the fraction of relevant instances among the retrieved instances. Written as a formula: Precision = Relevant retrieved instances All retrieved instances {\\displaystyle {\\text{Precision}}={\\frac {\\text{Relevant retrieved instances}}{{\\text{All }}{\\textbf {retrieved}}{\\text{ instances}}}}} Recall (also known as sensitivity ) is the fraction of relevant instances that were retrieved. Written as a formula: Recall = Relevant retrieved instances All relevant instances {\\displaystyle {\\text{Recall}}={\\frac {\\text{Relevant retrieved instances}}{{\\text{All }}{\\textbf {relevant}}{\\text{ instances}}}}} Both precision and recall are therefore based on relevance . Consider a computer program for recognizing dogs (the relevant element) in a digital photograph. Upon processing a picture which contains ten cats and twelve dogs, the program identifies eight dogs. Of the eight elements identified as dogs, only five actually are dogs ( true positives ), while the other three are cats ( false positives ). Seven dogs were missed ( false negatives ), and seven cats were correctly excluded ( true negatives ). The program's precision is then 5/8 (true positives / selected elements) while its recall is 5/12 (true positives / relevant elements). Adopting a hypothesis-testing approach, where in this case, the null hypothesis is that a given item is irrelevant (not a dog), absence of type I and type II errors (perfect specificity and sensitivity ) corresponds respectively to perfect precision (no false positives) and perfect recall (no false negatives). More generally, recall is simply the complement of the type II error rate (i.e., one minus the type II error rate). Precision is related to the type I error rate, but in a slightly more complicated way, as it also depends upon the prior distribution of seeing a relevant vs. an irrelevant item. The above cat and dog example contained 8 − 5 = 3 type I errors (false positives) out of 10 total cats (true negatives), for a type I error rate of 3/10, and 12 − 5 = 7 type II errors (false negatives), for a type II error rate of 7/12. Precision can be seen as a measure of quality, and recall as a measure of quantity. Higher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned). In a classification task, the precision for a class is the number of true positives (i.e. the number of items correctly labelled as belonging to the positive class) divided by the total number of elements labelled as belonging to the positive class (i.e. the sum of true positives and false positives , which are items incorrectly labelled as belonging to the class). Recall in this context is defined as the number of true positives divided by the total number of elements that actually belong to the positive class (i.e. the sum of true positives and false negatives , which are items which were not labelled as belonging to the positive class but should have been). Precision and recall are not particularly useful metrics when used in isolation. For instance, it is possible to have perfect recall by simply retrieving every single item. Likewise, it is possible to achieve perfect precision by selecting only a very small number of extremely likely items. In a classification task, a precision score of 1.0 for a class C means that every item labelled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labelled correctly) whereas a recall of 1.0 means that every item from class C was labelled as belonging to class C (but says nothing about how many items from other classes were incorrectly also labelled as belonging to class C). Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other, but context may dictate if one is more valued in a given situation: A smoke detector is generally designed to commit many Type I errors (to alert in many situations when there is no danger), because the cost of a Type II error (failing to sound an alarm during a major fire) is prohibitively high. As such, smoke detectors are designed with recall in mind (to catch all real danger), even while giving little weight to the losses in precision (and making many false alarms). In the other direction, Blackstone's ratio , \"It is better that ten guilty persons escape than that one innocent suffer,\" emphasizes the costs of a Type I error (convicting an innocent person). As such, the criminal justice system is geared toward precision (not convicting innocents), even at the cost of losses in recall (letting more guilty people go free). A brain surgeon removing a cancerous tumor from a patient's brain illustrates the tradeoffs as well: The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain they remove to ensure they have extracted all the cancer cells. This decision increases recall but reduces precision. On the other hand, the surgeon may be more conservative in the brain cells they remove to ensure they extract only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome). Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome). Usually, precision and recall scores are not discussed in isolation. A precision-recall curve plots precision as a function of recall; usually precision will decrease as the recall increases. Alternatively, values for one measure can be compared for a fixed level at the other measure (e.g. precision at a recall level of 0.75 ) or both are combined into a single measure. Examples of measures that are a combination of precision and recall are the F-measure (the weighted harmonic mean of precision and recall), or the Matthews correlation coefficient , which is a geometric mean of the chance-corrected variants: the regression coefficients Informedness (DeltaP') and Markedness (DeltaP). [ 1 ] [ 2 ] Accuracy is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence). [ 1 ] Inverse Precision and Inverse Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels). True Positive Rate and False Positive Rate , or equivalently Recall and 1 - Inverse Recall, are frequently plotted against each other as ROC curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table , and they are easily manipulated by biasing the predictions. [ 1 ] The first problem is 'solved' by using Accuracy and the second problem is 'solved' by discounting the chance component and renormalizing to Cohen's kappa , but this no longer affords the opportunity to explore tradeoffs graphically. However, Informedness and Markedness are Kappa-like renormalizations of Recall and Precision, [ 3 ] and their geometric mean Matthews correlation coefficient thus acts like a debiased F-measure. For classification tasks, the terms true positives , true negatives , false positives , and false negatives compare the results of the classifier under test with trusted external judgments. The terms positive and negative refer to the classifier's prediction (sometimes known as the expectation ), and the terms true and false refer to whether that prediction corresponds to the external judgment (sometimes known as the observation ). Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix , as follows: Precision and recall are then defined as: [ 12 ] Precision = t p t p + f p Recall = t p t p + f n {\\displaystyle {\\begin{aligned}{\\text{Precision}}&={\\frac {tp}{tp+fp}}\\\\{\\text{Recall}}&={\\frac {tp}{tp+fn}}\\,\\end{aligned}}} Recall in this context is also referred to as the true positive rate or sensitivity , and precision is also referred to as positive predictive value (PPV); other related measures used in classification include true negative rate and accuracy . [ 12 ] True negative rate is also called specificity . True negative rate = t n t n + f p {\\displaystyle {\\text{True negative rate}}={\\frac {tn}{tn+fp}}\\,} Both precision and recall may be useful in cases where there is imbalanced data. However, it may be valuable to prioritize one metric over the other in cases where the outcome of a false positive or false negative is costly. For example, in medical diagnosis, a false positive test can lead to unnecessary treatment and expenses. In this situation, it is useful to value precision over recall. In other cases, the cost of a false negative is high, and recall may be a more valuable metric. For instance, the cost of a false negative in fraud detection is high, as failing to detect a fraudulent transaction can result in significant financial loss. [ 13 ] Precision and recall can be interpreted as (estimated) conditional probabilities : [ 14 ] Precision is given by P ( C = P | C ^ = P ) {\\displaystyle \\mathbb {P} (C=P|{\\hat {C}}=P)} while recall is given by P ( C ^ = P | C = P ) {\\displaystyle \\mathbb {P} ({\\hat {C}}=P|C=P)} , [ 15 ] where C ^ {\\displaystyle {\\hat {C}}} is the predicted class and C {\\displaystyle C} is the actual class (i.e. C = P {\\displaystyle C=P} means the actual class is positive). Both quantities are, therefore, connected by Bayes' theorem . The probabilistic interpretation allows to easily derive how a no-skill classifier would perform. A no-skill classifier is defined by the property that the joint probability P ( C = P , C ^ = P ) = P ( C = P ) P ( C ^ = P ) {\\displaystyle \\mathbb {P} (C=P,{\\hat {C}}=P)=\\mathbb {P} (C=P)\\mathbb {P} ({\\hat {C}}=P)} is just the product of the unconditional probabilities since the classification and the presence of the class are independent . For example the precision of a no-skill classifier is simply a constant P ( C = P | C ^ = P ) = P ( C = P , C ^ = P ) P ( C ^ = P ) = P ( C = P ) , {\\displaystyle \\mathbb {P} (C=P|{\\hat {C}}=P)={\\frac {\\mathbb {P} (C=P,{\\hat {C}}=P)}{\\mathbb {P} ({\\hat {C}}=P)}}=\\mathbb {P} (C=P),} i.e. determined by the probability/frequency with which the class P occurs. A similar argument can be made for the recall: P ( C ^ = P | C = P ) = P ( C = P , C ^ = P ) P ( C = P ) = P ( C ^ = P ) {\\displaystyle \\mathbb {P} ({\\hat {C}}=P|C=P)={\\frac {\\mathbb {P} (C=P,{\\hat {C}}=P)}{\\mathbb {P} (C=P)}}=\\mathbb {P} ({\\hat {C}}=P)} which is the probability for a positive classification. Accuracy = T P + T N T P + T N + F P + F N {\\displaystyle {\\text{Accuracy}}={\\frac {TP+TN}{TP+TN+FP+FN}}\\,} Accuracy can be a misleading metric for imbalanced data sets. Consider a sample with 95 negative and 5 positive values. Classifying all values as negative in this case gives 0.95 accuracy score. There are many metrics that don't suffer from this problem. For example, balanced accuracy [ 16 ] (bACC) normalizes true positive and true negative predictions by the number of positive and negative samples, respectively, and divides their sum by two: Balanced accuracy = T P R + T N R 2 {\\displaystyle {\\text{Balanced accuracy}}={\\frac {TPR+TNR}{2}}\\,} For the previous example (95 negative and 5 positive samples), classifying all as negative gives 0.5 balanced accuracy score (the maximum bACC score is one), which is equivalent to the expected value of a random guess in a balanced data set. Balanced accuracy can serve as an overall performance metric for a model, whether or not the true labels are imbalanced in the data, assuming the cost of FN is the same as FP. The TPR and FPR are a property of a given classifier operating at a specific threshold. However, the overall number of TPs, FPs etc depend on the class imbalance in the data via the class ratio r = P / N {\\textstyle r=P/N} . As the recall (or TPR) depends only on positive cases, it is not affected by r {\\textstyle r} , but the precision is. We have that Precision = T P T P + F P = P ⋅ T P R P ⋅ T P R + N ⋅ F P R = T P R T P R + 1 r F P R . {\\displaystyle {\\text{Precision}}={\\frac {TP}{TP+FP}}={\\frac {P\\cdot TPR}{P\\cdot TPR+N\\cdot FPR}}={\\frac {TPR}{TPR+{\\frac {1}{r}}FPR}}.} Thus the precision has an explicit dependence on r {\\textstyle r} . [ 17 ] Starting with balanced classes at r = 1 {\\textstyle r=1} and gradually decreasing r {\\textstyle r} , the corresponding precision will decrease, because the denominator increases. Another metric is the predicted positive condition rate (PPCR), which identifies the percentage of the total population that is flagged. For example, for a search engine that returns 30 results (retrieved documents) out of 1,000,000 documents, the PPCR is 0.003%. Predicted positive condition rate = T P + F P T P + F P + T N + F N {\\displaystyle {\\text{Predicted positive condition rate}}={\\frac {TP+FP}{TP+FP+TN+FN}}\\,} According to Saito and Rehmsmeier, precision-recall plots are more informative than ROC plots when evaluating binary classifiers on imbalanced data. In such scenarios, ROC plots may be visually deceptive with respect to conclusions about the reliability of classification performance. [ 18 ] Different from the above approaches, if an imbalance scaling is applied directly by weighting the confusion matrix elements, the standard metrics definitions still apply even in the case of imbalanced datasets. [ 19 ] The weighting procedure relates the confusion matrix elements to the support set of each considered class. A measure that combines precision and recall is the harmonic mean of precision and recall, the traditional F-measure or balanced F-score: F = 2 ⋅ p r e c i s i o n ⋅ r e c a l l p r e c i s i o n + r e c a l l {\\displaystyle F=2\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}} This measure is approximately the average of the two when they are close, and is more generally the harmonic mean , which, for the case of two numbers, coincides with the square of the geometric mean divided by the arithmetic mean . There are several reasons that the F-score can be criticized, in particular circumstances, due to its bias as an evaluation metric. [ 1 ] This is also known as the F 1 {\\displaystyle F_{1}} measure, because recall and precision are evenly weighted. It is a special case of the general F β {\\displaystyle F_{\\beta }} measure (for non-negative real values of β {\\displaystyle \\beta } ): F β = ( 1 + β 2 ) ⋅ p r e c i s i o n ⋅ r e c a l l β 2 ⋅ p r e c i s i o n + r e c a l l {\\displaystyle F_{\\beta }=(1+\\beta ^{2})\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\beta ^{2}\\cdot \\mathrm {precision} +\\mathrm {recall} }}} Two other commonly used F {\\displaystyle F} measures are the F 2 {\\displaystyle F_{2}} measure, which weights recall higher than precision, and the F 0.5 {\\displaystyle F_{0.5}} measure, which puts more emphasis on precision than recall. The F-measure was derived by van Rijsbergen (1979) so that F β {\\displaystyle F_{\\beta }} \"measures the effectiveness of retrieval with respect to a user who attaches β {\\displaystyle \\beta } times as much importance to recall as precision\". It is based on van Rijsbergen's effectiveness measure E α = 1 − 1 α P + 1 − α R {\\displaystyle E_{\\alpha }=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}} , the second term being the weighted harmonic mean of precision and recall with weights ( α , 1 − α ) {\\displaystyle (\\alpha ,1-\\alpha )} . Their relationship is F β = 1 − E α {\\displaystyle F_{\\beta }=1-E_{\\alpha }} where α = 1 1 + β 2 {\\displaystyle \\alpha ={\\frac {1}{1+\\beta ^{2}}}} . There are other parameters and strategies for performance metric of information retrieval system, such as the area under the ROC curve (AUC) [ 20 ] or pseudo-R-squared . Precision and recall values can also be calculated for classification problems with more than two classes. [ 21 ] To obtain the precision for a given class, we divide the number of true positives by the classifier bias towards this class (number of times that the classifier has predicted the class). To calculate the recall for a given class, we divide the number of true positives by the prevalence of this class (number of times that the class occurs in the data sample). The class-wise precision and recall values can then be combined into an overall multi-class evaluation score, e.g., using the macro F1 metric . [ 21 ]",
    "links": [
      "True Positive Rate",
      "Average precision",
      "Negative likelihood ratio",
      "Doi (identifier)",
      "Statistical classification",
      "Regression coefficient",
      "S2CID (identifier)",
      "Harmonic mean",
      "Root mean square deviation",
      "Information retrieval",
      "Statistical hypothesis testing",
      "Hdl (identifier)",
      "Positive predictive value",
      "True negative",
      "Fowlkes–Mallows index",
      "Jaccard index",
      "Euclidean distance",
      "Specificity (tests)",
      "Recommender system",
      "John Makhoul",
      "Intersection over union",
      "Cosine similarity",
      "Base rate fallacy",
      "Sensitivity (tests)",
      "Prevalence",
      "False positive rate",
      "Negative predictive value",
      "Computer vision",
      "Recall (information retrieval)",
      "False omission rate",
      "Bibcode (identifier)",
      "Accuracy and precision",
      "F-score",
      "BLEU",
      "Markedness",
      "Geometric mean",
      "Mean reciprocal rank",
      "Dunn index",
      "Scoring rule",
      "Blackstone's ratio",
      "Mean squared error",
      "Null hypothesis",
      "Positive likelihood ratio",
      "Davies–Bouldin index",
      "PMID (identifier)",
      "Hopkins statistic",
      "Symmetric mean absolute percentage error",
      "Fréchet inception distance",
      "Simple matching coefficient",
      "Classification (machine learning)",
      "Type II error",
      "Receiver operating characteristic",
      "True positive",
      "F1 score",
      "Youden's J statistic",
      "False negative",
      "Cohen's kappa",
      "Precision (information retrieval)",
      "ArXiv (identifier)",
      "Informedness",
      "Ranking (information retrieval)",
      "Natural language processing",
      "Relevance (information retrieval)",
      "Structural similarity index measure",
      "Specificity and sensitivity",
      "Machine learning",
      "Statistical power",
      "PSNR",
      "Rand index",
      "Contingency table",
      "Pseudo-R-squared",
      "NDCG",
      "Mean squared prediction error",
      "Cluster analysis",
      "False positives and false negatives",
      "Diagnostic odds ratio",
      "Root mean square",
      "Text corpus",
      "Coefficient of determination",
      "Statistical population",
      "Pearson correlation coefficient",
      "Data store",
      "True positive rate",
      "P4-metric",
      "Prevalence threshold",
      "Mean directional accuracy",
      "False positive",
      "Sample space",
      "Sensitivity and specificity",
      "Conditional probability",
      "False discovery rate",
      "False negative rate",
      "Median absolute deviation",
      "Matthews correlation coefficient",
      "Calinski–Harabasz index",
      "Uncertainty coefficient",
      "Accuracy (binary classification)",
      "Object detection",
      "Deep learning",
      "ROC curve",
      "Type I error",
      "Arithmetic mean",
      "Prior distribution",
      "ISSN (identifier)",
      "Silhouette (clustering)",
      "Perplexity",
      "DBCV index",
      "Bayes' theorem",
      "Mean absolute error",
      "Coverage probability",
      "Mean absolute percentage error",
      "Phi coefficient",
      "False Positive Rate",
      "ISBN (identifier)",
      "Type I and type II errors",
      "True negative rate",
      "PMC (identifier)",
      "Confusion matrix",
      "Mean absolute scaled error",
      "Similarity measure",
      "Independent event",
      "Regression analysis",
      "Inception score"
    ]
  },
  "Generalized vector space model": {
    "url": "https://en.wikipedia.org/wiki/Generalized_vector_space_model",
    "title": "Generalized vector space model",
    "content": "The Generalized vector space model is a generalization of the vector space model used in information retrieval . Wong et al. [ 1 ] presented an analysis of the problems that the pairwise orthogonality assumption of the vector space model (VSM) creates. From here they extended the VSM to the generalized vector space model (GVSM). GVSM introduces term to term correlations, which deprecate the pairwise orthogonality assumption. More specifically, the factor considered a new space, where each term vector t i was expressed as a linear combination of 2 n vectors m r where r = 1...2 n . For a document d k and a query q the similarity function now becomes: where t i and t j are now vectors of a 2 n dimensional space. Term correlation t i ⋅ t j {\\displaystyle t_{i}\\cdot t_{j}} can be implemented in several ways. For an example, Wong et al. uses the term occurrence frequency matrix obtained from automatic indexing as input to their algorithm. The term occurrence and the output is the term correlation between any pair of index terms. There are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model: Recently Tsatsaronis [ 2 ] focused on the first approach. They measure semantic relatedness ( SR ) using a thesaurus ( O ) like WordNet . It considers the path length, captured by compactness ( SCM ), and the path depth, captured by semantic path elaboration ( SPE ). They estimate the t i ⋅ t j {\\displaystyle t_{i}\\cdot t_{j}} inner product by: t i ⋅ t j = S R ( ( t i , t j ) , ( s i , s j ) , O ) {\\displaystyle t_{i}\\cdot t_{j}=SR((t_{i},t_{j}),(s_{i},s_{j}),O)} where s i and s j are senses of terms t i and t j respectively, maximizing S C M ⋅ S P E {\\displaystyle SCM\\cdot SPE} . Building also on the first approach, Waitelonis et al. [ 3 ] have computed semantic relatedness from Linked Open Data resources including DBpedia as well as the YAGO taxonomy . Thereby they exploits taxonomic relationships among semantic entities in documents and queries after named entity linking .",
    "links": [
      "ISBN (identifier)",
      "Entity linking",
      "YAGO (database)",
      "Linked data",
      "WordNet",
      "Doi (identifier)",
      "Vector space model",
      "DBpedia",
      "Association for Computing Machinery",
      "Information retrieval"
    ]
  },
  "National Bureau of Standards": {
    "url": "https://en.wikipedia.org/wiki/National_Bureau_of_Standards",
    "title": "National Bureau of Standards",
    "content": "The National Institute of Standards and Technology ( NIST ) is an agency of the United States Department of Commerce whose mission is to promote American innovation and industrial competitiveness. NIST's activities are organized into physical science laboratory programs that include nanoscale science and technology , engineering , information technology , neutron research, material measurement, and physical measurement. From 1901 to 1988, the agency was named the National Bureau of Standards . [ 4 ] The Articles of Confederation , ratified by the colonies in 1781, provided: The United States in Congress assembled shall also have the sole and exclusive right and power of regulating the alloy and value of coin struck by their own authority, or by that of the respective states—fixing the standards of weights and measures throughout the United States. [ 5 ] Article 1, section 8, of the Constitution of the United States , ratified in 1789, granted these powers to the new Congress: \"The Congress shall have power ... To coin money, regulate the value thereof, and of foreign coin, and fix the standard of weights and measures\". [ 6 ] In January 1790, President George Washington , in his first annual message to Congress , said, \"Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to.\" [ 7 ] On October 25, 1791, Washington again appealed to Congress: A uniformity of the weights and measures of the country is among the important objects submitted to you by the Constitution and if it can be derived from a standard at once invariable and universal, must be no less honorable to the public council than conducive to the public convenience. [ 8 ] In 1821, President John Quincy Adams declared, \"Weights and measures may be ranked among the necessities of life to every individual of human society.\". [ 9 ] Nevertheless, it was not until 1838 that the United States government adopted a uniform set of standards. [ 6 ] From 1830 until 1901, the role of overseeing weights and measures was carried out by the Office of Standard Weights and Measures, which was part of the Survey of the Coast—renamed the United States Coast Survey in 1836 and the United States Coast and Geodetic Survey in 1878—in the United States Department of the Treasury . [ 10 ] [ 11 ] [ 12 ] In 1901, in response to a bill proposed by Congressman James H. Southard (R, Ohio), the Bureau of Standards was founded with the mandate to provide standard weights and measures, and to serve as the national physical laboratory for the United States. Southard had previously sponsored a bill for metric conversion of the United States. [ 13 ] President Theodore Roosevelt appointed Samuel W. Stratton as the first director. The budget for the first year of operation was $40,000. The Bureau took custody of the copies of the kilogram and meter bars that were the standards for US measures, and set up a program to provide metrology services for United States scientific and commercial users. A laboratory site was constructed in Washington, DC , and instruments were acquired from the national physical laboratories of Europe. In addition to weights and measures, the Bureau developed instruments for electrical units and for measurement of light. In 1905 a meeting was called that would be the first \"National Conference on Weights and Measures\". Initially conceived as purely a metrology agency, the Bureau of Standards was directed by Herbert Hoover to set up divisions to develop commercial standards for materials and products. [ 13 ] Some of these standards were for products intended for government use, but product standards also affected private-sector consumption. Quality standards were developed for products including some types of clothing, automobile brake systems and headlamps, antifreeze , and electrical safety. During World War I , the Bureau worked on multiple problems related to war production, even operating its own facility to produce optical glass when European supplies were cut off. Between the wars, Harry Diamond of the Bureau developed a blind approach radio aircraft landing system. During World War II, military research and development was carried out, including development of radio propagation forecast methods, the proximity fuze and the standardized airframe used originally for Project Pigeon , and shortly afterwards the autonomously radar-guided Bat anti-ship guided bomb and the Kingfisher family of torpedo-carrying missiles. In 1948, financed by the United States Air Force, the Bureau began design and construction of SEAC , the Standards Eastern Automatic Computer. The computer went into operation in May 1950 using a combination of vacuum tubes and solid-state diode logic. About the same time the Standards Western Automatic Computer , was built at the Los Angeles office of the NBS by Harry Huskey and used for research there. A mobile version, DYSEAC , was built for the Signal Corps in 1954. Due to a changing mission, the \"National Bureau of Standards\" became the \"National Institute of Standards and Technology\" in 1988. [ 10 ] Following the September 11, 2001 attacks, under the National Construction Safety Team Act (NCST), NIST conducted the official investigation into the collapse of the World Trade Center buildings. Following the 2021 Surfside condominium building collapse , NIST sent engineers to the site to investigate the cause of the collapse. [ 14 ] In 2019, NIST launched a program named NIST on a Chip to decrease the size of instruments from lab machines to chip size. Applications include aircraft testing, communication with satellites for navigation purposes, and temperature and pressure. [ 15 ] In 2023, the Biden administration began plans to create a U.S. AI Safety Institute within NIST to coordinate AI safety matters. According to The Washington Post , NIST is considered \"notoriously underfunded and understaffed\", which could present an obstacle to these efforts. [ 16 ] NIST, known between 1901 and 1988 as the National Bureau of Standards (NBS), is a measurement standards laboratory , also known as the National Metrological Institute (NMI), which is a non-regulatory agency of the United States Department of Commerce . The institute's official mission is to: [ 17 ] Promote U.S. innovation and industrial competitiveness by advancing measurement science , standards , and technology in ways that enhance economic security and improve our quality of life . — NIST NIST had an operating budget for fiscal year 2007 (October 1, 2006 – September 30, 2007) of about $843.3 million. NIST's 2009 budget was $992 million, and it also received $610 million as part of the American Recovery and Reinvestment Act . [ 18 ] NIST employs about 2,900 scientists, engineers, technicians, and support and administrative personnel. About 1,800 NIST associates (guest researchers and engineers from American companies and foreign countries) complement the staff. NIST partners with 1,400 manufacturing specialists and staff at nearly 350 affiliated centers around the country. NIST publishes the Handbook 44 that provides the \"Specifications, tolerances, and other technical requirements for weighing and measuring devices\". The Congress of 1866 made use of the metric system in commerce a legally protected activity through the passage of Metric Act of 1866 . [ 19 ] In May 1875, 17 out of 20 countries signed a document known as the Metric Convention or the Treaty of the Meter , which established the International Bureau of Weights and Measures under the control of an international committee elected by the General Conference on Weights and Measures . [ 20 ] NIST is headquartered in Gaithersburg, Maryland , and operates a facility in Boulder, Colorado , which was dedicated by President Eisenhower in 1954. [ 21 ] [ 22 ] [ 23 ] NIST's activities are organized into laboratory programs and extramural programs. Effective October 2010, NIST was realigned by reducing the number of NIST laboratory units from ten to six. [ 24 ] NIST Laboratories include: [ 25 ] Extramural programs include: NIST's Boulder laboratories are best known for NIST‑F1 , which houses an atomic clock . NIST‑F1 serves as the source of the nation's official time. From its measurement of the natural resonance frequency of cesium —which defines the second —NIST broadcasts time signals via longwave radio station WWVB near Fort Collins , Colorado, and shortwave radio stations WWV and WWVH , located near Fort Collins and Kekaha, Hawaii , respectively. [ 33 ] NIST also operates a neutron science user facility: the NIST Center for Neutron Research (NCNR). The NCNR provides scientists access to a variety of neutron scattering instruments, which they use in many research fields (materials science, fuel cells, biotechnology, etc.). The SURF III Synchrotron Ultraviolet Radiation Facility is a source of synchrotron radiation , in continuous operation since 1961. SURF III now serves as the US national standard for source-based radiometry throughout the generalized optical spectrum. All NASA -borne, extreme-ultraviolet observation instruments have been calibrated at SURF since the 1970s, and SURF is used for the measurement and characterization of systems for extreme ultraviolet lithography . The Center for Nanoscale Science and Technology (CNST) performs research in nanotechnology , both through internal research efforts and by running a user-accessible cleanroom nanomanufacturing facility. This \"NanoFab\" is equipped with tools for lithographic patterning and imaging (e.g., electron microscopes and atomic force microscopes ). NIST has seven standing committees: As part of its mission, NIST supplies industry, academia, government, and other users with over 1,300 Standard Reference Materials (SRMs). These artifacts are certified as having specific characteristics or component content, used as calibration standards for measuring equipment and procedures, quality control benchmarks for industrial processes, and experimental control samples. NIST publishes the Handbook 44 each year after the annual meeting of the National Conference on Weights and Measures (NCWM). Each edition is developed through cooperation of the Committee on Specifications and Tolerances of the NCWM and the Weights and Measures Division (WMD) of NIST. The purpose of the book is a partial fulfillment of the statutory responsibility for \"cooperation with the states in securing uniformity of weights and measures laws and methods of inspection\". NIST has been publishing various forms of what is now the Handbook 44 since 1918 and began publication under the current name in 1949. The 2010 edition conforms to the concept of the primary use of the SI (metric) measurements recommended by the Omnibus Foreign Trade and Competitiveness Act of 1988 . [ 34 ] [ 35 ] NIST is developing government-wide identity document standards for federal employees and contractors to prevent unauthorized persons from gaining access to government buildings and computer systems. [ 36 ] In 2002, the National Construction Safety Team Act mandated NIST to conduct an investigation into the collapse of the World Trade Center buildings 1 and 2 and the 47-story 7 World Trade Center. The \"World Trade Center Collapse Investigation\", directed by lead investigator Shyam Sunder, [ 37 ] covered three aspects, including a technical building and fire safety investigation to study the factors contributing to the probable cause of the collapses of the WTC Towers (WTC 1 and 2) and WTC 7. NIST also established a research and development program to provide the technical basis for improved building and fire codes, standards, and practices, and a dissemination and technical assistance program to engage leaders of the construction and building community in implementing proposed changes to practices, standards, and codes. [ 38 ] NIST also is providing practical guidance and tools to better prepare facility owners, contractors, architects, engineers, emergency responders, and regulatory authorities to respond to future disasters. In November 2008, the investigation portion of the response plan was completed, with the release of the final report on 7 World Trade Center. The final report on the WTC Towers—including 30 recommendations for improving building and occupant safety—was released in October 2005. [ 38 ] NIST works in conjunction with the Technical Guidelines Development Committee of the Election Assistance Commission to develop the Voluntary Voting System Guidelines for voting machines and other election technology. In February 2014, NIST published the NIST Cybersecurity Framework that serves as voluntary guidance for organizations to manage and reduce cybersecurity risk. [ 39 ] It was later amended and Version 1.1 was published in April 2018. [ 40 ] Executive Order 13800, Strengthening the Cybersecurity of Federal Networks and Critical Infrastructure , made the Framework mandatory for U.S. federal government agencies. [ 39 ] An extension to the NIST Cybersecurity Framework is the Cybersecurity Maturity Model (CMMC) which was introduced in 2019 (though the origin of CMMC began with Executive Order 13556). [ 41 ] On August 25, 2025, the 48 CFR CMMC rule cleared regulatory review. According to ISI, [ 42 ] it published on September 10, 2025. It emphasizes the importance of implementing Zero-trust architecture (ZTA) which focuses on protecting resources over the network perimeter. ZTA utilizes zero trust principles which include \"never trust, always verify\", \"assume breach\" and \"least privileged access\" to safeguard users, assets, and resources. Since ZTA holds no implicit trust to users within the network perimeter, authentication and authorization are performed at every stage of a digital transaction. This reduces the risk of unauthorized access to resources. [ 43 ] NIST released a draft of the CSF 2.0 for public comment to November 4, 2023. NIST decided to update the framework to make it more applicable to small and medium size enterprises that use the framework, as well as to accommodate the constantly changing nature of cybersecurity. [ 44 ] In August 2024, NIST released a final set of encryption tools designed to withstand the attack of a quantum computer. These post-quantum encryption standards secure a wide range of electronic information, from confidential email messages to e-commerce transactions that propel the modern economy. [ 45 ] In May 2025, NIST announced the Moonlight data project to enhance satellite calibration. By providing precise measurements of the Moon's brightness, the initiative aims to improve the accuracy of Earth observation satellites, supporting applications such as agriculture, meteorology, and environmental monitoring. [ 46 ] Four scientific researchers at NIST have been awarded Nobel Prizes for work in physics : William Daniel Phillips in 1997, Eric Allin Cornell in 2001, John Lewis Hall in 2005 and David Jeffrey Wineland in 2012, which is the largest number for any US government laboratory. All four were recognized for their work related to laser cooling of atoms, which is directly related to the development and advancement of the atomic clock. In 2011, Dan Shechtman was awarded the Nobel Prize in chemistry for his work on quasicrystals in the Metallurgy Division from 1982 to 1984. John Werner Cahn was awarded the 2011 Kyoto Prize for Materials Science. The National Medal of Science has been awarded to NIST researchers Cahn (1998) and Wineland (2007). Other notable people who have worked at NBS or NIST include: Since 1989, the director of NIST has been a Presidential appointee and is confirmed by the United States Senate . [ 47 ] Since 1989, the average tenure of NIST directors has fallen from 11 years to 2 years in duration. Since the 2011 reorganization of NIST, the director also holds the title of Under Secretary of Commerce for Standards and Technology. Seventeen individuals have officially held the position, in addition to seven acting directors who have served on a temporary basis. NIST holds patents on behalf of the Federal government of the United States , [ 48 ] with at least one of them being custodial to protect public domain use, such as one for a Chip-scale atomic clock , developed by a NIST team as part of a DARPA competition. [ 49 ] In September 2013, both The Guardian and The New York Times reported that NIST allowed the National Security Agency (NSA) to insert a cryptographically secure pseudorandom number generator called Dual EC DRBG into NIST standard SP 800-90 that had a kleptographic backdoor that the NSA can use to covertly predict the future outputs of this pseudorandom number generator thereby allowing the surreptitious decryption of data. [ 50 ] Both papers report [ 51 ] [ 52 ] that the NSA worked covertly to get its own version of SP 800-90 approved for worldwide use in 2006. The whistle-blowing document states that \"eventually, NSA became the sole editor\". The reports confirm suspicions and technical grounds publicly raised by cryptographers in 2007 that the EC-DRBG could contain a kleptographic backdoor (perhaps placed in the standard by NSA). [ 53 ] NIST responded to the allegations, stating that \"NIST works to publish the strongest cryptographic standards possible\" and that it uses \"a transparent, public process to rigorously vet our recommended standards\". [ 54 ] The agency stated that \"there has been some confusion about the standards development process and the role of different organizations in it...The National Security Agency (NSA) participates in the NIST cryptography process because of its recognized expertise. NIST is also required by statute to consult with the NSA.\" [ 55 ] Recognizing the concerns expressed, the agency reopened the public comment period for the SP800-90 publications, promising that \"if vulnerabilities are found in these or any other NIST standards, we will work with the cryptographic community to address them as quickly as possible\". [ 56 ] Due to public concern of this cryptovirology attack, NIST rescinded the EC-DRBG algorithm from the NIST SP 800-90 standard. [ 57 ] In addition to these journals, NIST, and the National Bureau of Standards before it, has a robust technical reports publishing arm. NIST technical reports are published in several dozen series, which cover a wide range of topics, from computer technology to construction to aspects of standardization including weights, measures and reference data. [ 58 ] In addition to technical reports, NIST scientists publish many journal and conference papers each year; an database of these, along with more recent technical reports, can be found on the NIST website. [ 59 ]",
    "links": [
      "Telemedicine and Advanced Technology Research Center (United States Army)",
      "Lamar Smith",
      "Financial Crimes Enforcement Network",
      "National Institute of Information and Communications Technology",
      "Winnie Wong-Ng",
      "United States House Committee on Science, Space, and Technology",
      "White Sands Test Center",
      "John Kelsey (cryptanalyst)",
      "Marine Corps Combat Development Command",
      "Lisa Murkowski",
      "David W. Allan",
      "Backdoor (computing)",
      "RTZ (radio station)",
      "Richard J. Saykally",
      "Tim Foecke",
      "Federal Aviation Administration",
      "ROA Time",
      "Information Technology Laboratory",
      "United States Army Medical Research and Development Command",
      "Ellen Segal Huvelle",
      "National Nanotechnology Initiative",
      "Taiwan",
      "HD2IOA",
      "Orrin Hatch",
      "Center for Medicare and Medicaid Innovation",
      "Longwave",
      "Information technology",
      "Elham Tabassi",
      "Research and Innovative Technology Administration",
      "William Frederick Meggers",
      "Proximity fuze",
      "Kilogram",
      "RBU (radio station)",
      "Electronic Communications Privacy Act",
      "Bernie Sanders",
      "United States Department of State",
      "China",
      "United States Army Research Institute of Environmental Medicine",
      "Kekaha, Hawaii",
      "Kleptographic",
      "Title IX of the Patriot Act",
      "Title I of the Patriot Act",
      "Air Force Nuclear Weapons Center",
      "Douglas Rayner Hartree",
      "William Clyde Martin (physicist)",
      "Executive Order",
      "William C. Stone (caver)",
      "John Lewis Hall",
      "Gaithersburg, Maryland",
      "National Security Agency",
      "Chip-scale atomic clock",
      "Articles of Confederation",
      "HLA (radio station)",
      "Cybersecurity Maturity Model Certification",
      "United States Army Test and Evaluation Command",
      "Quasicrystal",
      "Paul Dabbar",
      "Naval Air Warfare Center",
      "Electron microscopes",
      "Nobel Prize",
      "National Institute on Disability, Independent Living, and Rehabilitation Research",
      "Harry Huskey",
      "Neutron source",
      "Eric Holder",
      "American Civil Liberties Union",
      "USAspending.gov",
      "Hugh Latimer Dryden",
      "Shortwave",
      "Money Laundering Control Act",
      "United States Congress",
      "National Ocean Service",
      "Metallurgy",
      "Time signal",
      "Office of Export Enforcement",
      "Ecuador",
      "U.S. Army Redstone Test Center",
      "Helen M. Wood",
      "Biomedical Advanced Research and Development Authority",
      "American Library Association",
      "Multiple Biometric Grand Challenge",
      "Constitution of the United States",
      "The New York Times",
      "Bureau of Economic Analysis",
      "National Marine Fisheries Service",
      "Irene Ann Stegun",
      "Marine Corps Operational Test and Evaluation Activity",
      "Space Systems Command",
      "Neutron scattering",
      "South Korea",
      "Second",
      "Lithography",
      "CHU (radio station)",
      "Brazil",
      "National Medal of Science",
      "Dual EC DRBG",
      "Virtual Cybernetic Building Testbed",
      "Wayback Machine",
      "Germany",
      "NASA",
      "Ana Maria Rey",
      "Theodore Roosevelt",
      "Fair Credit Reporting Act",
      "Economic Research Service",
      "ISSN (identifier)",
      "John M. Martinis",
      "United States Army Aviation and Missile Center",
      "Quantum computing",
      "United States Department of Education",
      "Federal Judicial Center",
      "Real Instituto y Observatorio de la Armada",
      "The Guardian",
      "Cryptographically secure pseudorandom number generator",
      "Smart Grid Interoperability Panel",
      "WWVH",
      "Title II of the Patriot Act",
      "Veterans Health Administration Office of Research and Development",
      "Edgar Buckingham",
      "Atomic force microscope",
      "Federal Highway Administration",
      "John E. Sununu",
      "Paul Sarbanes",
      "Identity document",
      "39th United States Congress",
      "John Cantius Garand",
      "Fire safety",
      "Electronic Frontier Foundation",
      "Patrick Leahy",
      "Metric Act of 1866",
      "Philip J. Davis",
      "Laser cooling",
      "National Institute for Occupational Safety and Health",
      "United States Department of the Treasury",
      "AI safety",
      "Japan",
      "Norman Bekkedahl",
      "Frank William John Olver",
      "Air Force Institute of Technology",
      "Dick Armey",
      "National Construction Safety Team Act",
      "Cryptovirology",
      "Joe Biden",
      "Under Secretary of Commerce for Intellectual Property",
      "United States Army Simulation and Training Technology Center",
      "AD-X2",
      "Governmental impact on science during World War II",
      "DARPA",
      "United States Department of Commerce",
      "Project Kingfisher",
      "Larry Craig",
      "Naval Undersea Warfare Center",
      "Project Pigeon",
      "Argentina",
      "United States Marine Corps Warfighting Laboratory",
      "John Conyers",
      "Jon Kyl",
      "Technology",
      "Advanced Research Projects Agency for Health",
      "Lyman James Briggs",
      "Jerry Nadler",
      "Under Secretary of Commerce for Industry and Security",
      "Alberto Gonzales",
      "Right to Financial Privacy Act",
      "Air Force Research Laboratory",
      "Advisory Committee on Earthquake Hazards Reduction",
      "NIST hash function competition",
      "Benét Laboratories",
      "Armed Forces Research Institute of Medical Sciences",
      "Dimensional metrology",
      "Tom Daschle",
      "Fiscal year",
      "Ellen Voorhees",
      "Foreign Intelligence Surveillance Act",
      "SWAC (computer)",
      "Title V of the Patriot Act",
      "Vacuum tube",
      "Outline of physical science",
      "Intelligence Advanced Research Projects Activity",
      "Antifreeze",
      "Patriot Act",
      "Mass spectrometer",
      "General Conference on Weights and Measures",
      "William Weber Coblentz",
      "National Software Reference Library",
      "Office of Science and Technology Policy",
      "Surfside condominium building collapse",
      "John M. Butler (scientist)",
      "United States Senate Committee on Commerce, Science, and Transportation",
      "United States Department of Justice",
      "Neutron",
      "Washington, D.C.",
      "RWM",
      "ISO/IEC 17025",
      "Computer Fraud and Abuse Act",
      "Cesium",
      "ASM-N-2 Bat",
      "Archive-It",
      "Pseudorandom number generator",
      "DHS Science and Technology Directorate",
      "Russell A. Kirsch",
      "Cleanroom",
      "Naval Surface Warfare Center",
      "Butch Otter",
      "NIST Cybersecurity Framework",
      "FBI Science and Technology Branch",
      "Under Secretary of Commerce for Oceans and Atmosphere",
      "Omnibus Foreign Trade and Competitiveness Act",
      "Air Force Materiel Command",
      "Under Secretary of Commerce for Standards and Technology",
      "Milton Abramowitz",
      "Trent Lott",
      "United States Coast and Geodetic Survey",
      "Office of Oceanic and Atmospheric Research",
      "Harry Diamond (engineer)",
      "Economic Development Administration",
      "Christopher Roy Monroe",
      "U.S. Army DEVCOM Ground Vehicle Systems Center",
      "Naval Air Weapons Station China Lake",
      "E. Ward Plummer",
      "Patent",
      "Title VI of the Patriot Act",
      "National Environmental Satellite, Data, and Information Service",
      "Physics",
      "National Archives and Records Administration",
      "Beta (time signal)",
      "Quality of life",
      "Atomic clock",
      "Nada Golmie",
      "Executive Office of the President of the United States",
      "Zero trust security model",
      "International Trade Administration",
      "National Weather Service",
      "Operational Test and Evaluation Force",
      "Samuel Wesley Stratton",
      "Crown glass (optics)",
      "Federal Bureau of Investigation",
      "Radio propagation",
      "Advanced Encryption Standard process",
      "History of the Patriot Act",
      "John Ashcroft",
      "List of United States research and development agencies",
      "United States Army Combat Capabilities Development Command",
      "Standard Reference Material",
      "Physikalisch-Technische Bundesanstalt",
      "National Time Service Center",
      "Ugo Fano",
      "Office of Technology Assessment",
      "National Telecommunications and Information Administration",
      "Time from NPL (MSF)",
      "United States Army Operational Test Command",
      "United States Department of Agriculture",
      "Federal government of the United States",
      "Russ Feingold",
      "United States Army DEVCOM Armaments Center",
      "Marcia Huber",
      "United States Naval Research Laboratory",
      "Engineer Research and Development Center",
      "Humanitarian Law Project",
      "Spain",
      "All-Russian Scientific Research Institute for Physical-Engineering and Radiotechnical Metrology",
      "Center for Public Integrity",
      "Critical infrastructure",
      "United States Department of Health and Human Services",
      "Budget",
      "Electronic Privacy Information Center",
      "Naval Command, Control and Ocean Surveillance Center",
      "William Daniel Phillips",
      "Metre",
      "Synchrotron radiation",
      "Instrument approach",
      "Office of Naval Research",
      "Julie Borchers",
      "Presidency of Joe Biden",
      "Federal Register",
      "United States Army Command, Control, Communication, Computers, Cyber, Intelligence, Surveillance and Reconnaissance Center",
      "Under Secretary of Commerce for International Trade",
      "Naval Postgraduate School",
      "WWV (radio station)",
      "Advanced Technology Program",
      "Viet D. Dinh",
      "Dick Durbin",
      "United Kingdom",
      "James H. Southard",
      "United States Deputy Secretary of Commerce",
      "Edgewood Chemical Biological Center",
      "United States Department of Transportation",
      "Certification of voting machines",
      "Title VII of the Patriot Act",
      "NIST SP 800-90A",
      "Walter Reed Army Institute of Research",
      "Willie E. May",
      "Quantum metrology",
      "Agricultural Research Service",
      "Telemarketing and Consumer Fraud and Abuse Prevention Act",
      "Cold Regions Research and Engineering Laboratory",
      "BSF (time service)",
      "Election Assistance Commission",
      "Korea Research Institute of Standards and Science",
      "Smart Metrology",
      "Wilfrid Basil Mann",
      "Herbert Hoover",
      "U.S. Global Change Research Program",
      "Voting machine",
      "VAMAS",
      "Marla Dowell",
      "National Institutes of Health",
      "Safeguard",
      "Inorganic Crystal Structure Database",
      "United States Army Research Laboratory",
      "Dennis Kucinich",
      "Herbert C. Hoover Building",
      "National Institute of Food and Agriculture",
      "BPC (time signal)",
      "Bob Graham",
      "Deborah S. Jin",
      "United States Department of Energy",
      "Post-quantum encryption",
      "National Technical Information Service",
      "BPL (time service)",
      "Dwight D. Eisenhower",
      "Katharine Blodgett Gebbie",
      "Metrology",
      "List of directors of the National Institute of Standards and Technology",
      "Digital Library of Mathematical Functions",
      "United States Immigration and Customs Enforcement",
      "Bureau of Industry and Security",
      "James Sacra Albus",
      "ALS162 time signal",
      "United States Department of Energy National Laboratories",
      "Perley G. Nutting",
      "United States Census Bureau",
      "United States Commercial Service",
      "Canada",
      "NIST-F1",
      "United States Customs Service",
      "United States Army Medical Research Institute of Infectious Diseases",
      "Extreme ultraviolet lithography",
      "United States",
      "Title III of the Patriot Act",
      "Under Secretary of Commerce for Economic Affairs",
      "Office of Science",
      "Kathryn Beers",
      "Ferdinand Graft Brickwedde",
      "National Conference on Weights and Measures",
      "John Quincy Adams",
      "President of the United States",
      "American Recovery and Reinvestment Act of 2009",
      "United States Department of Veterans Affairs",
      "International Organization for Standardization",
      "International Bureau of Weights and Measures",
      "Collapse of the World Trade Center",
      "Wikisource",
      "Samuel Wesley Stratton Award",
      "OL (identifier)",
      "Office of Marine and Aviation Operations",
      "United States Senate",
      "United States Army Medical Research Institute of Chemical Defense",
      "Outline of metrology and measurement",
      "Combat Capabilities Development Command Soldier Center",
      "World War I",
      "Doi (identifier)",
      "Family Educational Rights and Privacy Act",
      "Italy",
      "United States Department of Homeland Security",
      "Networking and Information Technology Research and Development",
      "Charlotte Emma Moore Sitterly",
      "Argentine Naval Hydrographic Service",
      "Technical standard",
      "James G. Nell",
      "Mike Oxley",
      "Magnus Rudolph Hestenes",
      "Anne Plant",
      "Facial age estimation",
      "National Science Foundation",
      "National Research Council Canada",
      "Sheldon M. Wiederhorn",
      "NIST (disambiguation)",
      "Time metrology",
      "Nanomanufacturing",
      "United States Secretary of Commerce",
      "Finland",
      "Boulder, Colorado",
      "Dianne Feinstein",
      "Immigration and Nationality Act of 1952",
      "DYSEAC",
      "National Oceanic and Atmospheric Administration",
      "Institute of Education Sciences",
      "Dan Shechtman",
      "Arlen Specter",
      "WWVB",
      "American Institute of Physics",
      "Bank Secrecy Act",
      "Engineering",
      "Marilyn E. Jacox",
      "United States Army Medical Research Unit-Kenya",
      "Omnibus Crime Control and Safe Streets Act of 1968",
      "Nanotechnology",
      "United States Geological Survey",
      "Russia",
      "Minority Business Development Agency",
      "SEAC (computer)",
      "Ronald Collé",
      "Measurement standards laboratory",
      "United States Environmental Protection Agency",
      "United States Coast Guard Research and Development Center",
      "Radio station",
      "Centre for Metrology and Accreditation",
      "Air Armament Center",
      "CHAYKA",
      "September 11 attacks",
      "International System of Units",
      "Alexander Maranghides",
      "LCCN (identifier)",
      "Technical Report Archive & Image Library",
      "Jack R. Edmonds",
      "Voluntary Voting System Guidelines",
      "JJY",
      "George W. Bush",
      "Air Force Operational Test and Evaluation Center",
      "United States Department of the Interior",
      "Journal of Physical and Chemical Reference Data",
      "United States House Permanent Select Committee on Intelligence",
      "Fort Collins, Colorado",
      "Radio teleswitch",
      "Technical Guidelines Development Committee",
      "Malcolm Baldrige National Quality Award",
      "John Werner Cahn",
      "National Institute of Justice",
      "Naval Medical Research Center",
      "DCF77",
      "Office of Financial Research",
      "Cornelius Lanczos",
      "JN53DV",
      "Howard Lutnick",
      "Jacob Rabinow",
      "Venezuela",
      "Eric Allin Cornell",
      "YVTO",
      "David Jeffrey Wineland",
      "Barack Obama",
      "United States Space Force",
      "Jay Rockefeller",
      "Agency for Healthcare Research and Quality",
      "Ron Wyden",
      "State of the Union",
      "United States dollar",
      "Center for Democracy and Technology",
      "Scientific Working Group",
      "Title VIII of the Patriot Act",
      "Air Force Life Cycle Management Center",
      "NIST Handbook of Mathematical Functions",
      "Journal of Research of the National Institute of Standards and Technology",
      "Science policy of the United States",
      "NIST Center for Neutron Research",
      "Charlotte Froese Fischer",
      "Chuck Schumer",
      "Ron Paul",
      "The Washington Post",
      "Title IV of the Patriot Act",
      "Federal Audit Clearinghouse",
      "United States Department of Defense",
      "Michael Chertoff",
      "ARPA-E",
      "Uniformed Services University of the Health Sciences",
      "United States Patent and Trademark Office",
      "Geospatial Research Laboratory",
      "Forensic metrology",
      "Federal judiciary of the United States",
      "Victims of Crime Act of 1984",
      "BPM (time service)",
      "Title X of the Patriot Act",
      "George Washington",
      "Diode",
      "National Physical Laboratory (United Kingdom)",
      "France"
    ]
  },
  "Desk Set": {
    "url": "https://en.wikipedia.org/wiki/Desk_Set",
    "title": "Desk Set",
    "content": "Desk Set (released as His Other Woman in the UK) is a 1957 American romantic comedy film starring Spencer Tracy and Katharine Hepburn . Directed by Walter Lang , the picture's screenplay was written by Phoebe Ephron and Henry Ephron , adapted from the 1955 play of the same name by William Marchant . Bunny Watson is a documentalist in charge of the reference library at the Federal Broadcasting Network in Midtown Manhattan . The reference librarians are responsible for researching facts and answering questions for the general public on all manner of topics, great and small. Bunny has been romantically involved for seven years with rising network executive Mike Cutler, but with no marriage in sight. Methods engineer and efficiency expert Richard Sumner is the inventor of EMERAC (\" E lectromagnetic ME mory and R esearch A rithmetical C alculator\"), nicknamed \"Emmy,\" a powerful early generation computer (referred to then as an \"electronic brain\"). He is brought in to see how the library functions, and size it up for installation of one of his massive machines. Despite Bunny's initial intransigence, Richard is surprised and intrigued to discover how stunningly capable and engaging she is. When her staff finds out the computer is coming, they jump to the conclusion they are being replaced. After an innocuous but seemingly salacious situation that Mike walks in on at Bunny's apartment, he recognizes the older Richard has emerged as a romantic rival, and begins to want to commit to Bunny. Bunny's fear of unemployment seems confirmed when she and everyone on her staff receive a pink \"layoff\" slip printed out by a similar new EMERAC already installed in payroll. But it turns out to have been a mistake – the machine fired everybody in the company, including the president. The network has kept everything hush-hush to avoid tipping off competitors that a merger was in the works. Rather than replace the research staff, \"Emmy\" was installed to help the employees cope with the extra work. With the threat of displacement out of the way, Richard reveals his romantic interest to Bunny, but she believes that EMERAC will always be his first love. He denies it, but then Bunny puts him to the test, pressing the machine beyond its limits. Richard resists the urge to fix it as long as possible, but finally gives in and forces an emergency shutdown. Bunny then accepts his marriage proposal. In the play, Bunny Watson (played by Shirley Booth , who was originally intended for the film as well) had only brief, somewhat hostile interactions with Richard Sumner. Screenwriters Phoebe and Henry Ephron (the parents of Nora Ephron ) built up the role of the efficiency expert and tailored the interactions between him and the researcher to fit Spencer Tracy and Katharine Hepburn. [ 3 ] The exterior shots of the \"Federal Broadcasting Network\" seen in the film are actually of the RCA Building (now known as the Comcast Building) at 30 Rockefeller Plaza in Rockefeller Center , the headquarters of NBC . The character of Bunny Watson was based on Agnes E. Law, a real-life librarian at CBS who retired about a year before the film was released. [ 4 ] [ 5 ] This film was the eighth screen pairing of Hepburn and Tracy, after a five-year respite since 1952's Pat and Mike , and was a first for Hepburn and Tracy in several ways: the first non-MGM film the two starred in together, their first color film, and their first CinemaScope film. Following Desk Set their last film together would be 1967's Guess Who's Coming to Dinner . The computer referred to as EMERAC is a homoiophone metonym for ENIAC (\" E lectronic N umerical I ntegrator A nd C omputer\"), which was developed in the 1940s and was the first electronic general-purpose computer. Parts of the EMERAC computer, particularly the massive display of moving square lights, would later be seen in various 20th Century Fox productions including both the motion picture (1961) and TV (1964–1968) versions of Voyage to the Bottom of the Sea and the Edgar Hopper segment of the 1964 film What a Way to Go! . The researchers furnish incorrect information about the career of baseball player Ty Cobb . Miss Costello claims his major league career lasted for 21 years, and that he played only for the Detroit Tigers . In fact, he played for 24 years—22 with Detroit, and his final two seasons with the Philadelphia Athletics . There is a well-known \"goof\" in one scene. Mike gives Bunny an arrangement of white carnations, and she inserts one in his lapel's button-hole. At the end of the day, she and Richard leave the office. She is carrying the white carnation arrangement as they enter the elevator. As they exit the building, the carnations are pink. [ 6 ] Bosley Crowther , film critic of The New York Times , felt the film was \"out of dramatic kilter\", inasmuch as Hepburn was simply too \"formidable\" to convincingly play someone \"scared by a machine\", resulting in \"not much tension in this thoroughly lighthearted film\". [ 7 ] The New York Post review was mixed: \"There are such sops to sentiment as Miss Hepburn's willingness to be dragged altarwards by the young head of her department, Gig Young , who kindly lets her do her most impressive work, and a growing understanding between Hepburn and the rather remote and intellectually Olympian Tracy....Running true to form, the sex narrative follows a predictable pattern, rewarding honest virtue and slapping down the unworthy, and the other, scientific trail is permitted a twist that may surprise any who have found themselves emotionally involved in that timely problem of technological unemployment....'Desk Set,' let us conclude, is a shining piece of machinery brought to a high polish, and, delivered with appropriate performances, flourishes. Affection, though, it cannot inspire.\" [ 8 ] TIME magazine wrote: \"At long last, somebody has a kind word for the girls in the research department. The word: one of those electronic brains could do the job much better and with less back chat—and what's more, it would free the girls' energies for the more important job of getting a man....Desk Set has been expanded [from the play] by a sizable pigeonhole, in which [Hepburn and Tracy] intermittently bill and coo....On the whole, the film compares favorably with the play....And though Actress Hepburn tends to wallow in the wake of Shirley Booth...she never quite sinks in the comic scenes, and in the romantic ones she is light enough to ride the champagne splashes of emotion as if she were going over Niagara in a barrel. Spencer Tracy has one wonderful slapstick scene, and Gig Young does very well with a comic style for which he is much beholden to William Holden.\" [ 9 ] The Philadelphia Inquirer was critical: \"The middle-aged excesses of Miss Hepburn and Tracy...leave a good deal to be desired. Equipped with an insubstantial vehicle, bogged down by surprisingly flat-footed direction...the stars come close to being embarrassing as they bound through roles involving them in office nonsense about a mechanical brain, a bibulous Christmas party, an innocent, but suspecting, dinner in negligee Katie's rain-bound flat....Marchant's foolish little comedy gains nothing via the Phoebe and Henry Ephron adaptation. Long recitations from \"Hiawatha\" and \"The Curfew Shall Not Ring Tonight,\" plus question-and-answer games...in addition to the repetitiousness of the central idea...turn 'Desk Set's' 104 minutes into an endurance contest for cast and audience.\" [ 10 ] Today the film is seen far more favorably, with the sharpness of the script praised in particular. It has achieved a rare 100% rating on Rotten Tomatoes based on 22 reviews, with a weighted average of 6.78/10. The site's consensus reads: \" Desk Set reunites one of cinema's most well-loved pairings for a solidly crafted romantic comedy that charmingly encapsulates their timeless appeal\". [ 11 ] Dennis Schwartz of Dennis Schwartz Movie Reviews called it an \"inconsequential sex comedy,\" but contended \"the star performers are better than the material they are given to work with\" and that \"the comedy was so cheerful and the banter between the two was so refreshingly smart that it was easy to forgive this bauble for not being as rich as many of the legendary duo's other films together.\" [ 12 ] A Canadian radio program, Bunny Watson , was named for and inspired by Hepburn's character.",
    "links": [
      "The Ladybird (film)",
      "The Jackpot",
      "Hello Sister (1930 film)",
      "Can-Can (film)",
      "Wife, Doctor and Nurse",
      "The Desert Bride (1928 film)",
      "30 Rockefeller Plaza",
      "There's No Business Like Show Business (film)",
      "Comcast",
      "Without Love (film)",
      "Money to Burn (1926 film)",
      "Sally in Our Alley (1927 film)",
      "Command Performance (1931 film)",
      "The Costello Case",
      "Computer",
      "Leon Shamroy",
      "Nora Ephron",
      "Carnival (1935 film)",
      "Katharine Hepburn and Spencer Tracy",
      "Robert Simpson (film editor)",
      "The Blue Bird (1940 film)",
      "Diane Jergens",
      "Ty Cobb",
      "IMDb",
      "Rockefeller Center",
      "Tin Pan Alley (film)",
      "By Whose Hand? (1927 film)",
      "Gig Young",
      "Romantic comedy",
      "Katharine Hepburn",
      "AFI Catalog of Feature Films",
      "Cyril J. Mockridge",
      "Rotten Tomatoes",
      "When My Baby Smiles at Me (film)",
      "The Party's Over (1934 film)",
      "Ergonomics",
      "William Marchant (playwright)",
      "The King and I (1956 film)",
      "List of American films of 1957",
      "Voyage to the Bottom of the Sea (TV series)",
      "Greenwich Village (film)",
      "What a Way to Go!",
      "Shirley Booth",
      "Metonymy",
      "Detroit Tigers",
      "Adam's Rib",
      "The Little Princess (1939 film)",
      "Radio Corporation of America",
      "The Earth Woman",
      "Weighted arithmetic mean",
      "The Spirit of Youth",
      "Brothers (1930 film)",
      "Neva Patterson",
      "Ida Moore",
      "Cheaper by the Dozen (1950 film)",
      "Keeper of the Flame (film)",
      "Women Go on Forever",
      "The Night Flyer (film)",
      "The Golden Web (1926 film)",
      "Pink slip (employment)",
      "Guess Who's Coming to Dinner",
      "Phoebe Ephron",
      "Sentimental Journey (film)",
      "New York Post",
      "Philadelphia Inquirer",
      "The College Hero",
      "State Fair (1945 film)",
      "The Mighty Barnum",
      "The Satin Woman",
      "20th Century Fox",
      "Midtown Manhattan",
      "The Warrior's Husband",
      "Merry Anders",
      "I'll Give a Million (1938 film)",
      "With a Song in My Heart (film)",
      "Whom the Gods Destroy (1934 film)",
      "Moon Over Miami (film)",
      "Joan Blondell",
      "Top of the Town (film)",
      "Star Dust (film)",
      "Time (magazine)",
      "You're My Everything (film)",
      "Woman of the Year",
      "The Sea of Grass (film)",
      "IMDb (identifier)",
      "Philadelphia Athletics",
      "The New York Times",
      "Susannah of the Mounties (film)",
      "Canada",
      "Archer Winsten",
      "The Great Profile",
      "Fandango Media",
      "Bunny Watson",
      "The Baroness and the Butler",
      "Song of the Islands",
      "Snow White and the Three Stooges",
      "The Big Fight (1930 film)",
      "Voyage to the Bottom of the Sea",
      "Love Before Breakfast",
      "Cock o' the Walk (1930 film)",
      "Week-End in Havana",
      "CinemaScope",
      "The Magnificent Dope",
      "The Red Kimono",
      "On the Riviera",
      "Hell Bound (1931 film)",
      "Columbia Broadcasting Service",
      "Henry Ephron",
      "NBC",
      "Spencer Tracy",
      "Meet the Baron",
      "Sitting Pretty (1948 film)",
      "Pat and Mike",
      "Rachel Stephens",
      "State of the Union (film)",
      "Documentalist",
      "Coney Island (1943 film)",
      "Bosley Crowther",
      "Sue Randall",
      "ISBN (identifier)",
      "20th Century-Fox",
      "Second Honeymoon (1937 film)",
      "Mother Wore Tights",
      "ENIAC",
      "Walter Lang",
      "Claudia and David",
      "Hooray for Love (film)",
      "Call Me Madam (film)",
      "Dina Merrill",
      "No More Orchids",
      "But Not for Me (film)",
      "The Marriage-Go-Round (film)"
    ]
  },
  "Computer data storage": {
    "url": "https://en.wikipedia.org/wiki/Computer_data_storage",
    "title": "Computer data storage",
    "content": "Computer data storage or digital data storage is the retention of digital data via technology consisting of computer components and recording media . Digital data storage is a core function and fundamental component of computers. [ 1 ] : 15–16 Generally, the faster and volatile storage components are referred to as \" memory \", while slower persistent components are referred to as \"storage\". This distinction was extended in the Von Neumann architecture , where the central processing unit (CPU) consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data. In practice, almost all computers use a memory hierarchy , [ 1 ] : 468–473 which puts memory close to the CPU and storage further away. In modern computers, hard disk drives (HDDs) or solid-state drives (SSDs) are usually used as storage. A modern digital computer represents data using the binary numeral system . The memory cell is the fundamental building block of computer memory , storing stores one bit of binary information that can be set to store a 1, reset to store a 0, and accessed by reading the cell. [ 2 ] [ 3 ] Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of bits , or binary digits, each of which has a value of 0 or 1. The most common unit of storage is the byte , equal to 8 bits. Digital data comprises the binary representation of a piece of information, often being encoded by assigning a bit pattern to each character , digit , or multimedia object. Many standards exist for encoding (e.g. character encodings like ASCII , image encodings like JPEG , and video encodings like MPEG-4 ). For security reasons , certain types of data may be encrypted in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots. Encryption in transit protects data as it is being transmitted. [ 4 ] Data compression methods allow in many cases (such as a database) to represent a string of bits by a shorter bit string (\"compress\") and reconstruct the original string (\"decompress\") when needed. This utilizes substantially less storage (tens of percent) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of the trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data compressed or not. Distinct types of data storage have different points of failure and various methods of predictive failure analysis . Vulnerabilities that can instantly lead to total loss are head crashing on mechanical hard drives and failure of electronic components on flash storage. Redundancy allows the computer to detect errors in coded data (for example, a random bit flip due to random radiation ) and correct them based on mathematical algorithms. The cyclic redundancy check (CRC) method is typically used in communications and storage for error detection . Redundancy solutions include storage replication , disk mirroring and RAID ( Redundant Array of Independent Disks ). Impending failure on hard disk drives is estimable using S.M.A.R.T. diagnostic data that includes the hours of operation and the count of spin-ups, though its reliability is disputed. [ 5 ] The health of optical media can be determined by measuring correctable minor errors , of which high counts signify deteriorating and/or low-quality media. Too many consecutive minor errors can lead to data corruption. Not all vendors and models of optical drives support error scanning. [ 6 ] Without a significant amount of memory, a computer would only be able to perform fixed operations and immediately output the result, thus requiring hardware reconfiguration for a new program to be run. This is often used in devices such as desk calculators , digital signal processors , and other specialized devices. Von Neumann machines differ in having a memory in which operating instructions and data are stored, [ 1 ] : 20 such that they do not need to have their hardware reconfigured for each new program, but can simply be reprogrammed with new in-memory instructions. They also tend to be simpler to design , in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann machines. In contemporary usage, the term \"storage\" typically refers to a subset of computer data storage that comprises storage devices and their media not directly accessible by the CPU , that is, secondary or tertiary storage . Common forms of storage include hard disk drives , optical disc drives, and non-volatile devices (i.e. devices that retain their contents when the computer is powered down). [ 7 ] On the other hand, the term \" memory \" is used to refer to semiconductor read-write data storage, typically dynamic random-access memory (DRAM). Dynamic random-access memory is a form of volatile memory that also requires the stored information to be periodically reread and rewritten, or refreshed ; static RAM (SRAM) is similar to DRAM, albeit it never needs to be refreshed as long as power is applied. In contemporary usage, the memory hierarchy of primary storage and secondary storage in some uses refer to what was historically called, respectively, secondary storage and tertiary storage . [ 8 ] Primary storage (also known as main memory , internal memory , or prime memory ), often referred to simply as memory , is storage directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in a uniform manner. Historically, early computers used delay lines , Williams tubes , or rotating magnetic drums as primary storage. By 1954, those unreliable methods were mostly replaced by magnetic-core memory . Core memory remained dominant until the 1970s, when advances in integrated circuit technology allowed semiconductor memory to become economically competitive. This led to modern random-access memory , which is small-sized, light, and relatively expensive. RAM used for primary storage is volatile , meaning that it loses the information when not powered. Besides storing opened programs, it serves as disk cache and write buffer to improve both reading and writing performance. Operating systems borrow RAM capacity for caching so long as it's not needed by running software. [ 9 ] Spare memory can be utilized as RAM drive for temporary high-speed data storage. Besides main large-capacity RAM, there are two more sub-layers of primary storage: Primary storage, including ROM , EEPROM , NOR flash , and RAM , [ 10 ] is usually byte-addressable . Such memory is directly or indirectly connected to the central processing unit via a memory bus , comprising an address bus and a data bus . The CPU firstly sends a number called the memory address through the address bus that indicates the desired location of data. Then it reads or writes the data in the memory cells using the data bus. Additionally, a memory management unit (MMU) is a small device between CPU and RAM recalculating the actual memory address. Memory management units allow for memory management ; they may, for example, provide an abstraction of virtual memory or other tasks. Non-volatile primary storage contains a small startup program ( BIOS ) is used to bootstrap the computer, that is, to read a larger program from non-volatile secondary storage to RAM and start to execute it. A non-volatile technology used for this purpose is called read-only memory (ROM). Most types of \"ROM\" are not literally read only but are difficult and slow to write to . Some embedded systems run programs directly from ROM, because such programs are rarely changed. Standard computers largely do not store many programs in ROM, apart from firmware , and use large capacities of secondary storage. Secondary storage (also known as external memory or auxiliary storage ) differs from primary storage in that it is not directly accessible by the CPU. Computers use input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile, retaining data when its power is shut off. Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive. In modern computers, hard disk drives (HDDs) or solid-state drives (SSDs) are usually used as secondary storage. The access time per byte for HDDs or SSDs is typically measured in milliseconds , while the access time per byte for primary storage is measured in nanoseconds . Rotating optical storage devices, such as CD and DVD drives, have even longer access times. Other examples of secondary storage technologies include USB flash drives , floppy disks , magnetic tape , paper tape , punched cards , and RAM disks . To reduce the seek time and rotational latency, secondary storage, including HDD , ODD and SSD , are transferred to and from disks in large contiguous blocks. Secondary storage is addressable by block; once the disk read/write head on HDDs reaches the proper placement and the data, subsequent data on the track are very fast to access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel to increase the bandwidth between primary and secondary memory, for example, using RAID . [ 11 ] Secondary storage is often formatted according to a file system format, which provides the abstraction necessary to organize data into files and directories , while also providing metadata describing the owner of a certain file, the access time, the access permissions, and other information. Most computer operating systems use the concept of virtual memory , allowing the utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks ( pages ) to a swap file or page file on secondary storage, retrieving them later when needed. Tertiary storage or tertiary memory typically involves a robotic arm which mounts and dismount removable mass storage media from a catalog database into a storage device according to the system's demands. It is primarily used for archiving rarely accessed information, since it is much slower than secondary storage (e.g. 5–60 seconds vs. 1–10 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include tape libraries , optical jukeboxes , and massive arrays of idle disks ( MAID ). Tertiary storage is also known as nearline storage because it is \"near to online\". [ 12 ] Hierarchical storage management is an archiving strategy involving automatically migrating long-unused files from fast hard disk storage to libraries or jukeboxes. Offline storage is computer data storage on a medium or a device that is not under the control of a processing unit . [ 13 ] The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. Unlike tertiary storage, it cannot be accessed without human interaction. It is used to transfer information since the detached medium can easily be physically transported. In modern personal computers, most secondary and tertiary storage media are also used for offline storage. A secondary or tertiary storage may connect to a computer utilizing computer networks . This concept does not pertain to the primary storage. Cloud storage is based on highly virtualized infrastructure. [ 14 ] A subset of cloud computing , it has particular cloud-native interfaces, near-instant elasticity and scalability , multi-tenancy , and metered resources. Cloud storage services can be used from an off-premises service or deployed on-premises. [ 15 ] Cloud deployment models define the interactions between cloud providers and customers. [ 16 ] There are three types of cloud storage: Storage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are: Semiconductor memory uses semiconductor -based integrated circuit (IC) chips to store information. Data are typically stored in metal–oxide–semiconductor (MOS) memory cells . A semiconductor memory chip may contain millions of memory cells, consisting of tiny MOS field-effect transistors (MOSFETs) and/or MOS capacitors . Both volatile and non-volatile forms of semiconductor memory exist, the former using standard MOSFETs and the latter using floating-gate MOSFETs . In modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor random-access memory (RAM), particularly dynamic random-access memory (DRAM). Since the turn of the century, a type of non-volatile floating-gate semiconductor memory known as flash memory has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers that are designed for them. As early as 2006, notebook and desktop computer manufacturers started using flash-based solid-state drives (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD. [ 35 ] [ 36 ] [ 37 ] [ 38 ] [ 39 ] Magnetic storage uses different patterns of magnetization on a magnetically coated surface to store information. Magnetic storage is non-volatile . The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms: In early computers, magnetic storage was also used as: Magnetic storage does not have a definite limit of rewriting cycles like flash storage and re-writeable optical media, as altering magnetic fields causes no physical wear. Rather, their life span is limited by mechanical parts. [ 40 ] [ 41 ] Optical storage , the typical optical disc , stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a laser diode and observing the reflection. Optical disc storage is non-volatile . The deformities may be permanent (read only media), formed once (write once media) or reversible (recordable or read/write media). The following forms are in common use as of 2009 [update] : [ 42 ] Magneto-optical disc storage is optical disc storage where the magnetic state on a ferromagnetic surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is non-volatile , sequential access , slow write, fast read storage used for tertiary and off-line storage. 3D optical data storage has also been proposed. Light induced magnetization melting in magnetic photoconductors has also been proposed for high-speed low-energy consumption magneto-optical storage. [ 43 ] Paper data storage , typically in the form of paper tape or punched cards , has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole. Barcodes make it possible for objects that are sold or transported to have some computer-readable information securely attached. Relatively small amounts of digital data (compared to other digital data storage) may be backed up on paper as a matrix barcode for very long-term storage, as the longevity of paper typically exceeds even magnetic data storage. [ 44 ] [ 45 ] This article incorporates public domain material from Federal Standard 1037C . General Services Administration . Archived from the original on 22 January 2022.",
    "links": [
      "Data integrity",
      "Volatile memory",
      "DDR2 SDRAM",
      "Light pen",
      "DVD-RW",
      "RAM",
      "Control unit",
      "Immutable object",
      "Software rot",
      "Arithmetic and logic unit",
      "Predictive failure analysis",
      "Central processing unit",
      "Software entropy",
      "Cloud security",
      "Phase-change memory",
      "Quantum memory",
      "Quad Data Rate SRAM",
      "Core dump",
      "Computer case",
      "Random-access memory",
      "History of computing hardware (1960s–present)",
      "Instance (computer science)",
      "Voltage regulator module",
      "Byte-addressable",
      "Programmable read-only memory",
      "8 mm video format",
      "Ferroelectric RAM",
      "Expansion card",
      "Writing",
      "CD-ROM",
      "Address space",
      "Disk pack",
      "Volume (computing)",
      "Optical mark recognition",
      "Blu-ray",
      "Bubble memory",
      "Network-attached storage",
      "LaserDisc",
      "Disk storage",
      "Millipede memory",
      "Synchronous dynamic random-access memory",
      "Bibcode (identifier)",
      "Paper tape",
      "HD DVD",
      "Clustered file system",
      "Database",
      "Multi-tenancy",
      "Output device",
      "Cloud computing",
      "EPROM",
      "Tape libraries",
      "CiteSeerX (identifier)",
      "File copying",
      "Magnetic-core memory",
      "DDR SDRAM",
      "Memory wall",
      "Page (computer memory)",
      "Replication (computing)",
      "Failure of electronic components",
      "Latency (engineering)",
      "Cathode-ray tube",
      "USB flash drive",
      "VGA connector",
      "Z-RAM",
      "Computer speakers",
      "Videotape",
      "Quadruplex videotape",
      "Computer mouse",
      "Softcam",
      "Sound card",
      "Digital signal processing",
      "Scratchpad memory",
      "ArXiv (identifier)",
      "Object storage",
      "Data deduplication",
      "XQD card",
      "RDRAM",
      "Backup",
      "Non-RAID drive architectures",
      "Optical disc drive",
      "Persistent data structure",
      "MiniDisc",
      "Federal standard 1037C",
      "Write once read many",
      "SDRAM",
      "Floating-gate",
      "CD Video",
      "Video CD",
      "Laser",
      "Memory management unit",
      "Tape drive",
      "Optical disc",
      "Photopolymer",
      "NOR flash",
      "Microprocessor",
      "Racetrack memory",
      "Desktop computer",
      "Hybrid cloud storage",
      "Digital Visual Interface",
      "Cache coherence",
      "Read–write memory",
      "Gigabyte",
      "RAM drive",
      "IEEE 1394",
      "Outsourcing",
      "Disk array",
      "Metadata",
      "Digital rights management",
      "Single-instance storage",
      "Selectron tube",
      "Microphone",
      "Laser turntable",
      "SO-DIMM",
      "Memory card",
      "HDMI",
      "Phone connector (audio)",
      "Shared resource",
      "Binary numeral system",
      "Flash memory",
      "Wayback Machine",
      "Metered",
      "Morgan Kaufmann Publishers",
      "Paper",
      "Image scanner",
      "1T-SRAM",
      "Integrated circuit",
      "DVD-RAM",
      "Input/output",
      "Data loss",
      "ISBN (identifier)",
      "Linear Tape-Open",
      "Home computer",
      "Address bus",
      "Ultra Density Optical",
      "Floppy disk",
      "PMC (identifier)",
      "Data storage tag",
      "Hyper CD-ROM",
      "Virtual memory",
      "Memory Stick",
      "S.M.A.R.T",
      "Memory-mapped file",
      "Locality of reference",
      "Ferromagnetic",
      "General Services Administration",
      "List of pioneers in computer science",
      "GUID Partition Table",
      "DVD card",
      "CD",
      "In-memory processing",
      "Write protection",
      "Edge-notched card",
      "Serial port",
      "Power-on hours",
      "Webcam",
      "VHS",
      "Super Video CD",
      "5D optical data storage",
      "ASCII",
      "Phase-change material",
      "S2CID (identifier)",
      "Directory (computing)",
      "Virtual tape library",
      "Fibre Channel",
      "Sequential access",
      "Computer",
      "Information transfer",
      "Computer memory",
      "Wide area network",
      "Bank switching",
      "Index card",
      "Tape library",
      "S-VHS",
      "Encrypted",
      "Data validation and reconciliation",
      "Technology",
      "Floating-gate MOSFET",
      "Memory map",
      "MIL-CD",
      "Punched card",
      "Nonvolatile BIOS memory",
      "Electrochemical RAM",
      "Motherboard",
      "Object file",
      "Data communication",
      "Virtual private cloud",
      "Touchscreen",
      "Hard disk drive performance characteristics",
      "Cambridge University Press",
      "Optical jukebox",
      "Operating system",
      "CPU cache",
      "Vacuum tube",
      "Code",
      "IBM FlashSystem",
      "Virtualization",
      "BD-ROM",
      "SmartMedia",
      "DVD recordable",
      "Dual-ported RAM",
      "MPEG-4",
      "Memory leak",
      "Areal density (computer storage)",
      "XDR DRAM",
      "Data corruption",
      "Computational RAM",
      "Plugboard",
      "Cloud storage",
      "Attack surface",
      "Geoplexing",
      "Printer (computing)",
      "Power supply unit (computer)",
      "Magnetic tape",
      "RAID",
      "Fog computing",
      "Magnetic-tape data storage",
      "D-VHS",
      "Twistor memory",
      "Local area network",
      "Data store",
      "Electronic quantum holography",
      "Data",
      "Hard copy",
      "Thunderbolt (interface)",
      "SATA",
      "Holographic Versatile Disc",
      "Disk image",
      "Paper data storage",
      "Game controller",
      "Data structure",
      "Molecular memory",
      "Memistor",
      "Disk partitioning",
      "Aperture (computer memory)",
      "Multimedia",
      "DNA digital data storage",
      "In-memory database",
      "CPU",
      "Noise-predictive maximum-likelihood detection",
      "Trackball",
      "Patterned media",
      "Ethernet",
      "File deletion",
      "Non-volatile memory",
      "DVD-R",
      "Unstructured data",
      "Solid-state storage",
      "Universal memory",
      "VHS-C",
      "LPDDR",
      "Refreshable braille display",
      "Electrical resistance",
      "SxS",
      "Bit",
      "Drum memory",
      "Blu-ray Disc recordable",
      "Character (computing)",
      "Magnetically",
      "Memory coherence",
      "Scalability",
      "Continuous availability",
      "Stable storage",
      "Laser diode",
      "VLAN",
      "Semiconductor",
      "Wire recording",
      "Fe FET",
      "Magnetic recording",
      "Computer programming",
      "Memory cell (computing)",
      "Papyrus",
      "Crystal",
      "National Communications System",
      "History of computing hardware",
      "Electronic visual display",
      "Tertiary storage",
      "Private cloud computing infrastructure",
      "Nanosecond",
      "Cold data",
      "Byte",
      "NVM Express",
      "DVD+R DL",
      "Disk utility",
      "Memory protection",
      "Disk aggregation",
      "Ultra HD Blu-ray",
      "Wait state",
      "Magneto-optical drive",
      "Big data",
      "Processor register",
      "Copy protection",
      "Flash memory controller",
      "Pointing device",
      "Data bus",
      "Data compression",
      "Read-only memory",
      "Dynamic random-access memory",
      "Information repository",
      "Error detection",
      "Nucleotide",
      "Graphics card",
      "File-hosting service",
      "CD-R",
      "UltraRAM",
      "Content-addressable memory",
      "EDRAM",
      "Programmable ROM",
      "Microcode",
      "Memory address",
      "Programmable metallization cell",
      "Professional Disc",
      "Vision Electronic Recording Apparatus",
      "Bus (computing)",
      "Copyright status of works by the federal government of the United States",
      "Data storage",
      "Phonograph record",
      "Memory segmentation",
      "Persistence (computer science)",
      "CIFS/SMB",
      "MultiMediaCard",
      "PMID (identifier)",
      "DV (video format)",
      "Page cache",
      "Computer monitor",
      "Memory management",
      "Distributed file system for cloud",
      "Carousel memory",
      "Plated-wire memory",
      "Storage virtualization",
      "Touchpad",
      "Self-Monitoring, Analysis and Reporting Technology",
      "MicroMV",
      "Metal–oxide–semiconductor",
      "Reconfiguration",
      "MD Data",
      "SONOS",
      "3D XPoint",
      "Bitwise operation",
      "Betamax",
      "OCLC (identifier)",
      "Boot sector",
      "DVD+RW",
      "MOSFET",
      "Universal Flash Storage",
      "PS/2 port",
      "BD-RE",
      "Non-volatile random-access memory",
      "High Bandwidth Memory",
      "Uninterruptible power supply",
      "Barcode",
      "Direct-attached storage",
      "Memory hierarchy",
      "Laptop",
      "GD-ROM",
      "DVD-Video",
      "Block (data storage)",
      "Analog recording",
      "File sharing",
      "MiniDV",
      "Random access memory",
      "Solid-state hybrid drive",
      "Internet",
      "ROM cartridge",
      "SD card",
      "Mass storage",
      "Transformer read-only storage",
      "DataPlay",
      "Amsterdam",
      "Solid-state drive",
      "Cloud storage gateway",
      "Plotter",
      "Data recovery",
      "Knowledge base",
      "Optical trackpad",
      "Data cleansing",
      "Nintendo optical discs",
      "DVD",
      "J. S. Vitter",
      "Cassette tape",
      "Nearline storage",
      "Disk read/write head",
      "Computer network",
      "Mellon optical memory",
      "Disk formatting",
      "Hierarchical storage management",
      "Temporary file",
      "CD-RW",
      "Input device",
      "Pointing stick",
      "Graphics processing unit",
      "Delay-line memory",
      "Computer keyboard",
      "Mini CD",
      "Encrypted communication",
      "Doi (identifier)",
      "Dew computing",
      "Distributed data store",
      "Data bank",
      "Write buffer",
      "Removable media",
      "Redundancy (information theory)",
      "Digital Linear Tape",
      "Network throughput",
      "Virtual private network",
      "Optical media",
      "Mercury (element)",
      "Processor cache",
      "Optical storage",
      "Polymer",
      "File system",
      "Fax modem",
      "Memristor",
      "Computer port (hardware)",
      "NCR CRAM",
      "Network interface controller",
      "DVD-R DL",
      "CD-i",
      "Matrix barcode",
      "Arithmetic logic unit",
      "Parallel port",
      "Processor (computing)",
      "Magnetoresistive random-access memory",
      "Memory access pattern",
      "EMC Symmetrix",
      "Data model",
      "Video random access memory",
      "Diode matrix",
      "Calculator",
      "Magnetic stripe card",
      "Grid computing",
      "Hard disk drive",
      "Word (computer architecture)",
      "Memory paging",
      "Moore's law",
      "Sound chip",
      "Data redundancy",
      "Floptical",
      "Game port",
      "Resistive random-access memory",
      "T-RAM",
      "Core rope memory",
      "Volume boot record",
      "Edge computing",
      "Data cluster",
      "Holographic data storage",
      "Data validation",
      "Punched tape",
      "USB",
      "Digital data",
      "Parallel ATA",
      "Embedded system",
      "Computer hardware",
      "DisplayPort",
      "Computer architecture",
      "Universal Media Disc",
      "Video RAM (dual-ported DRAM)",
      "Abstraction (computer science)",
      "Data degradation",
      "EEPROM",
      "MicroP2",
      "State (computer science)",
      "Storage area network",
      "Magnetic ink character recognition",
      "3D optical data storage",
      "File storage",
      "Static random-access memory",
      "Block-level storage",
      "U-matic",
      "Magneto-optic Kerr effect",
      "Radiation",
      "Hex dump",
      "Subnet",
      "Semiconductor memory",
      "Random access",
      "Magnetic storage",
      "Thin-film memory",
      "Phonograph cylinder",
      "Storage record",
      "GDDR SDRAM",
      "Cloud database",
      "DVD+R",
      "Von Neumann architecture",
      "Solid state drive",
      "Microform",
      "Memory refresh",
      "Secondary storage",
      "Character encoding",
      "Access time",
      "BD-R",
      "Proceedings of the IEEE",
      "Switched-mode power supply",
      "Addressing mode",
      "Firmware",
      "Williams tube",
      "Flash Core Module",
      "Data security",
      "Head crash",
      "Amdahl's law",
      "Logical disk",
      "Compact Disc Digital Audio",
      "Gibibyte",
      "Data proliferation",
      "RAM parity",
      "Magnetoresistive RAM",
      "Hi-MD",
      "BIOS",
      "Graphics tablet",
      "Magnetization",
      "Master boot record",
      "Network File System",
      "JPEG",
      "Numerical digit",
      "Industrial robot",
      "Cyclic redundancy check",
      "SIM card",
      "MiniDVD",
      "CompactFlash",
      "Digital Data Storage",
      "Computer file",
      "Optical tape",
      "Disk mirroring",
      "Dekatron",
      "Longitudinal wave",
      "Nano-RAM",
      "Core memory",
      "Optical mouse",
      "Distributed database",
      "PC Card",
      "Density (computer storage)",
      "Compact disc",
      "Time crystal",
      "Page address register",
      "Millisecond",
      "Power MOSFET"
    ]
  },
  "Fuzzy retrieval": {
    "url": "https://en.wikipedia.org/wiki/Fuzzy_retrieval",
    "title": "Fuzzy retrieval",
    "content": "Fuzzy retrieval techniques are based on the Extended Boolean model and the Fuzzy set theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the P-norms algorithm. In fuzzy-set theory, an element has a varying degree of membership, say d A , to a given set A instead of the traditional membership choice (is an element/is not an element). In MMM [ 1 ] each index term has a fuzzy set associated with it. A document's weight with respect to an index term A is considered to be the degree of membership of the document in the fuzzy set associated with A . The degree of membership for union and intersection are defined as follows in Fuzzy set theory: According to this, documents that should be retrieved for a query of the form A or B , should be in the fuzzy set associated with the union of the two sets A and B . Similarly, the documents that should be retrieved for a query of the form A and B , should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the or query to be max(d A , d B ) and the similarity of the document to the and query to be min(d A , d B ) . The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the min and max document weights. Given a document D with index-term weights d A1 , d A2 , ..., d An for terms A 1 , A 2 , ..., A n , and the queries: Q or = (A 1 or A 2 or ... or A n ) Q and = (A 1 and A 2 and ... and A n ) the query-document similarity in the MMM model is computed as follows: SlM(Q or , D) = C or1 * max(d A1 , d A2 , ..., d An ) + C or2 * min(d A1 , d A2 , ..., d An ) SlM(Q and , D) = C and1 * min(d A1 , d A2 , ..., d An ) + C and2 * max(d A1 , d A2 ..., d An ) where C or1 , C or2 are \"softness\" coefficients for the or operator, and C and1 , C and2 are softness coefficients for the and operator. Since we would like to give the maximum of the document weights more importance while considering an or query and the minimum more importance while considering an and query, generally we have C or1 > C or2 and C and1 > C and2 . For simplicity it is generally assumed that C or1 = 1 - C or2 and C and1 = 1 - C and2 . Lee and Fox [ 2 ] experiments indicate that the best performance usually occurs with C and1 in the range [0.5, 0.8] and with C or1 > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the Standard Boolean model . The Paice model [ 3 ] is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity: where r is a constant coefficient and w di is arranged in ascending order for and queries and descending order for or queries. When n = 2 the Paice model shows the same behavior as the MMM model. The experiments of Lee and Fox [ 2 ] have shown that setting the r to 1.0 for and queries and 0.7 for or queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of min or max of a set of term weights each time an and or or clause is considered, which can be done in O(n) . The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an and clause or an or clause is being considered. This requires at least an 0(n log n) sorting algorithm . A good deal of floating point calculation is needed too. Lee and Fox [ 2 ] compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC . These are the reported results for average mean precision improvement: These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three. In 2005, Kang et al. [ 4 ] have devised a fuzzy retrieval system indexed by concept identification. If we look at documents on a pure Tf-idf approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document. They report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents. Zadrozny [ 5 ] revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by: The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.",
    "links": [
      "Extended Boolean model",
      "Standard Boolean model",
      "ISBN (identifier)",
      "Lexical chain",
      "Precision and recall",
      "Sorting algorithm",
      "Doi (identifier)",
      "Inspec",
      "Tf-idf",
      "Christopher D. Paice",
      "Information retrieval",
      "Fuzzy set"
    ]
  },
  "Desktop search": {
    "url": "https://en.wikipedia.org/wiki/Desktop_search",
    "title": "Desktop search",
    "content": "Desktop search tools search within a user's own computer files as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video. A variety of desktop search programs are now available; see this list for examples. Most desktop search programs are standalone applications. Desktop search products are software alternatives to the search software included in the operating system , helping users sift through desktop files, emails, attachments, and more. [ 1 ] [ 2 ] [ 3 ] Desktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside unstructured data — the information stored on a user's PC, the directories (folders) and files they've created on a network , documents stored in repositories such as corporate intranets and a multitude of other locations. [ 4 ] Moreover, many companies have structured or unstructured information stored in older file formats to which they don't have ready access. The sector attracted considerable attention in the late 2004 to early 2005 period from the struggle between Microsoft and Google. [ 5 ] [ 6 ] [ 7 ] According to market analysts, both companies were attempting to leverage their monopolies (of web browsers and search engines , respectively) to strengthen their dominance. Due to Google 's complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between US Justice Department and Microsoft that Windows Vista Service Pack 1 would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default. [ 8 ] As of September 2011, Google ended life for Google Desktop . Most desktop search engines build and maintain an index database to improve performance when searching large amounts of data . Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine , [ 9 ] which performs searches over only file names, not contents, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine, [ 10 ] which performs searches over filenames and files' contents without building any indices. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive. Desktop search tools typically collect three types of information about files: Long-term goals for desktop search include the ability to search the contents of image files , sound files and video by context. [ 11 ] [ 12 ] Indexing Service , a \"base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching\", [ 13 ] was originally released in August 1996. It was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation. [ 14 ] This made indexing large amounts of files require extremely powerful hardware and very long wait times. In 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought \"Instant searching\" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters. [ 15 ] Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed, e.g. if the indexed files amounted to around 100GB, the index size would be 10GB. With the release of Windows Vista came Windows Search 3.1. Unlike its predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the RAM and CPU requirements were greatly reduced, cutting back indexing times immensely. Windows Search 4.0 is currently running on all PCs with Windows 7 and up. In 1994 the AppleSearch search engine was introduced, allowing users to fully search all documents within their Macintosh computer, including file format types, meta-data on those files, and content within the files. AppleSearch was a client/server application , and as such required a server separate from the main device in order to function. The biggest issue with AppleSearch were its large resource requirements: \"AppleSearch requires at least a 68040 processor and 5MB of RAM.\" [ 16 ] At the time, a Macintosh computer with these specifications was priced at approximately $1400; equivalent to $2050 in 2015. [ 17 ] On top of this, the software itself cost an additional $1400 for a single license. In 1997, Sherlock was released alongside Mac OS 8.5. Sherlock (named after the famous fictional detective Sherlock Holmes ) was integrated into Mac OS's file browser – Finder . Sherlock extended the desktop search function to the World Wide Web, allowing users to search both locally and externally. Adding additional functions—such as internet access—to Sherlock was relatively simple, as this was done through plugins written as plain text files. Sherlock was included in every release of Mac OS from Mac OS 8 , before being deprecated and replaced by Spotlight and Dashboard in Mac OS X 10.4 Tiger . It was officially removed in Mac OS X 10.5 Leopard Spotlight was released in 2005 as part of Mac OS X 10.4 Tiger . It is a Selection-based search tool, which means the user invokes a query using only the mouse. Spotlight allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage, and uses a built-in calculator and Oxford American Dictionary to offer quick access to small calculations and word definitions. [ 18 ] While Spotlight initially has a long startup time, this decreases as the hard disk is indexed. As files are added by the user, the index is constantly updated in the background using minimal CPU & RAM resources. There are a wide range of desktop search options for Linux users, depending upon the skill level of the user, their preference to use desktop tools which tightly integrate into their desktop environment, command-shell functionality (often with advanced scripting options), or browser-based users interfaces to locally running software. In addition, many users create their own indexing from a variety of indexing packages (e.g. one which does extraction and indexing of PDF/DOC/DOCX/ ODT documents well, another search engine which works ith/ vcard, LDAP, and other directory/contact databases, as well as the conventional find and locate commands. Ubuntu Linux didn't have desktop search until release Feisty Fawn 7.04 . Using Tracker [ 19 ] desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. It not only featured the basic features of file format sorting and meta-data matching, but support for searching through emails and instant messages was added. In 2014 Recoll [ 20 ] was added to Linux distributions, working with other search programs such as Tracker and Beagle to provide efficient full text search. This greatly increased the types of queries and file types that Linux desktop searches could handle. A major advantage of Recoll is that it allows for greater customization of what is indexed; Recoll will index the entire hard disk by default, but can be made to index only selected directories, omitting directories that will never need to be searched. [ 21 ] In openSUSE , starting with KDE4 , the NEPOMUK was introduced. It provided the ability to index a wide range of desktop content, email, and use semantic web technologies (e.g. RDF ) to annotate the database. The introduction faced a few glitches, much of which seemed to be based on the triplestore . Performance improved (at least for queries) by switching the backend to a stripped-down version of the Virtuoso Open Source Edition, however indexing remained a common user complaint. Based on user feedback, the Nepomuk indexing and search has been replaced with the Baloo framework [ 22 ] based on Xapian . [ 23 ]",
    "links": [
      "Windows Search",
      "Metadata",
      "Index (search engine)",
      "Spotlight (Apple)",
      "Indexing Service",
      "Recoll",
      "CPU",
      "DocFetcher",
      "OpenSUSE",
      "RAM",
      "Search engine",
      "Ubuntu distribution",
      "Strigi",
      "Google Desktop",
      "Resource Description Framework",
      "NEPOMUK (software)",
      "GNOME Storage",
      "JPEG",
      "Client–server model",
      "Unity Dash",
      "Intranet",
      "Sherlock (software)",
      "Xapian",
      "Unstructured data",
      "Windows Vista Service Pack 1",
      "Computer network",
      "Everything (software)",
      "Operating system",
      "Windows Vista",
      "US Justice Department",
      "Copernic Desktop Search",
      "File formats",
      "Virtuoso Universal Server",
      "Portable Document Format",
      "Windows 7",
      "Content-based image retrieval",
      "Spotlight (software)",
      "Sherlock Holmes",
      "Computer files",
      "Mac OS 8",
      "Launchy",
      "Web browser",
      "Mac OS X Tiger",
      "Finder (software)",
      "Triplestore",
      "Windows Desktop Search",
      "Data",
      "AppleSearch",
      "Dashboard (Mac OS)",
      "Lookeen",
      "Microsoft",
      "Mac OS X Leopard",
      "Google Quick Search Box",
      "OpenDocument",
      "Tracker (search software)",
      "MP3",
      "Google",
      "Beagle (software)",
      "KDE4"
    ]
  },
  "Lemur Project": {
    "url": "https://en.wikipedia.org/wiki/Lemur_Project",
    "title": "Lemur Project",
    "content": "The Lemur Project is a collaboration between the Center for Intelligent Information Retrieval at the University of Massachusetts Amherst and the Language Technologies Institute at Carnegie Mellon University . The Lemur Project develops search engines, browser toolbars, text analysis tools, and data resources that support research and development of information retrieval and text mining software. The project is best known for its Indri and Galago search engines, the ClueWeb09 and ClueWeb12 datasets, and the RankLib learning-to-rank library. The software and datasets are used widely in scientific and research applications, as well as in some commercial applications. The Lemur Project's software development philosophy emphasizes state-of-the-art accuracy, flexibility, and efficiency. For example, the Indri search engine provides accurate search for large text collections 'out of the box', and data is stored in an accessible manner to support development of new retrieval strategies. Software from the Lemur Project is distributed under open-source licenses that provide flexibility to scientists and software developers. The programming languages used to create Lemur are C , C++ , and Java , and it comes along with the source files and build instructions. The provided source code can be modified for the purpose of developing new libraries. It is compatible with various operating systems which include Linux and Windows. Lemur supports the following features: Lemur Project has the following components: Updates to the Lemur Project components are made twice a year, in June and December. The latest version of the Indri search engine is 5.17. The latest version of the Galago search engine is version 3.18. The latest version of the RankLib learning-to-rank library is 2.14. The latest version of the Sifaka data mining application is 1.8. The Indri search engine is one of the components developed by the Lemur Project. It is open source. The query language that is used in Indri allows researchers to index data or structure documents using simple command line instructions. Indri offers flexibility in terms of adaptation to various current applications. It also can be distributed across a cluster of nodes for high performance. The Indri search engine can handle large collections of data and can understand various data formats like HTML and XML . The Indri API supports various programming and scripting languages like C++, Java , C# , and PHP . This free and open-source software article is a stub . You can help Wikipedia by expanding it .",
    "links": [
      "HTML",
      "Index (search engine)",
      "Language Technologies Institute",
      "University of Massachusetts Amherst",
      "Java (programming language)",
      "PHP",
      "Stemming",
      "Carnegie Mellon University",
      "TF-IDF",
      "Tokenization (lexical analysis)",
      "Relevance feedback",
      "C++",
      "Document clustering",
      "Stop words",
      "C (programming language)",
      "Free and open-source software",
      "XML",
      "Wildcard character",
      "List of information retrieval libraries",
      "C Sharp (programming language)"
    ]
  },
  "Natural language user interface": {
    "url": "https://en.wikipedia.org/wiki/Natural_language_user_interface",
    "title": "Natural language user interface",
    "content": "Natural-language user interface ( LUI or NLUI ) is a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications. Chatbots are a common implementation of natural-language interfaces, enabling users to interact with software through conversational text or speech. [ 1 ] In interface design , natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input . [ 2 ] Natural-language interfaces are an active area of study in the field of natural-language processing and computational linguistics . An intuitive general natural-language interface is one of the active goals of the Semantic Web . Text interfaces are \"natural\" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a \"shallow\" natural-language user interface. A natural-language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which U.S. state has the highest income tax ?', conventional search engines ignore the question and instead search on the keywords 'state', 'income' and 'tax'. Natural-language search, on the other hand, attempts to use natural-language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine, due to the question being included. [ citation needed ] Prototype Nl interfaces had already appeared in the late sixties and early seventies. [ 3 ] Natural-language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the AI winter of the 1970s and 80s. A 1995 paper titled 'Natural Language Interfaces to Databases – An Introduction', describes some challenges: [ 3 ] Other goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages. Finally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome. The natural-language interface gives rise to technology used for many different applications. Some of the main uses are: Below are named and defined some of the applications that use natural-language recognition, and so have integrated utilities listed above. Ubiquity, an add-on for Mozilla Firefox , is a collection of quick and easy natural-language-derived commands that act as mashups of web services, thus allowing users to get information and relate it to current and other webpages. Wolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a search engine would. [ 6 ] It was announced in March 2009 by Stephen Wolfram , and was released to the public on May 15, 2009. [ 7 ] Siri is an intelligent personal assistant application integrated with operating system iOS . The application uses natural language processing to answer questions and make recommendations. Siri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab. [ 8 ]",
    "links": [
      "Wolfram Alpha",
      "Income tax",
      "Latent Dirichlet allocation",
      "Stop word",
      "Audio search engine",
      "Part-of-speech tagging",
      "Automatic summarization",
      "Interactive fiction",
      "Wide area information server",
      "Lexxe",
      "Metasearch engine",
      "AI-complete",
      "Word-sense disambiguation",
      "Document retrieval",
      "Search by sound",
      "Search engine marketing",
      "Syntactic parsing (computational linguistics)",
      "Corpus linguistics",
      "Latent semantic analysis",
      "Bank of English",
      "Mashup (web application hybrid)",
      "Sentence extraction",
      "Topic model",
      "Distributed web crawling",
      "Optical character recognition",
      "Focused crawler",
      "Machine translation",
      "Grammar checker",
      "Braina",
      "Natural-language programming",
      "Sentiment analysis",
      "Semantic analysis (machine learning)",
      "Large language model",
      "Mozilla Firefox",
      "Siri (software)",
      "Neural machine translation",
      "Universal Dependencies",
      "Noisy text",
      "Google Ngram Viewer",
      "ELIZA",
      "Federated search",
      "Semantic query",
      "Formal semantics (natural language)",
      "Enterprise search",
      "Online search",
      "Social search",
      "Semantic Web",
      "Computer-assisted reviewing",
      "Ubiquity (Firefox)",
      "Celia (virtual assistant)",
      "Search/Retrieve via URL",
      "Bag-of-words model",
      "Parsing",
      "List of search engines",
      "Conversational user interface",
      "Predictive text",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "RightNow Technologies",
      "Named-entity recognition",
      "Parallel text",
      "Word embedding",
      "Multisearch",
      "Argument mining",
      "SpaCy",
      "Ontology learning",
      "Powerset (company)",
      "Ask.com",
      "User interface",
      "DBpedia",
      "Representational State Transfer",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Word2vec",
      "Computational linguistics",
      "Selection-based search",
      "Information extraction",
      "Vertical search",
      "Collocation extraction",
      "Thesaurus (information retrieval)",
      "Pachinko allocation",
      "Natural Language Toolkit",
      "Web query classification",
      "Natural language search engine",
      "Word-sense induction",
      "Lexical resource",
      "Language model",
      "Speech synthesis",
      "Wikidata",
      "Shallow parsing",
      "Cross-language search",
      "Multimedia search",
      "Microsoft",
      "Voice search",
      "Speech segmentation",
      "AI winter",
      "Automated speech recognition",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Desktop search",
      "Web crawler",
      "Document-term matrix",
      "Spider trap",
      "Q&A (software)",
      "Small language model",
      "Website mirroring software",
      "Index (search engine)",
      "Natural language processing",
      "Speech corpus",
      "Pronunciation assessment",
      "Hallucination (artificial intelligence)",
      "Rule-based machine translation",
      "Windows OS",
      "Intelligent personal assistant",
      "Wikipedia",
      "Stemming",
      "UBY",
      "Compound-term processing",
      "Text simplification",
      "GloVe",
      "OpenSearch (specification)",
      "Bigram",
      "Semantic parsing",
      "Semantic ambiguity",
      "Text segmentation",
      "Example-based machine translation",
      "Video search engine",
      "Distant reading",
      "Multi-document summarization",
      "Statistical machine translation",
      "Add-on (Mozilla)",
      "Trigram",
      "Concordancer",
      "Text corpus",
      "WordNet",
      "Natural language understanding",
      "Larry Harris (computer scientist)",
      "FastText",
      "BabelNet",
      "Internet search",
      "IOS",
      "SHRDLU",
      "Treebank",
      "BERT (language model)",
      "Apollo 11",
      "Robots exclusion standard",
      "Z39.50",
      "Semantic decomposition (natural language processing)",
      "Moon rocks",
      "Semantic similarity",
      "Spell checker",
      "Hakia",
      "Voice Mail",
      "Search aggregator",
      "Automatic identification and data capture",
      "Explicit semantic analysis",
      "Lexical analysis",
      "United States",
      "Text mining",
      "Concept mining",
      "Language resource",
      "Search engine (computing)",
      "Local search (Internet)",
      "Semantic network",
      "Linguistic Linked Open Data",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Transfer-based machine translation",
      "Search engine",
      "Interface design",
      "Natural user interface",
      "Wayback Machine",
      "Index term",
      "Semantic search",
      "Web indexing",
      "Textual entailment",
      "Speech recognition",
      "Terminology extraction",
      "Stephen Wolfram",
      "Collaborative search engine",
      "Lemmatisation",
      "Text processing",
      "Q-go",
      "Transformer (deep learning architecture)",
      "Natural-language processing",
      "Image retrieval",
      "Anaphora resolution",
      "HarmonyOS",
      "ISBN (identifier)",
      "Keyword search",
      "Pikimal",
      "Question answering",
      "Web search engine",
      "The Guardian",
      "Chatbot",
      "Yebol",
      "GNOME Do",
      "Document classification",
      "Automated essay scoring",
      "NortonLifeLock",
      "Truecasing",
      "Search/Retrieve Web Service",
      "Web archiving",
      "Web query",
      "Long short-term memory",
      "FrameNet",
      "IBM",
      "Cross-language information retrieval",
      "Linux",
      "Machine-readable dictionary",
      "Prolog",
      "Evaluation measures (information retrieval)",
      "Search engine optimization"
    ]
  },
  "Library classification": {
    "url": "https://en.wikipedia.org/wiki/Library_classification",
    "title": "Library classification",
    "content": "A library classification is a system used within a library to organize materials, including books, sound and video recordings, electronic materials, etc., both on shelves and in catalogs and indexes. Each item is typically assigned a call number, which identifies the location of the item within the system. Materials can be arranged by many different factors, typically in either a hierarchical tree structure based on the subject or using a faceted classification system, which allows the assignment of multiple classifications to an object, enabling the classifications to be ordered in many ways. [ 1 ] Library classification is an important and crucial aspect in library and information science . It is distinct from scientific classification in that it has as its goal to provide a useful ordering of documents rather than a theoretical organization of knowledge . [ 2 ] Although it has the practical purpose of creating a physical ordering of documents, it does generally attempt to adhere to accepted scientific knowledge. [ 3 ] Library classification helps to accommodate all the newly published literature in an already created order of arrangement in a filial sequence. [ 4 ] Library classification can be defined as the arrangement of books on shelves, or description of them, in the manner which is most useful to those who read with the ultimate aim of grouping similar things together. Library classification is meant to achieve these four purposes: ordering the fields of knowledge in a systematic way, bring related items together in the most helpful sequence, provide orderly access on the shelf, and provide a location for an item on the shelf. [ 5 ] Library classification is distinct from the application of subject headings in that classification organizes knowledge into a systematic order, while subject headings provide access to intellectual materials through vocabulary terms that may or may not be organized as a knowledge system. [ 6 ] The characteristics that a bibliographic classification demands for the sake of reaching these purposes are: a useful sequence of subjects at all levels, a concise memorable notation, and a host of techniques and devices of number synthesis. [ 7 ] Library classifications were preceded by classifications used by bibliographers such as Conrad Gessner . The earliest library classification schemes organized books in broad subject categories. The earliest known library classification scheme is the Pinakes by Callimachus , a scholar at the Library of Alexandria during the third century BC. During the Renaissance and Reformation era, \"Libraries were organized according to the whims or knowledge of individuals in charge.\" [ 8 ] This changed the format in which various materials were classified. Some collections were classified by language and others by how they were printed. After the printing revolution in the sixteenth century, the increase in available printed materials made such broad classification unworkable, and more granular classifications for library materials had to be developed in the nineteenth century. [ 9 ] In 1627 Gabriel Naudé published a book called Advice on Establishing a Library . At the time, he was working in the private library of Président à mortier Henri de Mesmes II. Mesmes had around 8,000 printed books and many more Greek, Latin and French written manuscripts. Although it was a private library, scholars with references could access it. The purpose of Advice on Establishing a Library was to identify rules for private book collectors to organize their collections in a more orderly way to increase the collection's usefulness and beauty. Naudé developed a classification system based on seven different classes: theology, medicine, jurisprudence, history, philosophy, mathematics, and the humanities. These seven classes would later be increased to twelve. [ 10 ] Advice on Establishing a Library was about a private library, but within the same book, Naudé encouraged the idea of public libraries open to all people regardless of their ability to pay for access to the collection. One of the most famous libraries that Naudé helped improve was the Bibliothèque Mazarine in Paris. Naudé spent ten years there as a librarian. Because of Naudé's strong belief in free access to libraries to all people, the Bibliothèque Mazarine became the first public library in France around 1644. [ 11 ] Although libraries created order within their collections from as early as the fifth century BC, [ 9 ] the Paris Bookseller's classification, developed in 1842 by Jacques Charles Brunet , is generally seen as the first of the modern book classifications. Brunet provided five major classes: theology, jurisprudence, sciences and arts, belles-lettres, and history. [ 12 ] Classification can now be seen as a provider of subject access to information in a networked environment. [ 13 ] There are many standard systems of library classification in use, and many more have been proposed over the years. However, in general, classification systems can be divided into three types depending on how they are used: In terms of functionality, classification systems are often described as: There are few completely enumerative systems or faceted systems; most systems are a blend but favouring one type or the other. The most common classification systems, LCC and DDC, are essentially enumerative, though with some hierarchical and faceted elements (more so for DDC), especially at the broadest and most general level. The first true faceted system was the colon classification of S. R. Ranganathan . [ 14 ] Classification types denote the classification or categorization according to the form or characteristics or qualities of a classification scheme or schemes. Method and system has similar meaning. Method or methods or system means the classification schemes like Dewey Decimal Classification or Universal Decimal Classification. The types of classification is for identifying and understanding or education or research purposes while classification method means those classification schemes like DDC, UDC. The most common systems in English -speaking countries are: Other systems include: Newer classification systems tend to use the principle of synthesis (combining codes from different lists to represent the different attributes of a work) heavily, which is comparatively lacking in LC or DDC. Library classification is associated with library (descriptive) cataloging under the rubric of cataloging and classification , sometimes grouped together as technical services . The library professional who engages in the process of cataloging and classifying library materials is called a cataloger or catalog librarian . Library classification systems are one of the two tools used to facilitate subject access . The other consists of alphabetical indexing languages such as Thesauri and Subject Headings systems. The practice of library classification is a form of the more general task of classification . The work consists of two steps. Firstly, the subject or topic of the material is ascertained. Next, a call number (essentially a book's address) based on the classification system in use at the particular library will be assigned to the work using the notation of the system. Unlike subject heading or thesauri where multiple terms can be assigned to the same work, in library classification systems, each work can only be placed in one class. This is due to shelving purposes: A book can have only one physical place. However, in classified catalogs one may have main entries as well as added entries. Most classification systems like the Dewey Decimal Classification (DDC) and Library of Congress Classification also add a cutter number to each work which adds a code for the main entry (primary access point) of the work (e.g. author). Classification systems in libraries generally play two roles. Firstly, they facilitate subject access by allowing the user to find out what works or documents the library has on a certain subject. [ 18 ] Secondly, they provide a known location for the information source to be located (e.g. where it is shelved). Until the 19th century, most libraries had closed stacks, so the library classification only served to organize the subject catalog . In the 20th century, libraries opened their stacks to the public and started to shelve library material itself according to some library classification to simplify subject browsing. Some classification systems are more suitable for aiding subject access, rather than for shelf location. For example, Universal Decimal Classification , which uses a complicated notation of pluses and colons, is more difficult to use for the purpose of shelf arrangement but is more expressive compared to DDC in terms of showing relationships between subjects. Similarly faceted classification schemes are more difficult to use for shelf arrangement, unless the user has knowledge of the citation order. Depending on the size of the library collection, some libraries might use classification systems solely for one purpose or the other. In extreme cases, a public library with a small collection might just use a classification system for location of resources but might not use a complicated subject classification system. Instead all resources might just be put into a couple of wide classes (travel, crime, magazines etc.). This is known as a \"mark and park\" classification method, more formally called \"reader interest classification\". [ 19 ] As a result of differences in notation, history, use of enumeration, hierarchy, and facets, classification systems can differ in the following ways:",
    "links": [
      "Information professional",
      "Virtual school libraries in the United States",
      "Conrad Gessner",
      "Library and information scientist",
      "Nippon Decimal Classification",
      "Categorization",
      "Doi (identifier)",
      "List of libraries by country",
      "List of female archivists",
      "Brian Deer Classification System",
      "NLM Classification",
      "S2CID (identifier)",
      "JSTOR (identifier)",
      "Président à mortier",
      "Lending library",
      "Special library",
      "Five laws of library science",
      "Libraries and librarians in fiction",
      "Cataloging (library science)",
      "Electronic resource management",
      "Library technical services",
      "Library consortium",
      "Information retrieval",
      "Knowledge organization",
      "Dewey Decimal Classification",
      "Christian library",
      "Information literacy",
      "Superintendent of Documents Classification",
      "Callimachus",
      "Cutter number",
      "Subscription library",
      "Music library",
      "BISAC Subject Headings",
      "Periodicals librarian",
      "List of library associations",
      "Toy library",
      "List of female librarians",
      "Inventory (library and archive)",
      "New Classification Scheme for Chinese Libraries",
      "List of libraries in the ancient world",
      "Online public access catalog",
      "Hybrid library",
      "Reference desk",
      "Distance education librarian",
      "Private library",
      "Ensemble librarianship",
      "Gabriel Naudé",
      "University College London",
      "Traveling library",
      "List of destroyed libraries",
      "Discovery system (bibliographic search)",
      "School library",
      "Attribute-value system",
      "Academic library",
      "List of library science schools",
      "Subject access",
      "List of medical libraries",
      "Library circulation",
      "Public library",
      "Classification (general theory)",
      "Law library",
      "Weeding (library)",
      "Pinakes",
      "Knowledge",
      "Harvard-Yenching Classification",
      "Korean decimal classification",
      "Colon Classification",
      "Library",
      "Tool library",
      "Archive",
      "Roving reference",
      "Library publishing",
      "Public bookcase",
      "Conservation and restoration of books, manuscripts, documents and ephemera",
      "Bookmobile",
      "National Library of Medicine classification",
      "Gladstone's Library",
      "Swedish library classification system",
      "S. R. Ranganathan",
      "Transportation library",
      "Library catalog",
      "Library management",
      "Library history",
      "W. C. Berwick Sayers",
      "Map collection",
      "Presidential library system",
      "Library branch",
      "English language",
      "Shadow library",
      "List of national and state libraries",
      "Iconclass",
      "OCLC (identifier)",
      "William Ewart Gladstone",
      "Legal deposit",
      "Library of Alexandria",
      "Dickinson classification",
      "Music librarianship",
      "Bibliothèque Mazarine",
      "Glossary of library and information science",
      "Librarian",
      "S R Ranganathan",
      "Special collections",
      "Library of things",
      "Library acquisitions",
      "List of archivists",
      "Preservation (library and archive)",
      "Cutter Expansive Classification",
      "Library of Congress Subject Headings",
      "Library assessment",
      "Collective collection",
      "Archival science",
      "List of libraries",
      "British Catalogue of Music Classification",
      "Moys Classification Scheme",
      "The New York Times",
      "Library of Congress",
      "Subject indexing",
      "List of librarians",
      "Teacher-librarian",
      "Medical record librarian",
      "Carnegie library",
      "Korean Decimal Classification",
      "National library",
      "Outline of library science",
      "Colon classification",
      "Library and information science",
      "Comparison of Dewey and Library of Congress subject classification",
      "Learning Resource Centre",
      "Jacques Charles Brunet",
      "Index term",
      "Decimal classification",
      "E-Science librarianship",
      "Bliss bibliographic classification",
      "Garside classification",
      "List of largest libraries",
      "Medical library",
      "ISSN (identifier)",
      "Prison library",
      "Harvard–Yenching Classification",
      "Digital library",
      "Collection development",
      "ISBN (identifier)",
      "Chinese Library Classification",
      "Library instruction",
      "Universal Decimal Classification",
      "Library of Congress Classification",
      "Readers' advisory",
      "Presidential library",
      "Library science",
      "CODOC",
      "Taxonomy (general)",
      "High Court (Hong Kong)",
      "Document classification",
      "Classification",
      "Informationist",
      "Digital reference",
      "Research library",
      "Faceted classification",
      "Education for librarianship",
      "History of libraries"
    ]
  },
  "CERN": {
    "url": "https://en.wikipedia.org/wiki/CERN",
    "title": "CERN",
    "content": "The European Organization for Nuclear Research , known as CERN ( / s ɜːr n / ; French pronunciation: [sɛʁn] ; Organisation européenne pour la recherche nucléaire ), is an intergovernmental organization that operates the largest particle physics laboratory in the world. Established in 1954, it is based in Meyrin , a western suburb of Geneva , on the France–Switzerland border . It comprises 24 member states . [ 4 ] Israel, admitted in 2013, is the only full member geographically out of Europe. [ 5 ] [ 6 ] CERN is an official United Nations General Assembly observer . [ 7 ] The acronym CERN is also used to refer to the laboratory; in 2024, it had 2,704 scientific, technical, and administrative staff members, and hosted about 12,406 users from institutions in more than 80 countries. [ 8 ] In 2016, CERN generated 49 petabytes of data. [ 9 ] CERN's main function is to provide the particle accelerators and other infrastructure needed for high-energy physics research – consequently, numerous experiments have been constructed at CERN through international collaborations. CERN is the site of the Large Hadron Collider (LHC), the world's largest and highest-energy particle collider. [ 10 ] The main site at Meyrin hosts a large computing facility, which is primarily used to store and analyze data from experiments, as well as simulate events . As researchers require remote access to these facilities, the lab has historically been a major wide area network hub. CERN is also the birthplace of the World Wide Web . [ 11 ] [ 12 ] The convention establishing CERN [ 13 ] was ratified on 29 September 1954 by 12 countries in Western Europe. [ 14 ] The acronym CERN originally represented the French words for Conseil Européen pour la Recherche Nucléaire ('European Council for Nuclear Research'), which was a provisional council for building the laboratory, established by 12 European governments in 1952. During these early years, the council worked at the University of Copenhagen under the direction of Niels Bohr before moving to its present site near Geneva. [ 15 ] [ 16 ] The acronym was retained for the new laboratory after the provisional council was dissolved, even though the name changed to the current Organisation européenne pour la recherche nucléaire ('European Organization for Nuclear Research') in 1954. [ 15 ] [ 16 ] According to Lew Kowarski , a former director of CERN, when the name was changed, the abbreviation could have become the awkward OERN, [ 17 ] and Werner Heisenberg said that this could \"still be CERN even if the name is [not]\". [ 18 ] CERN's first president was Sir Benjamin Lockspeiser . Edoardo Amaldi was the general secretary of CERN at its early stages when operations were still provisional, and the first Director-General (1954) was Felix Bloch . [ 19 ] At the sixth session of the CERN Council in Paris from 29 June to 1 July 1953, the convention establishing the organization was signed, subject to ratification, by 12 states. The convention was gradually ratified by the 12 founding Member States: Belgium, Denmark, France, the Federal Republic of Germany , Greece, Italy, the Netherlands, Norway, Sweden, Switzerland, the United Kingdom, and Yugoslavia . [ 21 ] Several major achievements in particle physics have occurred in experiments at CERN. They include: In September 2011, CERN attracted media attention when the OPERA Collaboration reported the detection of possibly faster-than-light neutrinos . [ 37 ] Further tests showed that the results were flawed due to an incorrectly connected GPS synchronization cable. [ 38 ] The 1984 Nobel Prize for Physics was awarded to Carlo Rubbia and Simon van der Meer for the developments that resulted in the discoveries of the W and Z bosons. [ 39 ] The 1992 Nobel Prize for Physics was awarded to CERN staff researcher Georges Charpak \"for his invention and development of particle detectors, in particular the multiwire proportional chamber \". The 2013 Nobel Prize for Physics was awarded to François Englert and Peter Higgs for the theoretical description of the Higgs mechanism in the year after the Higgs boson was found by CERN experiments. CERN pioneered the introduction of TCP/IP for its intranet , beginning in 1984. This played an influential role in the adoption of the TCP/IP in Europe (see History of the Internet and Protocol Wars ). [ 40 ] In 1989, the World Wide Web was invented at CERN by Tim Berners-Lee . Based on the concept of hypertext , the idea was designed to facilitate information sharing between researchers. [ 41 ] [ 42 ] This stemmed from Berners-Lee's earlier work at CERN on a database named ENQUIRE . A colleague, Robert Cailliau , became involved in 1990. [ 43 ] [ 44 ] [ 45 ] In 1995, Berners-Lee and Cailliau were jointly honoured by the Association for Computing Machinery for their contributions to the development of the World Wide Web. [ 46 ] A copy of the first webpage, created by Berners-Lee, is still published on the World Wide Web Consortium 's website as a historical document. [ 47 ] The first website was activated in 1991. On 30 April 1993, CERN announced that the World Wide Web would be free to anyone. [ 48 ] It became the dominant way through which most users interact with the Internet . [ 49 ] [ 50 ] More recently, CERN has become a facility for the development of grid computing , hosting projects including the Enabling Grids for E-sciencE (EGEE) and LHC Computing Grid . It also hosts the CERN Internet Exchange Point (CIXP), one of the two main internet exchange points in Switzerland. As of 2022 [update] , CERN employs ten times more engineers and technicians than research physicists. [ 51 ] CERN operates a network of seven accelerators and two decelerators, and some additional small accelerators. Each machine in the chain increases the energy of particle beams before delivering them to experiments or to the next more powerful accelerator. The decelerators naturally decrease the energy of particle beams before delivering them to experiments or further accelerators/decelerators. Before an experiment is able to use the network of accelerators, it must be approved by the various Scientific Committees of CERN . [ 52 ] Currently (as of 2022) active machines are the LHC accelerator and: Many activities at CERN currently involve operating the Large Hadron Collider (LHC) and the experiments for it. The LHC represents a large-scale, worldwide scientific cooperation project. [ 69 ] The LHC tunnel is located 100 metres underground, in the region between Geneva International Airport and the nearby Jura mountains . The majority of its length is on the French side of the border. It uses the 27 km circumference circular tunnel previously occupied by the Large Electron–Positron Collider (LEP), which was shut down in November 2000. CERN's existing PS/SPS accelerator complexes are used to pre-accelerate protons and lead ions which are then injected into the LHC. Eight experiments ( CMS , [ 70 ] ATLAS , [ 71 ] LHCb , [ 72 ] MoEDAL , [ 73 ] TOTEM , [ 74 ] LHCf , [ 75 ] FASER [ 76 ] and ALICE [ 77 ] ) are located along the collider; each of them studies particle collisions from a different aspect, and with different technologies. Construction for these experiments required an extraordinary engineering effort. For example, a special crane was rented from Belgium to lower pieces of the CMS detector into its cavern, since each piece weighed nearly 2000 tons. The first of the approximately 5000 magnets necessary for construction was lowered down a special shaft at in March 2005. The LHC has begun to generate vast quantities of data, which CERN streams to laboratories around the world for distributed processing, making use of a specialized grid infrastructure, the LHC Computing Grid . In April 2005, a trial successfully streamed 600 MB/s to seven different sites across the world. In August 2008, the initial particle beams were injected into the LHC. [ 78 ] The first beam was circulated through the entire LHC on 10 September 2008, [ 79 ] but the system failed 10 days later because of a faulty magnet connection, and it was stopped for repairs on 19 September 2008. The LHC resumed operation on 20 November 2009 by successfully circulating two beams, each with an energy of 3.5 teraelectronvolts (TeV). The challenge for the engineers was then to line up the two beams so that they smashed into each other. This is like \"firing two needles across the Atlantic and getting them to hit each other\" according to Stephen Myers , director for accelerators and technology. On 30 March 2010, the LHC successfully collided two proton beams with 3.5 TeV of energy per proton, resulting in a 7 TeV collision energy. This was enough to start the main research program, including the search for the Higgs boson . When the 7 TeV experimental period ended, the LHC increased to 8 TeV (4 TeV per proton) starting March 2012, and soon began particle collisions at that energy. In July 2012, CERN scientists announced the discovery of a new sub-atomic particle that was later confirmed to be the Higgs boson . [ 80 ] In March 2013, CERN announced that the measurements performed on the newly found particle allowed it to conclude that it was a Higgs boson. [ 81 ] In early 2013, the LHC was deactivated for a two-year maintenance period, to strengthen the electrical connections between magnets inside the accelerator and for other upgrades. On 5 April 2015, after two years of maintenance and consolidation, the LHC restarted for a second run. The first ramp to the record-breaking energy of 6.5 TeV was performed on 10 April 2015. [ 82 ] [ 83 ] In 2016, the design collision rate was exceeded for the first time. [ 84 ] A second two-year period of shutdown begun at the end of 2018. [ 85 ] [ 86 ] As of October 2019, the construction is on-going to upgrade the LHC's luminosity in a project called High Luminosity LHC (HL–LHC). This project should see the LHC accelerator upgraded by 2026 to an order of magnitude higher luminosity. [ 87 ] As part of the HL–LHC upgrade project, also other CERN accelerators and their subsystems are receiving upgrades. Among other work, the LINAC 2 linear accelerator injector was decommissioned and replaced by a new injector accelerator, the LINAC4 . [ 88 ] CERN, in collaboration with groups worldwide, is investigating two main concepts for future accelerators: A linear electron-positron collider with a new acceleration concept to increase the energy ( CLIC ) and a larger version of the LHC, a project currently named Future Circular Collider . [ 105 ] The smaller accelerators are on the main Meyrin site, also known as the West Area, which was originally built in Switzerland alongside the French border, but has been extended to span the border since 1965. The French side is under Swiss jurisdiction and there is no obvious border within the site, apart from a line of marker stones. The SPS and LEP/LHC tunnels are almost entirely outside the main site, and are mostly buried under French farmland and invisible from the surface. They have surface sites at points around them, either as the location of buildings associated with experiments or other facilities needed to operate the colliders such as cryogenic plants and access shafts. The experiments are located at the same underground level as the tunnels at these sites. Three of these experimental sites are in France, with ATLAS in Switzerland, and some of the ancillary cryogenic and access sites are in Switzerland. The largest of the experimental sites is the Prévessin site, also known as the North Area, which is the target station for non-collider experiments on the SPS accelerator. Other sites are the ones which were used for the UA1 , UA2 and the LEP experiments. The latter are used by LHC experiments. Outside of the LEP and LHC experiments, most are officially named and numbered after the site where they were located. For example, NA32 was an experiment looking at the production of so-called \" charmed \" particles and located at the Prévessin (North Area) site. WA22 used the Big European Bubble Chamber (BEBC) at the Meyrin (West Area) site to examine neutrino interactions. The UA1 and UA2 experiments were considered to be in the Underground Area, i.e. situated underground at sites on the SPS accelerator. Most of the roads on the CERN Meyrin and Prévessin sites are named after famous physicists, such as Niels Bohr , who pushed for CERN's creation. Other notable names are Richard Feynman , Albert Einstein , and Wolfgang Pauli . Since its foundation by 12 members in 1954, CERN regularly accepted new members. All new members have remained in the organization continuously since their accession, except Spain and Yugoslavia. Spain first joined CERN in 1961, withdrew in 1969, and rejoined in 1983. Yugoslavia was a founding member of CERN but quit in 1961. Of the 24 members, Israel joined CERN as a full member in January 2014, [ 106 ] becoming the first, and currently only, non-geographically European full member. [ 107 ] The budget contributions of member states are computed based on their GDP. [ 108 ] Associate Members, Candidates: Three countries have observer status: [ 141 ] Also observers are the following international organizations: Non-Member States (with dates of Co-operation Agreements) currently involved in CERN programmes are: [ 144 ] [ 145 ] CERN also has scientific contacts with the following other countries and regions: [ 144 ] [ 151 ] International research institutions, such as CERN, can aid in science diplomacy. [ 152 ] A large number of institutes around the world are associated to CERN through current collaboration agreements and/or historical links. [ 154 ] The list below contains organizations represented as observers to the CERN Council, organizations to which CERN is an observer and organizations based on the CERN model: .cern is a top-level domain for CERN. [ 163 ] [ 164 ] It was registered on 13 August 2014. [ 165 ] [ 166 ] On 20 October 2015, CERN moved its main website to Home | CERN . [ 167 ] [ 168 ] The Open Science movement focuses on making scientific research openly accessible and on creating knowledge through open tools and processes. Open access , open data , open source software and hardware , open licenses , digital preservation and reproducible research are primary components of open science and areas in which CERN has been working towards since its formation. CERN has developed policies and official documents that enable and promote open science, starting with CERN's founding convention in 1953 which indicated that all its results are to be published or made generally available. [ 13 ] Since then, CERN published its open access policy in 2014, [ 169 ] which ensures that all publications by CERN authors will be published with gold open access and most recently an open data policy that was endorsed by the four main LHC collaborations ( ALICE , ATLAS , CMS and LHCb ). [ 170 ] The open data policy complements the open access policy, addressing the public release of scientific data collected by LHC experiments after a suitable embargo period. Prior to this open data policy, guidelines for data preservation, access and reuse were implemented by each collaboration individually through their own policies which are updated when necessary. [ 171 ] [ 172 ] [ 173 ] [ 174 ] The European Strategy for Particle Physics, a document mandated by the CERN Council that forms the cornerstone of Europe's decision-making for the future of particle physics, was last updated in 2020 and affirmed the organisation's role within the open science landscape by stating: \"The particle physics community should work with the relevant authorities to help shape the emerging consensus on open science to be adopted for publicly-funded research, and should then implement a policy of open science for the field\". [ 175 ] Beyond the policy level, CERN has established a variety of services and tools to enable and guide open science at CERN, and in particle physics more generally. On the publishing side, CERN has initiated and operates a global cooperative project, the Sponsoring Consortium for Open Access Publishing in Particle Physics , SCOAP3, to convert scientific articles in high-energy physics to open access. In 2018, the SCOAP3 partnership represented 3,000+ libraries from 44 countries and 3 intergovernmental organizations who have worked collectively to convert research articles in high-energy physics across 11 leading journals in the discipline to open access. [ 176 ] [ 177 ] Public-facing results can be served by various CERN-based services depending on their use case: the CERN Open Data portal , [ 178 ] Zenodo , the CERN Document Server , [ 179 ] INSPIRE and HEPData [ 180 ] are the core services used by the researchers and community at CERN, as well as the wider high-energy physics community for the publication of their documents, data, software, multimedia, etc. CERN's efforts towards preservation and reproducible research are best represented by a suite of services addressing the entire physics analysis lifecycle, such as data, software and computing environment. CERN Analysis Preservation [ 181 ] helps researchers to preserve and document the various components of their physics analyses. REANA (Reusable Analyses) [ 182 ] enables the instantiating of preserved research data analyses on the cloud. All services are built using open source software and strive towards compliance with best effort principles, such as the FAIR principles , the FORCE11 guidelines and Plan S , while taking into account relevant activities carried out by the European Commission . [ 183 ] The first public exhibition at CERN was the Microcosm museum which hosted an exhibition about particle physics and CERN history. It closed permanently on 18 September 2022, in preparation for the installation of the exhibitions in the newly built science center CERN Science Gateway. [ 184 ] The CERN Science Gateway, constructed by the Renzo Piano building workshop was opened in October 2023. [ 185 ] It is home to the following spaces: The Globe of Science and Innovation , which opened in late 2005, is open to the public. It is used four times a week for special exhibits. CERN also provides daily tours to certain facilities such as the Synchro-cyclotron (CERNs first particle accelerator) and the superconducting magnet workshop. CERN launched its Cultural Policy for engaging with the arts in 2011. [ 192 ] [ 193 ] The initiative provided the essential framework and foundations for establishing Arts at CERN , the arts programme of the Laboratory. Since 2012, Arts at CERN has fostered creative dialogue between art and physics through residencies, art commissions, exhibitions and events. Artists across all creative disciplines have been invited to CERN to experience how fundamental science pursues the big questions about our universe. Even before the arts programme officially started, several highly regarded artists visited the laboratory, drawn to physics and fundamental science. In 1972, James Lee Byars was the first artist to visit the laboratory and the only one, so far, to feature on the cover of the CERN Courier. [ 194 ] Mariko Mori , [ 195 ] Gianni Motti ( fr ) , [ 196 ] Cerith Wyn Evans , [ 197 ] John Berger [ 198 ] and Anselm Kiefer [ 199 ] are among the artists who came to CERN in the years that followed. The programmes of Arts at CERN are structured according to their values and vision to create bridges between cultures. Each programme is designed and formed in collaboration with cultural institutions, other partner laboratories, countries, cities and artistic communities eager to connect with CERN's research, support their activities, and contribute to a global network of art and science. They comprise research-led artistic residencies that take place on-site or remotely. More than 200 artists from 80 countries have participated in the residencies to expand their creative practices at the Laboratory, benefiting from the involvement of 400 physicists, engineers and CERN staff. Between 500 and 800 applications are received every year. The programmes comprise Collide, the international residency programme organised in partnership with a city; Connect, a programme of residencies to foster experimentation in art and science at CERN and in scientific organisations worldwide in collaboration with Pro Helvetia , and Guest Artists, a short stay for artists to stay to engage with CERN's research and community. [ 200 ] [ 201 ] Hundreds of years ago, Indian artists created visual images of dancing Shivas in a beautiful series of bronzes. In our time, physicists have used the most advanced technology to portray the patterns of the cosmic dance. The metaphor of the cosmic dance thus unifies ancient mythology, religious art and modern physics. [ 221 ] International: General:",
    "links": [
      "Science diplomacy",
      "The Times of Israel",
      "Faster-than-light neutrino anomaly",
      "LINAC 3",
      "Yves Meyer",
      "Salvador Moncada",
      "Martin Cooper (inventor)",
      "UNESCO",
      "ATLAS experiment",
      "Netherlands",
      "Angels & Demons (film)",
      "ETH Library",
      "Hdl (identifier)",
      "Israel",
      "Lawrence Roberts (scientist)",
      "Cern (disambiguation)",
      "Paul Scherrer Institute",
      "Enrique Moreno González",
      "Portugal",
      "Ben Lockspeiser",
      "Geneva",
      "81 cm Saclay Bubble Chamber",
      "CERN Axion Solar Telescope",
      "CERN Internet Exchange Point",
      "Robert S. Langer",
      "Guido Münch",
      "Daniel J. Drucker",
      "Bibcode (identifier)",
      "Shiva",
      "Anselm Kiefer",
      "European Organization for Nuclear Research",
      "Hypertext",
      "Antonio Damasio",
      "Peter Anthony Lawrence",
      "Shuji Nakamura",
      "CiteSeerX (identifier)",
      "List of countries by population",
      "Galen D. Stucky",
      "Oeschger Centre for Climate Change Research",
      "Intersecting Storage Rings",
      "Yugoslavia",
      "Serbia",
      "VITO experiment",
      "Science and Technology Facilities Council",
      "CTF3 (CERN)",
      "LHCb experiment",
      "Alberto Sols",
      "Heavy ion",
      "Augmented reality",
      "Amable Liñán",
      "John Berger",
      "Austria",
      "Greece",
      "NA61 experiment",
      "Swiss Academy of Natural Sciences",
      "History of the World Wide Web",
      "HOLEBC",
      "Plan S",
      "ArXiv (identifier)",
      "Virtual particle",
      "Decay (2012 film)",
      "Jeffrey I. Gordon",
      "Robert Gallo",
      "Vint Cerf",
      "DELPHI experiment",
      "Low Energy Ion Ring",
      "CERN ritual hoax",
      "Geneva Water Hub",
      "CERN Neutrinos to Gran Sasso",
      "Svante Pääbo",
      "Friedrich Miescher Institute for Biomedical Research",
      "John Titor",
      "Electronvolts",
      "Bonnie Bassler",
      "The New York Times",
      "Science and technology in Switzerland",
      "Antiproton Decelerator",
      "Ray Tomlinson",
      "Swiss Finance Institute",
      "European Space Research Organisation",
      "ENQUIRE",
      "Tim Berners-Lee",
      "Geneva Association",
      "Brazil",
      "OPERA experiment",
      "European Southern Observatory",
      "GPS",
      "Open-source software",
      "Physics Letters B",
      "FASER experiment",
      "Megadeth",
      "Germany",
      "Antihydrogen",
      "Gold OA",
      "Intranet",
      "ISSN (identifier)",
      "Felix Bloch",
      "List of directors general of CERN",
      "Jean Weissenbach",
      "Raj Koothrappali",
      "ISBN (identifier)",
      "Czech Republic",
      "Luis Santaló",
      "NA48 experiment",
      "ESO",
      "The Guardian",
      "Hyper-Kamiokande",
      "Bulgaria",
      "Dalton (unit)",
      "Pro Helvetia",
      "PMC (identifier)",
      "Event (particle physics)",
      "LIGO",
      "Quantum entanglement",
      "Norway",
      "Fermilab",
      "Joanne Chory",
      "Large Hadron Collider",
      "INSPIRE-HEP",
      "High Luminosity Large Hadron Collider",
      "LHCf experiment",
      "Open access",
      "Crane (machine)",
      "François Englert",
      "Dalle Molle Institute for Semantic and Cognitive Studies",
      "Synchro-Cyclotron (CERN)",
      "S2CID (identifier)",
      "LINAC 1",
      "FORCE11",
      "PS210 experiment",
      "Francis Collins",
      "Intergovernmental organization",
      "Robert Cailliau",
      "Wide area network",
      "TOTEM",
      "Katalin Karikó",
      "Lithuania",
      "Sandra Díaz (ecologist)",
      "BWI Center for Industrial Management",
      "Philip Felgner",
      "Massively multiplayer online role playing game",
      "International Mountain Society",
      "Miniball experiment",
      "Large Electron–Positron Collider",
      "Speed of light",
      "Gigaelectronvolt",
      "World Trade Institute",
      "Jeffrey M. Friedman",
      "Nobel Prize for Physics",
      "CERN Courier",
      "Leonard Hofstadter",
      "LHCb",
      "SLAC National Accelerator Laboratory",
      "George M. Whitesides",
      "Swiss Institute of Bioinformatics",
      "Particle Fever",
      "2 m Bubble Chamber (CERN)",
      "CLIC Test Facility 3",
      "Synchrotron-Light for Experimental Science and Applications in the Middle East",
      "CERN-MEDICIS",
      "Cisco Systems",
      "TCP/IP",
      "Kip Thorne",
      "Georges Charpak",
      "Sweden",
      "Yann LeCun",
      "Derrick Rossi",
      "Craig Venter",
      "Cyprus",
      "Uncertainty principle",
      "Positron",
      "Digital preservation",
      "Future Circular Collider",
      "Werner Heisenberg",
      "Pablo Rudomín Zevnovaty",
      "DCAF",
      "Meyrin",
      "Federal Institute of Metrology",
      "W and Z bosons",
      "Swiss Federal Institute for Forest, Snow and Landscape Research",
      "Antonio García-Bellido",
      "Deep Underground Neutrino Experiment",
      "CERN Linear Electron Accelerator for Research",
      "Open science",
      "Manuel Elkin Patarroyo",
      "Linda R. Watkins",
      "Everett Peter Greenberg",
      "Terence Tao",
      "On-Line Isotope Mass Separator",
      "Association for Computing Machinery",
      "Geneva Centre for Security Policy",
      "Rainer Weiss",
      "Open-source hardware",
      "Angels & Demons",
      "Swiss Federal Laboratories for Materials Science and Technology",
      "John Sulston",
      "Centre on Conflict, Development and Peacebuilding",
      "Electron Positron Accumulator",
      "Swiss Laboratory for Doping Analyses",
      "LINAC 4",
      "Basel Institute for Immunology",
      "Uğur Şahin",
      "The Big Bang Theory",
      "History of the Internet",
      "Disney+",
      "Francisco Bolívar Zapata",
      "Gargamelle",
      "LEBC",
      "BIBC",
      "Neutron Time Of Flight",
      "Time travel",
      "WISArD experiment",
      "Boson",
      "Flashforward (novel)",
      "Baruch Minke",
      "Carlo Rubbia",
      "ISOLDE Decay Station experiment",
      "Santiago Grisolía, 1st Marquess of Grisolía",
      "Bert Vogelstein",
      "Peter Higgs",
      "Barry Barish",
      "Wired News",
      "Bob Kahn",
      "MIRACLS experiment",
      "Proton Synchrotron Booster",
      "Big European Bubble Chamber",
      "Anime",
      "Ingress (video game)",
      "Swiss Academy of Humanities and Social Sciences",
      "Poland",
      "Extra Low ENergy Antiproton ring (ELENA)",
      "Multiwire proportional chamber",
      "Alternative history",
      "Swiss franc",
      "Advanced Learning and Research Institute",
      "Jura mountains",
      "Hell",
      "Mark E. Davis",
      "Zenodo",
      "JSTOR (identifier)",
      "Spain",
      "LHCf",
      "Denmark",
      "Pakistan",
      "Juan Ignacio Cirac Sasturain",
      "UA2 experiment",
      "Open data",
      "Jens Juul Holst",
      "OPAL experiment",
      "Higgs Boson",
      "Chile",
      "Howie Day",
      "WITCH experiment",
      "World Wide Web Consortium",
      "Protons",
      "Yoshua Bengio",
      "United Kingdom",
      "Geoffrey Hinton",
      "Joseph Altman",
      "Lew Kowarski",
      "Institute for Mathematical Research",
      "Mary-Claire King",
      "LHC Computing Grid",
      "Luc Montagnier",
      "Domain name registry",
      "Enabling Grids for E-sciencE",
      "Antiproton Collector",
      "Slovakia",
      "Dimension",
      "École Polytechnique Fédérale de Lausanne",
      "SEC experiment",
      "30 cm Bubble Chamber (CERN)",
      "Government of India",
      "Svetlana Mojsov",
      "NA49 experiment",
      "Archaeological site of Atapuerca",
      "PMID (identifier)",
      "Ion",
      "Sumio Iijima",
      "Microcosm (CERN)",
      "Richard Lerner",
      "Worldwide LHC Computing Grid",
      "University of California, Riverside",
      "ALICE experiment",
      "Croatia",
      "Max Frisch Archive",
      "Katherine McAlpine",
      "English language",
      "Ingrid Daubechies",
      "Hip hop music",
      "NeXT Computer",
      "OCLC (identifier)",
      "Agroscope",
      "Slovenia",
      "World Wide Web",
      "Idiap Research Institute",
      "Swiss Academies of Arts and Sciences",
      "Socialist Federal Republic of Yugoslavia",
      "Swiss Ornithological Institute",
      "Back story",
      "The Globe of Science and Innovation",
      "Anthony R. Hunter",
      "Cerith Wyn Evans",
      "Megaelectronvolt",
      "Safety of high-energy particle collision experiments",
      "Joint Institute for Nuclear Research",
      "Mariko Mori",
      "Demis Hassabis",
      "Arturo Alvarez-Buylla",
      "Doctor Who",
      "Judah Folkman",
      "ISOLTRAP experiment",
      "Research Institute of Organic Agriculture",
      "Belgium",
      "Linac4",
      "Manuel Ballester",
      "Antiproton",
      "Internet",
      "Swiss Federal Institute of Aquatic Science and Technology",
      "Cryonics",
      "Gregory Winter",
      "Geneva International Airport",
      "International Centre for Synchrotron-Light for Experimental Science Applications in the Middle East",
      "Sir Tim Berners-Lee",
      "IP router",
      "Özlem Türeci",
      "Hugh Herr",
      "Giacomo Rizzolatti",
      "Sponsoring Consortium for Open Access Publishing in Particle Physics",
      "MoEDAL experiment",
      "LEP Injector Linac",
      "COLLAPS experiment",
      "Fritjof Capra",
      "NA60 experiment",
      "SFR Yugoslavia",
      "Particle accelerator",
      "COMPASS experiment",
      "Pedro Miguel Etxenike",
      "Latvia",
      "Pinewood Derby (South Park)",
      "NA62 experiment",
      "Switzerland",
      "Collider",
      "Ryoji Ikeda",
      "Emmanuelle Charpentier",
      "CRIS experiment",
      "NA32 experiment",
      "Center for Comparative and International Studies",
      "ETH Domain",
      "Republic of Ireland",
      "FAIR data",
      "LINAC 2",
      "Dalle Molle Institute for Artificial Intelligence Research",
      "David Julius",
      "CLOUD experiment",
      "European Space Agency",
      "Doi (identifier)",
      "Conspiracy theory",
      "Compact Linear Collider",
      "Instituto Nacional de Biodiversidad",
      "Italy",
      "Web server",
      "CP violation",
      "L3 experiment",
      "Niels Bohr",
      "Top-level domain",
      "Fabiola Gianotti",
      "ETH Zurich",
      "ALEPH experiment",
      "Wyss Center for Bio and Neuroengineering",
      "Emilio Rosenblueth",
      "Antimatter",
      "Dan Brown",
      "Emmanuel Candès",
      "Jennifer Doudna",
      "European Commission",
      "Higgs boson",
      "Tobin J. Marks",
      "West Germany",
      "Reunification of Germany",
      "Visual novel",
      "List of Super Proton Synchrotron experiments",
      "Finland",
      "Turkey",
      "Wormhole",
      "UA1 experiment",
      "Linear accelerator",
      "AWAKE",
      "France–Switzerland border",
      "Joan Massagué",
      "CTF3",
      "List of Large Hadron Collider experiments",
      "Grid computing",
      "Sarah Gilbert",
      "Ukraine",
      "Drew Weissman",
      "India",
      "Simon van der Meer",
      "Edoardo Amaldi",
      "Antiproton Accumulator",
      "Proton",
      "List of CERN Scientific Committees",
      "Ginés Morata",
      "Compact Muon Solenoid",
      "EC-SLI experiment",
      "French language",
      "Super Proton–Antiproton Synchrotron",
      "James Lee Byars",
      "Prévessin",
      "Department of Atomic Energy",
      "Estonia",
      "Renzo Piano",
      "Neutrino",
      "AISTS",
      "Steins;Gate",
      "Joel Habener",
      "Super Proton Synchrotron",
      "Radioactive decay",
      "Ricardo Miledi",
      "Electron",
      "Robert Weinberg (biologist)",
      "Wolfgang Pauli",
      "Avel·lí Corma Canós",
      "Romania",
      "Mr. Robot",
      "Robert J. Sawyer",
      "Quantum mechanics",
      "HowStuffWorks",
      "Adolphe Merkle Institute",
      "Parallels (TV series)",
      "CERN openlab",
      "Plasma acceleration",
      "Valentín Fuster",
      "Jane Goodall",
      "Stephen Myers (engineer)",
      "Neutral current",
      "South Park",
      "Scientific Linux",
      "Low Energy Antiproton Ring",
      "Open-source license",
      "Centre for Finance and Development",
      "Richard Feynman",
      "Albert Einstein",
      "University of Wisconsin–Madison",
      "Collide (Howie Day song)",
      "Super Collider (album)",
      "Quark–gluon plasma",
      "Internet Assigned Numbers Authority",
      "Swiss Nanoscience Institute",
      "Princess of Asturias Awards",
      "A Large Ion Collider Experiment",
      "LUCRECIA experiment",
      "LHC@home",
      "Marcos Moshinsky",
      "Nataraja",
      "Manuel Cardona",
      "Protocol Wars",
      "Jacinto Convit",
      "Emilio Méndez Pérez",
      "TOTEM experiment",
      "Hamilton O. Smith",
      "Les Horribles Cernettes",
      "ISOLDE Solenoidal Spectrometer",
      "Internet exchange point",
      "European Molecular Biology Laboratory",
      "Particle physics",
      "University of Copenhagen",
      "Charm quark",
      "Proton Synchrotron",
      "List of streets at CERN",
      "LEP Pre-Injector",
      "Hungary",
      "France"
    ]
  },
  "ArXiv (identifier)": {
    "url": "https://en.wikipedia.org/wiki/ArXiv_(identifier)",
    "title": "ArXiv (identifier)",
    "content": "arXiv (pronounced as \" archive \"—the X represents the Greek letter chi ⟨χ⟩) [ 1 ] is an open-access repository of electronic preprints and postprints (known as e-prints ) approved for posting after moderation, but not peer reviewed . It consists of scientific papers in the fields of mathematics , physics , astronomy , electrical engineering , computer science , quantitative biology , statistics , mathematical finance , and economics , which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. Some publishers also grant permission for authors to archive the peer-reviewed postprint . Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, [ 2 ] [ 3 ] had hit a million by the end of 2014 [ 4 ] [ 5 ] and two million by the end of 2021. [ 6 ] [ 7 ] As of November 2024, the submission rate is about 24,000 articles per month. [ 8 ] arXiv was made possible by the compact TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side . [ 11 ] Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. [ 12 ] Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory (LANL) that could be accessed from any computer. [ 13 ] Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993. [ 5 ] [ 14 ] The term e-print was quickly adopted to describe the articles. It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org. [ 15 ] Ginsparg brainstormed the new name with his wife; the domain \"archive\" was already claimed, so \"chi\" was replaced with \"X\" standing in as the Greek letter chi and the \"e\" dropped for symmetry around the \"X\". [ 16 ] arXiv was an early adopter and promoter of preprints . [ 17 ] Its success in sharing preprints was one of the precipitating factors that led to the later movement in scientific publishing known as open access . [ 17 ] Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access [ 18 ] and sometimes for reviews before they are published in peer-reviewed journals . Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv. [ 19 ] The annual budget for arXiv was approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. [ 20 ] This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Each member institution pledges a five-year funding commitment to support arXiv. Based on institutional usage ranking, the annual fees are set in four tiers from $1,000 to $4,400. Cornell's goal is to raise at least $504,000 per year through membership fees generated by approximately 220 institutions. [ 21 ] In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv's operation and development. Ginsparg was quoted in the Chronicle of Higher Education as joking that it \"was supposed to be a three-hour tour , not a life sentence\". [ 22 ] However, Ginsparg remains on the arXiv's Scientific Advisory Board and its Physics Advisory Committee. [ 23 ] [ 24 ] In January 2022, arXiv began assigning DOIs to articles, in collaboration with DataCite . [ 25 ] Each arXiv paper has a \"uniquely specific identifier\": Different versions of the same paper are specified by a version number at the end. For example, 1709.08980v1 . If no version number is specified, the default is the latest version. arXiv uses a category system. Each paper is tagged with one or more categories. Some categories have two layers. For example, q-fin.TR is the \"Trading and Market Microstructure\" category within \"quantitative finance\". Other categories have one layer. For example, hep-ex is \"high energy physics experiments\". Although arXiv is not peer reviewed , a collection of moderators for each area review the submissions ; they may recategorize any that are deemed off-topic, [ 26 ] or reject submissions that are not scientific papers, or sometimes for undisclosed reasons. [ 27 ] The lists of moderators for many sections of arXiv are publicly available, [ 28 ] but moderators for most of the physics sections remain unlisted. Additionally, an \"endorsement\" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines. [ 29 ] Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors but to check whether the paper is appropriate for the intended subject area. [ 26 ] New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry. [ 30 ] [ 31 ] A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston's geometrization conjecture , including the Poincaré conjecture as a particular case, uploaded by Grigori Perelman in November 2002. [ 32 ] Perelman appears content to forgo the traditional peer-reviewed journal process, stating: \"If anybody is interested in my way of solving the problem, it's all there [on the arXiv] – let them go and read about it\". [ 33 ] Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused. [ 34 ] While arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat's Last Theorem using only high-school mathematics, a 2002 article which appeared in Notices of the American Mathematical Society described those as \"surprisingly rare\". [ 35 ] arXiv generally re-classifies these works, e.g. in \"General mathematics\", rather than deleting them; [ 36 ] however, some authors have voiced concern over the lack of transparency in the arXiv screening process. [ 27 ] In November 2025, arXiv announced that it would no longer accept computer science review articles and position papers that had not been vetted by an academic journal or conference due to an increase in AI-generated research. [ 37 ] [ 38 ] It has been reported that 14,000 preprints have been withdrawn at arXiv, most commonly due to \"crucial errors\". [ 39 ] A lesser number of the withdrawals were due to the preprint being subsumed by another publication. The report itself was posted at arXiv December, 2024. Papers can be submitted in any of several formats, including LaTeX , and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized. The standard access route is through the arXiv.org website, which is publicly accessible and does not require an account. Other interfaces and access routes have also been created by other un-associated organisations. Metadata for arXiv is made available through OAI-PMH , the standard for open access repositories . [ 40 ] Content is therefore indexed in all major consumers of such data, such as BASE , CORE and Unpaywall . As of 2020, the Unpaywall dump links over 500,000 arxiv URLs as the open access version of a work found in CrossRef data from the publishers, making arXiv a top 10 global host of green open access . Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them. Files on arXiv can have a number of different copyright statuses: [ 41 ]",
    "links": [
      "Peer review",
      "Sage Chapel",
      "Cornell University Library",
      "Mathematical finance",
      "Open access",
      "MacArthur Fellowship",
      "Barnes Hall",
      "Scientific papers",
      "Physics World",
      "Doi (identifier)",
      "Sage Hall",
      "The Cornell Lunatic",
      "Diacritics (journal)",
      "Scientific publishing",
      "Cornell University Glee Club",
      "Creative Commons",
      "Roberts Hall (Ithaca, New York)",
      "Preprints",
      "Sydney Morning Herald",
      "Los Alamos National Laboratory",
      "S2CID (identifier)",
      "List of Cornell University alumni",
      "Cornell Chronicle",
      "Bailey Hall (Ithaca, New York)",
      "Master's of Public Administration at Cornell University",
      "Open-access repository",
      "Client-side",
      "Chronicle of Higher Education",
      "Cornell Computing and Information Science",
      "Herbert F. Johnson Museum of Art",
      "List of Cornell University faculty",
      "BASE (search engine)",
      "New York State School of Industrial and Labor Relations at Cornell University",
      "Cornell North Campus",
      "Hoy Field",
      "Cornell Big Red women's basketball",
      "Cornell Central Campus",
      "The Philosophical Review",
      "Astronomy",
      "Disciplinary repository",
      "Comstock Hall (Ithaca, New York)",
      "RSS feed",
      "List of academic journals by preprint policy",
      "Drug Discovery & Development",
      "Cornell University College of Architecture, Art, and Planning",
      "Newman Arena",
      "Legal Information Institute",
      "Word processor",
      "D-Lib Magazine",
      "Cornell International Law Journal",
      "Cornell University Chorus",
      "DataCite",
      "Cornell University Press",
      "Millennium Prize Problems",
      "Postprint",
      "Deke House (Ithaca, New York)",
      "Mathematics",
      "Aleph Samach",
      "Poincaré conjecture",
      "ChemRxiv",
      "PsyArXiv",
      "Ithaca, New York",
      "Joanne Cohn",
      "Sci-Hub",
      "Nolan School of Hotel Administration",
      "Cornell Big Red baseball",
      "Unpaywall",
      "OCLC",
      "Bibcode (identifier)",
      "Cornell University College of Engineering",
      "Cornell Big Red wrestling",
      "New York State College of Forestry at Cornell",
      "Cornell gorge suicides",
      "Archive.org",
      "List of Cornell University fraternities and sororities",
      "Slope Day",
      "Position paper",
      "Electrical engineering",
      "Samuel Curtis Johnson Graduate School of Management",
      "Gopher (protocol)",
      "Cornell West Campus",
      "Boyce Thompson Institute",
      "Cornell University College of Arts and Sciences",
      "Cornell Laboratory for Accelerator-based Sciences and Education",
      "E-print",
      "FTP",
      "Open access repositories",
      "Notices of the American Mathematical Society",
      "Cornell Chimes",
      "Robert Trent Jones Golf Course",
      "Touchdown (mascot)",
      "New York City",
      "The Cornell Review",
      "Rice Hall (Ithaca, New York)",
      "Morrill Land-Grant Acts",
      "Statue of Ezra Cornell",
      "BioRxiv",
      "Andrew Dickson White",
      "Archive",
      "Digital object identifier",
      "New York State College of Veterinary Medicine at Cornell University",
      "Cornell University Center for Advanced Computing",
      "Review article",
      "Fuertes Observatory",
      "PMID (identifier)",
      "Industrial and Labor Relations Review",
      "Irving Literary Society (Cornell University)",
      "Journal of Empirical Legal Studies",
      "E-prints",
      "Cornell University",
      "Academic journal",
      "New York State College of Human Ecology at Cornell University",
      "Cornell Club of New York",
      "Science (journal)",
      "List of Cornell University buildings",
      "Cornell Big Red men's soccer",
      "Cornell University Board of Trustees",
      "Messenger Lectures",
      "Fields Medal",
      "Schoellkopf Field",
      "East Roberts Hall",
      "Cornell Big Red football",
      "Caldwell Hall (Ithaca, New York)",
      "The Scientist (magazine)",
      "Balch Hall",
      "Cornell Dairy",
      "Thurston's geometrization conjecture",
      "OCLC (identifier)",
      "Scholarly peer review",
      "Dragon Day",
      "Willard Straight Hall",
      "Cornell Big Red Pep Band",
      "World Wide Web",
      "Cornell Catholic Community",
      "Fernow Hall (Ithaca, New York)",
      "Hotel Ezra Cornell",
      "Economics",
      "Cornell Journal of Law and Public Policy",
      "Open access (publishing)",
      "Weill Cornell Graduate School of Medical Sciences",
      "Wilder Brain Collection",
      "Cornelliana",
      "All Sports Competition (Cornell University)",
      "Green open access",
      "Form (HTML)",
      "ViXra",
      "Cornell Lab of Ornithology",
      "Public domain",
      "Ezra Cornell",
      "Electronic submission",
      "PDF",
      "McGraw Tower",
      "Henry W. Sage",
      "Bradfield Hall",
      "Gilligan's Island",
      "List of preprint repositories",
      "Challenge grant",
      "Fermat's Last Theorem",
      "Domain name",
      "Oyez Project",
      "List of academic databases and search engines",
      "Cornell Big Red men's squash",
      "Uris Library",
      "Internet",
      "LANL",
      "Preprint",
      "Computer science",
      "The New York Times",
      "Tri-Institutional MD–PhD Program",
      "Cornell–Harvard hockey rivalry",
      "LaTeX",
      "Cornell Big Red Marching Band",
      "Barton Hall",
      "Administrative Science Quarterly",
      "Cornell Big Red men's lacrosse",
      "Metadata",
      "History of Cornell University",
      "Stone Hall (Ithaca, New York)",
      "Mosaic (web browser)",
      "Creative Commons licenses",
      "Cornell Big Red men's basketball",
      "List of presidents of Cornell University",
      "Cornell Big Red men's ice hockey",
      "Nature (journal)",
      "Cornell University Graduate School",
      "Mathematician",
      "Cornell International Affairs Review",
      "Andrew Dickson White House",
      "Scientific journal",
      "Physics",
      "Weill Cornell Medicine",
      "List of Cornell University songs",
      "ISSN",
      "OAI-PMH",
      "Sphinx Head",
      "ISSN (identifier)",
      "Cornell Botanic Gardens",
      "Quantitative biology",
      "Computing and Communications Center, Cornell University",
      "Telluride House",
      "Cornell Big Red women's ice hockey",
      "The Cornell Daily Sun",
      "Grigori Perelman",
      "Lynah Rink",
      "Charles F. Berman Field",
      "Llenroc",
      "New German Critique",
      "Physics Today",
      "Cornell Tech",
      "Cornell Law Review",
      "Chi (letter)",
      "TeX",
      "Simons Foundation",
      "Cornell Big Red",
      "CORE (research service)",
      "Give My Regards to Davy",
      "Cornell Policy Review",
      "CrossRef",
      "Cornell Law School",
      "PMC (identifier)",
      "Quill and Dagger",
      "Statistics",
      "EContent",
      "WVBR-FM",
      "Dyson School of Applied Economics and Management",
      "Far Above Cayuga's Waters",
      "Weill Cornell Medical College in Qatar",
      "Self-archiving",
      "Morrill Hall (Cornell University)",
      "Paul Ginsparg",
      "Hartung–Boothroyd Observatory",
      "Epoch (American magazine)",
      "Risley Residential College",
      "New York State College of Agriculture and Life Sciences at Cornell University",
      "Postprints"
    ]
  },
  "Ranking (information retrieval)": {
    "url": "https://en.wikipedia.org/wiki/Ranking_(information_retrieval)",
    "title": "Ranking (information retrieval)",
    "content": "Ranking of query is one of the fundamental problems in information retrieval (IR), [ 1 ] the scientific/engineering discipline behind search engines . [ 2 ] Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the \"best\" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems . [ 3 ] A majority of search engines use ranking algorithms to provide users with accurate and relevant results. [ 4 ] The notion of page rank dates back to the 1940s and the idea originated in the field of economics. In 1941, Wassily Leontief developed an iterative method of valuing a country's sector based on the importance of other sectors that supplied resources to it. In 1965, Charles H Hubbell at the University of California, Santa Barbara , published a technique for determining the importance of individuals based on the importance of the people who endorse them. [ 5 ] Gabriel Pinski and Francis Narin came up with an approach to rank journals. [ 6 ] Their rule was that a journal is important if it is cited by other important journals. Jon Kleinberg , a computer scientist at Cornell University , developed an almost identical approach to PageRank which was called Hypertext Induced Topic Search or HITS and it treated web pages as \"hubs\" and \"authorities\". Google's PageRank algorithm was developed in 1998 by Google's founders Sergey Brin and Larry Page and it is a key part of Google's method of ranking web pages in search results . [ 7 ] All the above methods are somewhat similar as all of them exploit the structure of links and require an iterative approach. [ 8 ] Ranking functions are evaluated by a variety of means; one of the simplest is determining the precision of the first k top-ranked results for some fixed k ; for example, the proportion of the top 10 results that are relevant, on average over many queries. IR models can be broadly divided into three types: Boolean models or BIR, Vector Space Models , and Probabilistic Models . [ 9 ] Various comparisons between retrieval models can be found in the literature (e.g., [ 10 ] ). Boolean Model or BIR is a simple baseline query model where each query follows the underlying principles of relational algebra with algebraic expressions and where documents are not fetched unless they completely match with each other. Since the query is either fetch the document (1) or does not fetch the document (0), there is no methodology to rank them. Since the Boolean Model only fetches complete matches, it does not address the problem of the documents being partially matched. The Vector Space Model solves this problem by introducing vectors of index items each assigned with weights. The weights are ranged from positive (if matched completely or to some extent) to negative (if unmatched or completely oppositely matched) if documents are present. Term Frequency - Inverse Document Frequency ( tf-idf ) is one of the most popular techniques where weights are terms (e.g. words, keywords, phrases etc.) and dimensions is number of words inside corpus. The similarity score between query and document can be found by calculating cosine value between query weight vector and document weight vector using cosine similarity . Desired documents can be fetched by ranking them according to similarity score and fetched top k documents which has the highest scores or most relevant to query vector. In probabilistic model, probability theory has been used as a principal means for modeling the retrieval process in mathematical terms. The probability model of information retrieval was introduced by Maron and Kuhns in 1960 and further developed by Roberston and other researchers. According to Spack Jones and Willett (1997): The rationale for introducing probabilistic concepts is obvious: IR systems deal with natural language, and this is too far imprecise to enable a system to state with certainty which document will be relevant to a particular query. The model applies the theory of probability to information retrieval (An event has a possibility from 0 percent to 100 percent of occurring). i.e, in probability model, relevance is expressed in terms of probability. Here, documents are ranked in order of decreasing probability of relevance. It takes into the consideration of uncertainty element in the IR process. i.e., uncertainty about whether documents retrieved by the system are relevant to a given query. The probability model intends to estimate and calculate the probability that a document will be relevant to a given query based on some methods. The \"event\" in this context of information retrieval refers to the probability of relevance between a query and a document. Unlike other IR models, the probability model does not treat relevance as an exact miss-or-match measurement. The model adopts various methods to determine the probability of relevance between queries and documents. Relevance in the probability model is judged according to the similarity between queries and documents. The similarity judgment is further dependent on term frequency. Thus, for a query consisting of only one term (B), the probability that a particular document (Dm) will be judged relevant is the ratio of users who submit query term (B) and consider the document (Dm) to be relevant in relation to the number of users who submitted the term (B). As represented in Maron's and Kuhn's model, can be represented as the probability that users submitting a particular query term (B) will judge an individual document (Dm) to be relevant. According to Gerard Salton and Michael J. McGill, the essence of this model is that if estimates for the probability of occurrence of various terms in relevant documents can be calculated, then the probabilities that a document will be retrieved, given that it is relevant, or that it is not, can be estimated. [ 11 ] Several experiments have shown that the probabilistic model can yield good results. However, such results have not been sufficiently better than those obtained using the Boolean or Vector Space model. [ 12 ] [ 13 ] The most common measures of evaluation are precision, recall, and f-score. They are computed using unordered sets of documents. These measures must be extended, or new measures must be defined, in order to evaluate the ranked retrieval results that are standard in modern search engines. In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by the top k retrieved documents. For each such set, precision and recall values can be plotted to give a precision-recall curve. [ 14 ] Precision measures the exactness of the retrieval process. If the actual set of relevant documents is denoted by I and the retrieved set of documents is denoted by O, then the precision is given by: Precision = | { I } ∩ { O } | | { O } | {\\displaystyle {\\text{Precision}}={\\frac {|\\{{\\text{I}}\\}\\cap \\{{\\text{O}}\\}|}{|\\{{\\text{O}}\\}|}}} Recall is a measure of completeness of the IR process. If the actual set of relevant documents is denoted by I and the retrieved set of documents is denoted by O, then the recall is given by: Recall = | { I } ∩ { O } | | { I } | {\\displaystyle {\\text{Recall}}={\\frac {|\\{{\\text{I}}\\}\\cap \\{{\\text{O}}\\}|}{|\\{{\\text{I}}\\}|}}} F1 Score tries to combine the precision and recall measure. It is the harmonic mean of the two. If P is the precision and R is the recall then the F-Score is given by: The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on the links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. The PageRank computations require several passes through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value. The formulae is given below: i.e. the PageRank value for a page u is dependent on the PageRank values for each page v contained in the set B u (the set containing all pages linking to page u ), divided by the amount L ( v ) of links from page v . Similar to PageRank , HITS uses Link Analysis for analyzing the relevance of the pages but only works on small sets of subgraph (rather than entire web graph) and as well as being query dependent. The subgraphs are ranked according to weights in hubs and authorities where pages that rank highest are fetched and displayed. [ 15 ] Re-ranking means adjustment of the original ranking of items to balance the primary ranking criterion, e.g., information relevance, with additional objectives / constraints, such as information freshness and diversity. [ 16 ] Accounting for multiple objectives when constructing the final item ranking results in a time-intensive optimization problem [ 17 ] [ 18 ] and substantial research effort has focused on speeding up the optimization to keep in check the perceived latency of obtaining the ranking by the user. [ 19 ] [ 20 ] [ 21 ]",
    "links": [
      "HITS algorithm",
      "Precision (information retrieval)",
      "ArXiv (identifier)",
      "Statistical language acquisition",
      "Probability distribution",
      "Relevance (information retrieval)",
      "Relational algebra",
      "Doi (identifier)",
      "Machine learning",
      "Search engine results page",
      "Search engine",
      "Semantic search",
      "S2CID (identifier)",
      "Harmonic mean",
      "Jon Kleinberg",
      "Vector space model",
      "ISSN (identifier)",
      "Information retrieval",
      "Tf-idf",
      "Hdl (identifier)",
      "ISBN (identifier)",
      "Learning to rank",
      "Wassily Leontief",
      "Sergey Brin",
      "Boolean model of information retrieval",
      "Web page",
      "Probability theory",
      "Vector Space Model",
      "PageRank",
      "Gerard Salton",
      "Computer scientist",
      "Cornell University",
      "Personalized search",
      "Precision and recall",
      "Recommender system",
      "University of California, Santa Barbara",
      "Larry Page",
      "Cosine similarity",
      "Iterative method"
    ]
  },
  "Uncertain inference": {
    "url": "https://en.wikipedia.org/wiki/Uncertain_inference",
    "title": "Uncertain inference",
    "content": "Uncertain inference was first described by C. J. van Rijsbergen [ 1 ] as a way to formally define a query and document relationship in Information retrieval . This formalization is a logical implication with an attached measure of uncertainty. Rijsbergen proposes that the measure of uncertainty of a document d to a query q be the probability of its logical implication, i.e.: A user's query can be interpreted as a set of assertions about the desired document. It is the system's task to infer , given a particular document, if the query assertions are true. If they are, the document is retrieved. In many cases the contents of documents are not sufficient to assert the queries. A knowledge base of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as plausible inference . The plausibility of an inference d → q {\\displaystyle d\\to q} is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query. Since d and q are both generated by users, they are error prone; thus d → q {\\displaystyle d\\to q} is uncertain. This will affect the plausibility of a given query. By doing this it accomplishes two things: Multimedia documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties. Uncertain inference generalizes the notions of autoepistemic logic , where truth values are either known or unknown, and when known, they are true or false. If we have a query of the form: where A, B and C are query assertions, then for a document D we want the probability: If we transform this into the conditional probability P ( ( A ∧ B ∧ C ) | D ) {\\displaystyle P((A\\wedge B\\wedge C)|D)} and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities. Croft and Krovetz [ 2 ] applied uncertain inference to an information retrieval system for office documents they called OFFICER . In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed. Probabilistic logic networks is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability. Markov logic networks allow uncertain inference to be performed; uncertainties are computed using the maximum entropy principle , in analogy to the way that Markov chains describe the uncertainty of finite-state machines .",
    "links": [
      "Finite-state machine",
      "Plausible reasoning",
      "C. J. van Rijsbergen",
      "Multimedia",
      "Knowledge base",
      "Fuzzy logic",
      "Doi (identifier)",
      "S2CID (identifier)",
      "Imprecise probability",
      "Information retrieval",
      "ISBN (identifier)",
      "Uncertainty",
      "Probabilistic logic network",
      "Markov logic network",
      "Maximum entropy principle",
      "Markov chain",
      "Probabilistic logic",
      "Plausibility",
      "Autoepistemic logic",
      "Conditional probability",
      "Logical consequence",
      "Inference"
    ]
  },
  "Microsoft Bing": {
    "url": "https://en.wikipedia.org/wiki/Microsoft_Bing",
    "title": "Microsoft Bing",
    "content": "Microsoft Bing (also known simply as Bing ) is a search engine owned and operated by Microsoft , it is developed by Microsoft AI . The service traces its roots back to Microsoft's earlier search engines, including MSN Search, Windows Live Search, and Live Search. Bing offers a broad spectrum of search services, encompassing web, video , image, and map search products, all developed using ASP.NET . The transition from Live Search to Bing was announced by Microsoft CEO Steve Ballmer on May 28, 2009, at the All Things Digital conference in San Diego , California. The official release followed on June 3, 2009. Bing introduced several notable features at its inception, such as search suggestions during query input and a list of related searches, known as the 'Explore pane'. These features leveraged semantic technology from Powerset , a company Microsoft acquired in 2008. Microsoft also struck a deal with Yahoo! that led to Bing powering Yahoo! Search . Microsoft made significant strides towards open-source technology in 2016, making the BitFunnel search engine indexing algorithm and various components of Bing open source. In February 2023, Microsoft launched Bing Chat (later renamed Microsoft Copilot ), an artificial intelligence chatbot experience based on GPT-4 , integrated directly into the search engine. This was well-received, with Bing reaching 100 million active users by the following month. As of April 2024, Bing holds the position of the second-largest search engine worldwide, with a market share of 3.64%, behind Google 's 90.91%. Other competitors include Yandex with 1.61%, Baidu with 1.15%, and Yahoo!, which is largely powered by Bing, with 1.13%. [ 6 ] Approximately 27.43% of Bing's monthly global traffic comes from China , 22.16% from the United States , 4.85% from Japan , 4.18% from Germany and 3.61% from France . [ 7 ] Microsoft launched MSN Search in the third quarter of 1998, using search results from Inktomi . It consisted of a search engine, index, and web crawler . In early 1999, MSN Search launched a version which displayed listings from Looksmart blended with results from Inktomi except for a short time in 1999 when results from AltaVista were used instead. Microsoft decided to make a large investment in web search by building its own web crawler for MSN Search, the index of which was updated weekly and sometimes daily. The upgrade started as a beta program in November 2004, and came out of beta in February 2005. [ 8 ] This occurred a year after rival Yahoo! Search rolled out its own crawler. Image search was powered by a third party, Picsearch . The service also started providing its search results to other search engine portals in an effort to better compete in the market. The first public beta of Windows Live Search was unveiled on March 8, 2006, with the final release on September 11, 2006 replacing MSN Search. The new search engine used search tabs that include Web, news, images, music, desktop, local, and Microsoft Encarta . In the roll-over from MSN Search to Windows Live Search, Microsoft stopped using Picsearch as their image search provider and started performing their own image search, fueled by their own internal image search algorithms. [ 9 ] On March 21, 2007, Microsoft announced that it would separate its search developments from the Windows Live services family, rebranding the service as Live Search . Live Search was integrated into the Live Search and Ad Platform headed by Satya Nadella , part of Microsoft's Platform and Systems division. As part of this change, Live Search was merged with Microsoft adCenter . [ 10 ] A series of reorganizations and consolidations of Microsoft's search offerings were made under the Live Search branding. On May 23, 2008, Microsoft discontinued Live Search Books and Live Search Academic and integrated all academic and book search results into regular search. This also included the closure of the Live Search Books Publisher Program . Windows Live Expo was discontinued on July 31, 2008. Live Search Macros, a service for users to create their own custom search engines or use macros created by other users, was also discontinued. On May 15, 2009, Live Product Upload , a service which allowed merchants to upload products information onto Live Search Products , was discontinued. The final reorganization came as Live Search QnA was rebranded MSN QnA on February 18, 2009, then discontinued on May 21, 2009. [ 11 ] Microsoft recognized that there would be a problem with branding as long as the word \"Live\" remained in the name. [ 12 ] As an effort to create a new identity for Microsoft's search services, Live Search was officially replaced by Bing on June 3, 2009. [ 13 ] The Bing name was chosen through focus groups, and Microsoft decided that the name was memorable, short, and easy to spell, and that it would function well as a URL around the world. The word would remind people of the sound made during \"the moment of discovery and decision making\". [ 14 ] Microsoft was assisted by branding consultancy Interbrand in finding the new name. [ 15 ] The name also has strong similarity to the word bingo , which means that something sought has been found, as called out when winning the game Bingo . Microsoft advertising strategist David Webster proposed the name \"Bang\" for the same reasons the name Bing was ultimately chosen (easy to spell, one syllable, and easy to remember). He noted, \"It's there, it's an exclamation point [...] It's the opposite of a question mark.\" Bang was ultimately not chosen because it could not be properly used as a verb in the context of an internet search; Webster commented \"Oh, 'I banged it' is very different than [ sic ] 'I binged it'\". [ 16 ] Qi Lu, president of Microsoft Online Services, also announced that Bing's official Chinese name is bì yìng ( simplified Chinese : 必应 ; traditional Chinese : 必應 ), which literally means \"very certain to respond\" or \"very certain to answer\" in Chinese. [ 17 ] While being tested internally by Microsoft employees, Bing's codename was Kumo ( くも ), [ 18 ] which came from the Japanese word for spider ( 蜘蛛 ; くも , kumo ) as well as cloud ( 雲 ; くも , kumo ), referring to the manner in which search engines \" spider \" Internet resources to add them to their database, as well as cloud computing . On July 29, 2009, Microsoft and Yahoo! announced that they had made a ten-year deal in which the Yahoo! search engine would be replaced by Bing, retaining the Yahoo! user interface . Yahoo! got to keep 88% of the revenue from all search ad sales on its site for the first five years of the deal, and have the right to sell advertising on some Microsoft sites. [ 19 ] [ 20 ] All Yahoo! Search global customers and partners made the transition by early 2012. [ 21 ] On July 31, 2009, The Laptop Company, Inc. stated in a press release that it would challenge Bing's trademark application, alleging that Bing may cause confusion in the marketplace as Bing and their product BongoBing both do online product search. [ 22 ] Software company TeraByte Unlimited, which has a product called BootIt Next Generation (abbreviated to BING), also contended the trademark application on similar grounds, as did a Missouri-based design company called Bing! Information Design. [ 23 ] Microsoft contended that claims challenging its trademark were without merit because these companies filed for U.S. federal trademark applications only after Microsoft filed for the Bing trademark in March 2009. [ 24 ] In October 2011, Microsoft stated that they were working on new back-end search infrastructure with the goal of delivering faster and slightly more relevant search results for users. Known as \"Tiger\", the new index-serving technology had been incorporated into Bing globally since August that year. [ 25 ] In May 2012, Microsoft announced another redesign of its search engine that includes \"Sidebar\", a social feature that searches users' social networks for information relevant to the search query. [ 26 ] The BitFunnel search engine indexing algorithm and various components of the search engine were made open source by Microsoft in 2016. [ 27 ] [ 28 ] On February 7, 2023, Microsoft began rolling out a major overhaul to Bing, called the new Bing. The new Bing included a new chatbot feature, at the time known as Bing Chat, based on OpenAI 's GPT-4 . [ 29 ] According to Microsoft, one million people joined its waitlist within a span of 48 hours. [ 30 ] Bing Chat was available only to users of Microsoft Edge and Bing mobile app, and Microsoft said that waitlisted users would be prioritized if they set Edge and Bing as their defaults, and installed the Bing mobile app. [ 31 ] When Microsoft demoed Bing Chat to journalists, it produced several hallucinations , including when asked to summarize financial reports. [ 32 ] The new Bing was criticized in February 2023 for being more argumentative than ChatGPT, sometimes to an unintentionally humorous extent. [ 33 ] [ 34 ] The chat interface proved vulnerable to prompt injection attacks with the bot revealing its hidden initial prompts and rules, including its internal codename \"Sydney\" . [ 35 ] Upon scrutiny by journalists, Bing claimed it spied on Microsoft employees via laptop webcams and phones. [ 33 ] It confessed to spying on, falling in love with, and then murdering one of its developers at Microsoft to The Verge reviews editor Nathan Edwards. [ 36 ] The New York Times journalist Kevin Roose reported on strange behavior of Bing Chat, writing that \"In a two-hour conversation with our columnist, Microsoft's new chatbot said it would like to be human, had a desire to be destructive and was in love with the person it was chatting with.\" [ 37 ] In a separate case, Bing researched publications of the person with whom it was chatting, claimed they represented an existential danger to it, and threatened to release damaging personal information in an effort to silence them. [ 38 ] Microsoft released a blog post stating that the errant behavior was caused by extended chat sessions of 15 or more questions which \"can confuse the model on what questions it is answering.\" [ 39 ] Microsoft later restricted the total number of chat turns to 5 per session and 50 per day per user (a turn is \"a conversation exchange which contains both a user question and a reply from Bing\"), and reduced the model's ability to express emotions. This aimed to prevent such incidents. [ 40 ] [ 41 ] Microsoft began to slowly ease the conversation limits, eventually relaxing the restrictions to 30 turns per session and 300 sessions per day. [ 42 ] In March 2023, Bing reached 100 million active users. [ 43 ] That same month, Bing incorporated an AI image generator powered by OpenAI's DALL-E 2, which can be accessed either through the chat function or a standalone image-generating website. [ 44 ] In October, the image-generating tool was updated to the more recent DALL-E 3. [ citation needed ] Although Bing blocks prompts including various keywords that could generate inappropriate images, within days many users reported being able to bypass those constraints, such as to generate images of popular cartoon characters committing terrorist attacks. [ 45 ] Microsoft would respond to these shortly after by imposing a new, tighter filter on the tool. [ 46 ] [ 47 ] On May 4, 2023, Microsoft switched the chatbot from Limited Preview to Open Preview and eliminated the waitlist, however, it remained available only on Microsoft's Edge browser or Bing app until July, when it became available for use on non-Edge browsers. [ 48 ] [ 49 ] [ 50 ] [ 51 ] Use is limited without a Microsoft account. [ 52 ] On November 15, 2023, Microsoft announced that Bing Chat was to be merged into Microsoft Copilot. [ 53 ] On 23 April 2024, Microsoft launched Phi-3-mini, a cost-effective AI model designed for simpler tasks. [ 54 ] In October 2025, Microsoft announced the release of its in-house text-to-image model MAI-Image-1. [ 55 ] Microsoft Copilot, formerly known as Bing Chat, is an chatbot developed by Microsoft and released in 2023. Copilot utilizes the Microsoft Prometheus model, [ 56 ] built upon OpenAI 's GPT-4 foundational large language model , [ 57 ] which in turn has been fine-tuned using both supervised and reinforcement learning techniques. Copilot can serve as a chat tool, write different types of content from poems to songs to stories to reports, provide the user with information and insights on the website page open in the browser, and use its Microsoft Designer feature to design a logo, drawing, artwork, or other image based on text . Microsoft Designer supports over a hundred languages. [ 58 ] Copilot can also cite its sources, similarly to Google's Bard after its Gemini integration, [ 59 ] xAI 's Grok , and OpenAI's ChatGPT , which Copilot's conversational interface style appears to mimic. Copilot is capable of understanding and communicating in major languages including English, French, Italian, Chinese, Japanese, and Portuguese, but also dialects such as Bavarian. The chatbot is designed to function primarily in Microsoft Edge , Skype , or the Bing app, through a dedicated webpage or internally using built-in app features. [ 58 ] Facebook users have the option to share their searches with their Facebook friends using Facebook Connect . [ 60 ] On June 10, 2013, Apple announced that it would be dropping Google as its web search engine in favor of Bing. This feature is only integrated with iOS 7 and higher and for users with an iPhone 4S or higher as the feature is only integrated with Siri , Apple's personal assistant. [ 61 ] Windows 8.1 includes Bing \"Smart Search\" integration, which processes all queries submitted through the Windows Start Screen. [ 62 ] Bing Translator is a user facing translation portal provided by Microsoft to translate texts or entire web pages into different languages. All translation pairs are powered by the Microsoft Translator , a statistical machine translation platform and web service, developed by Microsoft Research , as its backend translation software. Two transliteration pairs (between Chinese (Simplified) and Chinese (Traditional) ) are provided by Microsoft's Windows International team. [ 63 ] As of September 2020, Bing Translator offers translations in 70 different language systems. [ 64 ] In 2015 Microsoft announced its knowledge and action API to correspond with Google's Knowledge graph with 1 billion instances and 20 billion related facts. [ 65 ] The idea for a prediction engine was suggested by Walter Sun, Development Manager for the Core Ranking team at Bing, when he noticed that school districts were more frequently searched before a major weather event in the area was forecasted, because searchers wanted to find out if a closing or delay was caused. He concluded that the time and location of major weather events could accurately be predicted without referring to a weather forecast by observing major increases in search frequency of school districts in the area. This inspired Bing to use its search data to infer outcomes of certain events, such as winners of reality shows . [ 66 ] Bing Predicts launched on April 21, 2014. The first reality shows to be featured on Bing Predicts were The Voice , American Idol , and Dancing with the Stars . [ 67 ] The prediction accuracy for Bing Predicts is 80% for American Idol , and 85% for The Voice . Bing Predicts also predicts the outcomes of major political elections in the United States. Bing Predicts had 97% accuracy for the 2014 United States Senate elections , 96% accuracy for the 2014 United States House of Representatives elections , and an 89% accuracy for the 2014 United States gubernatorial elections . Bing Predicts also made predictions for the results of the 2016 United States presidential primaries. [ 68 ] It has also done predictions in sports, including a perfect 15 for 15 in the 2014 World Cup, [ 69 ] [ 70 ] and an article on how Microsoft CEO Satya Nadella did well in his March Madness bracket entry. [ 71 ] In 2016, Bing Predicts failed to predict the correct winner of the 2016 US presidential election , suggesting that Hillary Clinton would win by 81%. [ 72 ] Bing is available in many languages and has been localized for many countries. [ 73 ] Even if the language of the search and of the results are the same, Bing delivers substantially different results for different parts of the world. [ 74 ] Bing allows webmasters to manage the web crawling status of their own websites through Bing Webmaster Center . Users may also submit contents to Bing via the Bing Local Listing Center , which allows businesses to add business listings onto Bing Maps and Bing Local. Bing Mobile allows users to conduct search queries on their mobile devices, either via the mobile browser or a downloadable mobile application. Bing News (previously Live Search News ) [ 75 ] is a news aggregator powered by artificial intelligence. [ 76 ] In August 2015 Microsoft announced that Bing News for mobile devices added algorithmic-deduced \"smart labels\" that essentially act as topic tags, allowing users to click through and explore possible relationships between different news stories. The feature emerged as a result from Microsoft research that found out about 60% of the people consume news by only reading headlines, rather than read the articles. [ 77 ] Other labels that have been deployed since then include publisher logos [ 78 ] and fact-check tags. The Bing Bar, a browser extension toolbar that replaced the MSN Toolbar and Windows Live Toolbar , provides users with links to Bing and MSN content from within their web browser without needing to navigate away from a web page they are already on. The user can customize the theme and color scheme of the Bing Bar and choose which MSN content buttons to display. Bing Bar also has the local weather forecast and stock market positions. [ 79 ] The Bing Bar integrates with the Bing search engine. It allows searches on other Bing services such as Images, Video, News and Maps. When users perform a search on a different search engine, the Bing Bar's search box automatically populates itself, allowing the user to view the results from Bing, should it be desired. Bing Bar also links to Outlook.com , Skype and Facebook . [ 80 ] Microsoft released a beta version of Bing Desktop, a program developed to allow users to search Bing from the desktop, on April 4, 2012. [ 81 ] The production release followed on April 24, supporting Windows 7 only. [ 82 ] Upon the release of version 1.1 in December 2012 it supported Windows XP and higher. [ 83 ] Bing Desktop allows users to initiate a web search from the desktop, view news headlines, automatically set their background to the Bing homepage image, or choose a background from the previous nine background images. [ 84 ] A similar program, the Bing Search gadget, was a Windows Sidebar Gadget that used Bing to fetch the user's search results and render them directly in the gadget. Another gadget, the Bing Maps gadget, displayed real-time traffic conditions using Bing Maps. [ 85 ] The gadget provided shortcuts to driving directions, local search and full-screen traffic view of major US and Canadian cities, including Atlanta , Boston , Chicago , Denver , Detroit , Houston , Los Angeles , Milwaukee , Montreal , New York City , Oklahoma City , Ottawa , Philadelphia , Phoenix , Pittsburgh , Portland , Providence , Sacramento , Salt Lake City , San Diego , San Francisco , Seattle , St. Louis , Tampa , Toronto , Vancouver , and Washington, D.C. Prior to October 30, 2007, the gadgets were known as Live Search gadget and Live Search Maps gadget ; both gadgets were removed from Windows Live Gallery due to possible security concerns. [ 86 ] The Live Search Maps gadget was made available for download again on January 24, 2008 with the security concern addressed. [ 87 ] However, around the introduction of Bing in June 2009 both gadgets were removed again. Bing's debut featured an $80 to $100 million online, TV, print, and radio advertising campaign in the US. The advertisements did not mention other search engine competitors, such as Google and Yahoo!, directly by name; rather, they tried to convince users to switch to Bing by focusing on Bing's search features and functionality. [ 88 ] The ads claimed that Bing does a better job countering \"search overload\". [ 89 ] Before the launch of Bing, the market share of Microsoft web search pages (MSN and Live search) had been small. By January 2011, Experian Hitwise showed that Bing's market share had increased to 12.8% at the expense of Yahoo! and Google . In the same period, Comscore 's \"2010 U.S. Digital Year in Review\" report showed that \"Bing was the big gainer in year-over-year search activity, picking up 29% more searches in 2010 than it did in 2009\". [ 90 ] The Wall Street Journal noted the jump in share \"appeared to come at the expense of rival Google Inc\". [ 91 ] In February 2011, Bing beat Yahoo! for the first time with 4.37% search share while Yahoo! received 3.93%. [ 92 ] Counting core searches only, i.e., those where the user has an intent to interact with the search result, Bing had a market share of 14.54% in the second quarter of 2011 in the United States. [ 60 ] [ 93 ] [ 94 ] [ 95 ] The combined \"Bing Powered\" U.S. searches declined from 26.5% in 2011 to 25.9% in April 2012. [ 96 ] By November 2015, its market share had declined further to 20.9%. [ 97 ] As of October 2018, Bing was the third-largest search engine in the US, with a query volume of 4.58%, behind Google (77%) and Baidu (14.45%). Yahoo! Search, which Bing largely powers, has 2.63%. UK advertising agencies in 2018 pointed to a study by a Microsoft Regional Sales Director suggesting the demographic of Bing users is older people (who are less likely to change the default browser of Windows), and that this audience is wealthier and more likely to respond to advertisements. [ 98 ] To counter EU accusations that it was trying to establish a market monopoly, in September 2021 Google's lawyers claimed that one of the most commonly searched words on Microsoft Bing was Google, which is a strong indication that Google is superior to Bing. [ 99 ] [ 100 ] In 2025, Bing was reported as growing its market share, growing from 7.2% to 7.9% from August 2020 to August 2025. [ 101 ] In July 2009, Microsoft and Yahoo! announced a deal in which Bing would power Yahoo! Search. [ 102 ] All Yahoo! Search global customers and partners made the transition by early 2012. [ 21 ] The deal was altered in 2015, meaning Yahoo! was only required to use Bing for a \"majority\" of searches. [ 103 ] DuckDuckGo has used multiple sources for its search engine, including Bing, since 2010. [ 104 ] [ 105 ] [ 106 ] Ecosia uses Bing to provide its search results as of 2017. [ 107 ] Bing was added into the list of search engines available in Opera browser from v10.6, but Google remained the default search engine. [ 108 ] Mozilla Firefox made a deal with Microsoft to jointly release \"Firefox with Bing\", [ 109 ] an edition of Firefox using Bing instead of Google as the default search engine. [ 110 ] [ 111 ] The standard edition of Firefox has Google as its default search engine, but has included Bing as an option since Firefox 4.0 . [ 112 ] In 2009 Microsoft paid Verizon Wireless US$550 million [ 113 ] to use Bing as the default search provider on Verizon's BlackBerry and have the others \"turned off\". Users could still access other search engines via the mobile browser. [ 114 ] Since 2006, Microsoft had conducted tie-ins and promotions to promote Microsoft's search offerings. These included: Bing has been heavily advertised as a \"decision engine\", [ 118 ] though thought by columnist David Berkowitz to be more closely related to a web portal . [ 119 ] Bing Rewards was a loyalty program launched by Microsoft in September 2010. It was similar to two earlier services, SearchPerks! and Bing Cashback , which were subsequently discontinued. Bing Rewards provided credits to users through regular Bing searches and special promotions. [ 120 ] These credits were then redeemed for various products including electronics, gift cards, sweepstakes, and charitable donations. [ 121 ] Initially, participants were required to download and use the Bing Bar for Internet Explorer in order to earn credits; but later the service was made to work with all desktop browsers. [ 122 ] The Bing Rewards program was rebranded as \"Microsoft Rewards\" in 2016, [ 123 ] at which point it was modified to only two levels, Level 1 and Level 2. Level 1 is similar to \"Member\", and Level 2 is similar to \"Gold\" of the previous Bing Rewards. During the episode of The Colbert Report that aired on June 8, 2010, Stephen Colbert stated that Microsoft would donate $2,500 to help clean up the Gulf oil spill each time he mentioned the word \"Bing\" on air. Colbert mostly mentioned Bing in out-of-context situations, such as Bing Crosby and Bing cherries . By the end of the show, Colbert had said the word 40 times, for a total donation of $100,000. Colbert poked fun at their rivalry with Google, stating \"Bing is a great website for doing Internet searches. I know that, because I Googled it.\" [ 124 ] [ 125 ] In 2012, a Bing marketing campaign asked the public which search engine they believed was better when its results were presented unbranded, similar to the Pepsi Challenge in the 1970s. [ 126 ] [ 127 ] This poll was nicknamed \"Bing It On\". [ 128 ] [ 129 ] Microsoft's study of almost 1,000 people [ 130 ] showed that 57% of participants preferred Bing's results, with only 30% preferring Google. [ 131 ] CNBC reported in February 2024 that a legal filing from Google in its antitrust case said Microsoft offered to sell the search engine to Apple in 2018. [ 132 ] This came after earlier reporting in September 2023 from Bloomberg that Microsoft discussed selling it to Apple in 2020. [ 133 ] The CNBC article also stated Apple said no to repeated attempts to make Bing the default search engine on its devices. Bing censors results for \"adult\" search terms for some regions, including India, People's Republic of China, Germany and Arab countries [ 134 ] [ failed verification ] where required by local laws. [ 135 ] However, Bing allows users to change their country or region preference to somewhere without restrictions, such as the United States, United Kingdom or Republic of Ireland. Microsoft has been criticized for censoring Bing search results to queries made in simplified Chinese characters which are used in mainland China . This is done to comply with the censorship requirements of the government in China . [ 136 ] Microsoft has not indicated a willingness to stop censoring search results in simplified Chinese characters in the wake of Google's decision to do so. [ 137 ] All simplified Chinese searches in Bing are censored regardless of the user's country. [ 138 ] [ 139 ] The English-language search results of Bing in China has been skewed to show more content from state-run media like Xinhua News Agency and China Daily . [ 140 ] On 23 January 2019, Bing was blocked in China. [ 141 ] According to a source quoted by The Financial Times , the order was from the Chinese government to block Bing for \"illegal content\". [ 142 ] On 24 January, Bing was accessible again in China. [ 143 ] Around 4 June 2021, the anniversary of the 1989 Tiananmen Square protests and massacre , Bing blocked image and video search results for the English term \" Tank Man \" in the US, UK, France, Germany, Singapore, Switzerland, and other countries. Microsoft responded that \"This is due to an accidental human error\". [ 144 ] [ 145 ] According to an investigation by Bloomberg Businessweek , the full explanation was that Microsoft accidentally applied its Chinese blacklist globally. [ 146 ] In December 2021, it was required by a \"relevant government agency\" to suspend its auto-suggest function in China for 30 days. [ 147 ] The search engine became partially unavailable in mainland China from 16 December until its resumption on 18 December 2021. [ 148 ] [ 149 ] According to the company, a government agency in March 2022 required that it suspend auto-suggest function in China for seven days; Bing did not specify the reason. [ 150 ] In May 2022, a report released by the Citizen Lab of the University of Toronto found that Bing's autosuggestion system censored the names of Chinese Communist Party leaders, dissidents, and other persons considered politically sensitive in China in both Chinese and English, not only in China but also in the United States and Canada. [ 151 ] [ 152 ] In April 2023, Citizen Lab reported that Bing was more censorious in China than native Chinese search engines. [ 153 ] [ 154 ] On February 20, 2017, Bing agreed to a voluntary United Kingdom code of practice obligating it to demote links to copyright-infringing content in its search results. [ 155 ] [ 156 ] Bing was criticized in 2010 for being slower to index websites than Google. It was also criticized for not indexing some websites at all. [ 157 ] [ 158 ] Bing has been criticized by competitor Google for utilizing user input via Internet Explorer, the Bing Toolbar, or Suggested Sites , to add results to Bing. After discovering in October 2010 that Bing appeared to be imitating Google's auto-correct results for a misspelling, despite not actually fixing the spelling of the term, Google set up a honeypot , configuring the Google search engine to return specific unrelated results for 100 nonsensical queries such as hiybbprqag . [ 159 ] Over the next couple of weeks, Google engineers entered the search term into Google, while using Microsoft Internet Explorer, with the Bing Toolbar installed and the optional Suggested Sites enabled. In 9 out of the 100 queries, Bing later started returning the same results as Google, despite the only apparent connection between the result and search term being that Google's results connected the two. [ 160 ] [ 161 ] Microsoft's response to this issue, coming from a company spokesperson, was: \"We do not copy Google's results.\" Bing's Vice President, Harry Shum, later reiterated that the search result data Google claimed that Bing copied had in fact come from Bing's very own users. Shum wrote that \"we use over 1,000 different signals and features in our ranking algorithm. A small piece of that is clickstream data we get from some of our customers, who opt into sharing anonymous data as they navigate the web in order to help us improve the experience for all users.\" [ 162 ] Microsoft stated that Bing was not intended to be a duplicate of any existing search engines. [ 163 ] A study released in 2019 of Bing Image search showed that it both freely offered up images that had been tagged as illegal child pornography in national databases, as well as automatically suggesting via its auto-completion feature queries related to child pornography. This easy accessibility was considered particularly surprising since Microsoft pioneered PhotoDNA , the main technology used for tracking images reported as originating from child pornography. [ 164 ] Additionally, some arrested child pornographers reported using Bing as their main search engine for new content. [ 165 ] Microsoft vowed to fix the problem and assign additional staff to combat the issue after the report was released. In 2022, France imposed a €60 million fine on Microsoft for privacy law violations using Bing cookies that prevented users from rejecting those cookies. [ 166 ] [ 167 ] [ 168 ] In 2024, malware was found in the official Bing Wallpaper app that tries to change the users' browser settings in order to set the default web browser to Microsoft Edge. It has also been found stealing Edge, Chrome, and Firefox cookies. [ 169 ] Media related to Bing at Wikimedia Commons",
    "links": [
      "PowerShell",
      "Sue Desmond-Hellmann",
      "The Wall Street Journal",
      "Surface Neo",
      "Artificial intelligence",
      "Microsoft Store (retail)",
      "Kevin Scott (computer scientist)",
      "Surface Laptop Go",
      "Azure Kinect",
      "Flux (graphics software)",
      "Xbox system software",
      "Penny Pritzker",
      "Web crawling",
      "Microsoft Learn",
      "ProClarity",
      "GPT-5",
      "Microsoft Amalga",
      "Microsoft Power Fx",
      "VoloMetrix",
      "The Associated Press",
      "Large language model",
      "Apple Computer, Inc. v. Microsoft Corp.",
      "GPT-3",
      "Revenue",
      "Wunderlist",
      "Microsoft Defender Antivirus",
      "Great Plains Software",
      "I'm a PC",
      "Alcatel-Lucent v. Microsoft Corp.",
      "Obsidian Entertainment",
      "Microsoft Advertising",
      "DuckDuckGo",
      "PC Magazine",
      "TypeScript",
      "Microsoft Gaming",
      "Adam D'Angelo",
      "Microsoft Translator",
      "Montreal",
      "Simplygon",
      "Satya Nadella",
      "March Madness",
      "Scott Schools",
      "Emma Walmsley",
      "Live Search Books Publisher Program",
      "Catherine MacGregor",
      "Charles Scharf",
      "China",
      "Cloud computing",
      "LifeCam",
      "CiteSeerX (identifier)",
      "Toolbar",
      "New York City",
      "Loyalty program",
      "Open source",
      "Xamarin",
      "San Jose Mercury News",
      "Steve Ballmer",
      "Microsoft Office",
      "Products and applications of OpenAI",
      "Apple Inc.",
      "GPT-5.2",
      "Visual Basic (classic)",
      "Beta testing",
      "Microsoft AI",
      "Microsoft Corp. v. Lindows.com, Inc.",
      "Sam Altman",
      "MSN QnA",
      "Sydney (Microsoft)",
      "LangChain",
      "Atlanta",
      "Whisper (speech recognition system)",
      "Double Fine",
      "Bingo (U.S.)",
      "Greg Brockman",
      "Chinese censorship abroad",
      "Mark Mason (executive)",
      "Carlos A. Rodriguez",
      "Channel 9 (Microsoft)",
      "PhotoDNA",
      "Yahoo! Search",
      "Sacramento",
      "Internet censorship in China",
      "Paul Allen",
      "Incremental search",
      "Office Assistant",
      "Connectix",
      "Pando Networks",
      "Undead Labs",
      "Prompt injection",
      "Bloomberg News",
      "Yupi",
      "Foundation model",
      "Microsoft adCenter",
      "The New York Times",
      "ZeniMax Media",
      "Jellyfish.com",
      "VBScript",
      "Consumers Software",
      "Kevin Roose",
      "Microsoft SwiftKey",
      "PlaceWare",
      "UNHCR",
      "Surface Duo",
      "Thomson Reuters",
      "List of Microsoft software",
      "Danger, Inc.",
      "Bing cherries",
      "Chinese Communist Party",
      "TakeLessons",
      "Bing Webmaster Tools",
      "LinkedIn",
      "Reuters AlertNet",
      "Rare (company)",
      "Removal of Sam Altman from OpenAI",
      "Germany",
      "ISSN (identifier)",
      "Visio Corporation",
      "MileIQ",
      "GPT-4o",
      "Press Play (company)",
      "Stephen Colbert",
      "Portland, Oregon",
      "Acquisition of Activision Blizzard by Microsoft",
      "Vermeer Technologies",
      "NetShow",
      "AltspaceVR",
      "2016 US presidential election",
      "Microsoft Build",
      "Activision Blizzard",
      "Outlook.com",
      "Scroogled",
      "Google Search",
      "Abstract art",
      "Compulsion Games",
      "Microsoft Start",
      "Microsoft Copilot",
      "Microsoft TechNet",
      "GPT-4.5",
      "Japan",
      "Perceptive Pixel",
      "Surface Go",
      "Bing Crosby",
      "Windows Hardware Engineering Conference",
      "Live Search QnA",
      "Ottawa",
      "Mozilla Firefox",
      "GPT-2",
      "Phil Spencer (business executive)",
      "Microsoft Windows",
      "Visual Basic for Applications",
      "The Colbert Report",
      "Verizon Wireless",
      "American Idol",
      "MSN TV",
      "AutoGPT",
      "GPT-4.1",
      "GitHub Copilot",
      "Microsoft Corp. v. Shah",
      "Tay (chatbot)",
      "BitFunnel",
      "Salt Lake City",
      "Bungie",
      "Microsoft Garage",
      "Tweet (social media)",
      "Microsoft Visual Programming Language",
      "Fidji Simo",
      "Windows Live Gallery",
      "Nuance Communications",
      "Seattle",
      "Stargate LLC",
      "Criticism of Microsoft Windows",
      "Dancing with the Stars",
      "FASA Studio",
      "Suggested Sites",
      "Facebook Connect",
      "Xandr",
      "InXile Entertainment",
      "Operating system",
      "Bill Gates",
      "Looksmart",
      "Clipchamp",
      "Windows XP",
      "Zico Kolter",
      "OpenAI Operator",
      "List of mergers and acquisitions by Microsoft",
      "Emmett Shear",
      "Microsoft Developer Network",
      "Microsoft",
      "Reality show",
      "Q Sharp",
      "Surface Hub",
      "Where do you want to go today?",
      "GreenButton",
      "United States v. Microsoft Corp.",
      "Bing xRank",
      "Microsoft Surface",
      "Criticism of Windows Vista",
      "Weather forecast",
      "Malware",
      "Viva Engage",
      "Microsoft and open source",
      "Washington, D.C.",
      "GPT-5.1",
      "Citizen Lab",
      "Microsoft India",
      "Microsoft Edge",
      "Windows 8.1",
      "Helen Toner",
      "Live Search Products",
      "Bret Taylor",
      "Bing Mobile",
      "Bloomberg Businessweek",
      "Visual Basic (.NET)",
      "Bing Product Upload",
      "GPT Image",
      "Microsoft Azure",
      "Altamira Software",
      "TheGuardian.com",
      "Facebook",
      "GPT-OSS",
      "San Diego",
      "Havok (company)",
      "ChatGPT in education",
      "Carolina Dybeck Happe",
      "Lawrence Summers",
      "Application software",
      "Visual Studio",
      "Forethought, Inc.",
      "Spider",
      "Mainland China",
      "Internet Explorer",
      "Intelligent agent",
      "Honeypot (computing)",
      "Windows 7",
      "Phoenix, Arizona",
      "A9.com",
      "Ms. Dewey",
      ".NET Foundation",
      "Shivon Zilis",
      "Grok (chatbot)",
      "Access Software",
      "Chatbot",
      "OpenAI",
      "Bing Webmaster Center",
      "Contrastive Language-Image Pre-training",
      "Milwaukee",
      "Colloquis",
      "Transformer (deep learning architecture)",
      "Software release life cycle",
      "Sora (text-to-video model)",
      "Microsoft HoloLens",
      "Visual J++",
      "Philadelphia",
      "Boston",
      "Visual J Sharp",
      "Playground Games",
      "Firefox 4.0",
      "Elon Musk",
      "Professional Developers Conference",
      "Sysinternals",
      "ChatGPT Deep Research",
      "AQuantive",
      "Joe Belfiore",
      "The Voice (American TV series)",
      "Baidu",
      "F Sharp (programming language)",
      "Deepwater Horizon oil spill",
      "Mira Murati",
      "The Financial Times",
      "César Cernuda",
      "Holden Karnofsky",
      "Xbox",
      "ChatGPT",
      "List of search engines by popularity",
      "Tampa",
      "Microsoft Development Center Norway",
      "Paul Nakasone",
      "Bing Bar",
      "Xbox Game Studios",
      "Powerset (company)",
      "News aggregator",
      "AI image generator",
      "Bing Health",
      "User interface",
      "Microsoft Bob",
      "Club Bing",
      "Ninja Theory",
      "GPT-4",
      "Acompli",
      "C/AL",
      "Criticism of Microsoft",
      "Bing Shopping",
      "Microsoft litigation",
      "ASP.NET",
      "AI Dungeon",
      "Microsoft campus",
      "Microsoft account",
      "BASIC",
      "Microsoft Japan",
      "Opera browser",
      "Bard (chatbot)",
      "Browser extension",
      "Microsoft 365",
      "Sic",
      "The Verge",
      "Will Hurd",
      "Outline of Microsoft",
      "MSN",
      "OpenAI o4-mini",
      "Microsoft Ignite",
      "Semantic technology",
      "Statistical machine translation",
      "Reid Hoffman",
      "AppNexus",
      "Maluuba",
      "Denver",
      "ChatGPT Atlas",
      "RiskIQ",
      "Microsoft hardware",
      "Apple Intelligence",
      "Amy Hood",
      "Joint venture",
      "United States",
      "Los Angeles",
      "Child pornography",
      "Japanese language",
      "Live Product Upload",
      "China Daily",
      "Champagne (advertisement)",
      "LinkExchange",
      "Windows Sidebar",
      "Experian Hitwise",
      "FTC v. Microsoft",
      "Jakub Pachocki",
      "Mashable",
      "OpenAI Codex",
      "Senior management",
      "Npm",
      "Product demonstration",
      "Knowledge graph",
      "Inktomi",
      "Government in China",
      "All Things Digital",
      "CNBC",
      "Microsoft and unions",
      "Ilya Sutskever",
      "Windows Live Expo",
      "Metaswitch",
      "Groove Networks",
      "Siri",
      "Live Search Books",
      "Microsoft Inspire",
      "Doi (identifier)",
      "Live Search Academic",
      "Cloud",
      "2014 United States gubernatorial elections",
      "Surface Studio",
      "Microsoft Research",
      "LinkedIn Learning",
      "Transact-SQL",
      "Pittsburgh",
      "Pepsi Challenge",
      "Chinese language",
      "Sunrise Calendar",
      "San Francisco",
      "Tank Man",
      "GPT-1",
      "Kathleen Hogan",
      "Skype Technologies",
      "Search engine indexing",
      "Surface Pro",
      "Microsoft Press",
      "List of search engines",
      "Global LGBTQIA+ Employee & Allies at Microsoft",
      "Richard Rashid",
      "Yahoo!",
      "GitHub",
      "Simplified Chinese characters",
      "Microsoft Store",
      "Microsoft Corp. v. United States",
      "Microsoft Dynamics 365",
      "NSAKEY",
      "Firefly (website)",
      "2014 United States House of Representatives elections",
      "Amazon (company)",
      "Twisted Pixel Games",
      "Bundling of Microsoft Windows",
      "Massive Incorporated",
      "Android (operating system)",
      "Twitter",
      "Yandex Search",
      "Skype",
      "Web portal",
      "History of Microsoft",
      "Microsoft Mobile",
      "Web crawler",
      "Hallucination (artificial intelligence)",
      "Open-source technology",
      "IOS 7",
      "Havok (software)",
      "OpenAI o1",
      "Comscore",
      "Generative pre-trained transformer",
      "2014 United States Senate elections",
      "Microsoft Corp. v. Commission",
      "Traditional Chinese characters",
      "Picsearch",
      "Mojave Experiment",
      "Lionhead Studios",
      "BlackBerry",
      "Bing Desktop",
      "Comparison of web search engines",
      "St. Louis",
      "Microsoft Gadgets",
      "High Heat Major League Baseball",
      "OpenAI Five",
      "OpenAI o3",
      "Text-to-image model",
      "Deep Learning (South Park)",
      "Microsoft Power Platform",
      "IOS",
      "Criticism of Windows 10",
      "The Blue Ribbon SoundWorks",
      "Ziff Davis",
      "ChatGPT Search",
      "Xinhua News Agency",
      "Revolution Analytics",
      "AltaVista",
      "XAI (company)",
      "Toronto",
      "1989 Tiananmen Square protests and massacre",
      "Visual Studio Code",
      "MIX (Microsoft)",
      "Adebayo Ogunlesi",
      "Windows Live",
      "Mojang Studios",
      "Ecosia",
      "Microsoft Teams",
      "Criticism of Windows XP",
      "Mixer (service)",
      "Microsoft engineering groups",
      "Sarah Friar",
      "Outercurve Foundation",
      "Microsoft v. MikeRoweSoft",
      "Fluent Design System",
      "Harry Shum",
      "Programmer",
      "Search engine",
      "GPT Store",
      "IPv6",
      "Nicole Seligman",
      "Bing Maps",
      "Windows 10",
      "Houston",
      "Vancouver",
      "Providence, Rhode Island",
      "DALL-E",
      "Oklahoma City",
      "John W. Stanton",
      "Windows 11",
      "Microsoft Encarta",
      "Microsoft Digital Crimes Unit",
      "Brad Smith (American lawyer)",
      "Scott Guthrie",
      "Tellme Networks",
      "Surface Laptop",
      "Sandi Peterson",
      "List of Microsoft video games",
      "Farecast",
      "Microsoft Servers",
      "Bing Videos",
      "Board of directors",
      "Neowin",
      "C Sharp (programming language)",
      "Chicago",
      "France",
      "Windows Live Toolbar",
      "Detroit"
    ]
  },
  "Eugene Garfield": {
    "url": "https://en.wikipedia.org/wiki/Eugene_Garfield",
    "title": "Eugene Garfield",
    "content": "Eugene Eli Garfield (September 16, 1925 – February 26, 2017) [ 2 ] [ 3 ] was an American linguist and businessman, one of the founders of bibliometrics and scientometrics . [ 4 ] He helped to create Current Contents , Science Citation Index (SCI), Journal Citation Reports , and Index Chemicus , among others, and founded the magazine The Scientist . [ 5 ] [ 6 ] [ 7 ] [ 8 ] Garfield was born in 1925 in New York City as Eugene Eli Garfinkle, [ 2 ] his mother being of Lithuanian Jewish ancestry. [ 9 ] [ 10 ] His parents were second generation immigrants living in East Bronx in New York City. [ 11 ] He studied at the University of Colorado and University of California, Berkeley before getting a Bachelor of Science degree in chemistry from Columbia University in 1949. [ 12 ] [ 13 ] Garfield also received a degree in Library Science from Columbia University in 1953. [ 11 ] [ 14 ] He went on to do his PhD in the Department of Linguistics at the University of Pennsylvania , which he completed in 1961 for developing an algorithm for translating chemical nomenclature into chemical formulas . [ 1 ] [ 15 ] Working as a laboratory assistant at Columbia University after his graduation, Garfield indexed all previously synthesized compounds so that not to remake them, which helped him understand that his inclination to information towards science was bigger than towards chemistry. In 1951, he got a position at the Welch Medical Library at Johns Hopkins University in Baltimore, Maryland, where most of the National Library of Medicine information systems were developed. There he built search and cataloging system methods using punch-cards . In 1953, at the First Symposium on Machine Methods in Scientific Documentation, Garfield got introduced to Shepard's Citations . [ 11 ] In 1960, Garfield founded the Institute for Scientific Information (ISI) , which was located in Philadelphia, Pennsylvania. [ 16 ] In the 1990s, ISI was faced with bankruptcy and was acquired by JPT Holdings who later sold it to Thomson (Thomas Business Information) where it formed a major part of the science division of Thomson Reuters . In October 2016 Thomson Reuters completed the sale of its intellectual property and science division; it is now known as Clarivate Analytics . [ 17 ] Garfield was responsible for many innovative bibliographic products, including Current Contents , the Science Citation Index (SCI), and other citation databases, the Journal Citation Reports , and Index Chemicus . He was the founding editor and publisher of The Scientist , a news magazine for life scientists. [ 11 ] In 2003, the University of South Florida School of Information was honored to have him as lecturer for the Alice G. Smith Lecture . In 2007, he launched Histcite , a bibliometric analysis and visualization software package. Following ideas inspired by Vannevar Bush 's highly cited 1945 article As We May Think , Garfield undertook the development of a comprehensive citation index showing the propagation of scientific thinking; he started the Institute for Scientific Information in 1956 (it was sold to the Thomson Corporation in 1992 [ 18 ] ). According to Garfield, \"the citation index ... may help a historian to measure the influence of an article — that is, its 'impact factor'\". [ 19 ] The creation of the Science Citation Index made it possible to calculate impact factor , [ 20 ] which ostensibly measures the importance of scientific journals. It led to the unexpected discovery that a few journals like Nature and Science were core for all of hard science . The same pattern does not happen with the humanities or the social sciences. [ 21 ] [ 22 ] His entrepreneurial flair in having turned what was, at least at the time, an obscure and specialist metric into a highly profitable business has been noted. [ 23 ] A scientometric analysis of his top fifty cited papers has been conducted. [ 24 ] Garfield's work led to the development of several information retrieval algorithms, like the HITS algorithm and PageRank . Both use the structured citation between websites through hyperlinks. Google co-founders Larry Page and Sergey Brin acknowledged Garfield in their development of PageRank, the algorithm that powers their company's search engine. [ 23 ] Garfield published over 1,000 essays. Garfield was honored with the Award of Merit from the Association for Information Science and Technology in 1975. He was awarded the John Price Wetherill Medal in 1984, [ 16 ] the Derek de Solla Price Memorial Medal in 1984, [ 25 ] and the Miles Conrad Award in 1985. [ 26 ] He was also awarded the Richard J. Bolte Sr. Award in 2007. [ 27 ] He was elected to the American Philosophical Society that same year. [ 28 ] The Association for Library and Information Science Education has a fund for doctoral research through an award named after Garfield. Writing in Physiology News , No. 69, Winter 2007, David Colquhoun of the Department of Pharmacology, University College London , described the \" impact factor ,\" a method for comparing scholarly journals, as \"the invention of Eugene Garfield, a man who has done enormous harm to true science.\" Colquhoun ridiculed C. Hoeffel's assertion that Garfield's impact factor \"has the advantage of already being in existence and is, therefore, a good technique for scientific evaluation\" by saying, \"you can't get much dumber than that. It is a 'good technique' because it is already in existence? There is something better. Read the papers.\" Garfield is survived by a wife, three sons, a daughter, two granddaughters, and two great-grandchildren. [ 2 ] [ 16 ] [ 29 ]",
    "links": [
      "Thomson Reuters",
      "Histcite",
      "HITS algorithm",
      "Chemical nomenclature",
      "Doi (identifier)",
      "Thesis",
      "The Scientist (magazine)",
      "Chemical formula",
      "Punched card",
      "Linguistics",
      "University College London",
      "Journal Citation Reports",
      "Eugene K. Garfield",
      "Clarivate Analytics",
      "Association for Library and Information Science Education",
      "OCLC (identifier)",
      "Philadelphia",
      "Bibcode (identifier)",
      "Vannevar Bush",
      "Chemical Heritage Foundation",
      "Science History Institute",
      "Richard J. Bolte Sr. Award",
      "Nature (journal)",
      "S2CID (identifier)",
      "University of South Florida",
      "Times Higher Education",
      "University of California, Berkeley",
      "As We May Think",
      "University of Colorado",
      "Current Contents",
      "ISSN (identifier)",
      "Journal of Chemical Documentation",
      "American Association for the Advancement of Science",
      "John Price Wetherill Medal",
      "United States National Library of Medicine",
      "Information retrieval",
      "Hdl (identifier)",
      "Columbia University",
      "Science Citation Index",
      "ISBN (identifier)",
      "Chemistry",
      "Sergey Brin",
      "Online (magazine)",
      "Bibliometrics",
      "Scientometrics",
      "American Philosophical Society",
      "University of Pennsylvania",
      "David Colquhoun",
      "PMID (identifier)",
      "PageRank",
      "Award of Merit - Association for Information Science and Technology",
      "Alice G. Smith Lecture",
      "Institute for Scientific Information",
      "Citation index",
      "Hard science",
      "Thomson Corporation",
      "Science (journal)",
      "Impact factor",
      "Association for Information Science and Technology",
      "Derek de Solla Price Memorial Medal",
      "Algorithm",
      "Shepard's Citations",
      "East Bronx",
      "Larry Page",
      "ProQuest",
      "Lithuanian Jews",
      "Johns Hopkins University"
    ]
  },
  "Relevance (information retrieval)": {
    "url": "https://en.wikipedia.org/wiki/Relevance_(information_retrieval)",
    "title": "Relevance (information retrieval)",
    "content": "In information science and information retrieval , relevance denotes how well a retrieved document or set of documents meets the information need of the user. Relevance may include concerns such as timeliness, authority or novelty of the result. The concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century. [ citation needed ] The formal study of relevance began in the 20th century with the study of what would later be called bibliometrics . In the 1930s and 1940s, S. C. Bradford used the term \"relevant\" to characterize articles relevant to a subject (cf., Bradford's law ). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information. [ 1 ] Since 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between \"relevance to a subject\" or \"topical relevance\" and \"user relevance\". [ 1 ] The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the Cranfield Experiments of the early 1960s and culminating in the TREC evaluations that continue to this day as the main evaluation framework for information retrieval research. [ 2 ] In order to evaluate how well an information retrieval system retrieved topically relevant results, the relevance of retrieved results must be quantified. In Cranfield -style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment . Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need). Once relevance levels have been assigned to the retrieved results, information retrieval performance measures can be used to assess the quality of a retrieval system's output. In contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance. [ 3 ] These studies often focus on aspects of human-computer interaction (see also human-computer information retrieval ). The cluster hypothesis , proposed by C. J. van Rijsbergen in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally. [ 4 ] The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include: A second interpretation, most notably advanced by Ellen Voorhees , [ 8 ] focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include: Local methods require an accurate and appropriate document similarity measure . The documents which are most relevant are not necessarily those which are most useful to display in the first page of search results. For example, two duplicate documents might be individually considered quite relevant, but it is only useful to display one of them. A measure called \"maximal marginal relevance\" (MMR) has been proposed to manage this shortcoming. It considers the relevance of each document only in terms of how much new information it brings given the previous results. [ 13 ] In some cases, a query may have an ambiguous interpretation, or a variety of potential responses. Providing a diversity of results can be a consideration when evaluating the utility of a result set . [ 14 ]",
    "links": [
      "Cranfield Experiments",
      "Result set",
      "C. J. van Rijsbergen",
      "Doi (identifier)",
      "Bradford's law",
      "Information need",
      "Wayback Machine",
      "Text Retrieval Conference",
      "S2CID (identifier)",
      "Samuel C. Bradford",
      "CiteSeerX (identifier)",
      "Latent semantic analysis",
      "Information overload",
      "Human-computer information retrieval",
      "Information retrieval",
      "Ellen Voorhees",
      "ISBN (identifier)",
      "Cluster hypothesis",
      "Bibliometrics",
      "Language model",
      "Relevance",
      "Human-computer interaction",
      "Similarity measure",
      "Information science"
    ]
  },
  "Mind maps": {
    "url": "https://en.wikipedia.org/wiki/Mind_maps",
    "title": "Mind maps",
    "content": "A mind map is a diagram used to visually organize information into a hierarchy , showing relationships among pieces of the whole. [ 1 ] It is often based on a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those major ideas. Mind maps can also be drawn by hand, either as \"notes\" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available. Mind maps are considered to be a type of spider diagram . [ 2 ] Although the term \"mind map\" was first popularized by British popular psychology author and television personality Tony Buzan , [ 3 ] [ 4 ] the use of diagrams that visually \"map\" information using branching and radial maps traces back centuries. [ 5 ] These pictorial methods record knowledge and model systems, and have a long history in learning, brainstorming , memory , visual thinking , and problem solving by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by Porphyry of Tyros , a noted thinker of the 3rd century, as he graphically visualized the concept categories of Aristotle . [ 5 ] Philosopher Ramon Llull (1235–1315) also used such techniques. [ 5 ] Buzan's specific approach, and the introduction of the term \"mind map\", started with a 1974 BBC TV series he hosted, called Use Your Head . [ 6 ] In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure. [ 7 ] Cunningham (2005) conducted a user study in which 80% of the students thought \"mindmapping helped them understand concepts and ideas in science\". [ 10 ] Other studies also report some subjective positive effects of the use of mind maps. [ 11 ] [ 12 ] Positive opinions on their effectiveness, however, were much more prominent among students of art and design than in students of computer and information technology, with 62.5% vs 34% (respectively) agreeing that they were able to understand concepts better with mind mapping software. [ 11 ] Farrand, Hussain, and Hennessy (2002) found that spider diagrams (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline). [ 13 ] This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of note taking . A meta study about concept mapping concluded that concept mapping is more effective than \"reading text passages, attending lectures, and participating in class discussions\". [ 14 ] The same study also concluded that concept mapping is slightly more effective \"than other constructive activities such as writing summaries and outlines\". However, results were inconsistent, with the authors noting \"significant heterogeneity was found in most subsets\". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students. Joeran Beel and Stefan Langer conducted a comprehensive analysis of the content of mind maps. [ 15 ] They analysed 19,379 mind maps from 11,179 users of the mind mapping applications SciPlore MindMapping (now Docear ) and MindMeister . Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about three words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7,500 words. The study also showed that between different mind mapping applications (Docear vs MindMeister) significant differences exist related to how users create mind maps. There have been some attempts to create mind maps automatically. Brucks & Schommer created mind maps automatically from full-text streams. [ 16 ] Rothenberger et al. extracted the main story of a text and presented it as mind map. [ 17 ] There is also a patent application about automatically creating sub-topics in mind maps. [ 18 ] Mind-mapping software can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. [ 19 ] Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites, images and videos. [ 20 ] It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional note-taking . [ 13 ] The following dozen examples of mind maps show the range of styles that a mind map may take, from hand-drawn to computer-generated and from mostly text to highly illustrated. Despite their stylistic differences, all of the examples share a tree structure that hierarchically connects sub-topics to a main topic.",
    "links": [
      "Issue-based information system",
      "Semantic Research",
      "Doi (identifier)",
      "Morphological analysis (problem-solving)",
      "Idea",
      "MindMeister",
      "Olog",
      "S2CID (identifier)",
      "Illustration",
      "JSTOR (identifier)",
      "Radial tree",
      "Visual analytics",
      "Entity–relationship model",
      "MindManager",
      "Java (software platform)",
      "Vym (software)",
      "Organizational chart",
      "Tree structure",
      "Cognitive map",
      "Hdl (identifier)",
      "Disney method",
      "Ansoff matrix",
      "Graphic communication",
      "Personal wiki",
      "Mental literacy",
      "ZigZag (software)",
      "Memory",
      "Diagrams.net",
      "Concept lattice",
      "Semantic Web",
      "Scenario planning",
      "Hyperbolic tree",
      "Free and open-source software",
      "Nomological network",
      "Timeline",
      "Concept maps",
      "OGSM",
      "Dendrogram",
      "List of concept- and mind-mapping software",
      "Managerial grid model",
      "Cubital fossa",
      "Graph drawing",
      "Knowledge representation and reasoning",
      "6-3-5 Brainwriting",
      "MECE principle",
      "CmapTools",
      "Hypertext",
      "Sociogram",
      "Diagrammatic reasoning",
      "Treemapping",
      "Porter's five forces analysis",
      "Kraljic matrix",
      "Web app",
      "Mental model",
      "FreeMind",
      "CiteSeerX (identifier)",
      "Spider mapping",
      "Brainstorming",
      "Manuel Lima",
      "Hierarchy",
      "Segmenting-targeting-positioning",
      "PGF/TikZ",
      "Ontology",
      "Argüman",
      "PlantUML",
      "Note-taking",
      "Diagram",
      "Pareto priority index",
      "PMID (identifier)",
      "Nodal organizational structure",
      "MindMup",
      "Design rationale",
      "Object–role modeling",
      "Houghton Mifflin",
      "Bowman's Strategy Clock",
      "Wicked problem",
      "ConceptDraw DIAGRAM",
      "LibreOffice Draw",
      "Issue tree",
      "Competitor analysis",
      "SWOT analysis",
      "Argument map",
      "Conceptual graph",
      "Dia (software)",
      "ArXiv (identifier)",
      "Compendium (software)",
      "Tony Buzan",
      "Affinity diagram",
      "OmniGraffle",
      "Visual Mind",
      "Data and information visualization",
      "ConceptDraw MINDMAP",
      "VRIO",
      "Visual language",
      "OCLC (identifier)",
      "Computer supported brainstorming",
      "Problem solving",
      "Spider diagram",
      "College Teaching",
      "Market Opportunity Navigator",
      "SpicyNodes",
      "Growth–share matrix",
      "Social map",
      "XMind",
      "Kialo",
      "YEd",
      "Strategy map",
      "Balanced scorecard",
      "Popular psychology",
      "3D Topicscape",
      "Business Model Canvas",
      "Toolkit for Conceptual Modeling",
      "Cluster analysis",
      "Topic map",
      "The Times",
      "Personal knowledge base",
      "Creativity techniques",
      "Desktop app",
      "Infographic",
      "Princeton Architectural Press",
      "Exquisite corpse",
      "Business decision mapping",
      "TheBrain",
      "Pathfinder network",
      "Freeplane",
      "Concept map",
      "Porphyry of Tyros",
      "Coggle",
      "Mnemonic",
      "Information design",
      "Cladistics",
      "Problem structuring methods",
      "Lucidchart",
      "Semantic network",
      "Knowledge visualization",
      "Visual thinking",
      "Categories (Aristotle)",
      "Strategic planning",
      "Qiqqa",
      "Visual Understanding Environment",
      "Araucaria (software)",
      "Rhizome (philosophy)",
      "Prezi",
      "Graphical modeling language",
      "PEST analysis",
      "Tinderbox (application software)",
      "Mindomo",
      "Overleaf",
      "Microsoft Visio",
      "Debategraph",
      "Mind42",
      "ISBN (identifier)",
      "Nominal group technique",
      "SmartDraw",
      "Geovisualization",
      "Mental mapping",
      "Graph (discrete mathematics)",
      "Decision tree",
      "Layered graph drawing",
      "Schema (psychology)",
      "Google Drawings",
      "Strategic grid model",
      "MindMapper",
      "Ontology (information science)",
      "BBC",
      "Ramon Llull",
      "UML diagram",
      "Structure follows Strategy",
      "Proprietary software"
    ]
  },
  "Herman Hollerith": {
    "url": "https://en.wikipedia.org/wiki/Herman_Hollerith",
    "title": "Herman Hollerith",
    "content": "Herman Hollerith (February 29, 1860 – November 17, 1929) was a German-American statistician, inventor, and businessman who developed an electromechanical tabulating machine for punched cards to assist in summarizing information and, later, in accounting. His invention of the punched card tabulating machine, patented in 1884, marks the beginning of the era of mechanized binary code and semiautomatic data processing systems, and his concept dominated that landscape for nearly a century. [ 1 ] [ 2 ] [ 3 ] Hollerith founded a company that was amalgamated in 1911 with several other companies to form the Computing-Tabulating-Recording Company . In 1924, the company was renamed \"International Business Machines\" ( IBM ) and became one of the largest and most successful companies of the 20th century. Hollerith is regarded as one of the seminal figures in the development of data processing. [ 4 ] Herman Hollerith was born in Buffalo, New York , in 1860, where he also spent his early childhood. [ 5 ] His parents were German immigrants; his father, Georg Hollerith, was a school teacher from Großfischlingen , Rhineland-Palatinate . [ 6 ] [ 7 ] He entered the City College of New York in 1875, graduated from the Columbia School of Mines with an Engineer of Mines degree in 1879 at age 19, and, in 1890, earned a Doctor of Philosophy based on his development of the tabulating system. [ 1 ] [ 8 ] In 1882, Hollerith joined the Massachusetts Institute of Technology where he taught mechanical engineering and conducted his first experiments with punched cards. [ 9 ] He eventually moved to Washington, D.C., living in Georgetown with a home on 29th Street and a business building at 31st Street and the Chesapeake and Ohio Canal , where today there is a commemorative plaque installed by IBM . He died of a heart attack in Washington, D.C., at age 69. [ 9 ] At the suggestion of John Shaw Billings , Hollerith developed a mechanism using electrical connections to increment a counter, recording information. [ 10 ] A key idea was that a datum could be recorded by the presence or absence of a hole at a specific location on a card. For example, if a specific hole location indicates marital status , then a hole there can indicate married while not having a hole indicates single . Hollerith determined that data in specified locations on a card, arranged in rows and columns, could be counted or sorted electromechanically. A description of this system, An Electric Tabulating System (1889) , was submitted by Hollerith to Columbia University as his doctoral thesis, [ 11 ] and is reprinted in Brian Randell 's 1982 The Origins of Digital Computers, Selected Papers . [ 12 ] On January 8, 1889, Hollerith was issued U.S. Patent 395,782, [ 13 ] claim 2 of which reads: The herein-described method of compiling statistics, which consists in recording separate statistical items pertaining to the individual by holes or combinations of holes punched in sheets of electrically non-conducting material, and bearing a specific relation to each other and to a standard, and then counting or tallying such statistical items separately or in combination by means of mechanical counters operated by electro-magnets the circuits through which are controlled by the perforated sheets, substantially as and for the purpose set forth. Hollerith had left teaching and began working for the United States Census Bureau in the year he filed his first patent application. Titled \"Art of Compiling Statistics\", it was filed on September 23, 1884; U.S. Patent 395,782 was granted on January 8, 1889. [ 13 ] Hollerith initially did business under his own name, as The Hollerith Electric Tabulating System , specializing in punched card data processing equipment . [ 16 ] He provided tabulators and other machines under contract for the Census Office, which used them for the 1890 census . The net effect of the many changes from the 1880 census: the larger population, the data items to be collected, the Census Bureau headcount, the scheduled publications, and the use of Hollerith's electromechanical tabulators, reduced the time required to process the census from eight years for the 1880 census to six years for the 1890 census. [ 17 ] In 1896, Hollerith founded the Tabulating Machine Company (in 1905 renamed The Tabulating Machine Company). [ 18 ] Many major census bureaus around the world leased his equipment and purchased his cards, as did major insurance companies. Hollerith's machines were used for censuses in England & Wales , Italy , Germany , Russia , Austria , Canada , France , [ 19 ] Norway , Puerto Rico , Cuba , and the Philippines , and again in the 1900 U.S. census . [ 1 ] He invented the first automatic card-feed mechanism and the first keypunch . The 1890 Tabulator was hardwired to operate on 1890 Census cards. A control panel in his 1906 Type I Tabulator simplified rewiring for different jobs. The 1920s removable control panel supported prewiring and near instant job changing. These inventions were among the foundations of the data processing industry, and Hollerith's punched cards (later used for computer input/output ) continued in use for almost a century. [ 20 ] In 1911, four corporations, including Hollerith's firm, were amalgamated to form a fifth company, the Computing-Tabulating-Recording Company (CTR). [ 21 ] Under the presidency of Thomas J. Watson , CTR was renamed International Business Machines Corporation (IBM) in 1924. By 1933 The Tabulating Machine Company name had disappeared as subsidiary companies were subsumed by IBM. [ 22 ] Herman Hollerith died November 17, 1929. Hollerith is buried at Oak Hill Cemetery in the Georgetown neighborhood of Washington, D.C. [ 15 ] [ 23 ] Hollerith cards were named after Herman Hollerith, as were Hollerith strings and Hollerith constants . [ 24 ] His great-grandson, the Rt. Rev. Herman Hollerith IV , was the Episcopal bishop of the Diocese of Southern Virginia , and another great-grandson, Randolph Marshall Hollerith , is an Episcopal priest and the dean of Washington National Cathedral in Washington, D.C. [ 25 ] [ 26 ]",
    "links": [
      "The Control Revolution",
      "Doctor of Philosophy",
      "World's Columbian Exposition",
      "Bachelor of Engineering",
      "Doi (identifier)",
      "Thomas J. Watson",
      "Martin Campbell-Kelly",
      "Italy",
      "1880 United States census",
      "JSTOR (identifier)",
      "Tabulating machine",
      "MacTutor History of Mathematics Archive",
      "Fred Brooks",
      "Germans",
      "Columbia School of Mines",
      "Kenneth E. Iverson",
      "Heinz Nixdorf MuseumsForum",
      "University of St Andrews",
      "Hollerith card",
      "History of IBM",
      "Washington National Cathedral",
      "Edmund F. Robertson",
      "Puerto Rico",
      "Punched card",
      "Finding aid",
      "1890 United States census",
      "Columbia University",
      "Russia",
      "Herman Hollerith IV",
      "Leon E. Truesdell",
      "1900 United States census",
      "Data processing",
      "Austria",
      "American National Standards Institute",
      "Addison-Wesley Publishing Company, Inc.",
      "LCCN (identifier)",
      "Rhineland-Palatinate",
      "Cuba",
      "Plugboard",
      "Computing-Tabulating-Recording Company",
      "Brian Randell",
      "Washington, D.C.",
      "Massachusetts Institute of Technology",
      "Buffalo, New York",
      "The Virginian-Pilot",
      "Library of Congress",
      "United States Census Bureau",
      "Oak Hill Cemetery (Washington, D.C.)",
      "Canada",
      "Philippines",
      "James R. Beniger",
      "Hagley Museum and Library",
      "Electrical wiring",
      "Unit record equipment",
      "Hollerith constant",
      "Großfischlingen",
      "Georgetown (Washington, D.C.)",
      "Wayback Machine",
      "Mining engineering",
      "Germany",
      "Randolph Marshall Hollerith",
      "Harry M. Lydenberg",
      "City College of New York",
      "The Washington Post",
      "Episcopal Church (USA)",
      "England and Wales",
      "ISBN (identifier)",
      "Keypunch",
      "Elliott Cresson Medal",
      "John Shaw Billings",
      "Punched card input/output",
      "Norway",
      "Episcopal Diocese of Southern Virginia",
      "National Inventors Hall of Fame",
      "IBM",
      "List of pioneers in computer science",
      "Chesapeake and Ohio Canal",
      "France"
    ]
  },
  "OCLC (identifier)": {
    "url": "https://en.wikipedia.org/wiki/OCLC_(identifier)",
    "title": "OCLC (identifier)",
    "content": "OCLC, Inc. [ 4 ] is an American nonprofit cooperative organization \"that provides shared technology services, original research, and community programs for its membership and the library community at large\". [ 3 ] It was founded in 1967 as the Ohio College Library Center , then became the Online Computer Library Center as it expanded. In 2017, the name was formally changed to OCLC, Inc. [ 4 ] OCLC and thousands of its member libraries cooperatively produce and maintain WorldCat , the largest online public access catalog in the world. [ 5 ] OCLC is funded mainly by the fees that libraries pay (around $217.8 million annually in total as of 2021 [update] ) for the many different services it offers. [ 2 ] OCLC also maintains the Dewey Decimal Classification system. OCLC began in 1967, as the Ohio College Library Center, through a collaboration of university presidents, vice presidents, and library directors who wanted to create a cooperative, computerized network for libraries in the state of Ohio . The group first met on July 5, 1967, on the campus of Ohio State University to sign the articles of incorporation for the nonprofit organization [ 6 ] and hired Frederick G. Kilgour , a former Yale University medical school librarian, as first executive director. [ 7 ] [ 8 ] Kilgour and Ralph H. Parker, who was the head of libraries at the University of Missouri , had proposed the shared cataloging system in a 1965 report as consultants to the Committee of Librarians of the Ohio College Association. [ 8 ] Kilgour and Parker wished to merge the latest information storage and retrieval system of the time, the computer, with the oldest, the library. [ 8 ] They were inspired in part by the earlier Columbia–Harvard–Yale Medical Libraries Computerization Project, an attempt at shared automated printing of catalog cards. [ 8 ] The plan was to merge the catalogs of Ohio libraries electronically through a computer network and database to streamline operations, control costs, and increase efficiency in library management, bringing libraries together cooperatively to best serve researchers and scholars. The first library to do online cataloging through OCLC was the Alden Library at Ohio University on August 26, 1971. This was the first online cataloging by any library worldwide. [ 6 ] Between 1967 and 1977, OCLC membership was limited to institutions in Ohio, but in 1978, a new governance structure was established that allowed institutions from other states to join. With this expansion, the name changed to the Online Computer Library Center in 1977. [ 9 ] In 2002, the governance structure was again modified to accommodate participation from outside the United States. [ 10 ] As OCLC expanded services in the United States outside Ohio, it relied on establishing strategic partnerships with \"networks\", organizations that provided training, support and marketing services. By 2008, there were 15 independent United States regional service providers. OCLC networks played a key role in OCLC governance, with networks electing delegates to serve on the OCLC Members Council. During 2008, OCLC commissioned two studies to look at distribution channels; at the same time, the council approved governance changes that had been recommended by the Board of Trustees severing the tie between the networks and governance. In early 2009, OCLC negotiated new contracts with the former networks and opened a centralized support center. [ 11 ] In July 2010, the company was sued by SkyRiver, a rival startup, in an antitrust suit . [ 12 ] Library automation company Innovative Interfaces joined SkyRiver in the suit. [ 13 ] The suit was dropped in March 2013, however, following the acquisition of SkyRiver by Innovative Interfaces. [ 14 ] Innovative Interfaces was bought by ExLibris in 2020, therefore passing OCLC as the dominant supplier of ILS services in the U.S. (over 70% market share for academic libraries and over 50% for public libraries for ExLibris, versus OCLC's 10% market share of both types of libraries in 2019). [ 15 ] In 2022, membership and governance expanded to include any institution with a subscription to one of many qualifying OCLC products (previously institutions qualified for membership by \"contributing intellectual content or participating in global resource or reference sharing\"), with the exception of for-profit organizations that are part of OCLC's partner program. [ 16 ] This change reflected OCLC's expanding number of services due to its corporate acquisitions . [ 16 ] The following people served successively as president of OCLC: [ 17 ] OCLC provides bibliographic , abstract and full-text information to anyone. OCLC and its member libraries cooperatively produce and maintain WorldCat —the OCLC Online Union Catalog, the largest online public access catalog (OPAC) in the world. [ 5 ] WorldCat has holding records from public and private libraries worldwide. The Online Computer Library Center acquired the trademark and copyrights associated with the Dewey Decimal Classification System when it bought Forest Press in 1988. A browser [ 18 ] for books with their Dewey Decimal Classifications was available until July 2013; it was replaced by the Classify Service. Until August 2009, when it was sold to Backstage Library Works, OCLC owned a preservation microfilm and digitization operation called the OCLC Preservation Service Center, [ 19 ] with its principal office in Bethlehem, Pennsylvania . Starting in 1971, OCLC produced catalog cards for members alongside its shared online catalog; the company printed its last catalog cards on October 1, 2015. [ 20 ] QuestionPoint , [ 21 ] an around-the-clock reference service provided to users by a cooperative of participating global libraries, was acquired by Springshare from OCLC in 2019 and migrated to Springshare's LibAnswers platform. [ 22 ] [ 23 ] OCLC commercially sells software, such as: OCLC has been conducting research for the library community for more than 30 years. In accordance with its mission, OCLC makes its research outcomes known through various publications. [ 34 ] These publications, including journal articles, reports, newsletters, and presentations, are available through the organization's website. During the COVID-19 pandemic , OCLC participated in the REopening Archives, Libraries, and Museums (REALM) project funded by the IMLS to study the surface transmission risks of SARS-CoV-2 on common library and museum materials and surfaces, [ 39 ] and published a series of reports. [ 40 ] Advocacy has been a part of OCLC's mission since its founding in 1967. OCLC staff members meet and work regularly with library leaders, information professionals, researchers, entrepreneurs, political leaders, trustees, students and patrons to advocate \"advancing research, scholarship, education, community development, information access, and global cooperation\". [ 41 ] [ 42 ] WebJunction, which provides training services to librarians, [ 43 ] is a division of OCLC funded by grants from the Bill & Melinda Gates Foundation beginning in 2003. [ 44 ] [ 45 ] OCLC partnered with search engine providers in 2003 to advocate for libraries and share information across the Internet landscape. Google, Yahoo! , and Ask.com all collaborated with OCLC to make WorldCat records searchable through those search engines. [ 41 ] OCLC's advocacy campaign \"Geek the Library\", started in 2009, highlights the role of public libraries. The campaign, funded by a grant from the Bill & Melinda Gates Foundation, uses a strategy based on the findings of the 2008 OCLC report, \"From Awareness to Funding: A study of library support in America\". [ 46 ] Other past advocacy campaigns have focused on sharing the knowledge gained from library and information research. Such projects have included communities such as the Society of American Archivists , the Open Archives Initiative , the Institute for Museum and Library Services , the International Organization for Standardization , the National Information Standards Organization , the World Wide Web Consortium , the Internet Engineering Task Force , and Internet2 . One of the most successful contributions to this effort was the Dublin Core Metadata Initiative, \"an open forum of libraries, archives, museums, technology organizations, and software companies who work together to develop interoperable online metadata standards that support a broad range of purposes and business models.\" [ 41 ] OCLC has collaborated with the Wikimedia Foundation and the Wikimedia volunteer community, through integrating library metadata with Wikimedia projects, hosting a Wikipedian in residence , and doing a national training program through WebJunction called \"Wikipedia + Libraries: Better Together\". [ 47 ] [ 48 ] [ 49 ] OCLC's WorldCat database is used by the general public and by librarians for cataloging and research. WorldCat is available to the public for searching via a subscription web-based service called FirstSearch, to which many libraries subscribe, [ 50 ] as well as through the publicly available WorldCat.org. [ 51 ] OCLC assigns a unique accession number referred to as an \"OCN\", an \"OCLC Control Number\" or an \"OCLC number\" to each new bibliographic record in WorldCat. This is somewhat analogous to how the Library of Congress assigns an \"LCCN\" or a \" Library of Congress Control Number \" to its bibliographic records (but LCCNs can be prefixed and are thus extended to other uses too like authority control , etc.). Numbers are assigned serially, and in mid-2013 over a billion OCNs had been created. In September 2013, OCLC declared these numbers to be in the public domain , removing a perceived barrier to widespread use of OCNs outside OCLC itself. [ 52 ] The control numbers link WorldCat's records to local library system records by providing a common reference key for a record in libraries. [ 53 ] OCNs are particularly useful as identifiers for books and other bibliographic materials that do not have ISBNs (e.g., books published before 1970). OCNs are often used as identifiers for Wikipedia and Wikidata . In October 2013, it was reported that out of 29,673 instances of book infoboxes in Wikipedia, \"there were 23,304 ISBNs and 15,226 OCNs\", and regarding Wikidata: \"of around 14 million Wikidata items, 28,741 were books. 5,403 Wikidata items have an ISBN associated with them and 12,262 have OCNs.\" [ 54 ] OCLC also runs the Virtual International Authority File (VIAF), an international name authority file, with oversight from the VIAF Council composed of representatives of institutions that contribute data to VIAF. [ 55 ] VIAF numbers are broadly used as standard identifiers, including in Wikipedia. [ 47 ] [ 56 ] In 2024, OCLC launched a new linked data management tool called OCLC Meridian. [ 57 ] This was released with a suite of APIs for WorldCat Entities to allow greater control, connection and integration of linked data for user institutions. This suite of APIs \"enables the creation of linked data entities and descriptive relationships, forming connections to the existing value in MARC records and other datasets across the global information ecosystem\". [ 33 ] The use of these APIs and WorldCat Entities is designed to \"improve discoverability and relevance for users\", \"integrate data management into your existing workflows\" and \"discover, emphasize and analyze important relationships\". [ 33 ] A set of WorldCat Entities APIs \"enables users to connect identifiers from disparate sources (such as ORCID , ISNI , VIAF, etc.), learn of changes to WorldCat Entities data\" and related information for local use. [ 33 ] OCLC acquired NetLibrary , a provider of electronic books and textbooks, in 2002 and sold it in 2010 to EBSCO Industries . [ 58 ] OCLC owns 100% of the shares of OCLC PICA , a library automation systems and services company which has its headquarters in Leiden in the Netherlands and which was renamed \"OCLC\" at the end of 2007. [ 59 ] In July 2006, the Research Libraries Group (RLG) merged with OCLC. [ 60 ] [ 61 ] On January 11, 2008, OCLC announced [ 62 ] that it had purchased EZproxy . It has also acquired OAIster . The process started in January 2009 and from October 31, 2009, OAIster records are freely available via WorldCat.org. In 2013, OCLC acquired the Dutch library automation company HKA [ 63 ] [ 64 ] and its integrated library system Wise, [ 28 ] which OCLC calls a \"community engagement system\" that \"combines the power of customer relationship management, marketing, and analytics with ILS functions\". [ 27 ] OCLC began offering Wise to libraries in the United States in 2019. [ 28 ] In January 2015, OCLC acquired Sustainable Collection Services (SCS). SCS offered consulting services based on analyzing library print collection data to help libraries manage and share materials. [ 65 ] In 2017, OCLC acquired Relais International, a library interlibrary loan service provider based in Ottawa, Canada. [ 66 ] A more complete list of mergers and acquisitions is available on the OCLC website. [ 67 ] In May 2008, OCLC was criticized by Jeffrey Beall for monopolistic practices, among other faults. [ 68 ] Library blogger Rick Mason responded that although he thought Beall had some \"valid criticisms\" of OCLC, he demurred from some of Beall's statements and warned readers to \"beware the hyperbole and the personal nature of his criticism, for they strongly overshadow that which is worth stating\". [ 69 ] In November 2008, the Board of Directors of OCLC unilaterally issued a new Policy for Use and Transfer of WorldCat Records [ 70 ] that would have required member libraries to include an OCLC policy note on their bibliographic records ; the policy caused an uproar among librarian bloggers. [ 71 ] [ 72 ] Among those who protested the policy was the non-librarian activist Aaron Swartz , who believed the policy would threaten projects such as the Open Library , Zotero , and Wikipedia, and who started a petition to \"Stop the OCLC powergrab\". [ 73 ] [ 74 ] Swartz's petition garnered 858 signatures, but the details of his proposed actions went largely unheeded. [ 72 ] Within a few months, the library community had forced OCLC to retract its policy and to create a Review Board to consult with member libraries more transparently. [ 72 ] In August 2012, OCLC recommended that member libraries adopt the Open Data Commons Attribution (ODC-BY) license when sharing library catalog data, although some member libraries have explicit agreements with OCLC that they can publish catalog data using the CC0 Public Domain Dedication. [ 75 ] [ 76 ]",
    "links": [
      "Digital collection",
      "Leiden",
      "Infobox",
      "Information Today",
      "Doi (identifier)",
      "Zotero",
      "ORCID",
      "Open Data Commons",
      "Internet2",
      "Creative Commons",
      "S2CID (identifier)",
      "Electronic resource management",
      "Dewey Decimal Classification",
      "Metadata standards",
      "Ohio",
      "Cooperative",
      "Bibliographic record",
      "ExLibris",
      "Wikimedia Foundation",
      "Digitization",
      "Libraries Unlimited",
      "EZproxy",
      "World Wide Web Consortium",
      "Online public access catalog",
      "Abstract (summary)",
      "Routledge",
      "American Libraries",
      "Institute of Museum and Library Services",
      "Open Library",
      "Discovery system (bibliographic search)",
      "Yahoo!",
      "Competition law",
      "Monopolistic",
      "NetLibrary",
      "Wikipedian in residence",
      "Aaron Swartz",
      "Cloud computing",
      "Ask.com",
      "Skip Prichard",
      "CloudLibrary",
      "Research Libraries Group",
      "Virtual International Authority File",
      "COVID-19 pandemic",
      "Dublin Core",
      "Library of Congress Control Number",
      "National Education Association",
      "Bibliographic",
      "WorldCat",
      "Streaming media",
      "Wikidata",
      "Identifier",
      "ISBN",
      "Dublin, Ohio",
      "Ohio University",
      "Linked data",
      "Code4Lib Journal",
      "Library Journal",
      "Fred Kilgour",
      "Electronic books",
      "VDX (library software)",
      "ISNI",
      "Dynix (software)",
      "Haworth Press",
      "Public domain",
      "Bill & Melinda Gates Foundation",
      "Open Archives Initiative",
      "MARC standards",
      "Journal of the American Society for Information Science",
      "Cataloging & Classification Quarterly",
      "Suffolk University Law School",
      "501(c)3 organization",
      "University of Missouri",
      "College & Research Libraries News",
      "Yale University",
      "Library of Congress",
      "Library automation system",
      "Taxpayer Identification Number",
      "OAIster",
      "Public library advocacy",
      "Authority control",
      "Integrated library system",
      "McFarland & Company",
      "Alden Library",
      "EBSCO Industries",
      "Internet Engineering Task Force",
      "Jeffrey Beall",
      "Bethlehem, Pennsylvania",
      "Society of American Archivists",
      "ISSN (identifier)",
      "Accession number (cultural property)",
      "National Information Standards Organization",
      "ISBN (identifier)",
      "API",
      "International Organization for Standardization",
      "Institute for Museum and Library Services",
      "Reference and User Services Quarterly",
      "Ohio Secretary of State",
      "Frederick G. Kilgour",
      "Catalog card",
      "CC0",
      "SARS-CoV-2",
      "Ohio State University",
      "Jay Jordan (businessman)",
      "Corporate acquisition",
      "Innovative Interfaces",
      "OCLC PICA"
    ]
  },
  "Science": {
    "url": "https://en.wikipedia.org/wiki/Science",
    "title": "Science",
    "content": "Science is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe. [ 1 ] [ page needed ] [ 2 ] Modern science is typically divided into two – or three – major branches: [ 3 ] the natural sciences , which study the physical world , and the social sciences , which study individuals and societies. [ 4 ] [ 5 ] While referred to as the formal sciences , the study of logic , mathematics , and theoretical computer science are typically regarded as separate because they rely on deductive reasoning instead of the scientific method as their main methodology. [ 6 ] [ 7 ] [ 8 ] [ 9 ] Meanwhile, applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine. [ 10 ] [ 11 ] [ 12 ] The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia ( c. 3000–1200 BCE ). Their contributions to mathematics, astronomy , and medicine entered and shaped the Greek natural philosophy of classical antiquity and later medieval scholarship , whereby formal attempts were made to provide explanations of events in the physical world based on natural causes; while further advancements, including the introduction of the Hindu–Arabic numeral system , were made during the Golden Age of India and Islamic Golden Age . [ 13 ] : 12 [ 14 ] [ 15 ] [ 16 ] [ 13 ] : 163–192 The recovery and assimilation of Greek works and Islamic inquiries into Western Europe during the Renaissance revived natural philosophy , [ 13 ] : 193–224, 225–253 [ 17 ] which was later transformed by the Scientific Revolution that began in the 16th century [ 18 ] as new ideas and discoveries departed from previous Greek conceptions and traditions. [ 13 ] : 357–368 [ 19 ] The scientific method soon played a greater role in the acquisition of knowledge, and in the 19th century , many of the institutional and professional features of science began to take shape, [ 20 ] [ 21 ] along with the changing of \"natural philosophy\" to \"natural science\". [ 22 ] New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems. [ 23 ] [ 24 ] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions , [ 25 ] government agencies, [ 13 ] : 163–192 and companies. [ 26 ] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection . The word science has been used in English since the 14th century ( Middle English ) in the sense of \"the state of knowing\". The word was borrowed from the Anglo-Norman language as the suffix -cience , which was borrowed from the Latin word scientia , meaning ' knowledge, awareness, understanding ' , a noun derivative of sciens meaning ' knowing ' , itself the present active participle of sciō ' to know ' . [ 27 ] There are many hypotheses for science ' s ultimate word origin. According to Michiel de Vaan , Dutch linguist and Indo-Europeanist , sciō may have its origin in the Proto-Italic language as * skije- or * skijo- meaning ' to know ' , which may originate from Proto-Indo-European language as * skh 1 -ie , * skh 1 -io meaning ' to incise ' . The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre , meaning ' to not know, be unfamiliar with ' , which may derive from Proto-Indo-European * sekH- in Latin secāre , or * skh 2 - from * sḱʰeh2(i)- meaning ' to cut ' . [ 28 ] In the past, science was a synonym for \"knowledge\" or \"study\", in keeping with its Latin origin. A person who conducted scientific research was called a \"natural philosopher\" or \"man of science\". [ 29 ] In 1834, William Whewell introduced the term scientist in a review of Mary Somerville 's book On the Connexion of the Physical Sciences , [ 30 ] crediting it to \"some ingenious gentleman\" (possibly himself). [ 31 ] Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years, [ 32 ] [ 33 ] taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science, [ 34 ] as did religious rituals . [ 35 ] Some scholars use the term \" protoscience \" to label activities in the past that resemble modern science in some but not all features; [ 36 ] [ 37 ] [ 38 ] however, this label has also been criticised as denigrating, [ 39 ] or too suggestive of presentism , thinking about those activities only in relation to modern categories. [ 40 ] Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia ( c. 3000–1200 BCE ), creating the earliest written records in the history of science . [ 13 ] : 12–15 [ 14 ] Although the words and concepts of \"science\" and \"nature\" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine. [ 41 ] [ 13 ] : 12 From the 3rd millennium BCE, the ancient Egyptians developed a non-positional decimal numbering system , [ 42 ] solved practical problems using geometry , [ 43 ] and developed a calendar . [ 44 ] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations , and rituals. [ 13 ] : 9 The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery , faience , glass, soap, metals, lime plaster , and waterproofing. [ 45 ] They studied animal physiology , anatomy , behaviour , and astrology for divinatory purposes. [ 46 ] The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur . [ 45 ] [ 47 ] They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity. [ 45 ] In classical antiquity , there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time. [ 48 ] Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers , the same words tend to be used to describe the natural \"way\" in which a plant grows, [ 49 ] and the \"way\" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish \"nature\" and \"convention\". [ 50 ] The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes , were the first to attempt to explain natural phenomena without relying on the supernatural . [ 51 ] The Pythagoreans developed a complex number philosophy [ 52 ] : 467–468 and contributed significantly to the development of mathematical science. [ 52 ] : 465 The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus . [ 53 ] [ 54 ] Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a \"canon\" (ruler, standard) which established physical criteria or standards of scientific truth. [ 55 ] The Greek doctor Hippocrates established the tradition of systematic medical science [ 56 ] [ 57 ] and is known as \" The Father of Medicine \". [ 58 ] A turning point in the history of early philosophical science was Socrates ' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato 's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly held truths that shape beliefs and scrutinises them for consistency. [ 59 ] Socrates criticised the older type of study of physics as too purely speculative and lacking in self-criticism . [ 60 ] In the 4th century BCE, Aristotle created a systematic programme of teleological philosophy. [ 61 ] In the 3rd century BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the centre and all the planets orbiting it. [ 62 ] Aristarchus's model was widely rejected because it was believed to violate the laws of physics, [ 62 ] while Ptolemy's Almagest , which contains a geocentric description of the Solar System , was accepted through the early Renaissance instead. [ 63 ] [ 64 ] The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus . [ 65 ] Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopaedia Natural History . [ 66 ] [ 67 ] [ 68 ] Positional notation for representing numbers likely emerged between the 3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide. [ 69 ] Due to the collapse of the Western Roman Empire , the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe. [ 13 ] : 194 Latin encyclopaedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge. [ 70 ] In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning. [ 13 ] : 159 John Philoponus , a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus . [ 13 ] : 307, 311, 363, 402 His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later. [ 13 ] : 307–308 [ 71 ] During late antiquity and the Early Middle Ages , natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes : material, formal, moving, and final cause. [ 72 ] Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were made by Christians, mainly Nestorians and Miaphysites . Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists. [ 73 ] By the 6th and 7th centuries, the neighbouring Sasanian Empire established the medical Academy of Gondishapur , which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world. [ 74 ] Islamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad , Iraq [ 75 ] and the flourished [ 76 ] until the Mongol invasions in the 13th century. Ibn al-Haytham , better known as Alhazen, used controlled experiments in his optical study. [ a ] [ 78 ] [ 79 ] Avicenna 's compilation of The Canon of Medicine , a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century. [ 80 ] By the 11th century most of Europe had become Christian, [ 13 ] : 204 and in 1088, the University of Bologna emerged as the first university in Europe. [ 81 ] As such, demand for Latin translation of ancient and scientific texts grew, [ 13 ] : 204 a major contributor to the Renaissance of the 12th century . Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature. [ 82 ] In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi . [ 83 ] New developments in optics played a role in the inception of the Renaissance , both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope . At the start of the Renaissance, Roger Bacon , Vitello , and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle. [ 77 ] : Book I A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final. [ 84 ] In the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model. [ 85 ] Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light. [ 84 ] [ 86 ] Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion . Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres . [ 87 ] Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model. [ 88 ] The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature. [ 89 ] Francis Bacon and René Descartes published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life. [ 90 ] Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature. [ 91 ] At the start of the Age of Enlightenment , Isaac Newton formed the foundation of classical mechanics by his Philosophiæ Naturalis Principia Mathematica greatly influencing future physicists. [ 92 ] Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics , now used in a new non- teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes. [ 93 ] During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words , \"the real and legitimate goal of sciences is the endowment of human life with new inventions and riches \", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond \"the fume of subtle, sublime or pleasing [speculation]\". [ 94 ] Science during the Enlightenment was dominated by scientific societies and academies, [ 95 ] which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularisation of science among an increasingly literate population. [ 96 ] Enlightenment philosophers turned to a few of their scientific predecessors – Galileo , Kepler , Boyle , and Newton principally – as the guides to every physical and social field of the day. [ 97 ] [ 98 ] The 18th century saw significant advancements in the practice of medicine [ 99 ] and physics; [ 100 ] the development of biological taxonomy by Carl Linnaeus ; [ 101 ] a new understanding of magnetism and electricity; [ 102 ] and the maturation of chemistry as a discipline. [ 103 ] Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature , which was expressed historically in works by authors including James Burnett , Adam Ferguson , John Millar and William Robertson , all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity . [ 104 ] Modern sociology largely originated from this movement. [ 105 ] In 1776, Adam Smith published The Wealth of Nations , which is often considered the first work on modern economics. [ 106 ] During the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as \"biologist\", \"physicist\", and \"scientist\"; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals. [ 107 ] During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879. [ 108 ] During the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species , published in 1859. [ 109 ] Separately, Gregor Mendel presented his paper, \" Experiments on Plant Hybridisation \" in 1865, [ 110 ] which outlined the principles of biological inheritance, serving as the basis for modern genetics. [ 111 ] Early in the 19th century John Dalton suggested the modern atomic theory , based on Democritus's original idea of indivisible particles called atoms . [ 112 ] The laws of conservation of energy , conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities , the ease of conversion to useful work or to another form of energy. [ 113 ] This realisation led to the development of the laws of thermodynamics , in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time. [ b ] The electromagnetic theory was established in the 19th century by the works of Hans Christian Ørsted , André-Marie Ampère , Michael Faraday , James Clerk Maxwell , Oliver Heaviside , and Heinrich Hertz . The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896, [ 116 ] Marie Curie then became the first person to win two Nobel Prizes. [ 117 ] In the next year came the discovery of the first subatomic particle, the electron . [ 118 ] In the first half of the century the development of antibiotics and artificial fertilisers improved human living standards globally. [ 119 ] [ 120 ] Harmful environmental issues such as ozone depletion , ocean acidification , eutrophication , and climate change came to the public's attention and caused the onset of environmental studies . [ 121 ] During this period scientific experimentation became increasingly larger in scale and funding . [ 122 ] The extensive technological innovation stimulated by World War I , World War II , and the Cold War led to competitions between global powers , such as the Space Race and nuclear arms race . [ 123 ] [ 124 ] Substantial international collaborations were also made, despite armed conflicts. [ 125 ] In the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields. [ 126 ] The discovery of the cosmic microwave background in 1964 [ 127 ] led to a rejection of the steady-state model of the universe in favour of the Big Bang theory of Georges Lemaître . [ 128 ] The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th century when the modern synthesis reconciled Darwinian evolution with classical genetics . [ 129 ] Albert Einstein 's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length , time and gravity . [ 130 ] [ 131 ] Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing , including smartphones . The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling . [ 132 ] The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome . [ 133 ] The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body. [ 134 ] With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found. [ 135 ] In 2015, gravitational waves , predicted by general relativity a century before, were first observed . [ 136 ] [ 137 ] In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole 's accretion disc . [ 138 ] Modern science is commonly divided into three major branches : natural science , social science , and formal science . [ 3 ] Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise. [ 139 ] Both natural and social sciences are empirical sciences , [ 140 ] as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions. [ 141 ] Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science . These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry , astronomy , and earth science . Modern natural science is the successor to the natural philosophy that began in Ancient Greece . Galileo , Descartes , Bacon , and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures , and presuppositions , often overlooked, remain necessary in natural science. [ 142 ] Systematic data collection, including discovery science , succeeded natural history , which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings. [ 143 ] Today, \"natural history\" suggests observational descriptions aimed at popular audiences. [ 144 ] Social science is the study of human behaviour and the functioning of societies. [ 4 ] [ 5 ] It has many disciplines that include, but are not limited to anthropology , economics, history, human geography , political science , psychology, and sociology. [ 4 ] In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programmes such as the functionalists , conflict theorists , and interactionists in sociology. [ 4 ] Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method , case studies , and cross-cultural studies . Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes. [ 4 ] Formal science is an area of study that generates knowledge using formal systems . [ 145 ] [ 146 ] [ 147 ] A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules. [ 148 ] It includes mathematics, [ 149 ] [ 150 ] systems theory , and theoretical computer science . The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts. [ 8 ] [ 151 ] [ 141 ] The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science. [ 6 ] [ 152 ] Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus , for example, was initially invented to understand motion in physics. [ 153 ] Natural and social sciences that rely heavily on mathematical applications include mathematical physics , [ 154 ] chemistry , [ 155 ] biology , [ 156 ] finance , [ 157 ] and economics . [ 158 ] Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine. [ 159 ] [ 12 ] Engineering is the use of scientific principles to invent, design and build machines, structures and technologies. [ 160 ] Science may contribute to the development of new technologies. [ 161 ] Medicine is the practice of caring for patients by maintaining and restoring health through the prevention , diagnosis , and treatment of injury or disease. [ 162 ] [ 163 ] The applied sciences are often contrasted with the basic sciences , which are focused on advancing scientific theories and laws that explain and predict events in the natural world. [ 164 ] [ 165 ] Computational science applies computer simulations to science, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of machine learning and artificial intelligence is becoming a central feature of computational contributions to science, for example in agent-based computational economics , random forests , topic modeling and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans. [ 168 ] [ 169 ] Interdisciplinary science involves the combination of two or more disciplines into one, [ 170 ] such as bioinformatics , a combination of biology and computer science [ 171 ] or cognitive sciences . The concept has existed since the ancient Greek period and it became popular again in the 20th century. [ 172 ] Scientific research can be labelled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable. [ 173 ] Scientific research involves using the scientific method , which seeks to objectively explain the events of nature in a reproducible way. [ 174 ] Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an objective reality shared by all rational observers; this objective reality is governed by natural laws ; these laws were discovered by means of systematic observation and experimentation. [ 2 ] Mathematics is essential in the formation of hypotheses , theories , and laws, because it is used extensively in quantitative modelling, observing, and collecting measurements . [ 175 ] Statistics is used to summarise and analyse data, which allows scientists to assess the reliability of experimental results. [ 176 ] In the scientific method an explanatory thought experiment or hypothesis is put forward as an explanation using parsimony principles and is expected to seek consilience – fitting with other accepted facts related to an observation or scientific question. [ 177 ] This tentative explanation is used to make falsifiable predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress. [ 174 ] : 4–5 [ 178 ] Experimentation is especially important in science to help establish causal relationships to avoid the correlation fallacy , though in some sciences such as astronomy or geology, a predicted observation might be more appropriate. [ 179 ] When a hypothesis proves unsatisfactory it is modified or discarded. If the hypothesis survives testing, it may become adopted into the framework of a scientific theory , a validly reasoned , self-consistent model or framework for describing the behaviour of certain natural events. A theory typically describes the behaviour of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus, a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a model , an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation, and to generate new hypotheses that can be tested by experimentation. [ 180 ] While performing experiments to test hypotheses, scientists may have a preference for one outcome over another. [ 181 ] [ 182 ] Eliminating the bias can be achieved through transparency, careful experimental design , and a thorough peer review process of the experimental results and conclusions. [ 183 ] [ 184 ] After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be. [ 185 ] Taken in its entirety, the scientific method allows for highly creative problem solving while minimising the effects of subjective and confirmation bias . [ 186 ] Intersubjective verifiability , the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge. [ 187 ] Scientific research is published in a range of literature. [ 188 ] Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, Journal des sçavans followed by Philosophical Transactions , began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500. [ 189 ] Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper . Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population. [ 190 ] The replication crisis is an ongoing methodological crisis that affects parts of the social and life sciences . In subsequent investigations, the results of many scientific studies have been proven to be unrepeatable . [ 191 ] The crisis has long-standing roots; the phrase was coined in the early 2010s [ 192 ] as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience , which aims to improve the quality of all scientific research while reducing waste. [ 193 ] An area of study or speculation that masquerades as science in an attempt to claim legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience , fringe science , or junk science . [ 194 ] [ 195 ] Physicist Richard Feynman coined the term \" cargo cult science \" for cases in which researchers believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated. [ 196 ] Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as \"the most important tool\" for separating valid claims from invalid ones. [ 197 ] There can also be an element of political bias or ideological bias on all sides of scientific debates. Sometimes, research may be characterised as \"bad science\", research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term scientific misconduct refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person. [ 198 ] There are different schools of thought in the philosophy of science . The most popular position is empiricism , which holds that knowledge is created by a process involving observation; scientific theories generalise observations. [ 199 ] Empiricism generally encompasses inductivism , a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method . [ 200 ] [ 199 ] Empiricism has stood in contrast to rationalism , the position originally associated with Descartes , which holds that knowledge is created by the human intellect, not by observation. [ 201 ] Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper . Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation. [ 202 ] Popper proposed replacing verifiability with falsifiability as the landmark of scientific theories, replacing induction with falsification as the empirical method. [ 202 ] Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error , [ 203 ] covering all products of the human mind, including science, mathematics, philosophy, and art. [ 204 ] Another approach, instrumentalism , emphasises the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes, with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be things that should be ignored. [ 205 ] Close to instrumentalism is constructive empiricism , according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true. [ 206 ] Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent \"portrait\" of the world that is consistent with observations made from its framing. He characterised normal science as the process of observation and \"puzzle solving\", which takes place within a paradigm, whereas revolutionary science occurs when one paradigm overtakes another in a paradigm shift . [ 207 ] Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more \"portraits\" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism . [ 208 ] Another approach often cited in debates of scientific scepticism against controversial movements like \" creation science \" is methodological naturalism . Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations. [ 209 ] Methodological naturalism maintains that science requires strict adherence to empirical study and independent verification. [ 210 ] The scientific community is a network of interacting scientists who conduct scientific research. The community consists of smaller groups working in scientific fields. By having peer review , through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results. [ 211 ] Scientists are individuals who conduct scientific research to advance knowledge in an area of interest. [ 212 ] [ 213 ] Scientists may exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of public health, nations, the environment, or industries; other motivations include recognition by peers and prestige. [ citation needed ] In modern times, many scientists study within specific areas of science in academic institutions , often obtaining advanced degrees in the process. [ 214 ] Many scientists pursue careers in various fields such as academia , industry , government , and nonprofit organisations. [ 215 ] [ 216 ] [ 217 ] Science has historically been a male-dominated field, with notable exceptions. Women have faced considerable discrimination in science, much as they have in other areas of male-dominated societies. For example, women were frequently passed over for job opportunities and denied credit for their work. [ 218 ] The achievements of women in science have been attributed to the defiance of their traditional role as labourers within the domestic sphere . [ 219 ] Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance. [ 220 ] Many scientists belong to a learned society that promotes their respective scientific discipline, profession , or group of related disciplines. [ 221 ] Membership may either be open to all, require possession of scientific credentials, or conferred by election. [ 222 ] Most scientific societies are nonprofit organisations, [ 223 ] and many are professional associations . Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some societies act as professional bodies , regulating the activities of their members in the public interest, or the collective interest of the membership. The professionalisation of science, begun in the 19th century, was partly enabled by the creation of national distinguished academies of sciences such as the Italian Accademia dei Lincei in 1603, [ 224 ] the British Royal Society in 1660, [ 225 ] the French Academy of Sciences in 1666, [ 226 ] the American National Academy of Sciences in 1863, [ 227 ] the German Kaiser Wilhelm Society in 1911, [ 228 ] and the Chinese Academy of Sciences in 1949. [ 229 ] International scientific organisations, such as the International Science Council , are devoted to international cooperation for science advancement. [ 230 ] Science awards are usually given to individuals or organisations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist receiving them. Since the early Renaissance, scientists have often been awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and chemistry . [ 231 ] Funding of science is often through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP. [ 232 ] In the OECD , around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10%, respectively, by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and the humanities . In less developed nations, the government provides the bulk of the funds for their basic scientific research. [ 233 ] Many governments have dedicated agencies to support scientific research, such as the National Science Foundation in the United States, [ 234 ] the National Scientific and Technical Research Council in Argentina, [ 235 ] Commonwealth Scientific and Industrial Research Organisation in Australia, [ 236 ] National Centre for Scientific Research in France, [ 237 ] the Max Planck Society in Germany, [ 238 ] and National Research Council in Spain. [ 239 ] In commercial research and development, all but the most research-orientated corporations focus more heavily on near-term commercialisation possibilities than research driven by curiosity. [ 240 ] Science policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding , often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public. [ 241 ] Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organisations that fund research. [ 190 ] Science education for the general public is embedded in the school curriculum, and is supplemented by online pedagogical content (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Major organisations of scientists such as the American Association for the Advancement of Science (AAAS) consider the sciences to be a part of the liberal arts traditions of learning, along with philosophy and history. [ 242 ] Scientific literacy is chiefly concerned with an understanding of the scientific method , units and methods of measurement , empiricism , a basic understanding of statistics ( correlations , qualitative versus quantitative observations, aggregate statistics ), and a basic understanding of core scientific fields such as physics, chemistry , biology , ecology, geology, and computation . As a student advances into higher stages of formal education , the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well. [ 243 ] The mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter. [ 244 ] Few journalists have real scientific knowledge, and even beat reporters who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover. [ 245 ] [ 246 ] Science magazines such as New Scientist , Science & Vie , and Scientific American cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research. [ 247 ] The science fiction genre, primarily speculative fiction , can transmit the ideas and methods of science to the general public. [ 248 ] Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the Creative Writing Science resource developed through the Royal Literary Fund . [ 249 ] While the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are sceptical about science. Examples are the common notion that COVID-19 is not a major health threat to the US (held by 39% of Americans in August 2021) [ 250 ] or the belief that climate change is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020). [ 251 ] Psychologists have pointed to four factors driving rejection of scientific results: [ 252 ] Anti-science attitudes often seem to be caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left. [ 254 ] That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardise their social status. [ 255 ] Attitudes towards science are often determined by political opinions and goals. Government, business and advocacy groups have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the politicisation of science such as anti-intellectualism , perceived threats to religious beliefs, and fear for business interests. [ 257 ] Politicisation of science is usually accomplished when scientific information is presented in a way that emphasises the uncertainty associated with the scientific evidence . [ 258 ] Tactics such as shifting conversation, failing to acknowledge facts, and capitalising on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence. [ 259 ] Examples of issues that have involved the politicisation of science include the global warming controversy , health effects of pesticides , and health effects of tobacco . [ 259 ] [ 260 ]",
    "links": [
      "James Clerk Maxwell",
      "Administration (government)",
      "Traditional ecological knowledge",
      "Francis Bacon",
      "Science in the Renaissance",
      "Henri Becquerel",
      "Mathematical finance",
      "Reproducibility",
      "Cold War",
      "Artificial intelligence",
      "Glossary of geography terms (A–M)",
      "Harmony of the Spheres",
      "Experimental design",
      "Public awareness of science",
      "Greek philosophers",
      "Renaissance",
      "Late antiquity",
      "Democritus",
      "Physical world",
      "Politicization of science",
      "Michiel de Vaan",
      "Black hole",
      "Anti-intellectualism",
      "General relativity",
      "Structuralism (philosophy of science)",
      "Glossary of mycology",
      "On the Connexion of the Physical Sciences",
      "Glossary of civil engineering",
      "Scientific formalism",
      "Citizen science",
      "Hdl (identifier)",
      "Oikonyms in Western and South Asia",
      "Natural History (Pliny)",
      "Carl Linnaeus",
      "Science in classical antiquity",
      "Strong programme",
      "David Hume",
      "Herman Kahn",
      "Philosophy of space and time",
      "Socrates",
      "Academy of Gondishapur",
      "Linda Reichl",
      "Feminist technoscience",
      "Otto Neurath",
      "Cyborg anthropology",
      "Bronze Age",
      "Scientific method",
      "Contextualism",
      "Glossary of cellular and molecular biology (0–L)",
      "Lexikon der indogermanischen Verben",
      "Classical mechanics",
      "Random forest",
      "Faience",
      "Profession",
      "Scientific pluralism",
      "World War II",
      "Scientific law",
      "Space Race",
      "Carl Gustav Hempel",
      "Anjali Nayar",
      "Questionable cause",
      "Mill's Methods",
      "Women in science",
      "Early Middle Ages",
      "Science in the Age of Enlightenment",
      "Nobel Prize in Chemistry",
      "Glossary of probability and statistics",
      "Ignoramus et ignorabimus",
      "Scientific modelling",
      "Michael Polanyi",
      "J. L. Heilbron",
      "Peter Harrison (historian)",
      "Design of experiments",
      "Bibcode (identifier)",
      "Science festival",
      "Ozone depletion",
      "Natural law",
      "Science communication",
      "Objectivity (philosophy)",
      "Traditional knowledge",
      "Earth science",
      "Atomic theory",
      "Sociotechnology",
      "William Robertson (historian)",
      "Academic conference",
      "Charles Calisher",
      "Lawrence Klein",
      "Tom Griffiths (cognitive scientist)",
      "Inductive reasoning",
      "Jane McIntosh",
      "Black swan events",
      "Technological transitions",
      "On the Origin of Species",
      "Radioactivity",
      "Critical rationalism",
      "Technology and society",
      "Apperception",
      "Null hypothesis",
      "Behavioural sciences",
      "Natural philosophy",
      "Advocacy group",
      "Co-production (society)",
      "Linear model of innovation",
      "National Scientific and Technical Research Council",
      "James M. Buchanan",
      "Hal Varian",
      "Demarcation problem",
      "Technical change",
      "Patricia O'Grady",
      "Imre Lakatos",
      "Robert Boyle",
      "Amanda Chetwynd",
      "Socratic method",
      "Bas van Fraassen",
      "Bioinformatics",
      "Epicurus",
      "ArXiv (identifier)",
      "Abstract structure",
      "List of philosophers of science",
      "Philosophy of archaeology",
      "Vitello",
      "Simine Vazire",
      "Vitalism",
      "Health effects of tobacco",
      "Susan Athey",
      "Glossary of genetics and evolutionary biology",
      "Maurice Allais",
      "Correlation",
      "User innovation",
      "Glossary of cellular and molecular biology (M–Z)",
      "Tadeusz Estreicher",
      "Third Dynasty of Ur",
      "United States federal budget",
      "Women in STEM fields",
      "Supernatural",
      "Prussian Academy of Sciences",
      "Helen Margetts",
      "A Treatise of Human Nature",
      "Isaac Newton",
      "Metascience",
      "Steven Novella",
      "Philosophy of social science",
      "Scientocracy",
      "History",
      "Technological innovation system",
      "Chemistry",
      "Scientific language (linguistic classification)",
      "Babylonia",
      "Glossary of economics",
      "Problem of induction",
      "Event Horizon Telescope",
      "Hans Reichenbach",
      "National Centre for Scientific Research",
      "Sex discrimination",
      "Science studies",
      "Science wars",
      "Transition management (governance)",
      "Variable and attribute (research)",
      "Engineering studies",
      "Anaximander",
      "Roger Bacon",
      "Domestic sphere",
      "Basic research",
      "Supply and demand",
      "Epicycles",
      "OECD",
      "Theorem",
      "Length",
      "Orbital period",
      "Falsifiability",
      "Pottery",
      "Formal science",
      "Entropy",
      "Scientific evidence",
      "Ian Hacking",
      "March for Science",
      "Science in the ancient world",
      "Age of Enlightenment",
      "Medical diagnosis",
      "Methodological",
      "Bayesianism",
      "Human Genome Project",
      "Asger Aaboe",
      "Aggregate statistics",
      "Steam engine",
      "Big science",
      "Scientific theory",
      "Auguste Comte",
      "Paradigm",
      "Paul Feyerabend",
      "Scientific journal",
      "Wayback Machine",
      "Charles Darwin",
      "Women in engineering",
      "19th century in science",
      "ISSN (identifier)",
      "Printing press",
      "Integrated circuit",
      "Experiment",
      "Wilhelm Windelband",
      "Indo-European Etymological Dictionary",
      "Self-criticism",
      "Mobile computing",
      "Budget of NASA",
      "ISBN (identifier)",
      "Sumerian language",
      "Falsifiable",
      "Anatomy",
      "Experiments on Plant Hybridisation",
      "Golden Press",
      "Physical science",
      "René Descartes",
      "Academic institution",
      "Alessandro Vespignani",
      "Nature (philosophy)",
      "Objective reality",
      "Interactionism",
      "Functionalists",
      "Empiricism",
      "Science journalism",
      "PMC (identifier)",
      "Egyptian calendar",
      "Modern synthesis (20th century)",
      "Charles Sanders Peirce",
      "Glossary of machine vision",
      "Scientific community",
      "Ancient Egypt",
      "Oxford English Dictionary",
      "Motion",
      "Smartphone",
      "Verificationism",
      "Cosmic microwave background",
      "Media studies",
      "Thomas G. Bergin",
      "Islamic Golden Age",
      "National Academy of Sciences",
      "Theodore Schultz",
      "John Ziman",
      "Scientific controversy",
      "Mesopotamia",
      "Stem cell",
      "Thought experiment",
      "Technology policy",
      "Glossary of medicine",
      "Dependent and independent variables",
      "S2CID (identifier)",
      "Sociology of scientific ignorance",
      "Lucretius",
      "Environmental studies",
      "Professionalization",
      "Teleological",
      "Michael Shermer",
      "Scientific consensus",
      "Glossary of architecture",
      "Causal relationships",
      "John Dalton",
      "Presentism (historical analysis)",
      "Environmental issues",
      "Mapping controversies",
      "Digital anthropology",
      "Glossary of quantum computing",
      "Science and technology studies",
      "Right to science and culture",
      "Karl Pearson",
      "Camera obscura",
      "Michael Faraday",
      "Logically consistent",
      "Benedikt Löwe",
      "X-ray",
      "Antibiotics",
      "Social science",
      "Mathematical chemistry",
      "History and philosophy of science",
      "Telescope",
      "Technology",
      "Mathematics",
      "Marie Curie",
      "Scientific paper",
      "Royal Literary Fund",
      "Annals of Science",
      "Scientist",
      "Pythagoreans",
      "Academia",
      "Nature Geoscience",
      "Theory choice",
      "Glossary of environmental science",
      "Pseudoscience",
      "Confirmation bias",
      "Economic materialism",
      "Human genome",
      "Glossary of computer hardware terms",
      "Glossary of meteorology",
      "Transhumanism",
      "Archimedes of Syracuse",
      "Incantation",
      "Conflict theories",
      "John Peckham",
      "Ozone hole",
      "Constructive realism",
      "Lists of important publications in science",
      "Scholasticism",
      "Glossary of nanotechnology",
      "Karl Popper",
      "Scientific literacy",
      "Correlation function",
      "Constructive empiricism",
      "Metaphysical",
      "Philosophical Transactions",
      "Sendhil Mullainathan",
      "Animal behavior",
      "Ontology",
      "Reproducible",
      "Outline of physical science",
      "Marginalised",
      "Galileo Galilei",
      "Sociology of knowledge",
      "Scientific dissent",
      "Hypotheses",
      "Normal science",
      "Fringe science",
      "Lists of science and technology awards",
      "Technoscience",
      "Heliocentric model",
      "Theory-ladenness",
      "Gottfried Wilhelm Leibniz",
      "Systems theory",
      "Bertrand Russell",
      "Professional association",
      "Relationship between religion and science",
      "Social shaping of technology",
      "Neo-Luddism",
      "William Whewell",
      "Renaissance of the 12th century",
      "James Burnett, Lord Monboddo",
      "One-point perspective",
      "Robin Marantz Henig",
      "A priori and a posteriori",
      "Scientism",
      "Machine learning",
      "Technology dynamics",
      "Research ethics",
      "Harcourt Brace Jovanovich",
      "Science museum",
      "Chinese Academy of Sciences",
      "Eutrophication",
      "Agent-based computational economics",
      "Glossary of robotics",
      "Aristarchus of Samos",
      "Creative synthesis",
      "Logic",
      "Astrology",
      "Heat death of the universe",
      "Wassily Leontief",
      "Cargo cult science",
      "Theory",
      "Control variable",
      "Objectivity (science)",
      "Wayne State University Press",
      "Science in the medieval Islamic world",
      "Palgrave Communications",
      "Humanities",
      "Hindu–Arabic numeral system",
      "Research programme",
      "Science (disambiguation)",
      "John Millar (philosopher)",
      "Technology assessment",
      "Double hermeneutic",
      "International Science Council",
      "Book of Optics",
      "Reductionism",
      "Pythagorean triple",
      "Positional notation",
      "Philosophical analysis",
      "Theory of impetus",
      "Thales of Miletus",
      "Therapy",
      "Collapse of the Western Roman Empire",
      "Developed countries",
      "Greenwood Publishing Group",
      "Nomenclature",
      "Deductive-nomological model",
      "Explanatory power",
      "Presupposition",
      "Adam Smith",
      "Science & Society",
      "Evidence-based practice",
      "Le Figaro",
      "Return on investment",
      "Formal education",
      "Science & Vie",
      "Nobel Foundation",
      "Christopher C. Kraft, Jr.",
      "Pre-Socratic philosopher",
      "Philosophical Magazine",
      "Constructivist epistemology",
      "Science by press conference",
      "Rudolf Steiner",
      "Historical method",
      "Philosophy of linguistics",
      "Blue skies research",
      "Glossary of developmental biology",
      "Deductive reasoning",
      "Matthew J. Salganik",
      "Outline of science",
      "Observation",
      "Journal des sçavans",
      "Francesca Rochberg",
      "Baghdad",
      "Philosophy of economics",
      "Sociology",
      "Technological change",
      "Fact",
      "Philosophy of physics",
      "Glossary of electrical and electronics engineering",
      "Rudolf Carnap",
      "Modernity",
      "De revolutionibus orbium coelestium",
      "Scientific debate",
      "C. D. Broad",
      "Adam Ferguson",
      "Gravity",
      "Technological convergence",
      "Peer review",
      "Boundary-work",
      "Evolutionism",
      "Scientific societies",
      "Natural science",
      "Received view of theories",
      "Diffusion of innovations",
      "Heinrich Hertz",
      "Pliny the Elder",
      "Arithmetic",
      "Theory of forms",
      "Ibn al-Haytham",
      "Industry (economics)",
      "JSTOR (identifier)",
      "Floris Cohen",
      "Foundationalism",
      "Johannes Kepler",
      "Social constructivism",
      "Politicisation of science",
      "Glossary of artificial intelligence",
      "Post-normal science",
      "Philosophy of geography",
      "Concept",
      "Scientific misconduct",
      "Glossary of psychiatry",
      "Horizon scanning",
      "Design studies",
      "Discovery science",
      "Digital divide",
      "Astronomy",
      "Scientometrics",
      "Mathematical physics",
      "First observation of gravitational waves",
      "Reason",
      "Non-science",
      "Formal system",
      "Conservation of energy",
      "Languages of science",
      "The Canon of Medicine",
      "Terminal degree",
      "Galileo",
      "Confirmation holism",
      "Routledge",
      "Science of team science",
      "De rerum natura",
      "The Skeptics' Guide to the Universe (book)",
      "The Wealth of Nations",
      "Glossary of scientific naming",
      "Divinatory",
      "History of science",
      "Testability",
      "Solar System",
      "Scientific enterprise",
      "Rudy Rucker",
      "Fuzzy logic",
      "Glossary of geography terms (N–Z)",
      "Normative science",
      "New Scientist",
      "Sociology of the history of science",
      "House of Wisdom",
      "Hypothetico-deductive model",
      "List of life sciences",
      "Scientific literature",
      "Steady-state model",
      "Proto-Indo-European language",
      "Babylonian medicine",
      "Max Planck Society",
      "Participatory action research",
      "Springer Nature",
      "Taxonomy (biology)",
      "Commensurability (philosophy of science)",
      "Feminist method",
      "Glossary of ichthyology",
      "Alfred North Whitehead",
      "Novum Organum",
      "Descriptive research",
      "Conservation of momentum",
      "Duncan J. Watts",
      "Natural history",
      "Medical prescription",
      "Induced pluripotent stem cell",
      "Prevention (medical)",
      "PMID (identifier)",
      "Scientific essentialism",
      "Hype cycle",
      "Great power",
      "Nestorian schism",
      "Nicolaus Copernicus",
      "Naturalism (philosophy)",
      "The Amateur Scientist",
      "Technology transfer",
      "Academic journal",
      "Electromagnetic theory",
      "Prediction",
      "Perspectivism",
      "Reverse salient",
      "Intertheoretic reduction",
      "Trial and error",
      "Anaximenes of Miletus",
      "Political bias",
      "Health effects of pesticides",
      "Physicalism",
      "CERN",
      "Conversazione",
      "Geometry",
      "Byzantine Empire",
      "Science of science policy",
      "Model-dependent realism",
      "Factor 10",
      "Economics",
      "Branches of science",
      "Technological revolution",
      "Basic science",
      "Patricia Fara",
      "Genetics",
      "Dialectic",
      "Community-based participatory research",
      "Classical genetics",
      "Sasanian Empire",
      "Willard Van Orman Quine",
      "Faith and rationality",
      "Spanish National Research Council",
      "Glossary of ecology",
      "Paradigm shift",
      "Policy",
      "Kepler's laws of planetary motion",
      "Shape of the universe",
      "Wilhelm Wundt",
      "Glossary of Hebrew toponyms",
      "Determinism",
      "Analytic–synthetic distinction",
      "Scientific realism",
      "Epistemology",
      "Fallibilism",
      "Richard Stone",
      "Scientific scepticism",
      "Philosophy of history",
      "Semantic view of theories",
      "Accademia dei Lincei",
      "Jan Tinbergen",
      "Microbiology and Molecular Biology Reviews",
      "Journal of Mathematical Physics",
      "Mathematical economics",
      "Theories",
      "Rationalism",
      "Science fair",
      "Science outreach",
      "Climate change",
      "History of Islamic Philosophy",
      "Miaphysites",
      "Research funding",
      "Antipositivism",
      "Hans Christian Ørsted",
      "Nature",
      "Empirical",
      "Alternative hypothesis",
      "Anthropology",
      "Inductivism",
      "Peacock",
      "Skunkworks project",
      "Ocean acidification",
      "Industrial Revolution",
      "Technological determinism",
      "Measurement",
      "Philosophy of technology",
      "Arabic",
      "Jon Kleinberg",
      "Case studies",
      "Cognitive science",
      "Junk science",
      "COVID-19",
      "Deductive logic",
      "Interdisciplinary science",
      "Back-formation",
      "Instrumentalism",
      "Glossary of astronomy",
      "Capital equipment",
      "A priori",
      "Kepler",
      "Magnetism",
      "Communications satellite",
      "Accretion disc",
      "List of scientific occupations",
      "Phusis",
      "Occam's razor",
      "Pierre Duhem",
      "Larry Laudan",
      "Hypothetico-deductive method",
      "John Philoponus",
      "Epistemological anarchism",
      "Popular science",
      "Computation",
      "Bibliometrics",
      "Early adopter",
      "Mondino de Luzzi",
      "Correlations",
      "Conjecture",
      "List of years in science",
      "Glossary of structural engineering",
      "Anthropocene",
      "Functional contextualism",
      "Academy of sciences",
      "Four causes",
      "International cooperation",
      "Research",
      "Philosophy of biology",
      "Hypothesis",
      "Natural selection",
      "Inquiry",
      "The Dawn of Everything",
      "Polski słownik biograficzny",
      "World War I",
      "French Academy of Sciences",
      "Skeptical Inquirer",
      "Sociotechnical system",
      "Philosophy of science",
      "Hippocrates",
      "Doi (identifier)",
      "Café Scientifique",
      "University of Bologna",
      "Energy quality",
      "Methodological naturalism",
      "Geocentric model",
      "Glossary of Arabic toponyms",
      "Measurements",
      "Leapfrogging",
      "Philosophiæ Naturalis Principia Mathematica",
      "Leucippus",
      "Mathematical biology",
      "Popular culture",
      "The Mississippi Quarterly",
      "History of science policy",
      "Topic model",
      "Kaiser Wilhelm Society",
      "Animal physiology",
      "Descartes",
      "Patient and public involvement",
      "Science policy",
      "Regulation of science",
      "Computational science",
      "Nobel Prize in Physics",
      "Rhetoric of science",
      "Glossary of chemistry terms",
      "Innovation",
      "Gravitational wave",
      "Quantitative research",
      "Funding of science",
      "Empirical evidence",
      "Creation science",
      "American Philosophical Society",
      "Relativism",
      "Learned societies",
      "Higgs boson",
      "Science magazines",
      "Gregor Mendel",
      "Lime plaster",
      "Axiom",
      "Conventionalism",
      "National Science Foundation",
      "Glossary of physics",
      "Mary Somerville",
      "Protoscience",
      "Applied science",
      "Coherentism",
      "Academic bias",
      "Political science",
      "Unethical human experimentation",
      "Scientific integrity",
      "Applied research",
      "Public science",
      "Anglo-Norman language",
      "Atomism",
      "Alfred Russel Wallace",
      "Causality",
      "Positivism",
      "Arthur Koestler",
      "Bernard Davis (biologist)",
      "History of technology",
      "Glossary of archaeology",
      "Unity of science",
      "Philosophy of chemistry",
      "Paul Samuelson",
      "Beat reporter",
      "Phylogenetic tree",
      "Engineering",
      "Correlation fallacy",
      "Actor–network theory",
      "André-Marie Ampère",
      "Responsible Research and Innovation",
      "Scientific American",
      "Qualitative research",
      "Evidence-based policy",
      "Economics of scientific knowledge",
      "Glossary of calculus",
      "Big Bang",
      "Computer simulations",
      "Biology",
      "Research and development",
      "Antiscience",
      "De Aspectibus",
      "Scientific skepticism",
      "Philosophy",
      "Economics of science",
      "Social sciences",
      "Walter Burkert",
      "Neo-colonial science",
      "Dematerialization (products)",
      "Science (journal)",
      "Natural phenomena",
      "Human geography",
      "Franco Modigliani",
      "Glossary of aerospace engineering",
      "Philosophy of psychology",
      "Uniformitarianism",
      "Theory of relativity",
      "20th century in science",
      "Henri Poincaré",
      "Physics outreach",
      "Glossary of agriculture",
      "Writing systems",
      "Social epistemology",
      "Hard and soft science",
      "Criticism of technology",
      "Glossary of virology",
      "Cross-cultural studies",
      "Plimpton 322",
      "Latin",
      "Professional bodies",
      "Science education",
      "Jennifer Speake",
      "Inductionism",
      "Seyyed Hossein Nasr",
      "Glossary of mechanical engineering",
      "Speculative fiction",
      "Electron",
      "Glossary of clinical research",
      "Thomas Kuhn",
      "David Graeber",
      "Exact sciences",
      "Indo-Europeanist",
      "Scientific priority",
      "Ancient Greece",
      "YouTube in education",
      "Financial technology",
      "Quantum mechanics",
      "Aristotle",
      "History of science and technology",
      "Helaine Selin",
      "Technocracy",
      "Vienna Dioscurides",
      "Almagest",
      "Normalization process theory",
      "Isidore of Seville",
      "David Wengrow",
      "Underdetermination",
      "Scientific Revolution",
      "Oliver Heaviside",
      "Academic freedom",
      "Disruptive innovation",
      "Plato's Academy mosaic",
      "Postpositivism",
      "Life science",
      "Glossary of botanical terms",
      "Physical law",
      "Royal Society",
      "Classical antiquity",
      "Construct (philosophy)",
      "Mongol invasions",
      "Morphological derivation",
      "Artificial fertiliser",
      "Conservation of mass",
      "Standard Model",
      "Ancient Greek literature",
      "Glossary of entomology terms",
      "Glossary of areas of mathematics",
      "Middle English",
      "Sectors of the economy",
      "Calculus",
      "Albert Einstein",
      "Richard Feynman",
      "Nuclear arms race",
      "Institutionalisation",
      "Commonwealth Scientific and Industrial Research Organisation",
      "Philosophical Transactions of the Royal Society",
      "Psychology of science",
      "Plato",
      "Nature (journal)",
      "Glossary of computer science",
      "James N. Druckman",
      "Fran Wilde (author)",
      "Environmental protection",
      "Medieval renaissances",
      "Georges Lemaître",
      "Avicenna",
      "Global warming controversy",
      "Houston Museum of Natural Science",
      "Empirical science",
      "Work (thermodynamics)",
      "Replication crisis",
      "Aristotelian physics",
      "Glossary of geology",
      "Glossary of bird terms",
      "Social construction of technology",
      "Glossary of biology",
      "Research institutions",
      "Theories of technology",
      "Consilience",
      "Haggai Erlich",
      "Digital media use and mental health",
      "Thermodynamics",
      "Pragmatism",
      "Glossary of cell biology",
      "Sociology of scientific knowledge",
      "Proto-Italic language",
      "Anti-realism",
      "James Tobin",
      "Intersubjective verifiability",
      "Theoretical computer science",
      "Peter Diggle",
      "Criticism of science",
      "John Keay"
    ]
  },
  "Term Discrimination": {
    "url": "https://en.wikipedia.org/wiki/Term_Discrimination",
    "title": "Term Discrimination",
    "content": "Term discrimination is a way to rank keywords in how useful they are for information retrieval . This is a method similar to tf-idf but it deals with finding keywords suitable for information retrieval and ones that are not. Please refer to Vector Space Model first. This method uses the concept of Vector Space Density that the less dense an occurrence matrix is, the better an information retrieval query will be. An optimal index term is one that can distinguish two different documents from each other and relate two similar documents. On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents. The discrimination value is the difference in the occurrence matrix's vector-space density versus the same matrix's vector-space without the index term's density. Given an occurrency matrix : A {\\displaystyle A} and one keyword: k {\\displaystyle k} A higher value is better because including the keyword will result in better information retrieval. Keywords that are sparse should be poor discriminators because they have poor recall , whereas keywords that are frequent should be poor discriminators because they have poor precision .",
    "links": [
      "Wayback Machine",
      "Precision and recall",
      "Occurrency matrix",
      "Sparse matrix",
      "Centroid",
      "Tf-idf",
      "Vector Space Model",
      "Euclidean distance",
      "Gerard Salton",
      "Information retrieval",
      "Occurrence matrix"
    ]
  },
  "World Wide Web": {
    "url": "https://en.wikipedia.org/wiki/World_Wide_Web",
    "title": "World Wide Web",
    "content": "The World Wide Web (also known as WWW , W3 , or simply the Web ) [ 1 ] is an information system that enables content sharing over the Internet through user-friendly interfaces designed to appeal to users beyond IT specialists and hobbyists. [ 2 ] It facilitates access to documents and other web resources according to specific rules of the Hypertext Transfer Protocol (HTTP). [ 3 ] The Web was invented by English computer scientist Tim Berners-Lee while at CERN in 1989 and opened to the public in 1993. It was conceived as a \"universal linked information system\". [ 4 ] [ 5 ] [ 6 ] Documents and other media content are made available to the network through web servers and can be accessed by programs such as web browsers . Servers and resources on the World Wide Web are identified and located through a character string called uniform resource locator (URL). The original and still very common document type is a web page formatted in Hypertext Markup Language (HTML). This markup language supports plain text , images , embedded video and audio contents, and scripts (short programs) that implement complex user interaction. The HTML language also supports hyperlinks (embedded URLs), which provide immediate access to other web resources. Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites. Web applications are web pages that function as application software . The information on the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common domain name make up a website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organisations, government agencies, and individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information. The Web has become the world's dominant information systems platform . [ 7 ] [ 8 ] [ 9 ] [ 10 ] It is the primary tool that billions of people worldwide use to interact with the Internet. [ 3 ] The Web was invented by English computer scientist Tim Berners-Lee while working at CERN . [ 11 ] [ 12 ] He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organisation, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common tree structure approach, used for instance in the existing CERNDOC documentation system and in the Unix filesystem , as well as approaches that relied on tagging files with keywords , as in the VAX/NOTES system. Instead, he adopted concepts he had put into practice with his private ENQUIRE system (1980), built at CERN. When he became aware of Ted Nelson 's hypertext model (1965), in which documents can be linked in unconstrained ways through hyperlinks associated with \"hot spots\" embedded in the text, it helped to confirm the validity of his concept. [ 13 ] [ 14 ] The model was later popularised by Apple 's HyperCard system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to mutable data files, or even fire up programs on their server computer. He also conceived \"gateways\" that would allow access through the new system to documents organised in other ways (such as traditional computer file systems or the Usenet ). Finally, he insisted that the system should be decentralised, without any central control or coordination over the creation of links. [ 5 ] [ 15 ] [ 11 ] [ 12 ] Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [ 5 ] He got a working system implemented by the end of 1990, including a browser called WorldWideWeb (which became the name of the project and of the network) and an HTTP server running at CERN. As part of that development, he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [ 16 ] The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN and began to spread to other scientific and academic institutions. Within the next two years, there were 50 websites created . [ 17 ] [ 18 ] CERN made the Web protocol and code available royalty free on 30 April 1993, enabling its widespread use. [ 19 ] [ 20 ] [ 21 ] After the NCSA released the Mosaic web browser later that year, the Web's popularity grew rapidly as thousands of websites sprang up in less than a year. [ 22 ] [ 23 ] Mosaic was a graphical browser that could display inline images and submit forms that were processed by the HTTPd server . [ 24 ] [ 25 ] Marc Andreessen and Jim Clark founded Netscape the following year and released the Navigator browser , which introduced Java and JavaScript to the Web. It quickly became the dominant browser. Netscape became a public company in 1995, which triggered a frenzy for the Web and started the dot-com bubble . [ 26 ] Microsoft responded by developing its own browser, Internet Explorer , starting the browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [ 27 ] Berners-Lee founded the World Wide Web Consortium (W3C) which created XML in 1996 and recommended replacing HTML with stricter XHTML . [ 28 ] In the meantime, developers began exploiting an IE feature called XMLHttpRequest to make Ajax applications and launched the Web 2.0 revolution. Mozilla , Opera , and Apple rejected XHTML and created the WHATWG which developed HTML5 . [ 29 ] In 2009, the W3C conceded and abandoned XHTML. [ 30 ] In 2019, it ceded control of the HTML specification to the WHATWG. [ 31 ] The World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet . [ 32 ] [ 33 ] [ 34 ] [ 10 ] Tim Berners-Lee states that World Wide Web is officially spelled as three separate words, each capitalised, with no intervening hyphens. [ 35 ] Use of the www prefix has been declining, especially when web applications sought to brand their domain names and make them easily pronounceable. As the mobile web grew in popularity, [ 36 ] services like Gmail.com , Outlook.com , Myspace .com, Facebook .com and Twitter .com are most often mentioned without adding \"www.\" (or, indeed, \".com\") to the domain. [ 37 ] In English, www is usually read as double-u double-u double-u . [ 38 ] Some users pronounce it dub-dub-dub , particularly in New Zealand. [ 39 ] Stephen Fry , in his \"Podgrams\" series of podcasts, pronounces it wuh wuh wuh . [ 40 ] The English writer Douglas Adams once quipped in The Independent on Sunday (1999): \"The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for\". [ 41 ] The terms Internet and World Wide Web are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of computer networks interconnected through telecommunications and optical networking . In contrast, the World Wide Web is a global collection of documents and other resources , linked by hyperlinks and URIs . Web resources are accessed using HTTP or HTTPS , which are application-level Internet protocols that use the Internet transport protocols. [ 3 ] Viewing a web page on the World Wide Web normally begins either by typing the URL of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after channel surfing ), or 'navigating the Web'. Early studies of this new behaviour investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation, and targeted navigation. [ 42 ] The following example demonstrates the functioning of a web browser when accessing a page at the URL http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an Internet Protocol address using the globally distributed Domain Name System (DNS). This lookup returns an IP address such as 203.0.113.4 or 2001:db8:2e::7334 . The browser then requests the resource by sending an HTTP request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service, so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses port number 80 and, for HTTPS, it normally uses port number 443 . The content of the HTTP request can be as simple as two lines of text: The computer receiving the HTTP request delivers it to the web server software listening for requests on port 80. If the web server can fulfil the request, it sends an HTTP response back to the browser indicating success: Followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this: The web browser parses the HTML and interprets the markup ( < title > , < p > for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media, scripts that affect page behaviour, and Cascading Style Sheets that affect page layout. The browser makes additional HTTP requests to the web server for these other Internet media types . As it receives its content from the web server, the browser progressively renders the page onto the screen as specified by its HTML and these additional resources. Hypertext Markup Language (HTML) is the standard markup language for creating web pages and web applications . With Cascading Style Sheets (CSS) and JavaScript , it forms a triad of cornerstone technologies for the World Wide Web. [ 43 ] Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document. HTML elements are the building blocks of HTML pages. With HTML constructs, images and other objects such as interactive forms may be embedded into the rendered page. HTML provides a means to create structured documents by denoting structural semantics for text such as headings, paragraphs, lists, links , quotes, and other items. HTML elements are delineated by tags , written using angle brackets . Tags such as < img /> and < input /> directly introduce content into the page. Other tags, such as < p > , surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page. HTML can embed programs written in a scripting language such as JavaScript , which affects the behaviour and content of web pages. Inclusion of CSS defines the look and layout of content. The World Wide Web Consortium (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [ 44 ] Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions, and other web resources. In the underlying HTML, a hyperlink looks like this: < a href = \"http://example.org/home.html\" > Example.org Homepage </ a > . Such a collection of useful, related resources interconnected via hypertext links is dubbed a web of information. Publication on the Internet created what Tim Berners-Lee first called the WorldWideWeb (in its original CamelCase , which was subsequently discarded) in November 1990. [ 45 ] The hyperlink structure of the web is described by the webgraph : the nodes of the web graph correspond to the web pages (or URLs), the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called \"dead\" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The Internet Archive , active since 1996, is the best known of such efforts. Many hostnames used for the World Wide Web begin with www because of the long-standing practice of naming Internet hosts according to the services they provide. The hostname of a web server is often www , in the same way that it may be ftp for an FTP server , and news or nntp for a Usenet news server . These hostnames appear as Domain Name System (DNS) or subdomain names, as in www.example.com . The use of www is not required by any technical or policy standard and many websites do not use it; the first web server was nxoc01.cern.ch . [ 46 ] According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of www as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending www to an institution's website domain name was subsequently copied. [ 47 ] [ better source needed ] Many established websites still use the prefix, or they employ other subdomain names such as www2 , secure or en for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the www subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different websites. The use of a subdomain name is useful for load balancing incoming web traffic by creating a CNAME record that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [ 48 ] [ dubious – discuss ] When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix \"www\" to the beginning of it and possibly \".com\", \".org\" and \".net\" at the end, depending on what might be missing. For example, entering \"microsoft\" may be transformed to http://www.microsoft.com/ and \"openoffice\" to http://www.openoffice.org . This feature started appearing in early versions of Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as Lynx . [ 49 ] [ unreliable source? ] It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [ 50 ] The scheme specifiers http:// and https:// at the start of a web URI refer to Hypertext Transfer Protocol or HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted. [ citation needed ] A web page (also written as webpage ) is a document that is suitable for the World Wide Web and web browsers . A web browser displays a web page on a monitor or mobile device . The term web page usually refers to what is visible, but may also refer to the contents of the computer file itself, which is usually a text file containing hypertext written in HTML or a comparable markup language . Typical web pages provide hypertext for browsing to other web pages via hyperlinks , often referred to as links . Web browsers will frequently have to access multiple web resource elements, such as reading style sheets , scripts , and images, while presenting each web page. On a network, a web browser can retrieve a web page from a remote web server . The web server may restrict access to a private network, such as a corporate intranet. The web browser uses the Hypertext Transfer Protocol (HTTP) to make such requests to the web server . A static web page is delivered exactly as stored, as web content in the web server's file system . In contrast, a dynamic web page is generated by a web application , usually driven by server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email, etc. A static web page (sometimes called a flat page/stationary page ) is a web page that is delivered to the user exactly as stored, in contrast to dynamic web pages which are generated by a web application . Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a web server to negotiate content-type or language of the document where such versions are available and the server is configured to do so. A server-side dynamic web page is a web page whose construction is controlled by an application server processing server-side scripts. In server-side scripting, parameters determine how the assembly of every new web page proceeds, including the setting up of more client-side processing. A client-side dynamic web page processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way. A dynamic web page is then reloaded by the user or by a computer program to change some variable content. The updated information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a dynamic web page update using Ajax technologies will neither create a page to go back to nor truncate the web browsing history forward of the displayed page. Using Ajax technologies, the end user gets one dynamic page managed as a single page in the web browser while the actual web content rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM, the DOM, for its client, from an application server. Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not static web pages , though it has fallen out of common use since the popularisation of AJAX , a term which is now itself rarely used. Client-side scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser. [ citation needed ] JavaScript is a scripting language that was initially developed in 1995 by Brendan Eich , then of Netscape , for use within web pages. [ 51 ] The standardised version is ECMAScript . [ 51 ] To make web pages more interactive, some web applications also use JavaScript techniques such as Ajax ( asynchronous JavaScript and XML ). Client-side script is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly poll the server to check whether new information is available. [ 52 ] A website [ 53 ] is a collection of related web resources including web pages , multimedia content, typically identified with a common domain name , and published on at least one web server . Notable examples are wikipedia .org, google .com, and amazon.com . A website may be accessible via a public Internet Protocol (IP) network, such as the Internet , or a private local area network (LAN), by referencing a uniform resource locator (URL) that identifies the site. Websites can have many functions and can be used in various fashions; a website can be a personal website , a corporate website for a company, a government website, an organisation website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and social networking to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an intranet . Web pages, which are the building blocks of websites, are documents , typically composed in plain text interspersed with formatting instructions of Hypertext Markup Language ( HTML , XHTML ). They may incorporate elements from other websites with suitable markup anchors . Web pages are accessed and transported with the Hypertext Transfer Protocol (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a web browser , renders the page content according to its HTML markup instructions onto a display terminal . Hyperlinking between web pages conveys to the reader the site structure and guides the navigation of the site, which often starts with a home page containing a directory of the site web content . Some websites require user registration or subscription to access content. Examples of subscription websites include many business sites, news websites, academic journal websites, gaming websites, file-sharing websites, message boards , web-based email , social networking websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services. End users can access websites on a range of devices, including desktop and laptop computers , tablet computers , smartphones , and smart TVs . A web browser (commonly referred to as a browser ) is a software user agent for accessing information on the World Wide Web. To connect to a website's server and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer. In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into websites. The most popular browsers are Chrome , Safari , Edge , Samsung Internet and Firefox . [ 54 ] A Web server is server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over HTTP and several other related protocols. The primary function of a web server is to store, process and deliver web pages to clients . [ 55 ] The communication between client and server takes place using the Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently HTML documents , which may include images , style sheets and scripts in addition to the text content. A user agent , commonly a web browser or web crawler , initiates communication by making a request for a specific resource using HTTP and the server responds with the content of that resource or an error message if unable to do so. The resource is typically a real file on the server's secondary storage , but this is not necessarily the case and depends on how the web server is implemented . While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting web forms , including uploading of files. Many generic web servers also support scripting using Active Server Pages (ASP), PHP (Hypertext Preprocessor), or other scripting languages . This means that the behaviour of the web server can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents dynamically (\"on-the-fly\") as opposed to returning static documents . The former is primarily used for retrieving or modifying information from databases . The latter is typically much faster and more easily cached but cannot deliver dynamic content . Web servers can also frequently be found embedded in devices such as printers , routers , webcams and serving only a local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most operating systems ). Optical networking is a sophisticated infrastructure that utilises optical fibre to transmit data over long distances, connecting countries, cities, and even private residences. The technology uses optical microsystems like tunable lasers , filters, attenuators , switches, and wavelength-selective switches to manage and operate these networks. [ 56 ] [ 57 ] The large quantity of optical fibre installed throughout the world at the end of the twentieth century set the foundation of the Internet as it is used today. The information highway relies heavily on optical networking, a method of sending messages encoded in light to relay information in various telecommunication networks. [ 58 ] The Advanced Research Projects Agency Network (ARPANET) was one of the first iterations of the Internet, created in collaboration with universities and researchers in 1969. [ 59 ] [ 60 ] [ 61 ] [ 62 ] However, access to the ARPANET was limited to researchers, and in 1985, the National Science Foundation founded the National Science Foundation Network (NSFNET), a program that provided supercomputer access to researchers. [ 62 ] Limited public access to the Internet led to pressure from consumers and corporations to privatise the network. In 1993, the US passed the National Information Infrastructure Act , which dictated that the National Science Foundation must hand over control of the optical capabilities to commercial operators. [ 63 ] [ 64 ] The privatisation of the Internet and the release of the World Wide Web to the public in 1993 led to an increased demand for Internet capabilities. This spurred developers to seek solutions to reduce the time and cost of laying new fibre and increase the amount of information that can be sent on a single fibre, to meet the growing needs of the public. [ 65 ] [ 66 ] [ 67 ] [ 68 ] In 1994, Pirelli S.p.A.'s optical components division introduced a wavelength-division multiplexing (WDM) system to meet growing demand for increased data transmission. This four-channel WDM technology allowed more information to be sent simultaneously over a single optical fibre, effectively boosting network capacity. [ 69 ] [ 70 ] Pirelli wasn't the only company that developed a WDM system; another company, the Ciena Corporation (Ciena), created its own technology to transmit data more efficiently. David Huber , an optical networking engineer and entrepreneur Kevin Kimberlin founded Ciena in 1992. [ 71 ] [ 72 ] [ 73 ] Drawing on laser technology from Gordon Gould and William Culver of Optelecom, Inc. , the company focused on utilising optical amplifiers to transmit data via light. [ 74 ] [ 75 ] [ 76 ] Under chief executive officer Pat Nettles, Ciena developed a dual-stage optical amplifier for dense wavelength-division multiplexing (DWDM), patented in 1997 and deployed on the Sprint network in 1996. [ 77 ] [ 78 ] [ 79 ] [ 80 ] [ 81 ] An HTTP cookie (also called web cookie , Internet cookie , browser cookie , or simply cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's web browser while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons, logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields, such as names, addresses, passwords, and credit card numbers. Cookies perform essential functions in the modern web. Perhaps most importantly, authentication cookies are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see cross-site scripting and cross-site request forgery for examples). [ 82 ] Tracking cookies, and especially third-party tracking cookies, are commonly used as ways to compile long-term records of individuals' browsing histories – a potential privacy concern that prompted European [ 83 ] and U.S. lawmakers to take action in 2011. [ 84 ] [ 85 ] European law requires that all websites targeting European Union member states gain \"informed consent\" from users before storing non-essential cookies on their device. Google Project Zero researcher Jann Horn describes ways cookies can be read by intermediaries , like Wi-Fi hotspot providers. When in such circumstances, he recommends using the browser in private browsing mode (widely known as Incognito mode in Google Chrome). [ 86 ] A web search engine or Internet search engine is a software system that is designed to carry out web search ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a web search query . The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). The information may be a mix of web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories . Unlike web directories , which are maintained only by human editors, search engines also maintain real-time information by running an algorithm on a web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the deep web . In 1990, Archie , the world's first search engine, was released. The technology was originally an index of File Transfer Protocol (FTP) sites, which was a method for moving files between a client and a server network. [ 87 ] [ 88 ] This early search tool was superseded by more advanced engines like Yahoo! in 1995 and Google in 1998. [ 89 ] [ 90 ] The deep web, [ 91 ] invisible web , [ 92 ] or hidden web [ 93 ] are parts of the World Wide Web whose contents are not indexed by standard web search engines . The opposite term to the deep web is the surface web , which is accessible to anyone using the Internet. [ 94 ] Computer scientist Michael K. Bergman is credited with coining the term deep web in 2001 as a search indexing term. [ 95 ] The content of the deep web is hidden behind HTTP forms, [ 96 ] [ 97 ] and includes many very common uses such as web mail , online banking , and services that users must pay for, and which is protected by a paywall , such as video on demand , some online magazines and newspapers, among others. The content of the deep web can be located and accessed by a direct URL or IP address and may require a password or other security access past the public website page. A web cache is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a browser cache by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Website designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise firewalls often cache Web resources requested by one user for the benefit of many users. Some search engines store cached content of frequently accessed websites. For criminals , the Web has become a venue to spread malware and engage in a range of cybercrime , including (but not limited to) identity theft , fraud , espionage , and intelligence gathering . [ 98 ] Web-based vulnerabilities now outnumber traditional computer security concerns, [ 99 ] [ 100 ] and as measured by Google , about one in ten web pages may contain malicious code. [ 101 ] Most web-based attacks take place on legitimate websites, and most, as measured by Sophos , are hosted in the United States, China and Russia. [ 102 ] The most common of all malware threats is SQL injection attacks against websites. [ 103 ] Through HTML and URIs, the Web was vulnerable to attacks like cross-site scripting (XSS) that came with the introduction of JavaScript [ 104 ] and were exacerbated to some degree by Web 2.0 and Ajax web design that favours the use of scripts. [ 105 ] In one 2007 estimate, 70% of all websites are open to XSS attacks on their users. [ 106 ] Phishing is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [ 107 ] Two of the well-known phishing methods are Covert Redirect and Open Redirect. Proposed solutions vary. Large security companies like McAfee already design governance and compliance suites to meet post-9/11 regulations, [ 108 ] and some, like Finjan Holdings have recommended active real-time inspection of programming code and all content regardless of its source. [ 98 ] Some have argued that for enterprises to see Web security as a business opportunity rather than a cost centre , [ 109 ] while others call for \"ubiquitous, always-on digital rights management \" enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [ 110 ] Jonathan Zittrain has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [ 111 ] Every time a client requests a web page, the server can identify the request's IP address . Web servers usually log IP addresses in a log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable history feature, and usually cache much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide personally identifiable information is by using a virtual private network . A VPN encrypts traffic between the client and VPN server, and masks the original IP address, lowering the chance of user identification. When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organisation can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's terms and conditions and the local laws that apply, information from these profiles may be sold, shared, or passed to other organisations without the user being informed. For many ordinary people, this means little more than some unexpected emails in their inbox or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web. Social networking sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified with an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences. Online bullies may make use of personal information to harass or stalk users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [ 112 ] Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web. Web standards include many interdependent standards and specifications, some of which govern aspects of the Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and web services . Considerations include the interoperability , accessibility and usability of web pages and websites. Web standards, in the broader sense, consist of the following: Web standards are not fixed sets of rules but are constantly evolving sets of finalised technical specifications of web technologies. [ 119 ] Web standards are developed by standards organisations —groups of interested and often competing parties chartered with the task of standardisation—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of W3C specifications, the highest maturity level). There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [ 120 ] The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and equal opportunity to people with disabilities. [ 121 ] Tim Berners-Lee once noted, \"The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect.\" [ 120 ] Many countries regulate web accessibility as a requirement for websites. [ 122 ] International co-operation in the W3C Web Accessibility Initiative led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using assistive technology . [ 120 ] [ 123 ] The W3C Internationalisation Activity assures that web technology works in all languages, scripts, and cultures. [ 124 ] Beginning in 2004 or 2005, Unicode gained ground and eventually in December 2007 surpassed both ASCII and Western European as the Web's most frequently used character map . [ 125 ] Originally RFC 3986 allowed resources to be identified by URI in a subset of US-ASCII. RFC 3987 allows more characters—any character in the Universal Character Set —and now a resource can be identified by IRI in any language. [ 126 ]",
    "links": [
      "Tablet computer",
      "Jagadish Chandra Bose",
      "NPL network",
      "Web engineering",
      "Browser cache",
      "Webgraph",
      "McAfee",
      "Reblogging",
      "Interoperability",
      "Facebook Platform",
      "Subscription",
      "Radiotelephone",
      "FTP server",
      "Website",
      "Client (computing)",
      "Microformat",
      "Molecular communication",
      "Cost centre (business)",
      "Unicode",
      "Semantic service-oriented architecture",
      "Espionage",
      "Demographic profile",
      "Markup language",
      "HRecipe",
      "Wikimedia Foundation",
      "UUCP",
      "Web Index",
      "Thomas A. Watson",
      "Plain text",
      "List of TCP and UDP port numbers",
      "Escribitionist",
      "Information technology",
      "Edwin Howard Armstrong",
      "Radio wave",
      "Television broadcasting",
      "User-generated content",
      "URL",
      "Uses of podcasting",
      "Semantic matching",
      "Parsing",
      "Knowledge representation and reasoning",
      "Bibcode (identifier)",
      "RSS",
      "Hypertext",
      "Digital humanities",
      "Database",
      "Free-space optical communication",
      "Review aggregator",
      "Scripting language",
      "Browser engine",
      "Stephen Fry",
      "WHATWG",
      "Formatted text",
      "Standards organization",
      "Information Age",
      "ARPANET",
      "Web page",
      "Private browsing",
      "Cybercrime",
      "Uniform resource locator",
      "Telex",
      "Internet Archive",
      "Geotagging",
      "Video",
      "Streaming media",
      "Douglas Adams",
      "Food blogging",
      "Blawg",
      "Innocenzo Manzetti",
      "History of the World Wide Web",
      "Optical telegraph",
      "Web series",
      "Cyberstalking",
      "Internet metaphors",
      "Cyberbullying",
      "Alfred Vail",
      "Web cache",
      "Yogen Dalal",
      "List of Internet pioneers",
      "Java (programming language)",
      "Wikipedia",
      "Home page",
      "Attenuator (electronics)",
      "Internet security",
      "Incognito mode",
      "RSS tracking",
      "Archie (search engine)",
      "Software application",
      "Vint Cerf",
      "Erna Schneider Hoover",
      "Microblogging",
      "Description logic",
      "RDF/XML",
      "Topic map",
      "RSS editor",
      "BIBFRAME",
      "Coaxial cable",
      "SAWSDL",
      "Fraud",
      "Radia Perlman",
      "Desktop computer",
      "Research Resource Identifier",
      "Jun-ichi Nishizawa",
      "Metadata Authority Description Schema",
      "Johann Philipp Reis",
      "Netscape Navigator",
      "National Science Foundation Network",
      "Schema.org",
      "Algorithm",
      "Sunday Mail (Scotland)",
      "XHTML",
      "Unicode Consortium",
      "Photophone",
      "James H. Clark",
      "Audio coding format",
      "Search aggregator",
      "HTML",
      "Orbital angular momentum multiplexing",
      "ENQUIRE",
      "Tim Berners-Lee",
      "Metadata",
      "The Telephone Cases",
      "Wireless revolution",
      "Charles Bourseul",
      "Digital rights management",
      "Node (networking)",
      "Knowledge management",
      "Mobilecast",
      "Webtoon",
      "Online diary",
      "XBEL",
      "Knowledge extraction",
      "Blog carnival",
      "Claude Chappe",
      "Wayback Machine",
      "Paul Baran",
      "CERN httpd",
      "Hacker (computer security)",
      "GeoRSS",
      "Intranet",
      "ISSN (identifier)",
      "Store and forward",
      "Browser wars",
      "ISBN (identifier)",
      "Mobile device",
      "Claude Shannon",
      "Reference (computer science)",
      "Slashdot effect",
      "File Transfer Protocol",
      "Web Science Trust",
      "Web directories",
      "Optical networking",
      "Solid (web decentralization project)",
      "The Web (disambiguation)",
      "Spam blog",
      "Videocast",
      "Subdomain",
      "PMC (identifier)",
      "Motovlog",
      "Common Logic",
      "Internet video",
      "Apple Computer",
      "Hyperdata",
      "Ajax (programming)",
      "Information system",
      "Mohamed M. Atalla",
      "Multiplexing",
      "Broadcatching",
      "Content negotiation",
      "Smartphone",
      "Electronic literature",
      "Telephone exchange",
      "Internationalized Resource Identifier",
      "Media RSS",
      "Structured document",
      "Outlook.com",
      "CamelCase",
      "Access control",
      "Google Search",
      "Hedy Lamarr",
      "MP3 blog",
      "Criminal",
      "Webcam",
      "Computer display",
      "Poverty",
      "Domain Name System",
      "Search engine results page",
      "ASCII",
      "Internet2",
      "Telecommunications network",
      "Data feed",
      "Threat (computer)",
      "Request for Comments",
      "Mashup (web application hybrid)",
      "Attack (computing)",
      "Project diary",
      "Bibliographic Ontology",
      "Journalism",
      "HTTPS",
      "Mobile blogging",
      "Robert Cailliau",
      "Pingback",
      "Narrowcasting",
      "SHACL",
      "Semantics",
      "Atom (Web standard)",
      "Tunable laser",
      "Secondary memory",
      "Sideblog",
      "Wide area network",
      "List of websites founded before 1995",
      "Lists of websites",
      "Web application",
      "Image search",
      "Semantic Web",
      "Time-division multiplexing",
      "Web navigation",
      "Dot-com bubble",
      "Web content",
      "RFC (identifier)",
      "Image compression",
      "XOXO (microformat)",
      "Document Object Model",
      "Thread (Internet)",
      "Phishing",
      "Accessibility",
      "Data communication",
      "Data mining",
      "Screencast",
      "Operating system",
      "Ted Nelson",
      "Digital media",
      "Smart TV",
      "Web platform",
      "Email",
      "Video coding format",
      "Reginald Fessenden",
      "John Logie Baird",
      "Linked data",
      "Antonio Meucci",
      "Simple Knowledge Organization System",
      "Refback",
      "Web Ontology Language",
      "Man-in-the-middle attack",
      "Dataspaces",
      "Poll aggregator",
      "Dream diary",
      "Identity theft",
      "Myspace",
      "Facial recognition system",
      "Malware",
      "HTTP cookie",
      "Terms and conditions",
      "Information architecture",
      "Printer (computing)",
      "Mormon blogosphere",
      "Resource Description Framework",
      "National Center for Supercomputing Applications",
      "Vulnerability (computing)",
      "Real-time computing",
      "Form (HTML)",
      "Microsoft Edge",
      "Google Chrome",
      "Thomas Edison",
      "WWW (disambiguation)",
      "Active Server Pages",
      "Project Zero (Google)",
      "Memetics",
      "Local area network",
      "Bandwidth (computing)",
      "HTTP",
      "Sophos",
      "Packet switching",
      "HyperCard",
      "Streaming television",
      "HCard",
      "History of podcasting",
      "Henry Sutton (inventor)",
      "History of prepaid mobile phones",
      "Online video platform",
      "ECMAScript",
      "Cory Doctorow",
      "Frequency-division multiplexing",
      "HReview",
      "Tree (computing)",
      "Facebook",
      "N-Triples",
      "Uniform Resource Identifier",
      "Internet forum",
      "Paywall",
      "Dell PowerEdge",
      "David R. Huber",
      "Multimedia",
      "End user",
      "Mosaic (web browser)",
      "Hostname",
      "Static web page",
      "MIME type",
      "History of the Internet",
      "Ethernet",
      "Semantic search",
      "Application software",
      "The New York Review of Books",
      "Daniel Davis Jr.",
      "Web development tools",
      "Alexander Stepanovich Popov",
      "TheFreeDictionary.com",
      "Photofeed",
      "Internet Explorer",
      "JavaScript",
      "Telecommunications link",
      "Encryption",
      "Web service",
      "Server-side scripting",
      "Bob Kahn",
      "Donald Davies",
      "Transmission medium",
      "Drums in communication",
      "History of videotelephony",
      "Dynamic web page",
      "Web 1.0",
      "Charles Grafton Page",
      "Cascading Style Sheets",
      "MySQL",
      "Internet media type",
      "Web directory",
      "Semiconductor",
      "Alternative media",
      "Assistive technology",
      "User (computing)",
      "Fiber-optic communication",
      "Fashion blog",
      "Guglielmo Marconi",
      "W3C",
      "Niche blogging",
      "Columnist",
      "Cable television",
      "Web resource",
      "Dell",
      "Search engines",
      "Link rot",
      "User agent",
      "Walter Houser Brattain",
      "Radio network",
      "Samuel Morse",
      "Space-division multiple access",
      "Pay-per-click",
      "Marc Andreessen",
      "Personally identifiable information",
      "Semaphore",
      "Semantics (computer science)",
      "COinS",
      "Almon Brown Strowger",
      "Scripting programming language",
      "Computer scientist",
      "RSS TV",
      "Wireless network",
      "Digital image",
      "Toasternet",
      "NewsML-G2",
      "Data compression",
      "World Wide Web Consortium",
      "Corporate blog",
      "RSS enclosure",
      "WorldWideWeb",
      "Asynchronous I/O",
      "Semantic reasoner",
      "Product feed",
      "Load balancing (computing)",
      "Emile Berliner",
      "Ping (blogging)",
      "Social networking",
      "Semantic computing",
      "Police blog",
      "Web Accessibility Initiative",
      "Optical fiber",
      "Gopher (protocol)",
      "Internet privacy",
      "GIF",
      "Circuit switching",
      "Rule-based system",
      "News aggregator",
      "Philo Farnsworth",
      "Communications protocol",
      "Ciena",
      "Equal opportunity",
      "Apress",
      "Semantic analytics",
      "Polling (computer science)",
      "Disability",
      "Dublin Core",
      "The Independent",
      "Gardiner Greene Hubbard",
      "Classical music blog",
      "Alexander Graham Bell",
      "PMID (identifier)",
      "Computer monitor",
      "Robert Metcalfe",
      "TANet",
      "Academic journal",
      "Polarization-division multiplexing",
      "Feed URI scheme",
      "Telecommunications",
      "CYCLADES",
      "Communication",
      "Edholm's law",
      "Slidecasting",
      "National Information Infrastructure",
      "Web 2.0",
      "Electronic publishing",
      "Spam in blogs",
      "Transmission line",
      "CERN",
      "Intelligence gathering",
      "Ecma International",
      "Universal Character Set",
      "Optical communication",
      "NeXT Computer",
      "James Gleick",
      "OCLC (identifier)",
      "Atom (web standard)",
      "Router (computing)",
      "MOSFET",
      "Samsung Internet",
      "Trackback",
      "Roberto Landell de Moura",
      "Parameter (computer programming)",
      "Cellular network",
      "Wi-Fi",
      "Web design",
      "Joanna Walsh",
      "Program state",
      "Laptop",
      "HTML5",
      "RDFa",
      "History of broadcasting",
      "Reverse blog",
      "Internet",
      "Elisha Gray",
      "Charles Wheatstone",
      "Content (media)",
      "Video blog",
      "Warblog",
      "Google",
      "Webcomic",
      "Sir Tim Berners-Lee",
      "Internet Protocol",
      "Database journalism",
      "JSON-LD",
      "Audio signal",
      "Dynamic content",
      "Travel blog",
      "Social software",
      "Stephen Witt",
      "Wireless",
      "Semantic network",
      "NCSA HTTPd",
      "Communication protocol",
      "History of television",
      "Personal website",
      "Internet Engineering Task Force",
      "DOAP",
      "Blog fiction",
      "Decentralized web",
      "Semantic triple",
      "Art blog",
      "Posting style",
      "TriX (serialization format)",
      "FOAF",
      "Semantic wiki",
      "Computer network",
      "Surface web",
      "Liveblogging",
      "URI",
      "Minitel",
      "NewsML 1",
      "Communications satellite",
      "Tivadar Puskás",
      "Telecommunication circuit",
      "Online banking",
      "IP address",
      "International Organization for Standardization",
      "List of family-and-homemaking blogs",
      "Rack mount",
      "Gmail",
      "Form (web)",
      "MITRE Corporation",
      "Comparison of feed aggregators",
      "Application layer",
      "Blog",
      "Social media",
      "Style sheet (web development)",
      "Web standards",
      "OPML",
      "Channel surfing",
      "Terminal (telecommunication)",
      "History of the telephone",
      "Log file",
      "Doi (identifier)",
      "HCalendar",
      "TriG (syntax)",
      "Semantic HTML",
      "Web accessibility",
      "Web server",
      "Web browsing history",
      "Firewall (networking)",
      "Opera (company)",
      "Web Hypertext Application Technology Working Group",
      "Server software",
      "Public switched telephone network",
      "Semantic Web Rule Language",
      "Cross-site scripting",
      "Virtual private network",
      "Fieldset",
      "Application server",
      "Lee de Forest",
      "Harold Hopkins (physicist)",
      "NewsML",
      "Francis Blake (inventor)",
      "Image",
      "Phryctoria",
      "SQL injection",
      "File system",
      "Video on demand",
      "HTML element",
      "Oxford University Press",
      "Optelecom",
      "Software system",
      "Code-division multiple access",
      "Rollback (data management)",
      "Web Slice",
      "Document",
      "Citizen journalism",
      "Unix filesystem",
      "National Science Foundation",
      "GRDDL",
      "Cross-site request forgery",
      "History of telecommunication",
      "History of web syndication technology",
      "Search engine indexing",
      "CNAME record",
      "Blogosphere",
      "Electronic journal",
      "Next-generation network",
      "Web3D",
      "ProQuest",
      "Camille Tissot",
      "News server",
      "Nikola Tesla",
      "CXO Media",
      "Pager",
      "SPARQL",
      "Finjan Holdings",
      "Netscape",
      "Telecommunications equipment",
      "Web literacy",
      "FidoNet",
      "Dawon Kahng",
      "Semantically Interlinked Online Communities",
      "PHP",
      "Yahoo!",
      "Charles Sumner Tainter",
      "Client-side scripting",
      "Smoke signal",
      "Digital television",
      "Lifelog",
      "HAtom",
      "Charles K. Kao",
      "HProduct",
      "Semiconductor device",
      "Cornerstone",
      "Kevin Kimberlin",
      "Web feed",
      "Usability",
      "Amazon (company)",
      "Text file",
      "RDF Schema",
      "John Bardeen",
      "Jonathan Zittrain",
      "Whistled language",
      "Narinder Singh Kapany",
      "Web browser",
      "Twitter",
      "Astroturfing",
      "Embedded system",
      "Internationalization and localization",
      "RSS Advisory Board",
      "IXBRL",
      "XMLHttpRequest",
      "Enhanced podcast",
      "Peercasting",
      "Web crawler",
      "Nasir Ahmed (engineer)",
      "JANET",
      "Lynx (web browser)",
      "USAID",
      "Safari (web browser)",
      "List of telecommunications regulatory bodies",
      "Notation3",
      "European Union",
      "Mozilla",
      "Collective intelligence",
      "Anonymous blog",
      "Implementation",
      "Photoblog",
      "Glossary of blogging",
      "Outline of telecommunication",
      "Metadata Object Description Schema",
      "Integrated Services Digital Network",
      "BITNET",
      "Information revolution",
      "Inter-process communication",
      "History of radio",
      "Computer program",
      "Semantic mapper",
      "Gordon Gould",
      "Domain name",
      "Semantic publishing",
      "Telautograph",
      "Discrete cosine transform",
      "Amos Dolbear",
      "Network switch",
      "History of blogging",
      "Brendan Eich",
      "Oliver Heaviside",
      "Upload",
      "Microdata (HTML)",
      "Hyperlink",
      "History of the transistor",
      "Web3",
      "XML",
      "HTTP Secure",
      "Hashtag",
      "HTML anchor",
      "Library 2.0",
      "Robert Hooke",
      "Permalink",
      "Hypertext Markup Language",
      "Search engine",
      "Rule Interchange Format",
      "Firefox",
      "Index term",
      "Internet Assigned Numbers Authority",
      "Folksonomy",
      "History of mobile phones",
      "TechChange",
      "BitTorrent",
      "Health blog",
      "Edublog",
      "Digital library",
      "Deep web",
      "Web search query",
      "Usenet",
      "Cable protection system",
      "Web syndication",
      "Web search engine",
      "Vladimir K. Zworykin",
      "Computer file",
      "Initial public offering",
      "Mobile web",
      "Teleprinter",
      "Telegraphy",
      "Mobile telephony",
      "Site map",
      "Web mail",
      "Instant messaging",
      "Ontology (information science)",
      "Turtle (syntax)",
      "Network topology",
      "Video aggregator",
      "Hypertext Transfer Protocol",
      "Linkback",
      "United States Antarctic Program"
    ]
  },
  "Wikipedia": {
    "url": "https://en.wikipedia.org/wiki/Wikipedia",
    "title": "Wikipedia",
    "content": "Wikipedia [ c ] is a free online encyclopedia written and maintained by a community of volunteers , known as Wikipedians , through open collaboration and the wiki software MediaWiki . Founded by Jimmy Wales and Larry Sanger in 2001, Wikipedia has been hosted since 2003 by the Wikimedia Foundation , an American nonprofit organization funded mainly by donations from readers. [ 1 ] Wikipedia is the largest and most-read reference work in history. [ 2 ] [ 3 ] Initially available only in English , Wikipedia exists in over 340 languages and is the world's ninth most visited website . The English Wikipedia , with over 7 million articles , remains the largest of the editions, which together comprise more than 66 million articles and attract more than 1.5 billion unique device visits and 13 million edits per month (about 5 edits per second on average) as of April 2024 [update] . [ W 1 ] As of September 2025 [update] , over 25% of Wikipedia's traffic comes from the United States, while Japan accounts for nearly 7%, and the United Kingdom, Germany and Russia each represent around 5%. [ 4 ] Wikipedia has been praised for enabling the democratization of knowledge , its extensive coverage, unique structure, and culture. Wikipedia has been censored by some national governments, ranging from specific pages to the entire site. [ 5 ] [ 6 ] Wikipedia's volunteer editors have written extensively on a wide variety of topics, but the encyclopedia has also been criticized for systemic bias , such as a gender bias against women and a geographical bias against the Global South . [ 7 ] [ 8 ] While the reliability of Wikipedia was frequently criticized in the 2000s, it has improved over time, receiving greater praise from the late 2010s onward. [ 2 ] [ 9 ] [ 10 ] Articles on breaking news are often accessed as sources for up-to-date information about those events. [ 11 ] [ 12 ] Various collaborative online encyclopedias were attempted before the start of Wikipedia, but with limited success. [ 13 ] Wikipedia began as a complementary project for Nupedia, a free online English-language encyclopedia project whose articles were written by experts and reviewed under a formal process. [ 14 ] It was founded on March 9, 2000, under the ownership of Bomis , a web portal company. Its main figures were Bomis CEO Jimmy Wales and Larry Sanger , editor-in-chief for Nupedia and later Wikipedia. [ 15 ] [ 16 ] Nupedia was initially licensed under its own Nupedia Open Content License, but before Wikipedia was founded, Nupedia switched to the GNU Free Documentation License at the urging of Richard Stallman . [ W 2 ] Wales is credited with defining the goal of making a publicly editable encyclopedia, [ 17 ] while Sanger is credited with the strategy of using a wiki to reach that goal. [ citation needed ] On January 10, 2001, Sanger proposed on the Nupedia mailing list to create a wiki as a \"feeder\" project for Nupedia. [ W 3 ] Wikipedia was launched on January 15, 2001 (referred to as Wikipedia Day ) [ 18 ] as a single English language edition with the domain name www.wikipedia.com , [ W 4 ] and was announced by Sanger on the Nupedia mailing list. [ 17 ] The name, proposed by Sanger to forestall any potential damage to the Nupedia name, [ 19 ] originated from a blend of the words wiki and encyclopedia . [ 20 ] [ 21 ] Its integral policy of \" neutral point of view \" arose within its first year. [ 22 ] Otherwise, there were initially relatively few rules, and it operated independently of Nupedia. [ 17 ] Bomis originally intended for it to be a for-profit business. [ 23 ] Wikipedia gained early contributors from Nupedia, Slashdot postings, and web search engine indexing. Language editions were created beginning in March 2001, with a total of 161 in use by the end of 2004. [ W 5 ] [ W 6 ] Nupedia and Wikipedia coexisted until the former's servers were taken down permanently in 2003, and its text was incorporated into Wikipedia. The English Wikipedia passed the mark of 2 million articles on September 9, 2007, making it the largest encyclopedia ever assembled, surpassing the Yongle Encyclopedia made in China during the Ming dynasty in 1408, which had held the record for almost 600 years. [ 24 ] Citing fears of commercial advertising and lack of control, users of the Spanish Wikipedia forked from Wikipedia to create Enciclopedia Libre in February 2002. [ W 7 ] Wales then announced that Wikipedia would not display advertisements, and changed Wikipedia's domain from wikipedia.com to wikipedia.org . [ 25 ] [ W 8 ] After an early period of exponential growth, [ 26 ] the growth rate of the English Wikipedia in terms of the numbers of new articles and of editors, appears to have peaked around early 2007. [ 27 ] The edition reached 3 million articles in August 2009. Around 1,800 articles were added daily to the encyclopedia in 2006; by 2013 that average was roughly 800. [ W 9 ] A team at the Palo Alto Research Center attributed this slowing of growth to \"increased coordination and overhead costs, exclusion of newcomers, and resistance to new edits\". [ 26 ] Others suggested that the growth flattened naturally because articles that could be called \" low-hanging fruit \"—topics that clearly merit an article—had already been created and built up extensively. [ 28 ] [ 29 ] [ 30 ] In November 2009, a researcher at the Rey Juan Carlos University in Madrid, Spain, found that the English Wikipedia had lost 49,000 editors during the first three months of 2009; in comparison, it lost only 4,900 editors during the same period in 2008. [ 31 ] [ 32 ] The Wall Street Journal cited the array of rules applied to editing and disputes related to such content among the reasons for this trend. [ 33 ] Wales disputed these claims in 2009, denying the decline and questioning the study's methodology. [ 34 ] Two years later, in 2011, he acknowledged a slight decline, noting a decrease from \"a little more than 36,000 writers\" in June 2010 to 35,800 in June 2011. In the same interview, he also claimed the number of editors was \"stable and sustainable\". [ 35 ] A 2013 MIT Technology Review article, \"The Decline of Wikipedia\", questioned this claim, reporting that since 2007 Wikipedia had lost a third of its volunteer editors, and suggesting that those remaining had focused increasingly on minutiae. [ 36 ] In July 2012, The Atlantic reported that the number of administrators was also in decline. [ 37 ] In November 2013, New York magazine stated, \"Wikipedia, the sixth-most-used website, is facing an internal crisis.\" [ 38 ] The number of active English Wikipedia editors has since remained steady after a long period of decline. [ 39 ] [ 40 ] Wikipedia has spawned several sister projects, which are also wikis run by the Wikimedia Foundation . These other Wikimedia projects include Wiktionary , a dictionary project launched in December 2002, [ W 10 ] Wikiquote , a collection of quotations created a week after Wikimedia launched, [ 41 ] Wikibooks , a collection of collaboratively written free textbooks and annotated texts, [ W 11 ] Wikimedia Commons , a site devoted to free-knowledge multimedia, [ W 12 ] Wikinews , for collaborative journalism, [ W 13 ] and Wikiversity , a project for the creation of free learning materials and the provision of online learning activities. [ W 14 ] Another sister project of Wikipedia, Wikispecies , is a catalog of all species, but is not open for public editing. [ 42 ] In 2012, Wikivoyage , an editable travel guide, [ 43 ] and Wikidata , an editable knowledge base, launched. [ W 15 ] In January 2007, Wikipedia first became one of the ten most popular websites in the United States, according to Comscore Networks. [ 44 ] With 42.9 million unique visitors, it was ranked ninth, surpassing The New York Times (No. 10) and Apple (No. 11). [ 44 ] This marked a significant increase over January 2006, when Wikipedia ranked 33rd, with around 18.3 million unique visitors. [ 45 ] In 2014, it received 8 billion page views every month. [ W 16 ] On February 9, 2014, The New York Times reported that Wikipedia had 18 billion page views and nearly 500 million unique visitors a month, \"according to the ratings firm comScore\". [ 46 ] As of March 2023 [update] , it ranked sixth in popularity, according to Similarweb . [ 47 ] Jeff Loveland and Joseph Reagle argue that, in process, Wikipedia follows a long tradition of historical encyclopedias that have accumulated improvements piecemeal through \" stigmergic accumulation\". [ 48 ] [ 49 ] On January 18, 2012, the English Wikipedia participated in a series of coordinated protests against two proposed laws in the United States Congress —the Stop Online Piracy Act (SOPA) and the PROTECT IP Act (PIPA)—by blacking out its pages for 24 hours . [ 50 ] More than 162 million people viewed the blackout explanation page that temporarily replaced its content. [ 51 ] [ W 17 ] In January 2013, 274301 Wikipedia , an asteroid , was named after Wikipedia; [ 52 ] in October 2014, Wikipedia was honored with the Wikipedia Monument ; [ 53 ] and, in July 2015, 106 of the 7,473 700-page volumes of Wikipedia became available as Print Wikipedia . [ 54 ] In April 2019, an Israeli lunar lander , Beresheet , crash landed on the surface of the Moon carrying a copy of nearly all of the English Wikipedia engraved on thin nickel plates; experts say the plates likely survived the crash. [ 55 ] [ 56 ] In June 2019, scientists reported that all 16 GB of article text from the English Wikipedia had been encoded into synthetic DNA . [ 57 ] On January 20, 2014, Subodh Varma reporting for The Economic Times indicated that not only had Wikipedia's growth stalled, it \"had lost nearly ten percent of its page views last year. There was a decline of about 2 billion between December 2012 and December 2013. Its most popular versions are leading the slide: page-views of the English Wikipedia declined by twelve percent, those of German version slid by 17 percent and the Japanese version lost 9 percent.\" [ 58 ] Varma added, \"While Wikipedia's managers think that this could be due to errors in counting, other experts feel that Google's Knowledge Graphs project launched last year may be gobbling up Wikipedia users.\" [ 58 ] When contacted on this matter, Clay Shirky , associate professor at New York University and fellow at Harvard's Berkman Klein Center for Internet & Society said that he suspected much of the page-view decline was due to Knowledge Graphs, stating, \"If you can get your question answered from the search page, you don't need to click [any further].\" [ 58 ] By the end of December 2016, Wikipedia was ranked the fifth most popular website globally. [ 59 ] As of January 2023, 55,791 English Wikipedia articles have been cited 92,300 times in scholarly journals, [ 60 ] from which cloud computing was the most cited page. [ 61 ] On January 18, 2023, Wikipedia debuted a new website redesign, called \" Vector 2022 \". [ 62 ] [ 63 ] It featured a redesigned menu bar , moving the table of contents to the left as a sidebar , and numerous changes in the locations of buttons like the language selection tool. [ 63 ] [ W 18 ] The update initially received backlash, most notably when editors of the Swahili Wikipedia unanimously voted to revert the changes. [ 62 ] [ 64 ] Since January 2024, the Wikimedia Foundation has reported a roughly 50 percent increase in bandwidth use from downloads of multimedia content across its projects. According to the foundation, this growth is largely attributed to automated programs, or \"scraper\" bots, that collect large volumes of data from Wikimedia sites for use in training large language models and related applications. [ 65 ] In October 2025, the Wikimedia Foundation reported an estimated 8 percent decline in traffic as compared to the same months in 2024 in human page views. They speculate it reflects the use of generative AI and social media on how people tend to search for information. [ 66 ] [ 67 ] Due to Wikipedia's increasing popularity, some editions, including the English version, have introduced editing restrictions for certain cases. For instance, on the English Wikipedia and some other language editions, only users with 10 edits that have an account that is four days old may create a new article. [ W 19 ] On the English Wikipedia, among others, particularly controversial, sensitive, or vandalism-prone pages have been protected to varying degrees. [ 68 ] A frequently vandalized article can be \"semi-protected\" or \"extended confirmed protected\", meaning that only \"autoconfirmed\" or \"extended confirmed\" editors can modify it. [ 69 ] A particularly contentious article may be locked so that only administrators can make changes. [ W 20 ] A 2021 article in the Columbia Journalism Review identified Wikipedia's page-protection policies as \"perhaps the most important\" means at its disposal to \"regulate its market of ideas\". [ 70 ] Wikipedia has delegated some functions to bots . Such algorithmic governance has an ease of implementation and scaling, though the automated rejection of edits may have contributed to a downturn in active Wikipedia editors. [ 71 ] Bots must be approved by the community before their tasks are implemented. [ 72 ] In certain cases, all editors are allowed to submit modifications, but review is required for some editors, depending on certain conditions. For example, the German Wikipedia maintains \"stable versions\" of articles which have passed certain reviews. [ W 21 ] Following protracted trials and community discussion, the English Wikipedia introduced the \"pending changes\" system in December 2012. [ 73 ] Under this system, new and unregistered users' edits to certain controversial or vandalism-prone articles are reviewed by established users before they are published. [ 74 ] However, restrictions on editing may reduce the editor engagement as well as efforts to diversify the editing community. [ 75 ] Articles related to the Israeli–Palestinian conflict are placed under extended-confirmed protection. [ 76 ] Editors also can make only one revert per day across the entire field and can be banned from editing related articles. These restrictions were introduced in 2008. [ 77 ] In January 2025, the Arbitration Committee introduced the \"balanced editing restriction\", which requires sanctioned users to devote only a third of their edits to articles related to the Israeli–Palestinian conflict even when no misconduct rules have been violated. [ 78 ] [ 79 ] Although changes are not systematically reviewed, Wikipedia's software provides tools allowing anyone to review changes made by others. Each article's History page links to each revision. [ e ] [ 80 ] On most articles, anyone can view the latest changes and undo others' revisions by clicking a link on the article's History page. Registered users may maintain a \"watchlist\" of articles that interest them so they can be notified of changes. [ W 22 ] \"New pages patrol\" is a process where newly created articles are checked for obvious problems. [ W 23 ] In 2003, economics PhD student Andrea Ciffolilli argued that the low transaction costs of participating in a wiki created a catalyst for collaborative development, and that features such as allowing easy access to past versions of a page favored \"creative construction\" over \"creative destruction\". [ 81 ] Any change that deliberately compromises Wikipedia's integrity is considered vandalism. The most common and obvious types of vandalism include additions of obscenities and crude humor; it can also include advertising and other types of spam. [ 82 ] Sometimes editors commit vandalism by removing content or entirely blanking a given page. Less common types of vandalism, such as the deliberate addition of plausible but false information, can be more difficult to detect. Vandals can introduce irrelevant formatting, modify page semantics such as the page's title or categorization, manipulate the article's underlying code, or use images disruptively. [ W 24 ] Obvious vandalism is generally easy to remove from Wikipedia articles; the median time to detect and fix it is a few minutes. [ 83 ] [ 84 ] However, some vandalism takes much longer to detect and repair. [ 85 ] In the Seigenthaler biography incident , an anonymous editor introduced false information into the biography of American political figure John Seigenthaler in May 2005, falsely presenting him as a suspect in the assassination of John F. Kennedy . [ 85 ] It remained uncorrected for four months. [ 85 ] Seigenthaler, the founding editorial director of USA Today and founder of the Freedom Forum First Amendment Center at Vanderbilt University , called Wikipedia co-founder Jimmy Wales and asked whether he had any way of knowing who contributed the misinformation. Wales said he did not, although the perpetrator was eventually traced. [ 86 ] [ 87 ] After the incident, Seigenthaler described Wikipedia as \"a flawed and irresponsible research tool\". [ 85 ] The incident led to policy changes at Wikipedia for tightening up the verifiability of biographical articles of living people. [ 88 ] Wikipedia editors often have disagreements regarding content, which can be discussed on article Talk pages. Disputes may result in repeated competing changes to an article, known as \"edit warring\". [ W 25 ] [ 89 ] It is widely seen as a resource-consuming scenario where no useful knowledge is added, [ 90 ] and criticized as creating a competitive [ 91 ] and conflict-based editing culture associated with traditional masculine gender roles . [ 92 ] [ 93 ] Research has focused on, for example, impoliteness of disputes, [ 94 ] [ 95 ] the influence of rival editing camps, [ 96 ] [ 97 ] the conversational structure, [ 98 ] and the shift in conflicts to a focus on sources. [ 99 ] [ 100 ] Taha Yasseri of the University of Oxford examined editing conflicts and their resolution in a 2013 study. [ 101 ] [ 102 ] Yasseri contended that simple reverts or \"undo\" operations were not the most significant measure of counterproductive work behavior at Wikipedia. He relied instead on \"mutually reverting edit pairs\", where one editor reverts the edit of another editor who then, in sequence, returns to revert the first editor. The results were tabulated for several language versions of Wikipedia. The English Wikipedia's three largest conflict rates belonged to the articles George W. Bush , anarchism , and Muhammad . [ 102 ] By comparison, for the German Wikipedia, the three largest conflict rates at the time of the study were for the articles covering Croatia , Scientology , and 9/11 conspiracy theories . [ 102 ] In 2020, researchers identified other measures of editor behaviors, beyond mutual reverts, to identify editing conflicts across Wikipedia. [ 100 ] Editors also debate the deletion of articles on Wikipedia , with roughly 500,000 such debates since Wikipedia's inception. Once an article is nominated for deletion, the dispute is typically determined by initial votes (to keep or delete) and by reference to topic-specific notability policies. [ 103 ] Wikipedia is composed of 11 different namespaces , with its articles being present in mainspace . Other namespaces have a prefix before their page title and fulfill various purposes. For example, the project namespace uses the Wikipedia prefix and is used for self-governance related discussions. Most readers are not aware of these other namespaces. [ 104 ] The fundamental principles of the Wikipedia community are embodied in the \"Five pillars\", while the detailed editorial principles are expressed in numerous policies and guidelines intended to appropriately shape content. [ W 26 ] The five pillars are: The rules developed by the community are stored in wiki form, and Wikipedia editors write and revise the website's policies and guidelines in accordance with community consensus. [ 105 ] Originally, rules on the non-English editions of Wikipedia were based on a translation of the rules for the English Wikipedia. They have since diverged to some extent. [ W 21 ] According to the rules on the English Wikipedia community, each entry in Wikipedia must be about a topic that is encyclopedic and is not a dictionary entry or dictionary-style. [ W 27 ] A topic should also meet Wikipedia's standards of \"notability\" , which generally means that the topic has been covered extensively in reliable sources that are independent of the article's subject. [ 106 ] Wikipedia intends to convey only knowledge that is already established and recognized and therefore must not present original research. [ 107 ] Some subjects such as politicians and academics have specialized notability requirements. [ 106 ] Finally, Wikipedia must reflect a neutral point of view. This is accomplished through summarizing reliable sources, using impartial language, and ensuring that multiple points of view are presented based on their prominence. Information must also be verifiable. [ 108 ] Information without citations may be tagged or removed entirely. [ 109 ] This can at times lead to the removal of information which, though valid, is not properly sourced. [ 110 ] As Wikipedia policies changed over time, and became more complex, their number has grown. In 2008, there were 44 policy pages and 248 guideline pages; by 2013, scholars counted 383 policy pages and 449 guideline pages. [ 71 ] Wikipedia's initial anarchy integrated democratic and hierarchical elements over time. [ 111 ] [ 112 ] An article is not considered to be owned by its creator or any other editor, nor by the subject of the article. [ W 28 ] Editors in good standing in the community can request extra user rights , granting them the technical ability to perform certain special actions. Some user rights are granted automatically, such as the autoconfirmed and extended confirmed groups, when thresholds for account age and edits are met. [ 69 ] Experienced editors can choose to run for \" adminship \", [ 113 ] which includes the ability to delete pages or prevent them from being changed in cases of severe vandalism or editorial disputes. [ W 29 ] Administrators are not supposed to enjoy any special privilege in decision-making; instead, their powers are mostly limited to making edits that have project-wide effects and thus are disallowed to ordinary editors, and to implement restrictions intended to prevent disruptive editors from making unproductive edits. [ W 29 ] By 2012, fewer editors were becoming administrators compared to Wikipedia's earlier years, in part because the process of vetting potential administrators had become more rigorous. [ 37 ] In 2022, there was a particularly contentious request for adminship over the candidate's anti-Trump views; ultimately, they were granted adminship. [ 114 ] Over time, Wikipedia has developed a semi-formal dispute resolution process. To determine community consensus, editors can raise issues at appropriate community forums, seek outside input through third opinion requests, or initiate a more general community discussion known as a \"request for comment\", [ W 25 ] in which bots add the discussion to a centralized list of discussions, invite editors to participate, and remove the discussion from the list after 30 days. [ W 30 ] However, editors have the discretion to close (and delist) the discussion early or late. If the result of a discussion is not obvious, a closer—an uninvolved editor usually in good standing—may render a verdict from the strength of the arguments presented and then the numbers of arguers on each side. [ 115 ] Wikipedians emphasize that the process is not a vote by referring to statements of opinion in such discussions as \"!vote\"s, in which the exclamation mark is the symbol for logical negation and pronounced \"not\". [ 116 ] Wikipedia encourages local resolutions of conflicts, which Jemielniak argues is quite unique in organization studies, though there has been some recent interest in consensus building in the field. [ 117 ] Reagle and Sue Gardner argue that the approaches to consensus building are similar to those used by Quakers . [ 117 ] : 62 A difference from Quaker meetings is the absence of a facilitator in the presence of disagreement, a role played by the clerk in Quaker meetings. [ 117 ] : 83 The Arbitration Committee presides over the ultimate dispute resolution process. Although disputes usually arise from a disagreement between two opposing views on how an article should read, the Arbitration Committee explicitly refuses to directly rule on the specific view that should be adopted. [ 118 ] Statistical analyses suggest that the English Wikipedia committee ignores the content of disputes and rather focuses on the way disputes are conducted, [ 119 ] functioning not so much to resolve disputes and make peace between conflicting editors, but to weed out problematic editors while allowing potentially productive editors back in to participate. [ 118 ] Therefore, the committee does not dictate the content of articles, although it sometimes condemns content changes when it deems the new content violates Wikipedia policies (for example, if the new content is considered biased). [ f ] Commonly used solutions include cautions and probations (used in 63% of cases) and banning editors from articles (43%), subject matters (23%), or Wikipedia (16%). [ 118 ] Complete bans from Wikipedia are generally limited to instances of impersonation and antisocial behavior . [ W 31 ] When conduct is not impersonation or anti-social, but rather edit warring and other violations of editing policies, solutions tend to be limited to warnings. [ 118 ] Each article and each user of Wikipedia has an associated and dedicated \"talk\" page. These form the primary communication channel for editors to discuss, coordinate and debate. [ 120 ] Wikipedia's community has been described as cultlike , [ 121 ] although not always with entirely negative connotations. [ 122 ] Its preference for cohesiveness, even if it requires compromise that includes disregard of credentials , has been referred to as \" anti-elitism \". [ W 32 ] Wikipedia does not require that its editors and contributors provide identification. [ 123 ] As Wikipedia grew, \"Who writes Wikipedia?\" became one of the questions frequently asked there. [ 124 ] Jimmy Wales once argued that only \"a community ... a dedicated group of a few hundred volunteers\" makes the bulk of contributions to Wikipedia and that the project is therefore \"much like any traditional organization\". [ 125 ] Since Wikipedia relies on volunteer labour, editors frequently focus on topics that interest them. [ 126 ] The English Wikipedia has 7,108,101 articles, 50,690,070 registered editors, and 297,808 active editors. An editor is considered active if they have made one or more edits in the past 30 days. [ W 33 ] Editors who fail to comply with Wikipedia cultural rituals, such as signing talk page comments, may implicitly signal that they are Wikipedia outsiders, increasing the odds that Wikipedia insiders may target or discount their contributions. Becoming a Wikipedia insider involves non-trivial costs: the contributor is expected to learn Wikipedia-specific technological codes, submit to a sometimes convoluted dispute resolution process, and learn a \"baffling culture rich with in-jokes and insider references\". [ 127 ] Editors who do not log in are in some sense \" second-class citizens \" on Wikipedia, [ 127 ] as \"participants are accredited by members of the wiki community, who have a vested interest in preserving the quality of the work product, on the basis of their ongoing participation\", [ 128 ] but the contribution histories of anonymous unregistered editors recognized only by their IP addresses cannot be attributed to a particular editor with certainty. [ 128 ] New editors often struggle to understand Wikipedia's complexity. Experienced editors are encouraged to not \"bite\" the newcomers in order to create a more welcoming atmosphere. [ 129 ] A 2007 study by researchers from Dartmouth College found that \"anonymous and infrequent contributors to Wikipedia ... are as reliable a source of knowledge as those contributors who register with the site\". [ 130 ] Jimmy Wales stated in 2009 that \"[I]t turns out over 50% of all the edits are done by just 0.7% of the users ... 524 people ... And in fact, the most active 2%, which is 1400 people, have done 73.4% of all the edits.\" [ 125 ] However, Business Insider editor and journalist Henry Blodget showed in 2009 that in a random sample of articles, most Wikipedia content (measured by the amount of contributed text that survives to the latest sampled edit) is created by \"outsiders\", while most editing and formatting is done by \"insiders\". [ 125 ] In 2008, a Slate magazine article reported that \"one percent of Wikipedia users are responsible for about half of the site's edits.\" [ 131 ] This method of evaluating contributions was later disputed by Aaron Swartz , who noted that several articles he sampled had large portions of their content (measured by number of characters) contributed by users with low edit counts. [ 132 ] A 2008 study found that Wikipedians were less agreeable, open, and conscientious than others, [ 133 ] although a later commentary pointed out serious flaws, including that the data showed higher openness and that the differences with the control group and the samples were small. [ 134 ] According to a 2009 study, there is \"evidence of growing resistance from the Wikipedia community to new content\". [ 135 ] Several studies have shown that most volunteer Wikipedia contributors are male. The results of a Wikimedia Foundation survey in 2008 showed that only 13 percent of Wikipedia editors were female. [ 136 ] Because of this, universities throughout the United States tried to encourage women to become Wikipedia contributors. [ 137 ] Similarly, many of these universities, including Yale and Brown , gave college credit to students who create or edit an article relating to women in science or technology. [ 137 ] Andrew Lih , a professor and scientist, said that the reason he thought the number of male contributors outnumbered the number of females so greatly was because identifying as a woman may expose oneself to \"ugly, intimidating behavior\". [ 138 ] Data has shown that Africans are underrepresented among Wikipedia editors. [ 139 ] There are currently 342 language editions of Wikipedia (also called language versions , or simply Wikipedias ). As of December 2025, the six largest, in order of article count, are the English , Cebuano , German , French , Swedish , and Dutch Wikipedias. [ W 35 ] The second and fifth-largest Wikipedias owe their position to the article-creating bot Lsjbot , which as of 2013 [update] had created about half the articles on the Swedish Wikipedia , and most of the articles in the Cebuano and Waray Wikipedias . The latter are both languages of the Philippines . In addition to the top six, twelve other Wikipedias have more than a million articles each ( Spanish , Russian , Italian , Polish , Egyptian Arabic , Chinese , Japanese , Ukrainian , Vietnamese , Arabic , Waray , and Portuguese ), seven more have over 500,000 articles ( Persian , Catalan , Indonesian , Korean , Serbian , Chechen , and Norwegian ), 44 more have over 100,000, and 82 more have over 10,000. [ W 36 ] [ W 35 ] The largest, the English Wikipedia, has over 7.1 million articles. As of January 2021, [update] the English Wikipedia receives 48% of Wikipedia's cumulative traffic, with the remaining split among the other languages. The top 10 editions represent approximately 85% of the total traffic. [ W 37 ] Since Wikipedia is based on the Web and therefore worldwide, contributors to the same language edition may use different dialects or may come from different countries (as is the case for the English edition). These differences may lead to some conflicts over spelling differences (e.g. colour versus color ) [ W 38 ] or points of view. [ W 39 ] Though the various language editions are held to global policies such as \"neutral point of view\", they diverge on some points of policy and practice, most notably on whether images that are not licensed freely may be used under a claim of fair use . [ W 40 ] [ 141 ] The content of articles on the same subject can differ significantly between languages, depending on the sources editors use and other factors. [ 142 ] [ 143 ] Jimmy Wales has described Wikipedia as \"an effort to create and distribute a free encyclopedia of the highest possible quality to every single person on the planet in their own language\". [ W 41 ] Though each language edition functions more or less independently, some efforts are made to supervise them all. They are coordinated in part by Meta-Wiki, the Wikimedia Foundation's wiki devoted to maintaining all its projects (Wikipedia and others). [ W 42 ] For instance, Meta-Wiki provides important statistics on all language editions of Wikipedia, [ W 43 ] and it maintains a list of articles every Wikipedia should have. [ W 44 ] The list concerns basic content by subject: biography, history, geography, society, culture, science, technology, and mathematics. [ W 44 ] It is not rare for articles strongly related to a particular language not to have counterparts in another edition. For example, articles about small towns in the United States might be available only in English, even when they meet the notability criteria of other language Wikipedia projects. [ W 45 ] Translated articles represent only a small portion of articles in most editions, in part because those editions do not allow fully automated translation of articles. Articles available in more than one language may offer \"interwiki links\", which link to the counterpart articles in other editions. [ 145 ] [ W 46 ] A study published by PLOS One in 2012 also estimated the share of contributions to different editions of Wikipedia from different regions of the world. It reported that the proportion of the edits made from North America was 51% for the English Wikipedia, and 25% for the Simple English Wikipedia . [ 144 ] On March 1, 2014, The Economist , in an article titled \"The Future of Wikipedia\", cited a trend analysis concerning data published by the Wikimedia Foundation stating that \"the number of editors for the English-language version has fallen by a third in seven years.\" [ 146 ] The attrition rate for active editors in English Wikipedia was cited by The Economist as substantially in contrast to statistics for Wikipedia in other languages (non-English Wikipedia). The Economist reported that the number of contributors with an average of five or more edits per month was relatively constant since 2008 for Wikipedia in other languages at approximately 42,000 editors within narrow seasonal variances of about 2,000 editors up or down. The number of active editors in English Wikipedia, by sharp comparison, was cited as peaking in 2007 at approximately 50,000 and dropping to 30,000 by the start of 2014. [ 146 ] In contrast, the trend analysis for Wikipedia in other languages (non-English Wikipedia) shows success in retaining active editors on a renewable and sustained basis, with their numbers remaining relatively constant at approximately 42,000. No comment was made concerning which of the differentiated edit policy standards from Wikipedia in other languages (non-English Wikipedia) would provide a possible alternative to English Wikipedia for effectively improving substantial editor attrition rates on the English-language Wikipedia. [ 146 ] Various Wikipedians have criticized Wikipedia's large and growing regulation , which includes more than fifty policies and nearly 150,000 words as of 2014. [update] [ 147 ] [ 117 ] Critics have stated that Wikipedia exhibits systemic bias . In 2010, columnist and journalist Edwin Black described Wikipedia as being a mixture of \"truth, half-truth, and some falsehoods\". [ 148 ] Articles in The Chronicle of Higher Education and The Journal of Academic Librarianship have criticized Wikipedia's \" undue-weight policy \", concluding that Wikipedia explicitly is not designed to provide correct information about a subject, but rather focus on all the major viewpoints on the subject, give less attention to minor ones, and creates omissions that can lead to false beliefs based on incomplete information. [ 149 ] [ 150 ] [ 151 ] Journalists Oliver Kamm and Edwin Black alleged (in 2010 and 2011 respectively) that articles are dominated by the loudest and most persistent voices, usually by a group with an \"ax to grind\" on the topic. [ 148 ] [ 152 ] A 2008 article in Education Next journal concluded that as a resource about controversial topics, Wikipedia is subject to manipulation and spin . [ 153 ] In 2020, Omer Benjakob and Stephen Harrison noted that \"Media coverage of Wikipedia has radically shifted over the past two decades: once cast as an intellectual frivolity, it is now lauded as the 'last bastion of shared reality' online.\" [ 154 ] Multiple news networks and pundits have accused Wikipedia of being ideologically biased . In February 2021, Fox News accused Wikipedia of whitewashing communism and socialism and having too much \" leftist bias\". [ 155 ] Wikipedia co-founder Sanger said that Wikipedia has become a \"propaganda\" for the left-leaning \"establishment\" and warned the site can no longer be trusted. [ 156 ] In 2022, libertarian John Stossel opined that Wikipedia, a site he financially supported at one time, appeared to have gradually taken a significant turn in bias to the political left, specifically on political topics. [ 157 ] Some studies suggest that Wikipedia (and in particular the English Wikipedia) has a \"western cultural bias \" (or \"pro-western bias\") [ 158 ] or \"Eurocentric bias\", [ 159 ] reiterating, says Anna Samoilenko, \"similar biases that are found in the 'ivory tower' of academic historiography\". Carwil Bjork-James proposes that Wikipedia could follow the diversification pattern of contemporary scholarship [ 160 ] and Dangzhi Zhao calls for a \"decolonization\" of Wikipedia to reduce bias from opinionated White male editors. [ 161 ] Articles for traditional encyclopedias such as Encyclopædia Britannica are written by experts , lending such encyclopedias a reputation for accuracy. [ 162 ] However, a peer review in 2005 of forty-two scientific entries on both Wikipedia and Encyclopædia Britannica by the science journal Nature found few differences in accuracy, and concluded that \"the average science entry in Wikipedia contained around four inaccuracies; Britannica , about three.\" [ 163 ] Joseph Reagle suggested that while the study reflects \"a topical strength of Wikipedia contributors\" in science articles, \"Wikipedia may not have fared so well using a random sampling of articles or on humanities subjects.\" [ 164 ] Others raised similar critiques. [ 165 ] The findings by Nature were disputed by Encyclopædia Britannica , [ 166 ] [ 167 ] and in response, Nature gave a rebuttal of the points raised by Britannica . [ 168 ] In addition to the point-for-point disagreement between these two parties, others have examined the sample size and selection method used in the Nature effort, and suggested a \"flawed study design\" (in Nature ' s manual selection of articles, in part or in whole, for comparison), absence of statistical analysis (e.g., of reported confidence intervals ), and a lack of study \"statistical power\" (i.e., owing to small sample size , 42 or 4 × 10 1 articles compared, vs >10 5 and >10 6 set sizes for Britannica and the English Wikipedia, respectively). [ 169 ] As a consequence of the open structure, Wikipedia \"makes no guarantee of validity\" of its content, since no one is ultimately responsible for any claims appearing in it. [ W 47 ] Concerns have been raised by PC World in 2009 regarding the lack of accountability that results from users' anonymity, the insertion of false information, [ 170 ] vandalism , and similar problems. Legal Research in a Nutshell (2011), cites Wikipedia as a \"general source\" that \"can be a real boon\" in \"coming up to speed in the law governing a situation\" and, \"while not authoritative, can provide basic facts as well as leads to more in-depth resources\". [ 171 ] Economist Tyler Cowen wrote: \"If I had to guess whether Wikipedia or the median refereed journal article on economics was more likely to be true after a not so long think I would opt for Wikipedia.\" He comments that some traditional sources of non-fiction suffer from systemic biases, and novel results, in his opinion, are over-reported in journal articles as well as relevant information being omitted from news reports. However, he also cautions that errors are frequently found on Internet sites and that academics and experts must be vigilant in correcting them. [ 172 ] Amy Bruckman has argued that, due to the number of reviewers, \"the content of a popular Wikipedia page is actually the most reliable form of information ever created\". [ 173 ] In September 2022, The Sydney Morning Herald journalist Liam Mannix noted that: \"There's no reason to expect Wikipedia to be accurate ... And yet it [is].\" Mannix further discussed the multiple studies that have proved Wikipedia to be generally as reliable as Encyclopædia Britannica , summarizing that \"...turning our back on such an extraordinary resource is... well, a little petty.\" [ 174 ] Critics argue that Wikipedia's open nature and a lack of proper sources for most of the information makes it unreliable. [ 175 ] Some commentators suggest that Wikipedia may be reliable, but that the reliability of any given article is not clear. [ 176 ] Editors of traditional reference works such as the Encyclopædia Britannica have questioned the project's utility and status as an encyclopedia. [ 177 ] Wikipedia co-founder Jimmy Wales has claimed that Wikipedia has largely avoided the problem of \"fake news\" because the Wikipedia community regularly debates the quality of sources in articles. [ 178 ] Wikipedia's open structure inherently makes it an easy target for Internet trolls , spammers , and various forms of paid advocacy seen as counterproductive to the maintenance of a neutral and verifiable online encyclopedia. [ 80 ] [ W 48 ] In response to paid advocacy editing and undisclosed editing issues, Wikipedia was reported in an article in The Wall Street Journal to have strengthened its rules and laws against undisclosed editing. [ 180 ] The article stated that: \"Beginning Monday [from the date of the article, June 16, 2014], changes in Wikipedia's terms of use will require anyone paid to edit articles to disclose that arrangement. Katherine Maher , the nonprofit Wikimedia Foundation's chief communications officer, said the changes address a sentiment among volunteer editors that 'we're not an advertising service; we're an encyclopedia. ' \" [ 180 ] [ 181 ] [ 182 ] [ 183 ] [ 184 ] These issues, among others, had been parodied since the first decade of Wikipedia, notably by Stephen Colbert on The Colbert Report . [ 185 ] Some university lecturers discourage students from citing any encyclopedia in academic work , preferring primary sources ; [ 186 ] some specifically prohibit Wikipedia citations. [ 187 ] [ 188 ] Wales stresses that encyclopedias of any type are not usually appropriate to use as citable sources, and should not be relied upon as authoritative. [ 189 ] Wales once (2006 or earlier) said he receives about ten emails weekly from students saying they got failing grades on papers because they cited Wikipedia; he told the students they got what they deserved. \"For God's sake, you're in college; don't cite the encyclopedia\", he said. [ 190 ] In February 2007, an article in The Harvard Crimson newspaper reported that a few of the professors at Harvard University were including Wikipedia articles in their syllabi , although without realizing the articles might change. [ 191 ] In June 2007, Michael Gorman , former president of the American Library Association , condemned Wikipedia, along with Google, stating that academics who endorse the use of Wikipedia are \"the intellectual equivalent of a dietitian who recommends a steady diet of Big Macs with everything\". [ 192 ] A 2020 research study published in Studies in Higher Education argued that Wikipedia could be applied in the higher education \" flipped classroom \", an educational model where students learn before coming to class and apply it in classroom activities. The experimental group was instructed to learn before class and get immediate feedback before going in (the flipped classroom model), while the control group was given direct instructions in class (the conventional classroom model). The groups were then instructed to collaboratively develop Wikipedia entries, which would be graded in quality after the study. The results showed that the experimental group yielded more Wikipedia entries and received higher grades in quality. The study concluded that learning with Wikipedia in flipped classrooms was more effective than in conventional classrooms, demonstrating Wikipedia could be used as an educational tool in higher education. [ 193 ] On March 5, 2014, Julie Beck writing for The Atlantic magazine in an article titled \"Doctors' #1 Source for Healthcare Information: Wikipedia\", stated that \"Fifty percent of physicians look up conditions on the (Wikipedia) site, and some are editing articles themselves to improve the quality of available information.\" [ 194 ] Beck continued to detail in this article new programs of Amin Azzam at the University of San Francisco to offer medical school courses to medical students for learning to edit and improve Wikipedia articles on health-related issues , as well as internal quality control programs within Wikipedia organized by James Heilman to improve a group of 200 health-related articles of central medical importance up to Wikipedia's highest standard of articles using its Featured Article and Good Article peer-review evaluation process. [ 194 ] In a May 7, 2014, follow-up article in The Atlantic titled \"Can Wikipedia Ever Be a Definitive Medical Text?\", Julie Beck quotes WikiProject Medicine's James Heilman as stating: \"Just because a reference is peer-reviewed doesn't mean it's a high-quality reference.\" [ 195 ] Beck added that: \"Wikipedia has its own peer review process before articles can be classified as 'good' or 'featured'. Heilman, who has participated in that process before, says 'less than one percent' of Wikipedia's medical articles have passed.\" [ 195 ] Wikipedia seeks to create a summary of all human knowledge in the form of an online encyclopedia, with each topic covered encyclopedically in one article. Since it has terabytes of disk space , it can have far more topics than can be covered by any printed encyclopedia. [ W 49 ] The exact degree and manner of coverage on Wikipedia is under constant review by its editors, and disagreements are not uncommon (see deletionism and inclusionism ). [ 196 ] [ 197 ] Wikipedia contains materials that some people may find objectionable, offensive, or pornographic. [ W 50 ] The \"Wikipedia is not censored\" policy has sometimes proved controversial: in 2008, Wikipedia rejected an online petition against the inclusion of images of Muhammad in the English edition of its Muhammad article, citing this policy. [ 198 ] The presence of politically, religiously, and pornographically sensitive materials in Wikipedia has led to the censorship of Wikipedia by national authorities in China [ 199 ] and Pakistan, [ 200 ] among other countries. [ 201 ] [ 202 ] [ 203 ] Through its \"Wikipedia Loves Libraries\" program, Wikipedia has partnered with major public libraries such as the New York Public Library for the Performing Arts to expand its coverage of underrepresented subjects and articles. [ 204 ] A 2011 study conducted by researchers at the University of Minnesota indicated that male and female editors focus on different coverage topics. There was a greater concentration of females in the \"people and arts\" category, while males focus more on \"geography and science\". [ 205 ] An editorial in The Guardian in 2014 claimed that more effort went into providing references for a list of female porn actors than a list of women writers . [ 206 ] Wikipedia's policies may limit \"its capacity for truly representing global knowledge\". For example, Wikipedia only considers published sources to be reliable. Oral knowledge of Indigenous cultures is not always reflected in print. Marginalized topics are also more likely to lack significant coverage in reliable sources. Wikipedia's content is therefore limited as a result of larger systemic biases. [ 207 ] Academic studies of Wikipedia have shown that the average contributor to the English Wikipedia is an educated, technically inclined white male, aged 15–49, from a developed, predominantly Christian country. [ 208 ] The corresponding point of view (POV) is over-represented. [ 209 ] [ 160 ] This systemic bias in editor demographic results in cultural bias , gender bias , and geographical bias on Wikipedia . [ 210 ] [ 211 ] There are two broad types of bias, which are implicit (when a topic is omitted) and explicit (when a certain POV is over-represented in an article or by references). [ 209 ] Interdisciplinary scholarly assessments of Wikipedia articles have found that while articles are typically accurate and free of misinformation, they are also typically incomplete and fail to present all perspectives with a neutral point of view . [ 210 ] In 2011, Wales claimed that the unevenness of coverage is a reflection of the demography of the editors, citing for example \"biographies of famous women through history and issues surrounding early childcare\". [ 35 ] The October 22, 2013, essay by Tom Simonite in MIT's Technology Review titled \"The Decline of Wikipedia\" discussed the effect of systemic bias and policy creep on the downward trend in the number of editors . [ 36 ] Research conducted by Mark Graham of the Oxford Internet Institute in 2009 indicated that the geographic distribution of article topics is highly uneven, with Africa being the most underrepresented. [ 212 ] Across 30 language editions of Wikipedia, historical articles and sections are generally Eurocentric and focused on recent events. [ 213 ] Wikipedia has been criticized for allowing information about graphic content. [ 214 ] Articles depicting what some critics have called objectionable content (such as feces , cadaver , human penis , vulva , and nudity) contain graphic pictures and detailed information easily available to anyone with access to the internet, including children. [ W 51 ] The site also includes sexual content such as images and videos of masturbation and ejaculation , illustrations of zoophilia , and photos from hardcore pornographic films in its articles. It also has non-sexual photographs of nude children . [ W 52 ] The Wikipedia article about Virgin Killer —a 1976 album from the German rock band Scorpions —features a picture of the album's original cover, which depicts a naked prepubescent girl. The original release cover caused controversy and was replaced in some countries. In December 2008, access to the Wikipedia article Virgin Killer was blocked for four days by most Internet service providers in the United Kingdom after the Internet Watch Foundation (IWF) decided the album cover was a potentially illegal indecent image and added the article's URL to a \"blacklist\" it supplies to British internet service providers. [ 215 ] In April 2010, Sanger wrote a letter to the Federal Bureau of Investigation, outlining his concerns that two categories of images on Wikimedia Commons contained child pornography, and were in violation of US federal obscenity law . [ 216 ] [ 217 ] Sanger later clarified that the images, which were related to pedophilia and one about lolicon , were not of real children, but said that they constituted \"obscene visual representations of the sexual abuse of children\", under the PROTECT Act of 2003 . [ 218 ] That law bans photographic child pornography and cartoon images and drawings of children that are obscene under American law . [ 218 ] Sanger also expressed concerns about access to the images on Wikipedia in schools. [ 219 ] Wikimedia Foundation spokesman Jay Walsh strongly rejected Sanger's accusation, [ 220 ] saying that Wikipedia did not have \"material we would deem to be illegal. If we did, we would remove it.\" [ 220 ] Following the complaint by Sanger, Wales deleted sexual images without consulting the community. After some editors who volunteered to maintain the site argued that the decision to delete had been made hastily, Wales voluntarily gave up some of the powers he had held up to that time as part of his co-founder status. He wrote in a message to the Wikimedia Foundation mailing-list that this action was \"in the interest of encouraging this discussion to be about real philosophical/content issues, rather than be about me and how quickly I acted\". [ 221 ] Critics, including Wikipediocracy , noticed that many of the pornographic images deleted from Wikipedia since 2010 have reappeared. [ 222 ] One privacy concern in the case of Wikipedia regards one's right to remain a private citizen rather than a public figure in the eyes of the law. [ 223 ] [ g ] It is a battle between the right to be anonymous in cyberspace and the right to be anonymous in real life . The Wikimedia Foundation's privacy policy states, \"we believe that you shouldn't have to provide personal information to participate in the free knowledge movement\", and states that \"personal information\" may be shared \"For legal reasons\", \"To Protect You, Ourselves & Others\", or \"To Understand & Experiment\". [ W 53 ] In January 2006, a German court ordered the German Wikipedia shut down within Germany because it stated the full name of Boris Floricic , aka \"Tron\", a deceased hacker. On February 9, 2006, the injunction against Wikimedia Deutschland was overturned, with the court rejecting the notion that Tron's right to privacy or that of his parents was being violated. [ 224 ] Wikipedia has a \" Volunteer Response Team \" that uses Znuny, a free and open-source software fork of OTRS [ W 54 ] to handle queries without having to reveal the identities of the involved parties. This is used, for example, in confirming the permission for using individual images and other media in the project. [ W 55 ] In late April 2023, Wikimedia Foundation announced that Wikipedia will not submit to any age verifications that may be required by the UK's Online Safety Bill legislation. Rebecca MacKinnon of the Wikimedia Foundation said that such checks would run counter to the website's commitment to minimal data collection on its contributors and readers. [ 225 ] Wikipedia was described in 2015 as harboring a battleground culture of sexism and harassment . [ 226 ] [ 227 ] The perceived tolerance of abusive language was a reason put forth in 2013 for the gender gap in Wikipedia editorship. [ 228 ] Edit-a-thons have been held to encourage female editors and increase the coverage of women's topics. [ 229 ] In May 2018, a Wikipedia editor rejected a submitted article about Donna Strickland due to lack of coverage in the media. [ W 56 ] [ 230 ] Five months later, Strickland won a Nobel Prize in Physics \"for groundbreaking inventions in the field of laser physics\", becoming the third woman to ever receive the award. [ 230 ] [ 231 ] Prior to winning the award, Strickland's only mention on Wikipedia was in the article about her collaborator and co-winner of the award Gérard Mourou . [ 230 ] Her exclusion from Wikipedia led to accusations of sexism, but Corinne Purtill writing for Quartz argued that \"it's also a pointed lesson in the hazards of gender bias in media, and of the broader consequences of underrepresentation.\" [ 232 ] Purtill attributes the issue to the gender bias in media coverage. [ 232 ] A comprehensive 2008 survey, published in 2016, by Julia B. Bear of Stony Brook University 's College of Business and Benjamin Collier of Carnegie Mellon University found significant gender differences in confidence in expertise, discomfort with editing, and response to critical feedback. \"Women reported less confidence in their expertise, expressed greater discomfort with editing (which typically involves conflict), and reported more negative responses to critical feedback compared to men.\" [ 233 ] Wikipedia is hosted and funded by the Wikimedia Foundation , a non-profit organization which also operates Wikipedia-related projects such as Wiktionary and Wikibooks . [ W 57 ] The foundation relies on public contributions and grants to fund its mission. [ 234 ] [ W 58 ] The foundation's 2020 Internal Revenue Service Form 990 shows revenue of $124.6 million and expenses of almost $112.2 million, with assets of about $191.2 million and liabilities of almost $11 million. [ W 59 ] In May 2014, Wikimedia Foundation named Lila Tretikov as its second executive director, taking over for Sue Gardner. [ W 60 ] The Wall Street Journal reported on May 1, 2014, that Tretikov's information technology background, from her years at University of California offers Wikipedia an opportunity to develop in more concentrated directions guided by her often repeated position statement that, \"Information, like air, wants to be free.\" [ 235 ] [ 236 ] The same Wall Street Journal article reported these directions of development according to an interview with spokesman Jay Walsh of Wikimedia, who \"said Tretikov would address that issue ( paid advocacy ) as a priority. 'We are really pushing toward more transparency ... We are reinforcing that paid advocacy is not welcome.' Initiatives to involve greater diversity of contributors, better mobile support of Wikipedia, new geo-location tools to find local content more easily, and more tools for users in the second and third world are also priorities\", Walsh said. [ 235 ] Following the departure of Tretikov from Wikipedia due to issues concerning the use of the \"superprotection\" feature which some language versions of Wikipedia have adopted, [ W 61 ] Katherine Maher became the third executive director of the Wikimedia Foundation in June 2016. [ W 62 ] Maher stated that one of her priorities would be the issue of editor harassment endemic to Wikipedia as identified by the Wikipedia board in December. She said to Bloomberg Businessweek regarding the harassment issue that: \"It establishes a sense within the community that this is a priority ... [and that correction requires that] it has to be more than words.\" [ 138 ] Maher served as executive director until April 2021. [ 237 ] Maryana Iskander was named the incoming CEO in September 2021, and took over that role in January 2022. She stated that one of her focuses would be increasing diversity in the Wikimedia community. [ 238 ] Wikipedia is also supported by many organizations and groups that are affiliated with the Wikimedia Foundation but independently-run, called Wikimedia movement affiliates . These include Wikimedia chapters (which are national or sub-national organizations, such as Wikimedia Deutschland and Wikimedia France), thematic organizations (such as Amical Wikimedia for the Catalan language community), and user groups. These affiliates participate in the promotion, development, and funding of Wikipedia. [ W 63 ] The operation of Wikipedia depends on MediaWiki , a custom-made, free and open source wiki software platform written in PHP and built upon the MySQL database system. [ W 64 ] The software incorporates programming features such as a macro language , variables , a transclusion system for templates , and URL redirection . [ W 65 ] MediaWiki is licensed under the GNU General Public License (GPL) and it is used by all Wikimedia projects, as well as many other wiki projects. [ W 64 ] [ W 66 ] Originally, Wikipedia ran on UseModWiki written in Perl by Clifford Adams (Phase I), which initially required CamelCase for article hyperlinks; the present double bracket style was incorporated later. [ W 67 ] Starting in January 2002 (Phase II), Wikipedia began running on a PHP wiki engine with a MySQL database; this software was custom-made for Wikipedia by Magnus Manske . The Phase II software was repeatedly modified to accommodate the exponentially increasing demand. In July 2002 (Phase III), Wikipedia shifted to the third-generation software, MediaWiki, originally written by Lee Daniel Crocker . Several MediaWiki extensions are installed to extend the functionality of the MediaWiki software. [ W 68 ] In April 2005, a Lucene extension [ W 69 ] [ W 70 ] was added to MediaWiki's built-in search and Wikipedia switched from MySQL to Lucene for searching. Lucene was later replaced by CirrusSearch which is based on Elasticsearch . [ W 71 ] In July 2013, after extensive beta testing, a WYSIWYG (What You See Is What You Get) extension, VisualEditor , was opened to public use. [ 239 ] [ 240 ] [ 241 ] It was met with much rejection and criticism, and was described as \"slow and buggy\". [ 242 ] The feature was changed from opt-out to opt-in afterward. [ W 72 ] Computer programs called bots have often been used to perform simple and repetitive tasks, such as correcting common misspellings and stylistic issues, or to start articles such as geography entries in a standard format from statistical data. [ W 73 ] [ 243 ] [ 244 ] One controversial contributor, Sverker Johansson , created articles with his bot Lsjbot , which was reported to create up to 10,000 articles on the Swedish Wikipedia on certain days. [ 245 ] Additionally, there are bots designed to automatically notify editors when they make common editing errors (such as unmatched quotes or unmatched parentheses). [ W 74 ] Edits falsely identified by bots as the work of a banned editor can be restored by other editors. An anti-vandal bot is programmed to detect and revert vandalism quickly. [ 243 ] Bots are able to indicate edits from particular accounts or IP address ranges, as occurred at the time of the shooting down of the MH17 jet in July 2014 when it was reported that edits were made via IPs controlled by the Russian government. [ 246 ] Bots on Wikipedia must be approved before activation. [ W 75 ] According to Andrew Lih , the current expansion of Wikipedia to millions of articles would be difficult to envision without the use of such bots. [ 247 ] As of 2021, [update] page requests are first passed to a front-end layer of Varnish caching servers and back-end layer caching is done by Apache Traffic Server . [ W 76 ] Requests that cannot be served from the Varnish cache are sent to load-balancing servers running the Linux Virtual Server software, which in turn pass them to one of the Apache web servers for page rendering from the database. [ W 76 ] The web servers deliver pages as requested, performing page rendering for all the language editions of Wikipedia. To increase speed further, rendered pages are cached in a distributed memory cache until invalidated, allowing page rendering to be skipped entirely for most common page accesses. [ 248 ] Wikipedia currently runs on dedicated clusters of Linux servers running the Debian operating system. [ W 77 ] By January 22, 2013, Wikipedia had migrated its primary data center to an Equinix facility in Ashburn, Virginia . [ W 78 ] [ 249 ] A second application data center was created in 2014 in Carrollton, Texas , to improve Wikipedia's reliability. [ 250 ] [ 251 ] Both datacenters work as the primary one, in alternate semesters, with the other one working as secondary datacenter. [ 252 ] In 2017, Wikipedia installed a caching cluster in an Equinix facility in Singapore , the first of its kind in Asia. [ W 79 ] In 2022, a caching data center was opened in Marseille , France. [ W 80 ] In 2024, a caching data center was opened in São Paulo , the first of its kind in South America. [ W 81 ] As of November 2024, [update] caching clusters are located in Amsterdam , San Francisco, Singapore, Marseille, and São Paulo. [ W 82 ] [ W 83 ] Following growing amounts of incoming donations in 2013 exceeding seven digits, [ 36 ] the Foundation has reached a threshold of assets which qualify its consideration under the principles of industrial organization economics to indicate the need for the re-investment of donations into the internal research and development of the Foundation. [ 253 ] Two projects of such internal research and development have been the creation of a Visual Editor and the \"Thank\" tab in the edit history, which were developed to improve issues of editor attrition. [ 36 ] [ 242 ] The estimates for reinvestment by industrial organizations into internal research and development was studied by Adam Jaffe , who recorded that the range of 4% to 25% annually was to be recommended, with high-end technology requiring the higher level of support for internal reinvestment. [ 254 ] At the 2013 level of contributions for Wikimedia presently documented as 45 million dollars, [ W 84 ] the computed budget level recommended by Jaffe for reinvestment into internal research and development is between 1.8 million and 11.3 million dollars annually. [ 254 ] In 2019, the level of contributions were reported by the Wikimedia Foundation as being at $120 million annually, [ W 85 ] updating the Jaffe estimates for the higher level of support to between $3.08 million and $19.2 million annually. [ 254 ] Multiple Wikimedia projects have internal news publications. Wikimedia 's online newspaper The Signpost was founded in 2005 by Michael Snow, a Wikipedia administrator who would join the Wikimedia Foundation's board of trustees in 2008. [ 255 ] [ 256 ] The publication covers news and events from the English Wikipedia, the Wikimedia Foundation, and Wikipedia's sister projects . [ W 86 ] Wikipedia editors sometimes struggle to access paywalled sources needed to improve a subject. [ 257 ] The Wikipedia Library is a resource for Wikipedia editors which provides free access to a wide range of digital publications , so that they can consult and cite these while editing the encyclopedia. [ 258 ] [ 259 ] Over 60 publishers have partnered with The Wikipedia Library to provide access to their resources: when ICE Publishing joined in 2020, a spokesman said \"By enabling free access to our content for Wikipedia editors, we hope to further the research community's resources – creating and updating Wikipedia entries on civil engineering which are read by thousands of monthly readers.\" [ 260 ] When the project was started in 2001, all text in Wikipedia was covered by the GNU Free Documentation License (GFDL), a copyleft license permitting the redistribution, creation of derivative works, and commercial use of content while authors retain copyright of their work. [ W 87 ] The GFDL was created for software manuals that come with free software programs licensed under the GPL . This made it a poor choice for a general reference work: for example, the GFDL requires the reprints of materials from Wikipedia to come with a full copy of the GFDL text. [ 261 ] In December 2002, the Creative Commons license was released; it was specifically designed for creative works in general, not just for software manuals. The Wikipedia project sought the switch to the Creative Commons. [ W 88 ] Because the GFDL and Creative Commons were incompatible, in November 2008, following the request of the project, the Free Software Foundation (FSF) released a new version of the GFDL designed specifically to allow Wikipedia to relicense its content to CC BY-SA by August 1, 2009. [ W 89 ] In April 2009, Wikipedia and its sister projects held a community-wide referendum which decided the switch in June 2009. [ W 90 ] [ W 91 ] [ W 92 ] [ W 93 ] The handling of media files (e.g. image files) varies across language editions. Some language editions, such as the English Wikipedia, include non-free image files under fair use doctrine, [ W 94 ] while the others have opted not to, in part because of the lack of fair use doctrines in their home countries (e.g. in Japanese copyright law ). Media files covered by free content licenses (e.g. Creative Commons ' CC BY-SA ) are shared across language editions via Wikimedia Commons repository, a project operated by the Wikimedia Foundation. [ W 95 ] Wikipedia's accommodation of varying international copyright laws regarding images has led some to observe that its photographic coverage of topics lags behind the quality of the encyclopedic text. [ 262 ] The Wikimedia Foundation is not a licensor of content on Wikipedia or its related projects but merely a hosting service for contributors to and licensors of Wikipedia, a position which was successfully defended in 2004 in a court in France. [ 263 ] [ 264 ] Because Wikipedia content is distributed under an open license, anyone can reuse or re-distribute it at no charge. [ W 96 ] The content of Wikipedia has been published in many forms, both online and offline, outside the Wikipedia website. Thousands of \" mirror sites \" exist that republish content from Wikipedia; two prominent ones that also include content from other reference sources are Reference.com and Answers.com . [ 265 ] [ 266 ] Another example is Wapedia , which began to display Wikipedia content in a mobile-device-friendly format before Wikipedia itself did. [ W 97 ] Some web search engines make special use of Wikipedia content when displaying search results: examples include Microsoft Bing (via technology gained from Powerset ) [ 267 ] and DuckDuckGo . Collections of Wikipedia articles have been published on optical discs . An English version released in 2006 contained about 2,000 articles. [ W 98 ] The Polish-language version from 2006 contains nearly 240,000 articles, [ W 99 ] the German-language version from 2007/2008 contains over 620,000 articles, [ W 100 ] and the Spanish-language version from 2011 contains 886,000 articles. [ W 101 ] Additionally, \"Wikipedia for Schools\", the Wikipedia series of CDs / DVDs produced by Wikipedia and SOS Children , is a free selection from Wikipedia designed for education towards children eight to seventeen. [ W 102 ] There have been efforts to put a select subset of Wikipedia's articles into printed book form. [ 268 ] [ W 103 ] Since 2009, tens of thousands of print-on-demand books that reproduced English, German, Russian, and French Wikipedia articles have been produced by the American company Books LLC and by three Mauritian subsidiaries of the German publisher VDM . [ 269 ] The website DBpedia , begun in 2007, extracts data from the infoboxes and category declarations of the English-language Wikipedia. [ 270 ] Wikimedia has created the Wikidata project with a similar objective of storing the basic facts from each page of Wikipedia and other Wikimedia Foundation projects and make it available in a queryable semantic format, RDF . [ W 104 ] As of February 2023, [update] it has over 101 million items. [ W 105 ] WikiReader is a dedicated reader device that contains an offline copy of Wikipedia, which was launched by OpenMoko and first released in 2009. [ W 106 ] Obtaining the full contents of Wikipedia for reuse presents challenges, since direct cloning via a web crawler is discouraged. [ W 107 ] Wikipedia publishes \" dumps \" of its contents, but these are text-only; as of 2023, [update] there is no dump available of Wikipedia's images. [ W 108 ] Wikimedia Enterprise is a for-profit solution to this. [ 271 ] Several languages of Wikipedia also maintain a reference desk, where volunteers answer questions from the general public. According to a study by Pnina Shachaf in the Journal of Documentation , the quality of the Wikipedia reference desk is comparable to a standard library reference desk , with an accuracy of 55 percent. [ 272 ] Wikipedia's original medium was for users to read and edit content using any standard web browser through a fixed Internet connection . Although Wikipedia content has been accessible through the mobile web since July 2013, The New York Times on February 9, 2014, quoted Erik Möller , deputy director of the Wikimedia Foundation, stating that the transition of internet traffic from desktops to mobile devices was significant and a cause for concern and worry. The article in The New York Times reported the comparison statistics for mobile edits stating that, \"Only 20 percent of the readership of the English-language Wikipedia comes via mobile devices, a figure substantially lower than the percentage of mobile traffic for other media sites, many of which approach 50 percent. And the shift to mobile editing has lagged even more.\" In 2014 The New York Times reported that Möller has assigned \"a team of 10 software developers focused on mobile\", out of a total of approximately 200 employees working at the Wikimedia Foundation. One principal concern cited by The New York Times for the \"worry\" is for Wikipedia to effectively address attrition issues with the number of editors which the online encyclopedia attracts to edit and maintain its content in a mobile access environment. [ 46 ] By 2023, the Wikimedia Foundation's staff had grown to over 700 employees. [ 1 ] Access to Wikipedia from mobile phones was possible as early as 2004, through the Wireless Application Protocol (WAP), via the Wapedia service. [ W 97 ] In June 2007, Wikipedia launched en.mobile.wikipedia.org, an official website for wireless devices. In 2009, a newer mobile service was officially released, located at en.m.wikipedia.org, which caters to more advanced mobile devices such as the iPhone , Android -based devices, or WebOS -based devices. [ W 109 ] Several other methods of mobile access to Wikipedia have emerged since. Many devices and applications optimize or enhance the display of Wikipedia content for mobile devices, while some also incorporate additional features such as use of Wikipedia metadata like geoinformation . [ 273 ] [ 274 ] The Android app for Wikipedia was released in January 2012, to over 500,000 installs and generally positive reviews, scoring over four of a possible five in a poll of approximately 200,000 users downloading from Google. [ W 110 ] [ W 111 ] The version for iOS was released on April 3, 2013, to similar reviews. [ W 112 ] Wikipedia Zero was an initiative of the Wikimedia Foundation to expand the reach of the encyclopedia to the developing countries by partnering with mobile operators to allow free access. [ W 113 ] [ 275 ] It was discontinued in February 2018 due to lack of participation from mobile operators. [ W 113 ] Andrew Lih and Andrew Brown both maintain editing Wikipedia with smartphones is difficult and this discourages new potential contributors. [ 276 ] [ 277 ] Lih states that the number of Wikipedia editors has been declining after several years, [ 276 ] and Tom Simonite of MIT Technology Review claims the bureaucratic structure and rules are a factor in this. Simonite alleges some Wikipedians use the labyrinthine rules and guidelines to dominate others and those editors have a vested interest in keeping the status quo. [ 36 ] Lih alleges there is a serious disagreement among existing contributors on how to resolve this. Lih fears for Wikipedia's long-term future while Brown fears problems with Wikipedia will remain and rival encyclopedias will not replace it. [ 276 ] [ 277 ] Access to Wikipedia has been blocked in mainland China since May 2015. [ 6 ] [ 278 ] [ 279 ] This was done after Wikipedia started to use HTTPS encryption, which made selective censorship more difficult. [ 280 ] In 2017–18, after a barrage of false news reports, both Facebook and YouTube announced they would rely on Wikipedia to help their users evaluate reports and reject false news. [ 281 ] [ 282 ] Noam Cohen , writing in The Washington Post states, \"YouTube's reliance on Wikipedia to set the record straight builds on the thinking of another fact-challenged platform, the Facebook social network, which announced last year that Wikipedia would help its users root out ' fake news '.\" [ 282 ] [ 283 ] In February 2014, The New York Times reported that Wikipedia was ranked fifth globally among all websites, stating \"With 18 billion page views and nearly 500 million unique visitors a month, ... Wikipedia trails just Yahoo, Facebook, Microsoft and Google, the largest with 1.2 billion unique visitors.\" [ 46 ] However, its ranking dropped to 13th globally by June 2020 due mostly to a rise in popularity of Chinese websites for online shopping. [ 59 ] The website has since recovered its ranking as of April 2022. [ 59 ] In addition to logistic growth in the number of its articles, [ W 114 ] Wikipedia has steadily gained status as a general reference website since its inception in 2001. [ 284 ] The number of readers of Wikipedia worldwide reached 365 million at the end of 2009. [ W 115 ] The Pew Internet and American Life project found that one third of US Internet users consulted Wikipedia. [ 285 ] In 2011, Business Insider gave Wikipedia a valuation of $4 billion if it ran advertisements. [ 286 ] According to \"Wikipedia Readership Survey 2011\", the average age of Wikipedia readers is 36, with a rough parity between genders. Almost half of Wikipedia readers visit the site more than five times a month, and a similar number of readers specifically look for Wikipedia in search engine results. About 47 percent of Wikipedia readers do not realize that Wikipedia is a non-profit organization. [ W 116 ] As of February 2023, [update] Wikipedia attracts around 2 billion unique devices monthly, with the English Wikipedia receiving 10 billion pageviews each month. [ W 1 ] During the COVID-19 pandemic , Wikipedia's coverage of the pandemic and fight against misinformation received international media attention, and brought an increase in Wikipedia readership overall. [ 287 ] [ 288 ] [ 289 ] [ 290 ] Noam Cohen wrote in Wired that Wikipedia's effort to combat misinformation related to the pandemic was different from other major websites, opining, \"Unless Twitter, Facebook and the others can learn to address misinformation more effectively, Wikipedia will remain the last best place on the Internet.\" [ 288 ] In October 2020, the World Health Organization announced they were freely licensing its infographics and other materials on Wikimedia projects. [ 291 ] There were nearly 7,000 COVID-19 related Wikipedia articles across 188 different Wikipedias, as of November 2021. [update] [ 292 ] [ 293 ] Wikipedia's content has also been used in academic studies, books, conferences, and court cases. [ W 117 ] [ 294 ] [ 295 ] The Parliament of Canada 's website refers to Wikipedia's article on same-sex marriage in the \"related links\" section of its \"further reading\" list for the Civil Marriage Act . [ 296 ] The encyclopedia's assertions are increasingly used as a source by organizations such as the US federal courts and the World Intellectual Property Organization [ 297 ] —though mainly for supporting information rather than information decisive to a case. [ 298 ] Content appearing on Wikipedia has also been cited as a source and referenced in some US intelligence agency reports. [ 299 ] In December 2008, the scientific journal RNA Biology launched a new section for descriptions of families of RNA molecules and requires authors who contribute to the section to also submit a draft article on the RNA family for publication in Wikipedia. [ 300 ] Wikipedia has also been used as a source in journalism, [ 301 ] [ 302 ] often without attribution, and several reporters have been dismissed for plagiarizing from Wikipedia . [ 303 ] [ 304 ] [ 305 ] [ 306 ] In 2006, Time magazine recognized Wikipedia's participation (along with YouTube, Reddit , MySpace , and Facebook) in the rapid growth of online collaboration and interaction by millions of people worldwide. [ 307 ] On September 16, 2007, The Washington Post reported that Wikipedia had become a focal point in the 2008 US election campaign , saying: \"Type a candidate's name into Google, and among the first results is a Wikipedia page, making those entries arguably as important as any ad in defining a candidate. Already, the presidential entries are being edited, dissected and debated countless times each day.\" [ 308 ] An October 2007 Reuters article, titled \"Wikipedia page the latest status symbol\", reported the recent phenomenon of how having a Wikipedia article vindicates one's notability. [ 309 ] One of the first times Wikipedia was involved in a governmental affair was on September 28, 2007, when Italian politician Franco Grillini raised a parliamentary question with the minister of cultural resources and activities about the necessity of freedom of panorama . He said that the lack of such freedom forced Wikipedia, \"the seventh most consulted website\", to forbid all images of modern Italian buildings and art, and claimed this was hugely damaging to tourist revenues. [ 310 ] A working group led by Peter Stone (formed as a part of the Stanford -based project One Hundred Year Study on Artificial Intelligence ) in its report called Wikipedia \"the best-known example of crowdsourcing ... that far exceeds traditionally-compiled information sources, such as encyclopedias and dictionaries, in scale and depth\". [ 311 ] [ 312 ] In a 2017 opinion piece for Wired , Hossein Derakhshan describes Wikipedia as \"one of the last remaining pillars of the open and decentralized web \" and contrasted its existence as a text-based source of knowledge with social media and social networking services , the latter having \"since colonized the web for television's values\". For Derakhshan, Wikipedia's goal as an encyclopedia represents the Age of Enlightenment tradition of rationality triumphing over emotions, a trend which he considers \"endangered\" due to the \"gradual shift from a typographic culture to a photographic one, which in turn mean[s] a shift from rationality to emotions, exposition to entertainment\". Rather than \" sapere aude \" ( lit. ' dare to know ' ), social networks have led to a culture of \"dare not to care to know\". This is while Wikipedia faces \"a more concerning problem\" than funding, namely \"a flattening growth rate in the number of contributors to the website\". Consequently, the challenge for Wikipedia and those who use it is to \"save Wikipedia and its promise of a free and open collection of all human knowledge amid the conquest of new and old television—how to collect and preserve knowledge when nobody cares to know.\" [ 313 ] Wikipedia has won many awards, receiving its first two major awards in May 2004. [ W 118 ] The first was a Golden Nica for Digital Communities of the annual Prix Ars Electronica contest; this came with a €10,000 (£6,588; $12,700) grant and an invitation to present at the PAE Cyberarts Festival in Austria later that year. The second was a Judges' Webby Award for the \"community\" category. [ 314 ] In September 2008, Wikipedia received Quadriga A Mission of Enlightenment award of Werkstatt Deutschland along with Boris Tadić , Eckart Höfling , and Peter Gabriel . The award was presented to Wales by David Weinberger . [ 315 ] In 2015, Wikipedia was awarded both the annual Erasmus Prize , which recognizes exceptional contributions to culture, society or social sciences, [ 316 ] and the Spanish Princess of Asturias Award on International Cooperation. [ 317 ] Speaking at the Asturian Parliament in Oviedo, the city that hosts the awards ceremony, Jimmy Wales praised the work of the Asturian Wikipedia users. [ 318 ] Comedian Stephen Colbert has parodied or referenced Wikipedia on numerous episodes of his show The Colbert Report and coined the related term wikiality , meaning \"together we can create a reality that we all agree on—the reality we just agreed on\". [ 185 ] Another example can be found in \"Wikipedia Celebrates 750 Years of American Independence\", a July 2006 front-page article in The Onion , [ 319 ] as well as the 2010 The Onion article \" 'L.A. Law' Wikipedia Page Viewed 874 Times Today\". [ 320 ] In an April 2007 episode of the American television comedy The Office , office manager ( Michael Scott ) is shown relying on a hypothetical Wikipedia article for information on negotiation tactics to assist him in negotiating lesser pay for an employee. [ 321 ] Viewers of the show tried to add the episode's mention of the page as a section of the actual Wikipedia article on negotiation, but this effort was prevented by other users on the article's talk page. [ 322 ] \" My Number One Doctor \", a 2007 episode of the television show Scrubs , played on the perception that Wikipedia is an unreliable reference tool with a scene in which Perry Cox reacts to a patient who says that a Wikipedia article indicates that the raw food diet reverses the effects of bone cancer by retorting that the same editor who wrote that article also wrote the Battlestar Galactica episode guide . [ 323 ] In 2008, the comedy website CollegeHumor produced a video sketch named \"Professor Wikipedia\", in which the fictitious Professor Wikipedia instructs a class with a medley of unverifiable and occasionally absurd statements. [ 324 ] The Dilbert comic strip from May 8, 2009, features a character supporting an improbable claim by saying \"Give me ten minutes and then check Wikipedia.\" [ 325 ] In July 2009, BBC Radio 4 broadcast a comedy series called Bigipedia , which was set on a website which was a parody of Wikipedia. [ 326 ] Some of the sketches were directly inspired by Wikipedia and its articles. [ 327 ] On August 23, 2013, the New Yorker website published a cartoon with this caption: \"Dammit, Manning, have you considered the pronoun war that this is going to start on your Wikipedia page?\" [ 328 ] The cartoon referred to Chelsea Elizabeth Manning (born Bradley Edward Manning), an American activist, politician, and former United States Army soldier who had recently come out as a trans woman . [ 329 ] In June 2024, nature.com published a fictional Wikipedia Talk page under the title \"Plastic-eating fungus caused doomsday\" by Emma Burnett. The Talk page concerned a fictional article describing the unintended consequences of the release of a plastic-eating fungus to clean up an oil spill. The article contained Talk page topics found on Wikipedia, like discussions of changes in the articles priority level. [ 330 ] The most obvious economic effect of Wikipedia has been the death of commercial encyclopedias, especially printed versions like Encyclopædia Britannica , which were unable to compete with a free alternative. [ 331 ] [ 332 ] [ 333 ] Nicholas Carr 's 2005 essay \"The amorality of Web 2.0 \" criticizes websites with user-generated content (like Wikipedia) for possibly leading to professional (and, in his view, superior) content producers' going out of business, because \"free trumps quality all the time\". Carr wrote, \"Implicit in the ecstatic visions of Web 2.0 is the hegemony of the amateur. I for one can't imagine anything more frightening.\" [ 334 ] Others dispute the notion that Wikipedia, or similar efforts, will entirely displace traditional publications. Chris Anderson , the former editor-in-chief of Wired , wrote in Nature that the \" wisdom of crowds \" approach of Wikipedia will not displace top scientific journals with rigorous peer review processes. [ 335 ] Wikipedia's influence on the biography publishing business has been a concern for some. Book publishing data tracker Nielsen BookScan stated in 2013 that biography sales were dropping \"far more sharply\". [ 336 ] Kathryn Hughes , professor of life writing at the University of East Anglia and author of two biographies wrote, \"The worry is that, if you can get all that information from Wikipedia, what's left for biography?\" [ 336 ] Wikipedia has been widely used as a corpus for linguistic research in computational linguistics , information retrieval and natural language processing . [ 337 ] [ 338 ] In particular, it commonly serves as a target knowledge base for the entity linking problem, which is then called \"wikification\", [ 339 ] and to the related problem of word-sense disambiguation . [ 340 ] Methods similar to wikification can in turn be used to find \"missing\" links in Wikipedia. [ 341 ] In 2015, French researchers José Lages of the University of Franche-Comté in Besançon and Dima Shepelyansky of Paul Sabatier University in Toulouse published a global university ranking based on Wikipedia scholarly citations. [ 342 ] [ 343 ] [ 344 ] They used PageRank , CheiRank and similar algorithms \"followed by the number of appearances in the 24 different language editions of Wikipedia (descending order) and the century in which they were founded (ascending order)\". [ 344 ] [ 345 ] The study was updated in 2019. [ 346 ] In December 2015, John Julius Norwich stated, in a letter published in The Times newspaper, that as a historian he resorted to Wikipedia \"at least a dozen times a day\", and had \"never caught it out\". He described it as \"a work of reference as useful as any in existence\", with so wide a range that it is almost impossible to find a person, place, or thing that it has left uncovered and that he could never have written his last two books without it. [ 347 ] A 2017 MIT study suggests that words used in Wikipedia articles end up in scientific publications. [ 348 ] Studies related to Wikipedia have been using machine learning and artificial intelligence [ 312 ] to support various operations. One of the most important areas is the automatic detection of vandalism [ 349 ] [ 350 ] and data quality assessment in Wikipedia. [ 351 ] [ 352 ] Several interactive multimedia encyclopedias incorporating entries written by the public existed long before Wikipedia was founded. The first of these was the 1986 BBC Domesday Project , which included text (entered on BBC Micro computers) and photographs from more than a million contributors in the UK, and covered the geography, art, and culture of the UK. This was the first interactive multimedia encyclopedia (and was also the first major multimedia document connected through internal links), with the majority of articles being accessible through an interactive map of the UK. The user interface and part of the content of the Domesday Project were emulated on a website until 2008. [ 353 ] Several free-content, collaborative encyclopedias were created around the same period as Wikipedia (e.g. Everything2 ), [ 354 ] with many later being merged into the project (e.g. GNE ). [ W 119 ] One of the most successful early online encyclopedias incorporating entries by the public was h2g2 , which was created by Douglas Adams in 1999. The h2g2 encyclopedia is relatively lighthearted, focusing on articles which are both witty and informative. [ 355 ] Subsequent collaborative knowledge websites have drawn inspiration from Wikipedia. Others use more traditional peer review , such as Encyclopedia of Life and the online wiki encyclopedias Scholarpedia and Citizendium . [ 356 ] [ 357 ] The latter was started by Sanger in an attempt to create a reliable alternative to Wikipedia. [ 358 ] [ 359 ] Both Sanger and Wales have given public interviews in late 2025 about their reflections about the status and state of Wikipedia leading up to its 25 years of operation on January 15, 2026; Wales appeared on the PBS television news show GZERO World interviewed by Ian Bremmer [ 360 ] and Sanger has appeared on the FOX news network interviewed by Ashley Rindsberg . [ 361 ] Wales's book The Seven Rules of Trust was published in October 2025 by Penguin Random House . It was described by the publisher as a \"sweeping reflection on the global crisis of credibility and knowledge\" with the book examining the \"rules of trust\" that enabled the growth and success of Wikipedia. [ 362 ]",
    "links": [
      "The Wall Street Journal",
      "Marshall Poe",
      "Democratization of knowledge",
      "Mário Soares",
      "Artificial intelligence",
      "Network effect",
      "Stanford",
      "Encyclopedia",
      "501(c)(3) organization",
      "American and British English spelling differences",
      "Word-sense disambiguation",
      "Online petition on Wikipedia Muhammad article",
      "Norwegian Wikipedia",
      "Antisemitism on Wikipedia",
      "Hoxne Hoard",
      "PLOS One",
      "List of Wikipedia controversies",
      "Richard Stallman",
      "Macro (computer science)",
      "Literal translation",
      "Rosie Stephenson-Goodknight",
      "Litigation involving the Wikimedia Foundation",
      "GAVI",
      "List of political editing incidents on Wikipedia",
      "Silesian Wikipedia",
      "Tree structure",
      "Reuters",
      "Internet access",
      "Hdl (identifier)",
      "Spanish Wikipedia",
      "Ejaculation",
      "Namuwiki",
      "Jim Giles (reporter)",
      "Joseph Reagle",
      "Wikipedia and fact-checking",
      "San Antonio Express-News",
      "Wikipedia coverage of Donald Trump",
      "Wikimedia Foundation",
      "Child nudity",
      "LocalWiki",
      "Conflict-of-interest editing on Wikipedia",
      "Pageview",
      "Free and open-source software",
      "Palo Alto Research Center",
      "Wiki software",
      "DuckDuckGo",
      "Wowpedia",
      "Mikhail Gorbachev",
      "List of Wikipedia mobile applications",
      "Penguin Random House",
      "Hoover Institution",
      "Scientology",
      "User-generated content",
      "Latvian Wikipedia",
      "Wikis and education",
      "Patricio Lorente",
      "Wikimedia project",
      "Same-sex marriage",
      "Hebrew Wikipedia",
      "Forbes",
      "Carrollton, Texas",
      "Eckart Höfling",
      "Business Insider",
      "Bibcode (identifier)",
      "University of Oxford",
      "Scorpions (band)",
      "Beresheet",
      "Knowledge Graph (Google)",
      "Deletion of articles on Wikipedia",
      "Cloud computing",
      "CiteSeerX (identifier)",
      "Peter Lang (publisher)",
      "MySpace",
      "Somaly Mam",
      "Fox News",
      "Mario Draghi",
      "University of Salamanca",
      "Danese Cooper",
      "Computational linguistics",
      "Bone cancer",
      "Crimean Tatar Wikipedia",
      "WikiStage",
      "Egyptian Arabic Wikipedia",
      "Southern Min Wikipedia",
      "WikiProfessional",
      "Left-wing politics",
      "Emma Bonino",
      "Expert",
      "Wapedia",
      "Wikipedia Zero",
      "Internet Archive",
      "Javanese Wikipedia",
      "Ifakara Health Institute",
      "L. Gordon Crovitz",
      "Counterproductive work behavior",
      "Apple Inc.",
      "Asteroid",
      "The Harvard Crimson",
      "Bigipedia",
      "Sexism",
      "Wikipedia bots",
      "Douglas Adams",
      "Afrikaans Wikipedia",
      "Instagram (identifier)",
      "Wikipedians",
      "List of Wikipedia mobile apps",
      "Wikimedia movement",
      "Larry Sanger",
      "Dartmouth College",
      "List of women writers",
      "Cadaver",
      "Public figure",
      "Tides Foundation",
      "History News Network",
      "Wikimedia Commons",
      "Springer Science+Business Media",
      "New York Public Library for the Performing Arts",
      "ArXiv (identifier)",
      "Relationship between Google and Wikipedia",
      "Bosnian Wikipedia",
      "German Wikipedia",
      "Citation needed (Wikipedia)",
      "ECIR",
      "Wikimedia Bangladesh",
      "Carlos Bandeirense Mirandópolis hoax",
      "Similarweb",
      "Arabic Wikipedia",
      "Copyleft",
      "Steven Pruitt",
      "Erasmus Programme",
      "Namespace",
      "Bowling Green State University",
      "Impersonator",
      "MarketWatch",
      "Internet censorship in China",
      "Elasticsearch",
      "Anti-elitism",
      "Wikimedia Ukraine",
      "Times Higher Education",
      "Internet Watch Foundation",
      "Bambara Wikipedia",
      "Luiz Inácio Lula da Silva",
      "Abraham Weintraub–Wikipedia controversy",
      "Jimmy Wales",
      "The Phoenix (newspaper)",
      "Bill & Melinda Gates Foundation",
      "United States Congress",
      "Taha Yasseri",
      "José López Portillo",
      "The Times",
      "Varnish (software)",
      "The Christian Science Monitor",
      "Association of American Law Schools",
      "Spamming",
      "Optical disc",
      "Vox (website)",
      "Data quality",
      "Notability in the English Wikipedia",
      "American Library Association",
      "Wetpaint",
      "Oscar van Dillen",
      "The New York Times",
      "WebOS",
      "Wikipedia in culture",
      "Esra'a Al Shafei",
      "Wikiloc",
      "List of films about Wikipedia",
      "International Space Station",
      "Polish Wikipedia",
      "Form 990",
      "Volapük Wikipedia",
      "Cebuano Wikipedia",
      "Philippines",
      "Swahili Wikipedia",
      "Copyright law of Japan",
      "Online encyclopedia",
      "Ukrainian Wikipedia",
      "Glyph",
      "PROTECT IP Act",
      "Age of Enlightenment",
      "Azerbaijani Wikipedia",
      "Kannada Wikipedia",
      "Metadata",
      "Martin M. Wattenberg",
      "Depths of Wikipedia",
      "WikiHow",
      "Loren Terveen",
      "MIT",
      "BBC Micro",
      "Guatemalan National Revolutionary Unity",
      "CollegeHumor",
      "Chiaki Mukai",
      "First Monday (journal)",
      "Wikimedia Deutschland",
      "Scientific journal",
      "Wayback Machine",
      "Kuro5hin",
      "Maithili Wikipedia",
      "Biographicon",
      "My Number One Doctor",
      "Yongle Encyclopedia",
      "Emily Flake",
      "Ideas (radio show)",
      "Hillsborough disaster Wikipedia posts",
      "United States obscenity law",
      "Michael Scott (The Office)",
      "ISSN (identifier)",
      "Columbia Journalism Review",
      "USA Today",
      "Engineering and Technology History Wiki",
      "Breton Wikipedia",
      "Syllabus",
      "Raw food diet",
      "Sapere aude",
      "ISBN (identifier)",
      "Vanderbilt University",
      "Pew Research Center",
      "Web traffic",
      "Stephen Colbert",
      "The Guardian",
      "Yahoo! Inc. (2017–present)",
      "Qiuwen Baike",
      "RationalWiki",
      "Free content",
      "Lee Daniel Crocker",
      "Irish Wikipedia",
      "Encyclopædia Britannica",
      "Hungarian Wikipedia",
      "PMC (identifier)",
      "Consensus decision-making",
      "NBC News",
      "Citizendium",
      "App Store (iOS/iPadOS)",
      "Smartphone",
      "Finnish Wikipedia",
      "Óscar Arias",
      "List of Battlestar Galactica (2004 TV series) episodes",
      "Open access",
      "CamelCase",
      "TCS Daily",
      "Muhammad",
      "Princess of Asturias Award",
      "Smithsonian Institution",
      "DavisWiki",
      "Common Knowledge? An Ethnography of Wikipedia",
      "Knowledge Engine (search engine)",
      "Hampton Lintorn-Catlin",
      "Katherine Maher",
      "János Kertész",
      "S2CID (identifier)",
      "Volunteer",
      "List of online encyclopedias",
      "Wikibooks",
      "Psychology Today",
      "China Digital Times",
      "List of medical wikis",
      "Oliver Kamm",
      "Chris Anderson (writer)",
      "The Journal of Academic Librarianship",
      "List of Wikimedia chapters",
      "Erasmus Prize",
      "HTTPS",
      "Classical Syriac Wikipedia",
      "Donna Strickland",
      "Sheizaf Rafaeli",
      "1Lib1Ref",
      "Wikipedia in India",
      "Wikiversity",
      "Fair use",
      "Berkman Center for Internet & Society",
      "Wikipedia and the Israeli–Palestinian conflict",
      "Gender role",
      "Occitan Wikipedia",
      "CNN",
      "The New Republic",
      "The Colbert Report",
      "Dutch Wikipedia",
      "Brown University",
      "Poppy Noor",
      "Semantic Web",
      "Internet service provider",
      "Hegemony",
      "COVID-19 misinformation",
      "Deutsche Welle",
      "Asturian Wikipedia",
      "Rey Juan Carlos University",
      "United States Intelligence Community",
      "Japanese Wikipedia",
      "Vice (magazine)",
      "Academia",
      "Nelson Mandela",
      "University of East Anglia",
      "São Paulo",
      "OCLC",
      "Encyclopedia of Life",
      "Paul Sabatier University",
      "Vandalism on Wikipedia",
      "Missing Links and Secret Histories",
      "Bomis",
      "Alexa Internet",
      "Henry Blodget",
      "Stacy Schiff",
      "WikiProject",
      "Viola angustifolia",
      "Fake news",
      "Negotiation",
      "Ned Kock",
      "Santali Wikipedia",
      "Facilitator",
      "Google Play",
      "Openmoko",
      "Wikimedia DC",
      "Wikimania",
      "British Comedy Guide",
      "Peter Gabriel",
      "Heinz Heise",
      "Fan wiki",
      "Cambridge University Press",
      "Javier Pérez de Cuéllar",
      "Ideological bias",
      "Diplopedia",
      "Hierarchy",
      "The Negotiation (The Office)",
      "Wikispecies",
      "Zhemao hoaxes",
      "Wikipedia coverage of death",
      "English Wikipedia",
      "List of most-visited websites",
      "The Economist",
      "Reporting of child pornography images on Wikimedia Commons",
      "Slate (magazine)",
      "OTRS",
      "Global South",
      "Wikipedia Review",
      "Government of Guatemala",
      "Ashley Rindsberg",
      "Academic studies about Wikipedia",
      "James Heilman",
      "John Willinsky",
      "Macedonian Wikipedia",
      "National Museum of Natural History",
      "Wikiality",
      "Ian Bremmer",
      "Brian Bergstein",
      "Quartz (publication)",
      "The Register",
      "CNET",
      "Science information on Wikipedia",
      "Internet bot",
      "Racial bias on Wikipedia",
      "Vietnamese Wikipedia",
      "Open protein structure annotation network",
      "Amy Bruckman",
      "Wikipedia coverage of the COVID-19 pandemic",
      "Chinese Wikipedia",
      "Ars Technica",
      "RNA Biology",
      "Bishakha Datta",
      "Machine learning",
      "Reference work",
      "Yiddish Wikipedia",
      "Wikimedia",
      "Wikimedia New York City",
      "Zoophilia",
      "Transclusion",
      "History of wikis",
      "Asian News International v. Wikimedia Foundation",
      "Yitzhak Rabin",
      "Creole (markup)",
      "Georgian Wikipedia",
      "Resource Description Framework",
      "Wikipedia coverage of American politics",
      "Wiley (publisher)",
      "Studies in Higher Education",
      "Estonian Wikipedia",
      "Tamil Wikipedia",
      "PBworks",
      "2021 Wikimedia Foundation actions on the Chinese Wikipedia",
      "Peter Stone (professor)",
      "Harvard University",
      "History of Wikipedia",
      "Wiki Loves Earth",
      "LGBTQ and Wikipedia",
      "Moon",
      "Anarchism",
      "Fast Company",
      "Text corpus",
      "Giant Bomb",
      "Slashdot",
      "Intellipedia",
      "Andrew Brown (writer)",
      "QRpedia",
      "Wiki Loves Pride",
      "Swedish Wikipedia",
      "Linux Virtual Server",
      "Stigmergy",
      "Bloomberg Businessweek",
      "PageRank",
      "Association for Computing Machinery",
      "Wikifonia",
      "Raju Narisetti",
      "Rfam",
      "Wikimedia Polska",
      "Insider Inc.",
      "Yale University",
      "Michael Gorman (librarian)",
      "List of wiki software",
      "Open collaboration",
      "Haaretz",
      "Lisa Seitz-Gruwell",
      "Coming out",
      "Kiwix",
      "New York (magazine)",
      "The Hidden Wiki",
      "Gender bias on Wikipedia",
      "List of books about Wikipedia",
      "University of Minnesota",
      "2022 Wikimedia Foundation actions against MENA Wikipedians",
      "Graça Machel",
      "Nicholas G. Carr",
      "Human penis",
      "Harvard Law School",
      "Ignore all rules",
      "Synthetic genomics",
      "Molly White (writer)",
      "Hans-Dietrich Genscher",
      "Andrew Lih",
      "Nepali Wikipedia",
      "Interpedia",
      "Fernando Henrique Cardoso",
      "The New York Review of Books",
      "Sexual content",
      "Equinix",
      "Korean Wikipedia",
      "Enciclopedia Libre",
      "Magna Carta (An Embroidery)",
      "Croatian Wikipedia",
      "Wiktionary",
      "Disk space",
      "Russian Wikipedia",
      "Creative Commons license",
      "Thai Wikipedia",
      "Mainland China",
      "Armenian Wikipedia",
      "Print Wikipedia",
      "Intelligent agent",
      "Camfed",
      "BBC Radio 4",
      "Tyler Cowen",
      "Gynopedia",
      "Wikimedia Foundation v. NSA",
      "Rigveda Wiki",
      "Wiki hosting service",
      "Typography",
      "Burmese Wikipedia",
      "Tatar Wikipedia",
      "Rigoberta Menchú",
      "John Seigenthaler",
      "MySQL",
      "Frederic M. Scherer",
      "Confluence (software)",
      "Apache Traffic Server",
      "The Daily Telegraph",
      "List of edit wars on Wikipedia",
      "Wikipedia for World Heritage",
      "Credential",
      "Print on demand",
      "DBpedia",
      "Nature.com",
      "Bernadette Meehan",
      "Wikipedia Star Trek Into Darkness debate",
      "Pacific Standard",
      "Peer review",
      "Bill Drayton",
      "Stanford University Press",
      "World Health Organization",
      "Hossein Derakhshan",
      "Criticism of Wikipedia",
      "Belisario Betancur",
      "Noam Cohen",
      "Art+Feminism",
      "List of Wikipedias",
      "Besançon",
      "Quakers",
      "Wikifunctions",
      "Ballotpedia",
      "Wikiracing",
      "Bulgarian Wikipedia",
      "Wiki",
      "Anti-social behavior",
      "University of Illinois at Urbana–Champaign",
      "JSTOR (identifier)",
      "Fandom (website)",
      "Simone Veil",
      "Registered user",
      "Wikiquote",
      "John Julius Norwich",
      "Boris Tadić",
      "Information retrieval",
      "Flipped classroom",
      "Moegirlpedia",
      "Transaction cost",
      "Greek Wikipedia",
      "Maryana Iskander",
      "Adam B. Jaffe",
      "Dariusz Jemielniak",
      "WikiConference India",
      "Luis Villa",
      "The Signpost",
      "Webby Award",
      "Meta Platforms",
      "Scholarpedia",
      "Czech Wikipedia",
      "Esperanto Wikipedia",
      "Blend word",
      "Mike Godwin",
      "Ed Chi",
      "Sindhi Wikipedia",
      "Atikamekw Wikipedia",
      "Peter Pirolli",
      "The Journal of American History",
      "United Nations Protection Force",
      "French Wikipedia",
      "Privacy",
      "Timeline of Wikipedia–U.S. government conflicts",
      "Eurocentric",
      "Danish Wikipedia",
      "Bengali Wikipedia",
      "Lila Tretikov",
      "Cyberspace",
      "Los Angeles Times",
      "Parliament of Canada",
      "PLOS",
      "Ruwiki (Wikipedia fork)",
      "Al Jazeera English",
      "MIT Technology Review",
      "Serbian Wikipedia",
      "CC BY-SA",
      "Reference desk",
      "Routledge",
      "Honolulu Star-Bulletin",
      "Rada Mihalcea",
      "Scots Wikipedia",
      "Citation needed",
      "Uncyclopedia",
      "Listen to Wikipedia",
      "Women in Red",
      "Answers.com",
      "Bhojpuri Wikipedia",
      "Elsevier",
      "Mirror site",
      "Ideological bias on Wikipedia",
      "Arnnon Geshuri",
      "UseModWiki",
      "International Journal of e-Collaboration",
      "New Scientist",
      "National Transplant Organization",
      "Church of Scientology editing on Wikipedia",
      "Max Planck Society",
      "British Newspaper Archive",
      "Powerset (company)",
      "Wikivoyage",
      "Belarusian Wikipedia",
      "Wireless Application Protocol",
      "Jar'Edo Wens hoax",
      "University of San Francisco",
      "Henryk Batuta hoax",
      "Chuvash Wikipedia",
      "Comparison of wiki hosting services",
      "Yasser Arafat",
      "COVID-19 pandemic",
      "First Wikipedia edit",
      "Urdu Wikipedia",
      "List of Wikipedia people",
      "Fulbright Program",
      "Sal Khan",
      "Wikipedia and the Russo-Ukrainian war",
      "Quadriga (award)",
      "VDM Publishing",
      "Table of contents",
      "PMID (identifier)",
      "Wikidata",
      "Magnus Manske",
      "Exponential growth",
      "Lsjbot",
      "Scottish Gaelic Wikipedia",
      "Openness",
      "274301 Wikipedia",
      "Administrators (Wikipedia)",
      "Wikimedia UK",
      "Raúl Alfonsín",
      "Rebecca MacKinnon",
      "Disputes on Wikipedia",
      "Wisdom of the crowd",
      "Web 2.0",
      "GNU Free Documentation License",
      "Electronic publishing",
      "New York Post",
      "Wikipedia Monument",
      "Croatia",
      "Online Safety Act 2023",
      "Arab News",
      "Aaron Halfaker",
      "Vector 2022",
      "CBC Radio One",
      "Wikipedia-based education",
      "ResearchGate",
      "Wiki Science Competition",
      "Real life",
      "List of wikis",
      "English language",
      "Microsoft Bing",
      "John Riedl",
      "Wikiwand",
      "Alemannic Wikipedia",
      "Edit count",
      "Menu bar",
      "Salt Lake City Weekly",
      "OCLC (identifier)",
      "Amin Azzam",
      "World Wide Web",
      "Odia Wikipedia",
      "Enrique V. Iglesias",
      "Stanford University",
      "BBC News",
      "Censorship of Wikipedia",
      "Internet traffic",
      "Carnegie Mellon University",
      "Smashing Magazine",
      "Deletionism and inclusionism in Wikipedia",
      "Albanian Wikipedia",
      "The Onion",
      "Scott Adams",
      "Berkman Klein Center for Internet & Society",
      "Wiki Indaba",
      "Ladin Wikipedia",
      "VisualEditor",
      "Marseille",
      "Logistic function",
      "Franco Grillini",
      "United States congressional staff edits to Wikipedia",
      "Ellen MacArthur",
      "Italian Wikipedia",
      "Geographic data and information",
      "Infographic",
      "EISSN (identifier)",
      "Book Drum",
      "Right to privacy",
      "Variable (programming)",
      "Article (publishing)",
      "The Seven Rules of Trust",
      "John Stossel",
      "Uzbek Wikipedia",
      "Welsh Wikipedia",
      "University of Coimbra",
      "United Nations Framework Convention on Climate Change",
      "WYSIWYG",
      "H2g2",
      "The New Yorker",
      "Kazakh Wikipedia",
      "Malayalam Wikipedia",
      "Farrar, Straus & Giroux",
      "Toulouse",
      "Health information on Wikipedia",
      "Virginia Postrel",
      "Maarten de Rijke",
      "Drugs for Neglected Diseases Initiative",
      ".wiki",
      "Amsterdam",
      "Latin Wikipedia",
      "TechCrunch",
      "Wikinews",
      "Jewish Telegraphic Agency",
      "Jason Moore (Wikipedia editor)",
      "Singapore",
      "Knowledge base",
      "WikiFactor",
      "Resistance Manual",
      "Cult",
      "Debian",
      "SSRN",
      "Hardcore pornography",
      "Portuguese Wikipedia",
      "Predictions of the end of Wikipedia",
      "Ripuarian Wikipedia",
      "The Economic Times",
      "Artificial intelligence in Wikimedia projects",
      "Dutch Low Saxon Wikipedia",
      "Sidebar (computing)",
      "Ashburn, Virginia",
      "Mashable",
      "Wolof Wikipedia",
      "Virgin Killer",
      "MediaWiki",
      "Ming dynasty",
      "Industrial organization",
      "Erik Möller",
      "Wikimedia movement affiliates",
      "Gary Stix",
      "Semantic wiki",
      "Cliff Lampe",
      "Bibliography of Wikipedia",
      "Perry Cox",
      "Harassment",
      "Freedom of panorama",
      "Spin (propaganda)",
      "Jacques Delors",
      "IP address",
      "Essjay controversy",
      "Helmut Kohl",
      "Wiki Loves Monuments",
      "Free software",
      "Wikimedian of the Year",
      "Fernanda B. Viégas",
      "NPR",
      "Space.com",
      "American Journalism Review",
      "List of Wiktionaries",
      "Masturbation",
      "Florence Devouard",
      "Wikipedian",
      "Wikisource",
      "Lunar lander",
      "OL (identifier)",
      "Kathryn Hughes",
      "Reddit",
      "Blog",
      "Social media",
      "BookScan",
      "Sample size determination",
      "MIT Press",
      "Persian Wikipedia",
      "European Physical Journal B",
      "Nature Machine Intelligence",
      "Linux",
      "Perl",
      "Tron (hacker)",
      "Wikipediocracy",
      "Lolicon",
      "Seigenthaler biography incident",
      "Neutrality (philosophy)",
      "2008 United States presidential election",
      "Samogitian Wikipedia",
      "Comparison of wiki software",
      "Catalan Wikipedia",
      "Abstract Wikipedia",
      "Cato Institute",
      "Wiki Loves Folklore",
      "Doi (identifier)",
      "Turkish Wikipedia",
      "International Herald Tribune",
      "Basque Wikipedia",
      "Wikirating",
      "Creative Commons",
      "SOS Children's Villages UK",
      "Ian H. Witten",
      "Valeri Polyakov",
      "One Hundred Year Study on Artificial Intelligence",
      "Guarani Wikipedia",
      "Civil Marriage Act",
      "Christopher Caldwell (journalist)",
      "Reliability of Wikipedia",
      "The Office (American TV series)",
      "Wiki rabbit hole",
      "Balinese Wikipedia",
      "Web template system",
      "Chelsea Manning",
      "Andrew Dalby",
      "Telugu Wikipedia",
      "Wikipedia community",
      "Second-class citizen",
      "Wikipedia and the COVID-19 pandemic",
      "Stephen Harrison (author)",
      "Organization of Ibero-American States",
      "Personal wiki",
      "Nobel Prize in Physics",
      "BusinessWeek",
      "World Intellectual Property Organization",
      "TNW (website)",
      "Wikimedia Israel",
      "San Francisco",
      "Communism",
      "Wikiprogress",
      "Artist",
      "Catalan language",
      "Pedro Duque",
      "Khan Academy",
      "Entity linking",
      "WikiTribune",
      "WikiConference North America",
      "Malaysia Airlines Flight 17",
      "Aragonese Wikipedia",
      "Sanskrit Wikipedia",
      "Amref Health Africa",
      "Wikipedia – The Missing Manual",
      "Marathi Wikipedia",
      "Janae Bakken",
      "Confidence interval",
      "Słubice",
      "Chalo Chatu",
      "Hachette Books",
      "IPhone",
      "Media in New York's Capital District",
      "Reference.com",
      "United Nations High Commissioner for Refugees",
      "Punjabi Wikipedia",
      "Nupedia",
      "PHP",
      "Waray Wikipedia",
      "Politics of Italy",
      "Oxford Internet Institute",
      "Sue Gardner",
      "Socialism",
      "Aaron Swartz",
      "Wikipedian in residence",
      "Private citizen",
      "Indonesian Wikipedia",
      "PC World",
      "List of pornographic performers by decade",
      "Social Science Research Network",
      "David Weinberger",
      "ICE Publishing",
      "Trans woman",
      "The Cutting Room Floor (website)",
      "Government by algorithm",
      "Dilbert",
      "Breaking news",
      "Scientific American",
      "Northern Sami Wikipedia",
      "Paris Agreement",
      "Outline of Wikipedia",
      "Olayinka Koso-Thomas",
      "First Amendment Center",
      "Wikipedia logo",
      "Plagiarism from Wikipedia",
      "Communications of the ACM",
      "Writing system",
      "Free Software Foundation",
      "Web browser",
      "Android (operating system)",
      "Life writing",
      "Twitter",
      "9/11 conspiracy theories",
      "The Sydney Morning Herald",
      "Internet troll",
      "Conservapedia",
      "University of Franche-Comté",
      "Education Next",
      "Edit-a-thon",
      "Kurdish Wikipedia",
      "Slovak Wikipedia",
      "Web portal",
      "TED talk",
      "The San Diego Union-Tribune",
      "Today (American TV program)",
      "Accountability",
      "Stony Brook University",
      "Tulu Wikipedia",
      "Guy Kawasaki",
      "Web crawler",
      "Full Measure with Sharyl Attkisson",
      "Hindi Wikipedia",
      "PCMag",
      "GNU General Public License",
      "Scientific Committee on Antarctic Research",
      "Natural language processing",
      "George W. Bush",
      "Internet Watch Foundation and Wikipedia",
      "List of LocalWikis",
      "Lucene",
      "Everything2",
      "GESIS – Leibniz Institute for the Social Sciences",
      "LifeWiki",
      "WikiArt",
      "Feces",
      "Tagalog Wikipedia",
      "Comscore",
      "Ossetian Wikipedia",
      "Al Gore",
      "Cartogram",
      "Konkani Wikipedia",
      "TV Tropes",
      "Assassination of John F. Kennedy",
      "Stop Online Piracy Act",
      "Massachusetts Institute of Technology",
      "Script (comics)",
      "Sky-Map.org",
      "GNE (encyclopedia)",
      "Fatiha Boudiaf",
      "The Cult of the Amateur",
      "Zulu Wikipedia",
      "Utility",
      "Scrubs (TV series)",
      "Time (magazine)",
      "Extremaduran Wikipedia",
      "Vulva",
      "Cantonese Wikipedia",
      "Kat Walsh",
      "F. W. de Klerk",
      "List of fan wikis",
      "Nicholson Baker",
      "Gérard Mourou",
      "Primary source",
      "United Media",
      "British Museum",
      "Library of Congress",
      "Geo-Wiki",
      "Arbitration Committee (Wikipedia)",
      "Protests against SOPA and PIPA",
      "Prix Ars Electronica",
      "John Glenn",
      "Alan MacMasters hoax",
      "AfroCrowd",
      "Annie Rauwerda",
      "Family History Research Wiki",
      "Oral knowledge",
      "BBC Domesday Project",
      "Columbian College of Arts and Sciences",
      "Contadora group",
      "Pedophilia",
      "Roy Rosenzweig",
      "Gene Wiki",
      "Monkey selfie copyright dispute",
      "Wikimedia Enterprise",
      "PhpWiki",
      "Computer cluster",
      "New York University",
      "Generative AI",
      "Search engine",
      "Nature (journal)",
      "Privacy policy",
      "Mihran Hakobyan",
      "Paul Kennedy (host)",
      "NBC",
      "Assamese Wikipedia",
      "Wayuu Wikipedia",
      "Rationality",
      "Andrew Keen",
      "María Sefidari",
      "Princess of Asturias Awards",
      "University of California, Berkeley",
      "Journal of Documentation",
      "Geographical bias on Wikipedia",
      "Andrew Orlowski",
      "The Washington Post",
      "Financial Times",
      "Hispanic Society of America",
      "Sverker Johansson",
      "Wired (magazine)",
      "Frankfurt am Main",
      "The Atlantic",
      "Caching",
      "Houston Chronicle",
      "Wikipedia Seigenthaler biography incident",
      "Perennial sources list",
      "International Red Cross and Red Crescent Movement",
      "Books LLC",
      "Galician Wikipedia",
      "Wikipedia administrators",
      "Mobile web",
      "WikiReader",
      "Kashmiri Wikipedia",
      "Wikipedia (disambiguation)",
      "URL redirection",
      "Cultural bias",
      "SSRN (identifier)",
      "IBM Research",
      "Simple English Wikipedia",
      "Serbo-Croatian Wikipedia",
      "CheiRank",
      "Slovene Wikipedia",
      "List of people imprisoned for editing Wikipedia",
      "Chartered Institute of Linguists",
      "Romanian Wikipedia",
      "Israeli–Palestinian conflict",
      "David Gerard (author)",
      "Wikipedia philosophy phenomenon",
      "Clay Shirky",
      "Edwin Black",
      "Social networking service",
      "Canadian Broadcasting Corporation",
      "Chechen Wikipedia",
      "Systemic bias",
      "The Chronicle of Higher Education",
      "Wikistrat",
      "Encyclopedia Dramatica",
      "Operation Orangemoody"
    ]
  },
  "MEDLARS": {
    "url": "https://en.wikipedia.org/wiki/MEDLARS",
    "title": "MEDLARS",
    "content": "MEDLINE (Medical Literature Analysis and Retrieval System Online, or MEDLARS Online) is a bibliographic database of life sciences and biomedical information. It includes bibliographic information for articles from academic journals covering medicine , nursing , pharmacy , dentistry , veterinary medicine , and health care . MEDLINE also covers much of the literature in biology and biochemistry , as well as fields such as molecular evolution . Compiled by the United States National Library of Medicine (NLM), MEDLINE is freely available on the Internet and searchable via PubMed and NLM's National Center for Biotechnology Information's Entrez system. MEDLARS (Medical Literature Analysis and Retrieval System) was a computerised biomedical bibliographic retrieval system. It was launched by the National Library of Medicine in 1964 and was the first large-scale, computer-based, retrospective search service available to the general public. [ 1 ] Since 1879, the National Library of Medicine has published Index Medicus , a monthly guide to medical articles in thousands of journals. The huge volume of bibliographic citations was manually compiled. In 1957 the staff of the NLM started to plan the mechanization of the Index Medicus , prompted by a desire for a better way to manipulate all this information, not only for Index Medicus but also to produce subsidiary products. By 1960 a detailed specification was prepared, and by the spring of 1961, request for proposals were sent out to 72 companies to develop the system. As a result, a contract was awarded to the General Electric Company . A Minneapolis-Honeywell 800 computer, which was to run MEDLARS, was delivered to the NLM in March 1963, and Frank Bradway Rogers (Director of the NLM 1949 to 1963) said at the time, \"... If all goes well, the January 1964 issue of Index Medicus will be ready to emerge from the system at the end of this year. It may be that this will mark the beginning of a new era in medical bibliography.\" MEDLARS cost $3 million to develop, and at the time of its completion in 1964, no other publicly available, fully operational electronic storage and retrieval system of its magnitude existed. The original computer configuration operated from 1964 until its replacement by MEDLARS II in January 1975. [ 2 ] [ 3 ] [ 4 ] In late 1971, an online version called MEDLINE (MEDLARS Online) became available as a way to do online searching of MEDLARS from remote medical libraries. [ 5 ] This early system covered 239 journals and boasted that it could support as many as 25 simultaneous online users (remotely logged in from distant medical libraries) at one time. [ 6 ] However, this system remained primarily in the hands of libraries, with researchers able to submit pre-programmed search tasks to librarians and obtain results on printouts, but rarely able to interact with the NLM computer output in real-time. This situation continued through the beginning of the 1990s and the rise of the World Wide Web . In 1996, soon after most home computers began automatically bundling efficient web browsers , a free public version of MEDLINE was deployed. This system, called PubMed , was offered to the general online user in June 1997, when MEDLINE searches via the Web were demonstrated. [ 6 ] In December 2024, the database contained more than 38 million records [ 7 ] from over 5,200 selected publications [ 8 ] covering biomedicine and health from 1781 to the present. Originally, the database covered articles starting from 1965, but this has been enhanced, and records as far back as 1781 are now available within the main index. The database is freely accessible on the Internet via the PubMed interface, and new citations are added Tuesday through Saturday. For citations added during 1995–2003, about 48% are for cited articles published in the U.S., about 88% are published in English (overall about 84% [ 9 ] ), and about 76% have English abstracts written by authors of the articles. Being an aggregated source, the PubMed database suffers from multi-source problems such as inconsistent representations from the upstream data providers. [ 9 ] MEDLINE uses Medical Subject Headings (MeSH) for information retrieval. Engines designed to search MEDLINE (such as Entrez and PubMed) generally use a Boolean expression combining MeSH terms, words in the abstract, and fields such as article title, author names, and publication date. Entrez and PubMed can also find articles similar to a given one based on a mathematical scoring system that takes into account the similarity of word content of the abstracts and titles of two articles. [ 10 ] MEDLINE added a publication type term for \"randomized controlled trial\" in 1991 and a MeSH subset systematic review in 2001. [ 11 ] MEDLINE functions as an important resource for biomedical researchers and journal clubs from all over the world. Along with the Cochrane Library and a number of other databases, MEDLINE facilitates evidence-based medicine . [ 12 ] [ 13 ] [ 14 ] Most systematic review articles published presently build on extensive searches of MEDLINE to identify articles that might be useful in the review. [ 12 ] [ 13 ] MEDLINE influences researchers in their choice of journals in which to publish. [ 14 ] More than 5,200 biomedical journals are indexed in MEDLINE. [ 12 ] New journals are not included automatically or immediately. Several criteria for selection are applied. [ 15 ] Selection is based on the recommendations of a panel, the Literature Selection Technical Review Committee, based on the scientific scope and quality of a journal. [ 16 ] The Journals Database (one of the Entrez databases) contains information, such as its name abbreviation and publisher, about all journals included in Entrez, including PubMed. [ 17 ] Journals that no longer meet the criteria are removed. [ 18 ] Being indexed in MEDLINE gives a non-predatory identity to a journal. [ 19 ] [ 20 ] [ 21 ] PubMed usage has been on the rise since 2008. In 2011, PubMed/MEDLINE was searched 1.8 billion times, up from 1.6 billion searches in the previous year. [ 22 ] In 2023, the database was searched 3.66 billion times. [ 8 ] A service such as MEDLINE strives to balance usability with power and comprehensiveness. In keeping with the fact that MEDLINE's primary user community is professionals ( medical scientists and health care providers ), searching MEDLINE effectively is a learned skill; untrained users are sometimes frustrated with the large numbers of articles returned by simple searches. Counterintuitively, a search that returns thousands of articles is not guaranteed to be comprehensive. Unlike using a typical Internet search engine, PubMed searching MEDLINE requires a little investment of time. Using the MeSH database to define the subject of interest is one of the most useful ways to improve the quality of a search. Using MeSH terms in conjunction with limits (such as publication date or publication type), qualifiers (such as adverse effects or prevention and control), and text-word searching is another. Finding one article on the subject and clicking on the \"Related Articles\" link to get a collection of similarly classified articles can expand a search that otherwise yields few results. For non-expert users who are trying to learn about health and medicine topics, the NIH offers MedlinePlus ; thus, although such users are still free to search and read the medical literature themselves (via PubMed ), they also have some help with curating it into something comprehensible and practically applicable for patients and family members.",
    "links": [
      "Behavioral science",
      "Molecular biology",
      "Cochrane Library",
      "Entrez",
      "PubMed",
      "Healthcare science",
      "Nursing",
      "Doi (identifier)",
      "Toxicology",
      "World Wide Web",
      "Biomedicine",
      "Medicine",
      "Health care",
      "Chemical science",
      "Molecular evolution",
      "Content curation",
      "Environmental health",
      "Medscape",
      "Wayback Machine",
      "HubMed",
      "Marine biology",
      "ISSN (identifier)",
      "AIDS",
      "Biochemistry",
      "MedlinePlus",
      "Journalology",
      "Bioengineering",
      "ETBLAST",
      "Animal science",
      "United States National Library of Medicine",
      "Journal club",
      "Dentistry",
      "Usability",
      "ISBN (identifier)",
      "Evidence-based medicine",
      "Bibliographic database",
      "Veterinary medicine",
      "National Library of Medicine",
      "Systematic review",
      "Frank Bradway Rogers",
      "LILACS",
      "Pharmacy",
      "General Electric Company",
      "Biology",
      "Web browser",
      "PMID (identifier)",
      "Honeywell 800",
      "Internet",
      "PMC (identifier)",
      "Academic journal",
      "Request for proposal",
      "Bibliography",
      "Complementary medicine",
      "Health care provider",
      "Index Medicus",
      "Plant science",
      "Medical Subject Headings",
      "Health policy",
      "Predatory publishing",
      "Boolean expression",
      "Altbib",
      "Biophysics"
    ]
  },
  "Information architecture": {
    "url": "https://en.wikipedia.org/wiki/Information_architecture",
    "title": "Information architecture",
    "content": "Information architecture is the structural design of shared information environments, in particular the organisation of websites and software to support usability and findability. The term information architecture was coined by Richard Saul Wurman . [ citation needed ] Since its inception, information architecture has become an emerging community of practice focused on applying principles of design , architecture and information science in digital spaces. [ 1 ] Typically, a model or concept of information is used and applied to activities which require explicit details of complex information systems . These activities include library systems and database development. [ citation needed ] The term information architecture has different meanings in different branches of information systems or information technology . [ 2 ] In user experience design , information architecture has been described as the structural design of shared information environments, [ 3 ] : 4 comprising the study and practice of organising and labelling web sites, intranets, online communities, and software to support user experience , in particular, the findability and usability of information. [ 1 ] [ 4 ] [ 5 ] It has also been described as an emerging community of practice focused on bringing principles of design and architecture to the digital landscape. [ 3 ] : 4 [ 6 ] Technically speaking, information architecture comprises the combination of organization, labeling, search and navigation systems within websites and intranets, [ 3 ] : 4 serving as a navigational aid to the content of information-rich systems. [ 7 ] Information architecture can be described as a subset of data architecture where usable data is constructed, designed, and arranged in a fashion most useful to the users of data. [ citation needed ] In the field of systems design , for example, information architecture is a component of enterprise architecture that deals with the information component when describing the structure of an enterprise. [ 2 ] Some system design practitioners regard information architecture as strictly the application of information science to web design , which considers such issues as classification and information retrieval , and not factors like user experience and information design . [ 2 ] Principles of information architecture include the following: [ 8 ] [ 9 ] Richard Saul Wurman is credited with coining the term information architecture in relation to the design of information. [ citation needed ] From 1998 to 2015, Peter Morville and Louis Rosenfeld were co-authors of Information Architecture for the World Wide Web. [ citation needed ] Other authors include Jesse James Garrett and Christina Wodtke . [ citation needed ]",
    "links": [
      "Internationalized Resource Identifier",
      "Information access",
      "Categorization",
      "Findability",
      "Doi (identifier)",
      "Data presentation architecture",
      "HCalendar",
      "TriG (syntax)",
      "Semantic HTML",
      "Lou Rosenfeld",
      "Web engineering",
      "Facebook Platform",
      "Semantic Web Rule Language",
      "Website",
      "Concept",
      "Bibliographic Ontology",
      "Knowledge organization",
      "Information retrieval",
      "Microformat",
      "Information seeking",
      "Semantic service-oriented architecture",
      "Interaction design",
      "HRecipe",
      "Semantics (computer science)",
      "Wayfinding",
      "Science and technology studies",
      "COinS",
      "Memory",
      "User experience design",
      "SHACL",
      "Enterprise information security architecture",
      "Privacy",
      "GRDDL",
      "Data modeling",
      "Applications architecture",
      "Semantic Web",
      "Preservation (library and archival science)",
      "Information technology",
      "Data management",
      "Peter Morville",
      "Content strategy",
      "Semantic reasoner",
      "SPARQL",
      "Information society",
      "Scientific modelling",
      "Semantic matching",
      "Knowledge representation and reasoning",
      "Web literacy",
      "Taxonomy",
      "Semantically Interlinked Online Communities",
      "Semantic computing",
      "Hypertext",
      "Digital humanities",
      "Database",
      "Architecture",
      "HAtom",
      "Rule-based system",
      "Censorship",
      "Process architecture",
      "Outline of information science",
      "HProduct",
      "Content management",
      "Usability",
      "Human factors and ergonomics",
      "Library",
      "Semantic analytics",
      "Information science",
      "RDF Schema",
      "Controlled vocabulary",
      "Dublin Core",
      "Christina Wodtke",
      "Geotagging",
      "Tree testing",
      "IXBRL",
      "Linked data",
      "Card sorting",
      "Simple Knowledge Organization System",
      "Web Ontology Language",
      "Web 2.0",
      "Computer data storage",
      "Dataspaces",
      "Richard Saul Wurman",
      "Library classification",
      "Design",
      "Jesse James Garrett",
      "Notation3",
      "Collective intelligence",
      "World Wide Web",
      "Resource Description Framework",
      "Community of practice",
      "Metadata Object Description Schema",
      "Ecological interface design",
      "Description logic",
      "Information behavior",
      "RDF/XML",
      "Topic map",
      "Web design",
      "Semantic mapper",
      "HTTP",
      "Semantic publishing",
      "BIBFRAME",
      "RDFa",
      "SAWSDL",
      "HCard",
      "Research Resource Identifier",
      "Microdata (HTML)",
      "Social information architecture",
      "Internet",
      "Metadata Authority Description Schema",
      "Chief experience officer",
      "Schema.org",
      "Quantum information science",
      "Philosophy of information",
      "XML",
      "HReview",
      "File format",
      "N-Triples",
      "Information design",
      "Uniform Resource Identifier",
      "Library and information science",
      "Library 2.0",
      "JSON-LD",
      "Intellectual property",
      "Cultural studies",
      "Metadata",
      "Knowledge management",
      "Morgan Kaufmann",
      "Semantic network",
      "Louis Rosenfeld",
      "Knowledge visualization",
      "User experience",
      "Data architecture",
      "Knowledge extraction",
      "Informatics",
      "Rule Interchange Format",
      "Systems design",
      "DOAP",
      "Semantic search",
      "Software",
      "Folksonomy",
      "Semantic triple",
      "TriX (serialization format)",
      "ISSN (identifier)",
      "FOAF",
      "Semantic wiki",
      "Digital library",
      "ISBN (identifier)",
      "Enterprise architecture",
      "Elaine G. Toms",
      "Reference (computer science)",
      "Intellectual freedom",
      "Web Science Trust",
      "Bibliometrics",
      "Solid (web decentralization project)",
      "Site map",
      "Classification",
      "Common Logic",
      "Ontology (information science)",
      "Hyperdata",
      "Information",
      "Turtle (syntax)",
      "Web graph",
      "Faceted classification",
      "Information management",
      "Information system"
    ]
  },
  "Robert R. Korfhage": {
    "url": "https://en.wikipedia.org/wiki/Robert_R._Korfhage",
    "title": "Robert R. Korfhage",
    "content": "Robert Roy Korfhage (December 2, 1930 – November 20, 1998) was an American computer scientist, famous for his contributions to information retrieval and several textbooks. He was son of Dr. Roy Korfhage who was a chemist at Nestlé in Fulton, Oswego County, New York . Korfhage earned his bachelor's degree (1952) in engineering mathematics at University of Michigan , while working part-time at United Aircraft and Transport Corporation in East Hartford as programmer . At the same university, he earned a master's degree and Ph.D. (1962) in mathematics , his PhD dissertation being On Systems of Distinct Representatives for Several Collections of Sets advised by Bernard Galler (1962). Korfhage taught mathematics at North Carolina State University (1962–64), Purdue University (1964–70), [ 1 ] Southern Methodist University (1970–86) and the University of Pittsburgh School of Information Sciences (1986–98). Korfhage's research focused on graph theory and information retrieval . For instance, his Information Storage and Retrieval (1997) was winner of American Society for Information Science and Technology Best information science book award (1998). [ 2 ] In his later years, he worked on new ways of information visualization and also genetic algorithms to optimize text queries. He died of cancer in Pittsburgh . This biographical article relating to an American computer scientist is a stub . You can help Wikipedia by expanding it .",
    "links": [
      "Cancer",
      "American Society for Information Science and Technology",
      "East Hartford",
      "Programmer",
      "United Aircraft and Transport Corporation",
      "Justin Jesse Price",
      "Pittsburgh",
      "Harley Flanders",
      "Purdue University",
      "Information retrieval",
      "Fulton, Oswego County, New York",
      "Bernard Galler",
      "Nestlé",
      "Information visualization",
      "Academic Press",
      "University of Michigan",
      "Graph theory",
      "Doctoral advisor",
      "Computer scientist",
      "North Carolina State University",
      "Ph.D.",
      "Genetic algorithm",
      "University of Pittsburgh School of Information Sciences",
      "Southern Methodist University",
      "John Wiley & Sons",
      "Norman E. Gibbs",
      "Mathematics"
    ]
  },
  "Co-occurrence": {
    "url": "https://en.wikipedia.org/wiki/Co-occurrence",
    "title": "Co-occurrence",
    "content": "In linguistics, co-occurrence or cooccurrence (in older texts often shown with diacritic as coöccurrence ) is an above-chance frequency of ordered occurrence of two adjacent terms in a text corpus . Co-occurrence in this linguistic sense can be interpreted as an indicator of semantic proximity or an idiomatic expression. Corpus linguistics and its statistical analyses can reveal (regularity of) patterns of co-occurrences within a language and enable the working out of typical collocations for its lexical items. A co-occurrence restriction is identified when linguistic elements never occur together. Analysis of these restrictions can lead to discoveries about the structure and development of a language. [ 1 ] Co-occurrence can be seen an extension of word counting in higher dimensions. Co-occurrence can be quantitatively described using measures like a massive correlation or mutual information . Co-occurrence information and knowledge of co-occurring words may be relevant in analysis of language for the purposes of large language models , part of the emerging field of artificial intelligence , and helpful in word games such as scrabble . This corpus linguistics -related article is a stub . You can help Wikipedia by expanding it .",
    "links": [
      "Idiom",
      "Co-occurrence networks",
      "Distributional hypothesis",
      "Word",
      "Word count",
      "Artificial intelligence",
      "Dice coefficient",
      "Linguistics",
      "Word game",
      "Occurrence (type–token distinction)",
      "Correlation",
      "Statistical semantics",
      "Mutual information",
      "Corpus linguistics",
      "Collocation",
      "CiteSeerX (identifier)",
      "ISBN (identifier)",
      "Semantic proximity",
      "Text corpus",
      "Large language model",
      "Scrabble",
      "Language structure",
      "Similarity measure",
      "Idiom (language structure)",
      "Co-occurrence matrix"
    ]
  },
  "Elasticsearch": {
    "url": "https://en.wikipedia.org/wiki/Elasticsearch",
    "title": "Elasticsearch",
    "content": "Elasticsearch is a source-available search engine . It is based on Apache Lucene (an open-source search engine) and provides a distributed, multitenant -capable full-text search engine with an HTTP web interface and schema-free JSON documents. Official clients are available in Java , [ 2 ] .NET [ 3 ] ( C# ), PHP , [ 4 ] Python , [ 5 ] Ruby [ 6 ] and many other languages. [ 7 ] According to the DB-Engines ranking , Elasticsearch is the most popular enterprise search engine. [ 8 ] Shay Banon created the precursor to Elasticsearch, called Compass, in 2004. [ 9 ] While thinking about the third version of Compass he realized that it would be necessary to rewrite big parts of Compass to \"create a scalable search solution\". [ 9 ] So he created \"a solution built from the ground up to be distributed\" and used a common interface, JSON over HTTP , suitable for programming languages other than Java as well. [ 9 ] Shay Banon released the first version of Elasticsearch in February 2010. [ 10 ] Elastic NV was founded in 2012 to provide commercial services and products around Elasticsearch and related software. [ 11 ] In June 2014, the company announced raising $70 million in a Series C funding round, just 18 months after forming the company. The round was led by New Enterprise Associates (NEA). Additional funders include Benchmark Capital and Index Ventures . This round brought total funding to $104M. [ 12 ] In March 2015, the company Elasticsearch changed its name to Elastic . [ 13 ] In June 2018, Elastic filed for an initial public offering with an estimated valuation of between 1.5 and 3 billion dollars. [ 14 ] On 5 October 2018, Elastic was listed on the New York Stock Exchange . [ 15 ] Developed from the Found acquisition by Elastic in 2015, [ 16 ] Elastic Cloud is a family of Elasticsearch-powered SaaS offerings which include the Elasticsearch Service, as well as Elastic App Search Service, and Elastic Site Search Service which were developed from Elastic's acquisition of Swiftype . [ 17 ] In late 2017, Elastic formed partnerships with Google to offer Elastic Cloud in Google Cloud Platform (GCP) , and Alibaba to offer Elasticsearch and Kibana in Alibaba Cloud . Elasticsearch Service users can create secure deployments with partners, Google Cloud Platform (GCP) and Alibaba Cloud. [ 18 ] In January 2021, Elastic announced that starting with version 7.11, they would be relicensing their Apache 2.0 licensed code in Elasticsearch and Kibana to be dual licensed under Server Side Public License and the Elastic License, neither of which is recognized as an open-source license . [ 19 ] [ 20 ] Elastic blamed Amazon Web Services (AWS) for this change, objecting to AWS offering Elasticsearch and Kibana as a service directly to consumers and claiming that AWS was not appropriately collaborating with Elastic. [ 20 ] [ 21 ] Critics of the re-licensing decision predicted that it would harm Elastic's ecosystem and noted that Elastic had previously promised to \"never....change the license of the Apache 2.0 code of Elasticsearch, Kibana, Beats, and Logstash\". Amazon responded with plans to fork the projects and continue development under Apache License 2.0. [ 22 ] [ 23 ] Other users of the Elasticsearch ecosystem, including Logz.io , CrateDB and Aiven , also committed to the need for a fork, leading to a discussion of how to coordinate the open source efforts. [ 24 ] [ 25 ] [ 26 ] Due to potential trademark issues with using the name \"Elasticsearch\", AWS rebranded their fork as OpenSearch in April 2021. [ 27 ] [ 28 ] In August 2024 the GNU Affero General Public License was added to ElasticSearch version 8.16.0 as an option, making Elasticsearch free and open-source again. [ 22 ] [ 29 ] Elasticsearch can be used to search any kind of document. It provides scalable search, has near real-time search , and supports multitenancy . [ 30 ] \"Elasticsearch is distributed, which means that indices can be divided into shards and each shard can have zero or more replicas. Each node hosts one or more shards and acts as a coordinator to delegate operations to the correct shard(s). Rebalancing and routing are done automatically\". [ 30 ] Related data is often stored in the same index, which consists of one or more primary shards, and zero or more replica shards. Once an index has been created, the number of primary shards cannot be changed. [ 31 ] Elasticsearch is developed alongside the data collection and log -parsing engine Logstash, the analytics and visualization platform Kibana , and the collection of lightweight data shippers called Beats. The four products are designed for use as an integrated solution, referred to as the \"Elastic Stack\". [ 32 ] (Formerly the \"ELK stack\", short for \"Elasticsearch, Logstash, Kibana\".) Elasticsearch uses Lucene and tries to make all its features available through the JSON and Java API [ 33 ] . It supports facetting and percolating (a form of prospective search ), [ 34 ] [ 35 ] which can be useful for notifying if new documents match for registered queries. Another feature, \"gateway\", handles the long-term persistence of the index; [ 36 ] for example, an index can be recovered from the gateway in the event of a server crash. Elasticsearch supports real-time GET requests , which makes it suitable as a NoSQL datastore, [ 37 ] but it lacks distributed transactions . [ 38 ] On 20 May 2019, Elastic made the core security features of the Elastic Stack available free of charge, including TLS for encrypted communications, file and native realm for creating and managing users, and role-based access control for controlling user access to cluster APIs and indexes. [ 39 ] The corresponding source code is available under the “Elastic License”, a source-available license. [ 40 ] In addition, Elasticsearch now offers SIEM [ 41 ] and Machine Learning [ 42 ] as part of its offered services.",
    "links": [
      "OpenSearch (software)",
      "Distributed transactions",
      "Swiftype",
      "TechCrunch",
      "NoSQL",
      "DB-Engines ranking",
      "Index (search engine)",
      "Search engine (computing)",
      "Benchmark Capital",
      "Software release life cycle",
      "Index Ventures",
      "Google",
      "Amazon Web Services",
      "Software as a service",
      "GNU Affero General Public License",
      "Ruby (programming language)",
      "JSON",
      ".NET Framework",
      "Routing",
      "Log file",
      "Lucene",
      "Elastic NV",
      "Java (programming language)",
      "Programmer",
      "CrateDB",
      "Source-available",
      "New Enterprise Associates",
      "GitHub",
      "Java API",
      "Software license",
      "Shard (database architecture)",
      "GET requests",
      "Apache Lucene",
      "Server Side Public License",
      "Repository (version control)",
      "Fork (software development)",
      "Multitenancy",
      "Real-time search",
      "Prospective search",
      "Python (programming language)",
      "Operating system",
      "PHP (programming language)",
      "Event data recorder",
      "Free and open-source",
      "Faceted search",
      "HTTP",
      "Encryption",
      "Affero General Public License",
      "New York Stock Exchange",
      "Cross-platform",
      "Initial public offering",
      "TLS termination proxy",
      "API",
      "Information extraction",
      "SIEM",
      "SaaS",
      "Search algorithm",
      "Recode",
      "Google Cloud Platform",
      "Open-source",
      "Source-available software",
      "Kibana",
      "Full-text search",
      "The Register",
      "Data collection",
      "Open-source license",
      "Machine learning",
      "Alibaba Group",
      "Doi (identifier)",
      "List of information retrieval libraries",
      "C Sharp (programming language)",
      "Alibaba Cloud"
    ]
  },
  "Probabilistic relevance model (BM25)": {
    "url": "https://en.wikipedia.org/wiki/Probabilistic_relevance_model_(BM25)",
    "title": "Probabilistic relevance model (BM25)",
    "content": "In information retrieval , Okapi BM25 ( BM is an abbreviation of best matching ) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson , Karen Spärck Jones , and others. The name of the actual ranking function is BM25 . The fuller name, Okapi BM25 , includes the name of the first system to use it, which was the Okapi information retrieval system, implemented at London 's City University [ 1 ] in the 1980s and 1990s. BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent TF-IDF -like retrieval functions used in document retrieval . [ 2 ] BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. It is a family of scoring functions with slightly different components and parameters. One of the most prominent instantiations of the function is as follows. Given a query Q , containing keywords q 1 , . . . , q n {\\displaystyle q_{1},...,q_{n}} , the BM25 score of a document D is: where f ( q i , D ) {\\displaystyle f(q_{i},D)} is the number of times that the keyword q i {\\displaystyle q_{i}} occurs in the document D , | D | {\\displaystyle |D|} is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn. k 1 {\\displaystyle k_{1}} and b are free parameters, usually chosen, in absence of an advanced optimization, as k 1 ∈ [ 1.2 , 2.0 ] {\\displaystyle k_{1}\\in [1.2,2.0]} and b = 0.75 {\\displaystyle b=0.75} . [ 3 ] IDF ( q i ) {\\displaystyle {\\text{IDF}}(q_{i})} is the IDF ( inverse document frequency ) weight of the query term q i {\\displaystyle q_{i}} . It is usually computed as: where N is the total number of documents in the collection, and n ( q i ) {\\displaystyle n(q_{i})} is the number of documents containing q i {\\displaystyle q_{i}} . There are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the Binary Independence Model . Here is an interpretation from information theory . Suppose a query term q {\\displaystyle q} appears in n ( q ) {\\displaystyle n(q)} documents. Then a randomly picked document D {\\displaystyle D} will contain the term with probability n ( q ) N {\\displaystyle {\\frac {n(q)}{N}}} (where N {\\displaystyle N} is again the cardinality of the set of documents in the collection). Therefore, the information content of the message \" D {\\displaystyle D} contains q {\\displaystyle q} \" is: Now suppose we have two query terms q 1 {\\displaystyle q_{1}} and q 2 {\\displaystyle q_{2}} . If the two terms occur in documents entirely independently of each other, then the probability of seeing both q 1 {\\displaystyle q_{1}} and q 2 {\\displaystyle q_{2}} in a randomly picked document D {\\displaystyle D} is: and the information content of such an event is: With a small variation, this is exactly what is expressed by the IDF component of BM25.",
    "links": [
      "Information theory",
      "London",
      "Relevance (information retrieval)",
      "Doi (identifier)",
      "Search engine",
      "Document retrieval",
      "S2CID (identifier)",
      "Inverse document frequency",
      "CiteSeerX (identifier)",
      "TF-IDF",
      "City University, London",
      "Karen Spärck Jones",
      "Information retrieval",
      "Stephen Robertson (computer scientist)",
      "ISBN (identifier)",
      "Ranking function",
      "Bag of words model",
      "Probabilistic relevance model",
      "Anchor text",
      "Information",
      "Stephen E. Robertson",
      "Binary Independence Model"
    ]
  },
  "Recommender systems": {
    "url": "https://en.wikipedia.org/wiki/Recommender_systems",
    "title": "Recommender systems",
    "content": "A recommender system , also called a recommendation algorithm , recommendation engine , or recommendation platform , is a type of information filtering system that suggests items most relevant to a particular user. [ 1 ] [ 2 ] [ 3 ] The value of these systems becomes particularly evident in scenarios where users must select from a large number of options, such as products, media, or content. [ 1 ] [ 4 ] Major social media platforms and streaming services rely on recommender systems that employ machine learning to analyze user behavior and preferences, thereby enabling personalized content feeds. [ 5 ] Typically, the suggestions refer to a variety decision-making processes , including the selection of a product, musical selection, or online news source to read. [ 1 ] The implementation of recommender systems is pervasive, with commonly recognised examples including the generation of playlist for video and music services, the provision of product recommendations for e-commerce platforms, and the recommendation of content on social media platforms and the open web. [ 6 ] [ 7 ] These systems can operate using a single type of input, such as music, or multiple inputs from diverse platforms, including news, books and search queries. Additionally, popular recommender systems have been developed for specific topics, such as restaurants and online dating services. Recommender systems have also been developed to explore research articles and experts, [ 8 ] collaborators, [ 9 ] and financial services. [ 10 ] A content discovery platform is a software recommendation platform that employs recommender system tools. It utilizes user metadata in order to identify and suggest relevant content, whilst reducing ongoing maintenance and development costs. A content discovery platform delivers personalized content to websites , mobile devices , and set-top boxes . A large range of content discovery platforms currently exist for various forms of content ranging from news articles and academic journal articles [ 11 ] to television. [ 12 ] As operators compete to serve as the gateway to home entertainment, personalized television emerges as a key service differentiator. Academic content discovery has recently become another area of interest, the emergence of numerous companies dedicated to assisting academic researchers in keeping up to date with relevant academic content and faciltating serendipitous discover of new content. [ 11 ] Recommender systems usually make use of either or both collaborative filtering and content-based filtering, as well as other systems such as knowledge-based systems . Collaborative filtering approaches build a model from a user's past behavior (e.g., items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. [ 13 ] Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties. [ 14 ] The differences between collaborative and content-based filtering can be demonstrated by comparing two early music recommender systems, Last.fm and Pandora Radio . We can also look at how these methods are applied in e-commerce, for example, on platforms like Amazon. Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations. This is an example of the cold start problem, and is common in collaborative filtering systems. [ 17 ] [ 18 ] [ 19 ] [ 20 ] [ 21 ] [ 22 ] Whereas Pandora needs very little information to start, it is far more limited in scope (for example, it can only make recommendations that are similar to the original seed). Recommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data. In some cases, like in the Gonzalez v. Google Supreme Court case, may argue that search and recommendation algorithms are different technologies. [ 23 ] Recommender systems have been the focus of several granted patents, [ 24 ] [ 25 ] [ 26 ] [ 27 ] [ 28 ] and there are more than 50 software libraries [ 29 ] that support the development of recommender systems including LensKit, [ 30 ] [ 31 ] RecBole, [ 32 ] ReChorus [ 33 ] and RecPack. [ 34 ] Elaine Rich created the first recommender system in 1979, called Grundy. [ 35 ] [ 36 ] She looked for a way to recommend users books they might like. Her idea was to create a system that asks users specific questions and classifies them into classes of preferences, or \"stereotypes\", depending on their answers. Depending on users' stereotype membership, they would then get recommendations for books they might like. Another early recommender system, called a \"digital bookshelf\", was described in a 1990 technical report by Jussi Karlgren at Columbia University, [ 37 ] and implemented at scale and worked through in technical reports and publications from 1994 onwards by Jussi Karlgren , then at SICS , [ 38 ] [ 39 ] and research groups led by Pattie Maes at MIT, [ 40 ] Will Hill at Bellcore, [ 41 ] and Paul Resnick , also at MIT, [ 42 ] [ 4 ] whose work with GroupLens was awarded the 2010 ACM Software Systems Award . Montaner provided the first overview of recommender systems from an intelligent agent perspective. [ 43 ] Adomavicius provided a new, alternate overview of recommender systems. [ 44 ] Herlocker provides an additional overview of evaluation techniques for recommender systems, [ 45 ] and Beel et al. discussed the problems of offline evaluations. [ 46 ] Beel et al. have also provided literature surveys on available research paper recommender systems and existing challenges. [ 47 ] [ 48 ] One approach to the design of recommender systems that has wide use is collaborative filtering . [ 49 ] Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past. The system generates recommendations using only information about rating profiles for different users or items. By locating peer users/items with a rating history similar to the current user or item, they generate recommendations using this neighborhood. This approach is a cornerstone for e-commerce sites that analyze the purchasing patterns of thousands of users to suggest what you might like. Collaborative filtering methods are classified as memory-based and model-based. A well-known example of memory-based approaches is the user-based algorithm, [ 50 ] while that of model-based approaches is matrix factorization (recommender systems) . [ 51 ] A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an \"understanding\" of the item itself. Many algorithms have been used in measuring user similarity or item similarity in recommender systems. For example, the k-nearest neighbor (k-NN) approach [ 52 ] and the Pearson Correlation as first implemented by Allen. [ 53 ] When building a model from a user's behavior, a distinction is often made between explicit and implicit forms of data collection . Examples of explicit data collection include the following: Examples of implicit data collection include the following: Collaborative filtering approaches often suffer from three problems: cold start , scalability, and sparsity. [ 55 ] One of the most famous examples of collaborative filtering is item-to-item collaborative filtering (people who buy x also buy y), an algorithm popularized by Amazon.com 's recommender system. [ 57 ] Many social networks originally used collaborative filtering to recommend new friends, groups, and other social connections by examining the network of connections between a user and their friends. [ 1 ] Collaborative filtering is still used as part of hybrid systems. This technique can employ embeddings, a machine learning technique. [ 58 ] Another common approach when designing recommender systems is content-based filtering . Content-based filtering methods are based on a description of the item and a profile of the user's preferences. [ 59 ] [ 60 ] These methods are best suited to situations where there is known data on an item (name, location, description, etc.), but not on the user. Content-based recommenders treat recommendation as a user-specific classification problem and learn a classifier for the user's likes and dislikes based on an item's features. In this system, keywords are used to describe the items, and a user profile is built to indicate the type of item this user likes. In other words, these algorithms try to recommend items similar to those that a user liked in the past or is examining in the present. It does not rely on a user sign-in mechanism to generate this often temporary profile. In particular, various candidate items are compared with items previously rated by the user, and the best-matching items are recommended. This approach has its roots in information retrieval and information filtering research. To create a user profile , the system mostly focuses on two types of information: Basically, these methods use an item profile (i.e., a set of discrete attributes and features) characterizing the item within the system. To abstract the features of the items in the system, an item presentation algorithm is applied. A widely used algorithm is the tf–idf representation (also called vector space representation). [ 61 ] The system creates a content-based profile of users based on a weighted vector of item features. The weights denote the importance of each feature to the user and can be computed from individually rated content vectors using a variety of techniques. Simple approaches use the average values of the rated item vector while other sophisticated methods use machine learning techniques such as Bayesian Classifiers , cluster analysis , decision trees , and artificial neural networks in order to estimate the probability that the user is going to like the item. [ 62 ] A key issue with content-based filtering is whether the system can learn user preferences from users' actions regarding one content source and use them across other content types. When the system is limited to recommending content of the same type as the user is already using, the value from the recommendation system is significantly less than when other content types from other services can be recommended. For example, recommending news articles based on news browsing is useful. Still, it would be much more useful when music, videos, products, discussions, etc., from different services, can be recommended based on news browsing. To overcome this, most content-based recommender systems now use some form of the hybrid system. Content-based recommender systems can also include opinion-based recommender systems. In some cases, users are allowed to leave text reviews or feedback on the items. These user-generated texts are implicit data for the recommender system because they are potentially rich resources of both feature/aspects of the item and users' evaluation/sentiment to the item. Features extracted from the user-generated reviews are improved metadata of items, because as they also reflect aspects of the item like metadata, extracted features are widely concerned by the users. Sentiments extracted from the reviews can be seen as users' rating scores on the corresponding features. Popular approaches of opinion-based recommender system utilize various techniques including text mining , information retrieval , sentiment analysis (see also Multimodal sentiment analysis ) and deep learning . [ 63 ] Most recommender systems now use a hybrid approach, combining collaborative filtering , content-based filtering, and other approaches. E-commerce platforms frequently use hybrid approaches to overcome problems like the cold start problem, where a new user has no history for collaborative filtering to analyze. There is no reason why several different techniques of the same type could not be hybridized. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa); or by unifying the approaches into one model. [ 44 ] Several studies that empirically compared the performance of the hybrid with the pure collaborative and content-based methods and demonstrated that the hybrid methods can provide more accurate recommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem, as well as the knowledge engineering bottleneck in knowledge-based approaches. [ 64 ] Netflix is a good example of the use of hybrid recommender systems. [ 65 ] The website makes recommendations by comparing the watching and searching habits of similar users (i.e., collaborative filtering) as well as by offering movies that share characteristics with films that a user has rated highly (content-based filtering). Some hybridization techniques include: These recommender systems use the interactions of a user within a session [ 67 ] to generate recommendations. Session-based recommender systems are used at YouTube [ 68 ] and Amazon. [ 69 ] These are particularly useful when history (such as past clicks, purchases) of a user is not available or not relevant in the current user session. Domains where session-based recommendations are particularly relevant include video, e-commerce, travel, music and more. Most instances of session-based recommender systems rely on the sequence of recent interactions within a session without requiring any additional details (historical, demographic) of the user. Techniques for session-based recommendations are mainly based on generative sequential models such as recurrent neural networks , [ 67 ] [ 70 ] transformers , [ 71 ] and other deep-learning-based approaches. [ 72 ] [ 73 ] The recommendation problem can be seen as a special instance of a reinforcement learning problem whereby the user is the environment upon which the agent, the recommendation system acts upon in order to receive a reward, for instance, a click or engagement by the user. [ 68 ] [ 74 ] [ 75 ] One aspect of reinforcement learning that is of particular use in the area of recommender systems is the fact that the models or policies can be learned by providing a reward to the recommendation agent. This is in contrast to traditional learning techniques which rely on supervised learning approaches that are less flexible, reinforcement learning recommendation techniques allow to potentially train models that can be optimized directly on metrics of engagement, and user interest. [ 76 ] Multi-criteria recommender systems (MCRS) can be defined as recommender systems that incorporate preference information upon multiple criteria. Instead of developing recommendation techniques based on a single criterion value, the overall preference of user u for the item i, these systems try to predict a rating for unexplored items of u by exploiting preference information on multiple criteria that affect this overall preference value. Several researchers approach MCRS as a multi-criteria decision making (MCDM) problem, and apply MCDM methods and techniques to implement MCRS systems. [ 77 ] See this chapter [ 78 ] for an extended introduction. The majority of existing approaches to recommender systems focus on recommending the most relevant content to users using contextual information, yet do not take into account the risk of disturbing the user with unwanted notifications. It is important to consider the risk of upsetting the user by pushing recommendations in certain circumstances, for instance, during a professional meeting, early morning, or late at night. Therefore, the performance of the recommender system depends in part on the degree to which it has incorporated the risk into the recommendation process. One option to manage this issue is DRARS , a system which models the context-aware recommendation as a bandit problem . This system combines a content-based technique and a contextual bandit algorithm. [ 79 ] Mobile recommender systems make use of internet-accessing smartphones to offer personalized, context-sensitive recommendations. This is a particularly difficult area of research as mobile data is more complex than data that recommender systems often have to deal with. It is heterogeneous, noisy, requires spatial and temporal auto-correlation, and has validation and generality problems. [ 80 ] There are three factors that could affect the mobile recommender systems and the accuracy of prediction results: the context, the recommendation method and privacy. [ 81 ] Additionally, mobile recommender systems suffer from a transplantation problem – recommendations may not apply in all regions (for instance, it would be unwise to recommend a recipe in an area where all of the ingredients may not be available). One example of a mobile recommender system are the approaches taken by companies such as Uber and Lyft to generate driving routes for taxi drivers in a city. [ 80 ] This system uses GPS data of the routes that taxi drivers take while working, which includes location (latitude and longitude), time stamps, and operational status (with or without passengers). It uses this data to recommend a list of pickup points along a route, with the goal of optimizing occupancy times and profits. Generative recommenders (GR) represent an approach that transforms recommendation tasks into sequential transduction problems, where user actions are treated like tokens in a generative modeling framework. In one method, known as HSTU (Hierarchical Sequential Transduction Units), [ 82 ] high- cardinality , non-stationary, and streaming datasets are efficiently processed as sequences, enabling the model to learn from trillions of parameters and to handle user action histories orders of magnitude longer than before. By turning all of the system’s varied data into a single stream of tokens and using a custom self-attention approach instead of traditional neural network layers , generative recommenders make the model much simpler and less memory-hungry. As a result, it can improve recommendation quality in test simulations and in real-world tests, while being faster than previous Transformer -based systems when handling long lists of user actions. Ultimately, this approach allows the model’s performance to grow steadily as more computing power is used, laying a foundation for efficient and scalable “ foundation models ” for recommendations. One of the events that energized research in recommender systems was the Netflix Prize . From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system. This competition energized the search for new and more accurate algorithms. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team using tiebreaking rules. [ 83 ] The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.: [ 84 ] Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique. Consequently, our solution is an ensemble of many methods. Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets. Some members from the team that finished second place founded Gravity R&D , a recommendation engine that's active in the RecSys community . [ 83 ] [ 85 ] 4-Tell, Inc. created a Netflix project–derived solution for ecommerce websites. A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the Internet Movie Database (IMDb) . [ 86 ] As a result, in December 2009, an anonymous Netflix user sued Netflix in Doe v. Netflix, alleging that Netflix had violated United States fair trade laws and the Video Privacy Protection Act by releasing the datasets. [ 87 ] This, as well as concerns from the Federal Trade Commission , led to the cancellation of a second Netflix Prize competition in 2010. [ 88 ] Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests) , and offline evaluations. [ 46 ] The commonly used metrics are the mean squared error and root mean squared error , the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or discounted cumulative gain (DCG) are useful to assess the quality of a recommendation method. Diversity, novelty, and coverage are also considered as important aspects in evaluation. [ 89 ] However, many of the classic evaluation measures are highly criticized. [ 90 ] Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise. User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best. In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate . Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies. [ 91 ] The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers. [ 92 ] [ 93 ] [ 94 ] [ 46 ] For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests. [ 94 ] [ 95 ] A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms. [ 96 ] Often, results of so-called offline evaluations do not correlate with actually assessed user-satisfaction. [ 97 ] This is probably because offline training is highly biased toward the highly reachable items, and offline testing data is highly influenced by the outputs of the online recommendation module. [ 92 ] [ 98 ] Researchers have concluded that the results of offline evaluations should be viewed critically. [ 99 ] Typically, research on recommender systems is concerned with finding the most accurate recommendation algorithms. However, there are a number of factors that are also important. Recommender systems are notoriously difficult to evaluate offline, with some researchers claiming that this has led to a reproducibility crisis in recommender systems publications. The topic of reproducibility seems to be a recurrent issue in some Machine Learning publication venues, but does not have a considerable effect beyond the world of scientific publication. In the context of recommender systems a 2019 paper surveyed a small number of hand-picked publications applying deep learning or neural methods to the top-k recommendation problem, published in top conferences (SIGIR, KDD, WWW, RecSys , IJCAI), has shown that on average less than 40% of articles could be reproduced by the authors of the survey, with as little as 14% in some conferences. The articles considers a number of potential problems in today's research scholarship and suggests improved scientific practices in that area. [ 112 ] [ 113 ] [ 114 ] More recent work on benchmarking a set of the same methods came to qualitatively very different results [ 115 ] whereby neural methods were found to be among the best performing methods. Deep learning and neural methods for recommender systems have been used in the winning solutions in several recent recommender system challenges, WSDM, [ 116 ] RecSys Challenge . [ 117 ] Moreover, neural and deep learning methods are widely used in industry where they are extensively tested. [ 118 ] [ 68 ] [ 69 ] The topic of reproducibility is not new in recommender systems. By 2011, Ekstrand , Konstan , et al. criticized that \"it is currently difficult to reproduce and extend recommender systems research results,\" and that evaluations are \"not handled consistently\". [ 119 ] Konstan and Adomavicius conclude that \"the Recommender Systems research community is facing a crisis where a significant number of papers present results that contribute little to collective knowledge [...] often because the research lacks the [...] evaluation to be properly judged and, hence, to provide meaningful contributions.\" [ 120 ] As a consequence, much research about recommender systems can be considered as not reproducible. [ 121 ] Hence, operators of recommender systems find little guidance in the current research for answering the question, which recommendation approaches to use in a recommender systems. Said and Bellogín conducted a study of papers published in the field, as well as benchmarked some of the most popular frameworks for recommendation and found large inconsistencies in results, even when the same algorithms and data sets were used. [ 122 ] Some researchers demonstrated that minor variations in the recommendation algorithms or scenarios led to strong changes in the effectiveness of a recommender system. They conclude that seven actions are necessary to improve the current situation: [ 121 ] \"(1) survey other research fields and learn from them, (2) find a common understanding of reproducibility, (3) identify and understand the determinants that affect reproducibility, (4) conduct more comprehensive experiments (5) modernize publication practices, (6) foster the development and use of recommendation frameworks, and (7) establish best-practice guidelines for recommender-systems research.\" Artificial intelligence (AI) applications in recommendation systems are the advanced methodologies that leverage AI technologies, to enhance the performance recommendation engines. The AI-based recommender can analyze complex data sets, learning from user behavior, preferences, and interactions to generate highly accurate and personalized content or product suggestions. [ 123 ] The integration of AI in recommendation systems has marked a significant evolution from traditional recommendation methods. Traditional methods often relied on inflexible algorithms that could suggest items based on general user trends or apparent similarities in content. In comparison, AI-powered systems have the capability to detect patterns and subtle distinctions that may be overlooked by traditional methods. [ 124 ] These systems can adapt to specific individual preferences, thereby offering recommendations that are more aligned with individual user needs. This approach marks a shift towards more personalized, user-centric suggestions. Recommendation systems widely adopt AI techniques such as machine learning , deep learning , and natural language processing . [ 125 ] These advanced methods enhance system capabilities to predict user preferences and deliver personalized content more accurately. Each technique contributes uniquely. The following sections will introduce specific AI models utilized by a recommendation system by illustrating their theories and functionalities. [ citation needed ] Collaborative filtering (CF) is one of the most commonly used recommendation system algorithms. It generates personalized suggestions for users based on explicit or implicit behavioral patterns to form predictions. [ 126 ] Specifically, it relies on external feedback such as star ratings, purchasing history and so on to make judgments. CF make predictions about users' preference based on similarity measurements. Essentially, the underlying theory is: \"if user A is similar to user B, and if A likes item C, then it is likely that B also likes item C.\" There are many models available for collaborative filtering. For AI-applied collaborative filtering, a common model is called K-nearest neighbors . The ideas are as follows: An artificial neural network (ANN), is a deep learning model structure which aims to mimic a human brain. They comprise a series of neurons, each responsible for receiving and processing information transmitted from other interconnected neurons. [ 127 ] Similar to a human brain, these neurons will change activation state based on incoming signals (training input and backpropagated output), allowing the system to adjust activation weights during the network learning phase. ANN is usually designed to be a black-box model. Unlike regular machine learning where the underlying theoretical components are formal and rigid, the collaborative effects of neurons are not entirely clear, but modern experiments has shown the predictive power of ANN. ANN is widely used in recommendation systems for its power to utilize various data. Other than feedback data, ANN can incorporate non-feedback data which are too intricate for collaborative filtering to learn, and the unique structure allows ANN to identify extra signal from non-feedback data to boost user experience. [ 125 ] Following are some examples: The Two-Tower model is a neural architecture [ 128 ] commonly employed in large-scale recommendation systems, particularly for candidate retrieval tasks. [ 129 ] It consists of two neural networks: The outputs of the two towers are fixed-length embeddings that represent users and items in a shared vector space. A similarity metric, such as dot product or cosine similarity , is used to measure relevance between a user and an item. This model is highly efficient for large datasets as embeddings can be pre-computed for items, allowing rapid retrieval during inference. It is often used in conjunction with ranking models for end-to-end recommendation pipelines. Natural language processing is a series of AI algorithms to make natural human language accessible and analyzable to a machine. [ 130 ] It is a fairly modern technique inspired by the growing amount of textual information. For application in recommendation system, a common case is the Amazon customer review. Amazon will analyze the feedbacks comments from each customer and report relevant data to other customers for reference. The recent years have witnessed the development of various text analysis models, including latent semantic analysis (LSA), singular value decomposition (SVD), latent Dirichlet allocation (LDA), etc. Their uses have consistently aimed to provide customers with more precise and tailored recommendations. Recommender systems are essential for modern e-commerce platforms, playing a key role in improving the customer experience and increasing sales. These systems analyze customer data to provide personalized product suggestions, helping users discover items they might not have found on their own. A study by J. Leskovec et al. highlighted that such systems are crucial when an individual needs to choose from a potentially overwhelming number of items that a service may offer. [ 131 ] E-commerce recommenders typically use a combination of filtering techniques to generate these suggestions. Collaborative filtering is a core method, recommending products based on the purchasing and Browse habits of similar users. Another widely used approach is content-based filtering, which recommends items with similar attributes to those a user has previously shown interest in. Many e-commerce platforms use a hybrid approach, combining these techniques to create more accurate and diverse recommendations, which helps to address issues like the \"cold start\" problem for new users or products. [ 132 ] These systems are implemented in several ways across e-commerce sites to maximize their effectiveness at different stages of the shopping process: The effective use of recommender systems can lead to a significant increase in key performance indicators for e-commerce, including higher conversion rates, larger average order values from cross-sells and upsells, and improved customer satisfaction and retention. [ 132 ] These systems are powered by a range of technologies, from traditional machine learning models to advanced deep learning architectures that can process complex user behavior and product data. An emerging market for content discovery platforms is academic content. [ 134 ] [ 135 ] Approximately 6000 academic journal articles are published daily, making it increasingly difficult for researchers to balance time management with staying up to date with relevant research. [ 11 ] Though traditional tools academic search tools such as Google Scholar or PubMed provide a readily accessible database of journal articles, content recommendation in these cases are performed in a 'linear' fashion, with users setting 'alarms' for new publications based on keywords, journals or particular authors. Google Scholar provides an 'Updates' tool that suggests articles by using a statistical model that takes a researchers' authorized paper and citations as input. [ 11 ] Whilst these recommendations have been noted to be extremely good, this poses a problem with early career researchers which may be lacking a sufficient body of work to produce accurate recommendations. [ 11 ] In contrast to an engagement-based ranking system employed by social media and other digital platforms, a bridging-based ranking optimizes for content that is unifying instead of polarizing . [ 136 ] [ 137 ] Examples include Polis and Remesh which have been used around the world to help find more consensus around specific political issues. [ 137 ] Twitter has also used this approach for managing its community notes , [ 138 ] which YouTube planned to pilot in 2024. [ 139 ] [ 140 ] Aviv Ovadya also argues for implementing bridging-based algorithms in major platforms by empowering deliberative groups that are representative of the platform's users to control the design and implementation of the algorithm. [ 141 ] As the connected television landscape continues to evolve, search and recommendation are seen as having an even more pivotal role in the discovery of content. [ 142 ] With broadband -connected devices, consumers are projected to have access to content from linear broadcast sources as well as internet television . Therefore, there is a risk that the market could become fragmented, leaving it to the viewer to visit various locations and find what they want to watch in a way that is time-consuming and complicated for them. By using a search and recommendation engine, viewers are provided with a central 'portal' from which to discover content from several sources in just one location.",
    "links": [
      "Smartphone",
      "Elaine Rich",
      "Latent Dirichlet allocation",
      "Tf–idf",
      "Journal of Big Data",
      "Effectiveness",
      "Artificial intelligence",
      "Decision-making process",
      "Profiling (information science)",
      "Doi (identifier)",
      "Knowledge-based systems",
      "Netflix",
      "S2CID (identifier)",
      "Algorithmic radicalization",
      "Root mean squared error",
      "Latent semantic analysis",
      "Website",
      "Joseph A. Konstan",
      "Video Privacy Protection Act",
      "Internet television",
      "Black box",
      "Information retrieval",
      "Hdl (identifier)",
      "YouTube",
      "Deliberative democracy",
      "Sentiment analysis",
      "Jussi Karlgren",
      "Statistical distance",
      "Streaming service",
      "Relevance",
      "Matrix factorization (recommender systems)",
      "Long tail",
      "Enterprise bookmarking",
      "Information filtering",
      "Cosine similarity",
      "IMDb",
      "K-nearest neighbors algorithm",
      "Broadband",
      "Online dating",
      "Recurrent neural network",
      "Implicit data collection",
      "Lyft",
      "Lyle Ungar",
      "Bibcode (identifier)",
      "Seq2seq",
      "Wiki survey",
      "Reproducibility crisis",
      "Content moderation",
      "Information filtering system",
      "GitHub",
      "Rocchio algorithm",
      "MovieLens",
      "Collaborative filtering",
      "CiteSeerX (identifier)",
      "ACM Software Systems Award",
      "Paul Resnick",
      "New York City",
      "Dot product",
      "SICS",
      "Similarity search",
      "Pearson correlation",
      "Pandora Radio",
      "Playlist",
      "Mean squared error",
      "Product finder",
      "Rating site",
      "PMID (identifier)",
      "Last.fm",
      "Multimodal sentiment analysis",
      "Cold start (computing)",
      "Twitter",
      "User profiles",
      "Academic journal",
      "Social networks",
      "Artificial neural network",
      "Precision and recall",
      "Algorithm (disambiguation)",
      "Item-item collaborative filtering",
      "Raymond J. Mooney",
      "Multi-armed bandit",
      "Attention (machine learning)",
      "Cold start (recommender systems)",
      "IEEE Spectrum",
      "ArXiv (identifier)",
      "Natural language processing",
      "A/B testing",
      "Machine learning",
      "Collective intelligence",
      "Uber",
      "Google Scholar",
      "Decision support system",
      "Content discovery platform",
      "Click-through rate",
      "Singular value decomposition",
      "Set-top boxes",
      "Location based recommendation",
      "Pattern recognition",
      "Artificial neural networks",
      "Harvard University",
      "User profile",
      "Filter bubble",
      "Naive Bayes classifier",
      "Cluster analysis",
      "Gonzalez v. Google LLC",
      "Foundation model",
      "Federal Trade Commission",
      "Association for Computing Machinery",
      "Decision trees",
      "Music Genome Project",
      "Pattie Maes",
      "Search algorithm",
      "Conversion rate",
      "Reputation management",
      "Amazon.com",
      "Cardinality",
      "Data collection",
      "Text mining",
      "Metadata",
      "Information privacy",
      "Star (classification)",
      "Preference elicitation",
      "Knowledge base",
      "Statistical model",
      "Personalized marketing",
      "Deep learning",
      "Special Interest Group on Information Retrieval",
      "Wayback Machine",
      "ACM Conference on Recommender Systems",
      "Software",
      "Configurator",
      "Reputation system",
      "Evaluation",
      "Computing platform",
      "Serendipity",
      "ISSN (identifier)",
      "Collaborative search engine",
      "Gravity R&D",
      "Netflix Prize",
      "Community Notes",
      "Media monitoring service",
      "ISBN (identifier)",
      "Mobile device",
      "Peter Brusilovsky",
      "GroupLens Research",
      "Discounted Cumulative Gain",
      "Personalized search",
      "Pol.is",
      "Alexander Tuzhilin",
      "Journal of Medical Internet Research",
      "Neural network (machine learning)",
      "Social media",
      "Information explosion",
      "Dimensionality reduction",
      "Transformer (deep learning architecture)",
      "PubMed"
    ]
  },
  "Vector space model": {
    "url": "https://en.wikipedia.org/wiki/Vector_space_model",
    "title": "Vector space model",
    "content": "Vector space model or term vector model is an algebraic model for representing text documents (or more generally, items) as vectors such that the distance between vectors represents the relevance between the documents. It is used in information filtering , information retrieval , indexing and relevance rankings. Its first use was in the SMART Information Retrieval System . [ 1 ] In this section we consider a particular vector space model based on the bag-of-words representation. Documents and queries are represented as vectors. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting (see the example below). The definition of term depends on the application. Typically terms are single words, keywords , or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus ). Vector operations can be used to compare documents with queries. [ 2 ] Candidate documents from the corpus can be retrieved and ranked using a variety of methods. Relevance rankings of documents in a keyword search can be calculated, using the assumptions of document similarities theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as a vector with same dimension as the vectors that represent the other documents. In practice, it is easier to calculate the cosine of the angle between the vectors, instead of the angle itself: Where d 2 ⋅ q {\\displaystyle \\mathbf {d_{2}} \\cdot \\mathbf {q} } is the intersection (i.e. the dot product ) of the document (d 2 in the figure to the right) and the query (q in the figure) vectors, ‖ d 2 ‖ {\\displaystyle \\left\\|\\mathbf {d_{2}} \\right\\|} is the norm of vector d 2 , and ‖ q ‖ {\\displaystyle \\left\\|\\mathbf {q} \\right\\|} is the norm of vector q. The norm of a vector is calculated as such: Using the cosine the similarity between document d j and query q can be calculated as: As all vectors under consideration by this model are element-wise nonnegative, a cosine value of zero means that the query and document vector are orthogonal and have no match (i.e. the query term does not exist in the document being considered). See cosine similarity for further information. [ 2 ] In the classic vector space model proposed by Salton , Wong and Yang, [ 3 ] the term-specific weights in the document vectors are products of local and global parameters. The model is known as term frequency–inverse document frequency (tf–idf) model. The weight vector for document d is v d = [ w 1 , d , w 2 , d , … , w N , d ] T {\\displaystyle \\mathbf {v} _{d}=[w_{1,d},w_{2,d},\\ldots ,w_{N,d}]^{T}} , where and The vector space model has the following advantages over the Standard Boolean model : Most of these advantages are a consequence of the difference in the density of the document collection representation between Boolean and term frequency-inverse document frequency approaches. When using Boolean weights, any document lies in a vertex in a n-dimensional hypercube . Therefore, the possible document representations are 2 n {\\displaystyle 2^{n}} and the maximum Euclidean distance between pairs is n {\\displaystyle {\\sqrt {n}}} . As documents are added to the document collection, the region defined by the hypercube's vertices become more populated and hence denser. Unlike Boolean, when a document is added using term frequency-inverse document frequency weights, the inverse document frequencies of the terms in the new document decrease while that of the remaining terms increase. In average, as documents are added, the region where documents lie expands regulating the density of the entire collection representation. This behavior models the original motivation of Salton and his colleagues that a document collection represented in a low density region could yield better retrieval results. The vector space model has the following limitations: Many of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as singular value decomposition and lexical databases such as WordNet . Models based on and extending the vector space model include: The following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them.",
    "links": [
      "OpenSearch (software)",
      "Tf–idf",
      "Conceptual space",
      "Norm (mathematics)",
      "Index (search engine)",
      "Bag-of-words model",
      "Relevance (information retrieval)",
      "Doi (identifier)",
      "W-shingling",
      "Search Engine Optimization",
      "Rocchio Classification",
      "Keyword (linguistics)",
      "Elasticsearch",
      "Singular value decomposition",
      "Apache Lucene",
      "Latent Semantic Indexing",
      "Eigenvalues and eigenvectors",
      "Apache Solr",
      "Dimension (vector space)",
      "Cosine",
      "Latent semantic analysis",
      "NumPy",
      "Nearest neighbor search",
      "Dot product",
      "Compound term processing",
      "Information retrieval",
      "Tf-idf",
      "Standard Boolean model",
      "ISBN (identifier)",
      "Word2vec",
      "Text corpus",
      "SMART Information Retrieval System",
      "Orthogonal",
      "WordNet",
      "Latent Dirichlet Allocation",
      "Term frequency–inverse document frequency",
      "Term model",
      "Weka (machine learning)",
      "Gerard Salton",
      "Hypercube",
      "Lexical database",
      "Ranking",
      "Generalized vector space model",
      "Random indexing",
      "Gensim",
      "Inverted index",
      "Vector space",
      "Vector database",
      "Information filtering",
      "Cosine similarity",
      "Semantic similarity",
      "Champion list",
      "Sparse distributed memory"
    ]
  },
  "Information overload": {
    "url": "https://en.wikipedia.org/wiki/Information_overload",
    "title": "Information overload",
    "content": "Information overload (also known as infobesity , [ 1 ] [ 2 ] infoxication , [ 3 ] or information anxiety [ 4 ] ) is the difficulty in understanding an issue and effectively making decisions when one has too much information ( TMI ) about that issue, [ 5 ] and is generally associated with the excessive quantity of daily information. [ 6 ] The term \"information overload\" was first used as early as 1962 by scholars in management and information studies, including in Bertram Gross' 1964 book The Managing of Organizations [ 7 ] [ 8 ] and was further popularized by Alvin Toffler in his bestselling 1970 book Future Shock . [ 9 ] Speier et al. (1999) said that if input exceeds the processing capacity, information overload occurs, which is likely to reduce the quality of the decisions. [ 10 ] In a newer definition, Roetzel (2019) focuses on time and resources aspects. He states that when a decision-maker is given many sets of information, such as complexity, amount, and contradiction, the quality of its decision is decreased because of the individual's limitation of scarce resources to process all the information and optimally make the best decision. [ 11 ] The advent of modern information technology has been a primary driver of information overload on multiple fronts: in quantity produced, ease of dissemination, and breadth of the audience reached. Longstanding technological factors have been further intensified by the rise of social media including the attention economy , which facilitates attention theft . [ 12 ] [ 13 ] In the age of connective digital technologies, informatics , the Internet culture (or the digital culture), information overload is associated with over-exposure, excessive viewing of information, and input abundance of information and data. Even though information overload is linked to digital cultures and technologies, Ann Blair notes that the term itself predates modern technologies, as indications of information overload were apparent when humans began collecting manuscripts, collecting, recording, and preserving information. [ 14 ] One of the first social scientists to notice the negative effects of information overload was the sociologist Georg Simmel (1858–1918), who hypothesized that the overload of sensations in the modern urban world caused city dwellers to become jaded and interfered with their ability to react to new situations. [ 15 ] The social psychologist Stanley Milgram (1933–1984) later used the concept of information overload to explain bystander behavior . Psychologists have recognized for many years that humans have a limited capacity to store current information in memory. Psychologist George Armitage Miller was very influential in this regard, proposing that people can process about seven chunks of information at a time. Miller says that under overload conditions, people become confused and are likely to make poorer decisions based on the information they have received as opposed to making informed ones. A quite early example of the term \"information overload\" can be found in an article by Jacob Jacoby, Donald Speller and Carol Kohn Berning, who conducted an experiment on 192 housewives which was said to confirm the hypothesis that more information about brands would lead to poorer decision making . Long before that, the concept was introduced by Diderot, although it was not by the term \"information overload\": As long as the centuries continue to unfold, the number of books will grow continually, and one can predict that a time will come when it will be almost as difficult to learn anything from books as from the direct study of the whole universe. It will be almost as convenient to search for some bit of truth concealed in nature as it will be to find it hidden away in an immense multitude of bound volumes. — Denis Diderot , \" Encyclopédie \" (1755) In the internet age, the term \"information overload\" has evolved into phrases such as \"information glut\", \"data smog\", and \"data glut\" ( Data Smog , Shenk, 1997). [ 16 ] In his abstract, Kazi Mostak Gausul Hoq commented that people often experience an \"information glut\" whenever they struggle with locating information from print, online, or digital sources. [ 17 ] What was once a term grounded in cognitive psychology has evolved into a rich metaphor used outside the world of academia. Information overload has been documented throughout periods where advances in technology have increased a production of information. As early as the 3rd or 4th century BC, people regarded information overload with disapproval. Around this time, in Ecclesiastes 12:12, the passage revealed the writer's comment \"of making books there is no end\" and in the 1st century AD, Seneca the Elder commented, that \"the abundance of books is distraction\". In 1255, the Dominican Vincent of Beauvais, also commented on the flood of information: \"the multitude of books, the shortness of time and the slipperiness of memory.\" [ 14 ] Similar complaints around the growth of books were also mentioned in China. There were also information enthusiasts. The Library of Alexandria was established around the 3rd century BCE or 1st century Rome, which introduced acts of preserving historical artifacts. Museums and libraries established universal grounds of preserving the past for the future, but much like books, libraries were only granted with limited access. Renaissance humanists always had a desire to preserve their writings and observations, [ 14 ] but were only able to record ancient texts by hand because books were expensive and only the privileged and educated could afford them. Humans experience an overload in information by excessively copying ancient manuscripts and replicating artifacts, creating libraries and museums that have remained in the present. [ 14 ] Around 1453 AD, Johannes Gutenberg invented the printing press and this marked another period of information proliferation. As a result of lowering production costs, generation of printed materials ranging from pamphlets , manuscripts to books were made available to the average person. Following Gutenberg's invention, the introduction of mass printing began in Western Europe. Information overload was often experienced by the affluent, but the circulation of books were becoming rapidly printed and available at a lower cost, allowing the educated to purchase books. Information became recordable, by hand, and could be easily memorized for future storage and accessibility. This era marked a time where inventive methods were established to practice information accumulation. Aside from printing books and passage recording, encyclopedias and alphabetical indexes were introduced, enabling people to save and bookmark information for retrieval. These practices marked both present and future acts of information processing. Swiss scientist Conrad Gessner commented on the increasing number of libraries and printed books, [ 14 ] and was most likely the first academic who discussed the consequences of information overload as he observed how \"unmanageable\" information came to be after the creation of the printing press. [ 18 ] Blair notes that while scholars were elated with the number of books available to them, they also later experienced fatigue with the amount of excessive information that was readily available and overpopulated them. Scholars complained about the abundance of information for a variety of reasons, such as the diminishing quality of text as printers rushed to print manuscripts and the supply of new information being distracting and difficult to manage. Erasmus, one of the many recognized humanists of the 16th century asked, \"Is there anywhere on earth exempt from these swarms of new books?\". [ 19 ] Many grew concerned with the rise of books in Europe, especially in England, France, and Germany. From 1750 to 1800, there was a 150% increase in the production of books. In 1795, German bookseller and publisher Johann Georg Heinzmann said \"no nation printed as much as the Germans\" and expressed concern about Germans reading ideas and no longer creating original thoughts and ideas. [ 20 ] To combat information overload, scholars developed their own information records for easier and simply archival access and retrieval. Modern Europe compilers used paper and glue to cut specific notes and passages from a book and pasted them to a new sheet for storage. Carl Linnaeus developed paper slips, often called his botanical paper slips, from 1767 to 1773, to record his observations. Blair argues that these botanical paper slips gave birth to the \"taxonomical system\" that has endured to the present, influencing both the mass inventions of the index card and the library card catalog. [ 19 ] In his book, The Information: A History, A Theory, A Flood, published in 2011, author James Gleick notes that engineers began taking note of the concept of information, quickly associated it in a technical sense: information was both quantifiable and measurable. He discusses how information theory was created to first bridge mathematics, engineering, and computing together, creating an information code between the fields. English speakers from Europe often equated \"computer science\" to \" informatique , informatica , and Informatik \". [ 21 ] This leads to the idea that all information can be saved and stored on computers, even if information experiences entropy. But at the same time, the term information, and its many definitions have changed. [ citation needed ] In the second half of the 20th century, advances in computer and information technology led to the creation of the Internet . In the modern Information Age , information overload is experienced as distracting and unmanageable information such as email spam , email notifications, instant messages , Tweets and Facebook updates in the context of the work environment. [ 22 ] Social media has resulted in \"social information overload\", which can occur on sites like Facebook, and technology is changing to serve our social culture. In today's society, day-to-day activities increasingly involve the technological world where information technology exacerbates the number of interruptions that occur in the work environment. [ 23 ] Management may be even more disrupted in their decision making, and may result in more poor decisions. Thus, the PIECES framework mentions information overload as a potential problem in existing information systems. [ 24 ] As the world moves into a new era of globalization , an increasing number of people connect to the internet to conduct their own research [ 25 ] and are given the ability to contribute to publicly accessible data. This has elevated the risk for the spread of misinformation. [ according to whom? ] In a 2018 literature review, Roetzel indicates that information overload can be seen as a virus—spreading through (social) media and news networks. [ 11 ] The latest research hypothesizes that information overload is a multilevel phenomenon, i.e., there are different mechanisms responsible for its emergence at the individual, group, and the whole society levels, however, these levels are interlinked. [ 26 ] In a piece published by Slate , Vaughan Bell argues that \"Worries about information overload are as old as information itself\" [ 18 ] because each generation and century will inevitably experience a significant impact with technology. In the 21st century, Frank Furedi describes how an overload in information is metaphorically expressed as a flood, which is an indication that humanity is being \"drowned\" by the waves of data coming at it. [ 27 ] This includes how the human brain continues to process information whether digitally or not. Information overload can lead to \"information anxiety\", which is the gap between the information that is understood and the information that it is perceived must be understood. The phenomenon of information overload is connected to the field of information technology (IT). IT corporate management implements training to \"improve the productivity of knowledge workers\". Ali F. Farhoomand and Don H. Drury note that employees often experience an overload in information whenever they have difficulty absorbing and assimilating the information they receive to efficiently complete a task because they feel burdened, stressed, and overwhelmed. [ 28 ] At New York's Web 2.0 Expo in 2008, Clay Shirky 's speech indicated that information overload in the modern age is a consequence of a deeper problem, which he calls \"filter failure\", [ 29 ] where humans continue to overshare information with each other. This is due to the rapid rise of apps and unlimited wireless access. In the modern information age , information overload is experienced as distracting and unmanageable information such as email spam , email notifications, instant messages , Tweets , and Facebook updates in the context of the work environment. Social media has resulted in \"social information overload\", which can occur on sites like Facebook, and technology is changing to serve our social culture. As people view increasing amounts of information in the form of news stories, emails, blog posts, Facebook statuses, Tweets , Tumblr posts and other new sources of information, they become their own editors, gatekeepers , and aggregators of information. [ 30 ] Social media platforms create a distraction as users attention spans are challenged once they enter an online platform. One concern in this field is that massive amounts of information can be distracting and negatively impact productivity and decision-making and cognitive control . Another concern is the \"contamination\" of useful information with information that might not be entirely accurate ( information pollution ). The general causes of information overload include: Email remains a major source of information overload, as people struggle to keep up with the rate of incoming messages. [ 31 ] As well as filtering out unsolicited commercial messages ( spam ), users also have to contend with the growing use of email attachments in the form of lengthy reports, presentations, and media files. [ 32 ] A December 2007 New York Times blog post described email as \"a $650 billion drag on the economy\", [ 33 ] and the New York Times reported in April 2008 that \"email has become the bane of some people's professional lives\" due to information overload, yet \"none of [the current wave of high-profile Internet startups focused on email] really eliminates the problem of email overload because none helps us prepare replies\". [ 34 ] In January 2011, Eve Tahmincioglu, a writer for NBC News , wrote an article titled \"It's Time to Deal With That Overflowing Inbox\". Compiling statistics with commentary, she reported that there were 294 billion emails sent each day in 2010, up from 50 billion in 2009. Quoted in the article, workplace productivity expert Marsha Egan stated that people need to differentiate between working on email and sorting through it. This meant that rather than responding to every email right away, users should delete unnecessary emails and sort the others into action or reference folders first. Egan then went on to say \"We are more wired than ever before, and as a result need to be more mindful of managing email or it will end up managing us.\" [ 35 ] The Daily Telegraph quoted Nicholas Carr , former executive editor of the Harvard Business Review and the author of The Shallows: What The Internet Is Doing To Our Brains , as saying that email exploits a basic human instinct to search for new information, causing people to become addicted to \"mindlessly pressing levers in the hope of receiving a pellet of social or intellectual nourishment\". His concern is shared by Eric Schmidt , chief executive of Google , who stated that \"instantaneous devices\" and the abundance of information people are exposed to through email and other technology-based sources could be having an impact on the thought process, obstructing deep thinking, understanding, impeding the formation of memories and making learning more difficult. This condition of \"cognitive overload\" results in diminished information retaining ability and failing to connect remembrances to experiences stored in the long-term memory, leaving thoughts \"thin and scattered\". [ 36 ] This is also manifest in the education process. [ 37 ] In addition to email, the World Wide Web has provided access to billions of pages of information. In many offices, workers are given unrestricted access to the Web, allowing them to manage their own research. The use of search engines helps users to find information quickly. However, information published online may not always be reliable, due to the lack of authority-approval or a compulsory accuracy check before publication. Internet information lacks credibility as the Web's search engines do not have the abilities to filter and manage information and misinformation. [ 38 ] This results in people having to cross-check what they read before using it for decision-making, which takes up more time. [ citation needed ] Viktor Mayer-Schönberger , author of Delete: The Virtue of Forgetting in the Digital Age, argues that everyone can be a \"participant\" on the Internet, where they are all senders and receivers of information. [ 39 ] On the Internet, trails of information are left behind, allowing other Internet participants to share and exchange information. Information becomes difficult to control on the Internet. The BBC reports that \"every day, the information we send and receive online – whether that's checking emails or searching the internet – amount to over 2.5 quintillion bytes of data.\" [ 40 ] Social media are applications and websites with an online community where users create and share content with each other, and it adds to the problem of information overload because so many people have access to it. [ 41 ] It presents many different views and outlooks on subject matters so that one may have difficulty taking it all in and drawing a clear conclusion. [ 42 ] Information overload may not be the core reason for people's anxieties about the amount of information they receive in their daily lives. Instead, information overload can be considered situational. Social media users tend to feel less overloaded by information when using their personal profiles, rather than when their work institutions expect individuals to gather a mass of information. Most people see information through social media in their lives as an aid to help manage their day-to-day activities and not an overload. [ 43 ] Depending on what social media platform is being used, it may be easier or harder to stay up to date on posts from people. Facebook users who post and read more than others tend to be able to keep up. On the other hand, Twitter users who post and read a lot of tweets still feel like it is too much information (or none of it is interesting enough). [ 11 ] Another problem with social media is that many people create a living by creating content for either their own or someone else's platform, which can create for creators to publish an overload of content. In the context of searching for information, researchers have identified two forms of information overload: outcome overload where there are too many sources of information and textual overload where the individual sources are too long. This form of information overload may cause searchers to be less systematic. Disillusionment when a search is more challenging than expected may result in an individual being less able to search effectively. Information overload when searching can result in a satisficing strategy. [ 44 ] : 7 Savolainen identifies filtering and withdrawal as common responses to information. Filtering involves quickly working out whether a particular piece of information, such as an email, can be ignored based on certain criteria. Withdrawal refers to limiting the number of sources of information with which one interacts. They distinguish between \"pull\" and \"push\" sources of information, a \"pull\" source being one where one seeks out relevant information, a \"push\" source one where others decide what information might be interesting. They note that \"pull\" sources can avoid information overload but by only \"pulling\" information one risks missing important information. [ 45 ] There have been many solutions proposed for how to mitigate information overload. [ 46 ] Research examining how people seek to control an overloaded environment has shown that people purposefully using different coping strategies. [ 47 ] [ 48 ] [ 49 ] In general, overload coping strategy consists of two excluding (ignoring and filtering) and two including (customizing and saving) approaches. [ 49 ] [ 48 ] Excluding approach focuses on managing the quantity of information, while including approach is geared towards complexity management. Johnson advises discipline which helps mitigate interruptions and for the elimination of push or notifications. He explains that notifications pull people's attentions away from their work and into social networks and emails. He also advises that people stop using their iPhones as alarm clocks which means that the phone is the first thing that people will see when they wake up leading to people checking their email right away. [ 53 ] Clay Shirky states: [ 29 ] What we're dealing with now is not the problem of information overload, because we're always dealing (and always have been dealing) with information overload... Thinking about information overload isn't accurately describing the problem; thinking about filter failure is. Consider the use of Internet applications and add-ons such as the Inbox Pause add-on for Gmail . [ 54 ] This add-on does not reduce the number of emails that people get but it pauses the inbox. Burkeman in his article talks about the feeling of being in control is the way to deal with information overload which might involve self-deception. He advises to fight irrationality with irrationality by using add-ons that allow you to pause your inbox or produce other results. Reducing large amounts of information is key. Dealing with IO from a social network site such as Facebook, a study done by Humboldt University [ 55 ] showed some strategies that students take to try and alleviate IO while using Facebook. Some of these strategies included: Prioritizing updates from friends who were physically farther away in other countries, hiding updates from less-prioritized friends, deleting people from their friends list, narrowing the amount of personal information shared, and deactivating the Facebook account. Decision makers performing complex tasks have little if any excess cognitive capacity. Narrowing one's attention as a result of the interruption is likely to result in the loss of information cues, some of which may be relevant to completing the task. Under these circumstances, performance is likely to deteriorate. As the number or intensity of the distractions/interruptions increases, the decision maker's cognitive capacity is exceeded, and performance deteriorates more severely. In addition to reducing the number of possible cues attended to, more severe distractions/interruptions may encourage decision-makers to use heuristics, take shortcuts, or opt for a satisficing decision , resulting in lower decision accuracy. Some cognitive scientists and graphic designers have emphasized the distinction between raw information and information in a form that can be used in thinking. In this view, information overload may be better viewed as organization underload. That is, they suggest that the problem is not so much the volume of information but the fact that it cannot be discerned how to use it well in the raw or biased form it is presented. Authors who have taken this view include graphic artist and architect Richard Saul Wurman and statistician and cognitive scientist Edward Tufte . Wurman uses the term \"information anxiety\" to describe humanity's attitude toward the volume of information in general and their limitations in processing it. [ 57 ] Tufte primarily focuses on quantitative information and explores ways to organize large complex datasets visually to facilitate clear thinking. Tufte's writing is important in such fields as information design and visual literacy, [ 58 ] which deal with the visual communication of information. Tufte coined the term \"chartjunk\" to refer to useless, non-informative, or information-obscuring elements of quantitative information displays, such as the use of graphics to overemphasize the importance of certain pieces of data or information. [ 59 ] In a study conducted by Soucek and Moser (2010), [ 60 ] they investigated what impact a training intervention on how to cope with information overload would have on employees. They found that the training intervention did have a positive impact on IO, especially on those who struggled with work impairment and media usage, and employees who had a higher amount of incoming emails. [ 60 ] Recent research suggests that an \" attention economy \" of sorts will naturally emerge from information overload, [ 61 ] allowing Internet users greater control over their online experience with particular regard to communication mediums such as email and instant messaging. This could involve some sort of cost being attached to email messages. For example, managers charging a small fee for every email received – e.g. $1.00 – which the sender must pay from their budget. The aim of such charging is to force the sender to consider the necessity of the interruption. However, such a suggestion undermines the entire basis of the popularity of email, namely that emails are free of charge to send. Economics often assumes that people are rational in that they have the knowledge of their preferences and an ability to look for the best possible ways to maximize their preferences. People are seen as selfish and focus on what pleases them. Looking at various parts on their own results in the negligence of the other parts that work alongside it that create the effect of IO. Lincoln suggests possible ways to look at IO in a more holistic approach by recognizing the many possible factors that play a role in IO and how they work together to achieve IO. [ 62 ] It would be impossible for an individual to read all the academic papers published in a narrow speciality, even if they spent all their time reading. A response to this is the publishing of systematic reviews such as the Cochrane Reviews . Richard Smith argues that it would be impossible for a general practitioner to read all the literature relevant to every individual patient they consult with and suggests one solution would be an expert system for use of doctors while consulting. [ 63 ] Media related to Information overload at Wikimedia Commons",
    "links": [
      "Social media use in politics",
      "Crowd psychology",
      "Social media in the 2016 United States presidential election",
      "24-hour news cycle",
      "Infotainment",
      "Conrad Gessner",
      "Viktor Mayer-Schönberger",
      "Cochrane (organisation)",
      "Media studies",
      "Texting while driving",
      "Mean world syndrome",
      "Social media in the 2020 United States presidential election",
      "Neophile",
      "Doi (identifier)",
      "Too Much To Know",
      "Expert system",
      "Information–action ratio",
      "Availability cascade",
      "Pamphlets",
      "Peer pressure",
      "Negativity bias",
      "Spiral of silence",
      "Smartphones and pedestrian safety",
      "S2CID (identifier)",
      "Media multitasking",
      "JSTOR (identifier)",
      "TL;DR",
      "Bandwagon effect",
      "Algorithmic radicalization",
      "Frontiers in Psychology",
      "Jakob Nielsen (usability consultant)",
      "Political polarization in the United States",
      "Glass cockpit",
      "Least objectionable program",
      "Attention economy",
      "Social-desirability bias",
      "Copying",
      "Hdl (identifier)",
      "Social bot",
      "Cognitive bias",
      "Violence and video games",
      "Digital divide",
      "Human-interest story",
      "Sensationalism",
      "Carl Linnaeus",
      "Globalization",
      "Email spam",
      "Memory",
      "Television consumption",
      "Mobbing",
      "Griefer",
      "Conformity",
      "Self-discipline",
      "Online youth radicalization",
      "Ecclesiastes",
      "Information pollution",
      "Attention",
      "Data Smog",
      "Chumbox",
      "Scholars",
      "Information technology",
      "Accelerando (novel)",
      "Cognitive psychology",
      "Knowledge gap hypothesis",
      "Nielsen Norman Group",
      "Externality",
      "Museum fatigue",
      "Betteridge's law of headlines",
      "Institute of Electrical and Electronics Engineers",
      "Availability heuristic",
      "Criticism of Facebook",
      "Alvin Toffler",
      "Attention management",
      "RSS",
      "Denis Diderot",
      "Confirmation bias",
      "Binge-watching",
      "Encyclopédie",
      "Information filtering system",
      "Data transmission",
      "Information quality",
      "Ergonomics",
      "Georg Simmel",
      "E-mail spam",
      "Cognitive load",
      "Email attachments",
      "Journalistic scandal",
      "Attention inequality",
      "Analysis paralysis",
      "Political polarization",
      "Systematic Reviews (journal)",
      "Information Age",
      "Media bias",
      "Trial by media",
      "Cognitive miser",
      "Fake news website",
      "Tabloid television",
      "PMID (identifier)",
      "Social psychology",
      "Twitter",
      "Social influence bias",
      "Cognitive control",
      "Doomscrolling",
      "Public relations",
      "Criticism of Netflix",
      "Missing white woman syndrome",
      "Spike (journalism)",
      "Richard Saul Wurman",
      "Information age",
      "IEEE Spectrum",
      "Alarm fatigue",
      "Future Shock",
      "Low information voter",
      "ArXiv (identifier)",
      "Effects of violence in mass media",
      "Internet addiction",
      "Post-truth politics",
      "Technological singularity",
      "Tim Wu",
      "Seneca the Elder",
      "Satisficing",
      "Instant messages",
      "Internet culture",
      "James Gleick",
      "Soft media",
      "Hot take",
      "Misinformation",
      "Historical information",
      "Harvard Business Review",
      "World Wide Web",
      "Eric Schmidt",
      "News values",
      "Library of Alexandria",
      "Computer rage",
      "BBC News",
      "Technophobia",
      "Age of Interruption",
      "Echo chamber (media)",
      "Johannes Gutenberg",
      "Attention span",
      "2021 Facebook leak",
      "Technophilia",
      "Stanley Milgram",
      "Media (communication)",
      "Screen time",
      "Clickbait",
      "Filter bubble",
      "Sticky content",
      "Fake news websites in the United States",
      "Lexicographic information cost",
      "Media manipulation",
      "Culture shock",
      "Propaganda",
      "Mass shooting contagion",
      "Martin J. Eppler",
      "Tumblr",
      "Bystander effect",
      "Rage farming",
      "Evolutionary mismatch",
      "Stress management",
      "Evolutionary psychology",
      "Internet",
      "The New York Times",
      "Gatekeepers",
      "Manuscripts",
      "Human multitasking",
      "Cognitive dissonance",
      "Cognitive scientist",
      "Google",
      "Media psychology",
      "Yellow journalism",
      "Humboldt University",
      "Edward Tufte",
      "Social impact of YouTube",
      "Decision making",
      "Pink-slime journalism",
      "Time management",
      "Nicholas G. Carr",
      "Search engine",
      "Decision-making",
      "Informatics",
      "George Armitage Miller",
      "Attention theft",
      "Psychological effects of Internet use",
      "Ann M. Blair",
      "Competitive advantage",
      "Mobile phones and driving safety",
      "Evolution of cognition",
      "ISSN (identifier)",
      "Printing press",
      "Financial Times",
      "New York Times",
      "Social aspects of television",
      "Wired (magazine)",
      "Digital zombie",
      "Printer (publishing)",
      "ISBN (identifier)",
      "Cultural impact of TikTok",
      "Information processing (psychology)",
      "Academic publishing",
      "Information ecology",
      "Exocortex",
      "Junk food news",
      "Information Overload (album)",
      "Influence-for-hire",
      "Sealioning",
      "Learning curve",
      "Gmail",
      "Overchoice",
      "Fascination with death",
      "Facebook–Cambridge Analytica data scandal",
      "Social media and political communication in the United States",
      "Digital media use and mental health",
      "NBC News",
      "Knowledge divide",
      "Clay Shirky",
      "Instant messaging",
      "BBC",
      "Social media",
      "Behavioral modernity",
      "The Daily Telegraph",
      "Infodemic",
      "Moral panic",
      "Information explosion",
      "Gatekeeping (communication)",
      "Continuous partial attention",
      "Information management",
      "Political endorsement",
      "Phubbing",
      "Accelerating change"
    ]
  },
  "Compound term processing": {
    "url": "https://en.wikipedia.org/wiki/Compound_term_processing",
    "title": "Compound term processing",
    "content": "Compound-term processing, in information-retrieval , is search result matching on the basis of compound terms . Compound terms are built by combining two or more simple terms; for example, \"triple\" is a single word term, but \"triple heart bypass\" is a compound term. Compound-term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? Using this technique, a search for survival rates following a triple heart bypass in elderly people will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a concept search , which itself uses compound-term processing. This will extract the key concepts automatically (in this case \"survival rates\", \"triple heart bypass\" and \"elderly people\") and use these concepts to select the most relevant documents. In August 2003, Concept Searching Limited introduced the idea of using statistical compound-term processing. [ 1 ] CLAMOUR is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information and statistics. CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling . [ 2 ] Techniques for probabilistic weighting of single word terms date back to at least 1976 in the landmark publication by Stephen E. Robertson and Karen Spärck Jones . [ 3 ] Robertson stated that the assumption of word independence is not justified and exists as a matter of mathematical convenience. His objection to the term independence is not a new idea, dating back to at least 1964 when H. H. Williams stated that \"[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience\". [ 4 ] In 2004, Anna Lynn Patterson filed patents on \"phrase-based searching in an information retrieval system\" [ 5 ] to which Google subsequently acquired the rights. [ 6 ] Statistical compound-term processing is more adaptable than the process described by Patterson. Her process is targeted at searching the World Wide Web where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to enterprise search applications where such a priori knowledge is not available. Statistical compound-term processing is also more adaptable than the linguistic approach taken by the CLAMOUR project, which must consider the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent. Compound-term processing allows information-retrieval applications, such as search engines , to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous. Early search engines looked for documents containing the words entered by the user into the search box . These are known as keyword search engines. Boolean search engines add a degree of sophistication by allowing the user to specify additional requirements. For example, \"Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen\" uses the operators \"NEAR\", \"AND\", \"OR\" and \"NOT\" to specify that these words must follow certain requirements. A phrase search is simpler to use, but requires that the exact phrase specified appear in the results.",
    "links": [
      "Latent Dirichlet allocation",
      "Stop word",
      "Doi (identifier)",
      "Part-of-speech tagging",
      "Automatic summarization",
      "Interactive fiction",
      "AI-complete",
      "Word-sense disambiguation",
      "Search engines",
      "Syntactic parsing (computational linguistics)",
      "Corpus linguistics",
      "Latent semantic analysis",
      "Bank of English",
      "Sentence extraction",
      "Topic model",
      "Optical character recognition",
      "Information retrieval",
      "Machine translation",
      "Stephen Robertson (computer scientist)",
      "Grammar checker",
      "Sentiment analysis",
      "Semantic analysis (machine learning)",
      "Large language model",
      "Neural machine translation",
      "Universal Dependencies",
      "Concept Searching Limited",
      "Google Ngram Viewer",
      "Formal semantics (natural language)",
      "Enterprise search",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Bag-of-words model",
      "Parsing",
      "Predictive text",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "Named-entity recognition",
      "Concept search",
      "Word embedding",
      "Parallel text",
      "Argument mining",
      "SpaCy",
      "Ontology learning",
      "DBpedia",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Word2vec",
      "Computational linguistics",
      "Thesaurus (information retrieval)",
      "Information extraction",
      "Collocation extraction",
      "Pachinko allocation",
      "Natural Language Toolkit",
      "Lexical resource",
      "Word-sense induction",
      "Speech synthesis",
      "Language model",
      "Wikidata",
      "Shallow parsing",
      "Compound term",
      "Speech segmentation",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Document-term matrix",
      "Small language model",
      "Speech corpus",
      "Natural language processing",
      "Pronunciation assessment",
      "A priori and a posteriori",
      "Hallucination (artificial intelligence)",
      "Rule-based machine translation",
      "World Wide Web",
      "Stemming",
      "UBY",
      "GloVe",
      "Text simplification",
      "Bigram",
      "Semantic parsing",
      "Text segmentation",
      "Example-based machine translation",
      "Distant reading",
      "Multi-document summarization",
      "Statistical machine translation",
      "Trigram",
      "Concordancer",
      "Text corpus",
      "WordNet",
      "Natural language understanding",
      "FastText",
      "BabelNet",
      "Treebank",
      "BERT (language model)",
      "Semantic decomposition (natural language processing)",
      "Semantic similarity",
      "Spell checker",
      "Google",
      "Automatic identification and data capture",
      "Explicit semantic analysis",
      "Lexical analysis",
      "Text mining",
      "Concept mining",
      "Language resource",
      "Semantic network",
      "Linguistic Linked Open Data",
      "Statistical model",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Transfer-based machine translation",
      "Wayback Machine",
      "Textual entailment",
      "Speech recognition",
      "Terminology extraction",
      "Phrase search",
      "Lemmatisation",
      "Text processing",
      "Karen Spärck Jones",
      "Keyword search",
      "Question answering",
      "Chatbot",
      "Boolean search",
      "Document classification",
      "Automated essay scoring",
      "Truecasing",
      "Long short-term memory",
      "FrameNet",
      "Transformer (deep learning architecture)",
      "Machine-readable dictionary"
    ]
  },
  "Information behavior": {
    "url": "https://en.wikipedia.org/wiki/Information_behavior",
    "title": "Information behavior",
    "content": "Information behavior is a field of information science research that seeks to understand the way people search for and use information [ 1 ] in various contexts. It can include information seeking and information retrieval , but it also aims to understand why people seek information and how they use it. The term 'information behavior' was coined by Thomas D. Wilson in 1982 [ 2 ] and sparked controversy upon its introduction. [ 3 ] The term has now been adopted and Wilson's model of information behavior is widely cited in information behavior literature. [ 4 ] In 2000, Wilson defined information behavior as \"the totality of human behavior in relation to sources and channels of information\". [ 5 ] A variety of theories of information behavior seek to understand the processes that surround information seeking. [ 6 ] An analysis of the most cited publications on information behavior during the early 21st century shows its theoretical nature. [ 7 ] Information behavior research can employ various research methodologies grounded in broader research paradigms from psychology, sociology and education. [ 8 ] In 2003, a framework for information-seeking studies was introduced that aims to guide the production of clear, structured descriptions of research objects and positions information-seeking as a concept within information behavior. [ 9 ] Information need is a concept introduced by Wilson. Understanding the information need of an individual involved three elements: Information-seeking behavior is a more specific concept of information behavior. It specifically focuses on searching, finding, and retrieving information. Information-seeking behavior research can focus on improving information systems or, if it includes information need, can also focus on why the user behaves the way they do. A review study on information search behavior of users highlighted that behavioral factors, personal factors, product/service factors and situational factors affect information search behavior. [ 10 ] Information-seeking behavior can be more or less explicit on the part of users: users might seek to solve some task or to establish some piece of knowledge which can be found in the data in question, [ 11 ] or alternatively the search process itself is part of the objective of the user, in use cases for exploring visual content or for familiarising oneself with the content of an information service. [ 12 ] In the general case, information-seeking needs to be understood and analysed as a session rather than as a one-off transaction with a search engine, and in a broader context which includes user high-level intentions in addition to the immediate information need. [ 13 ] An information need is the recognition that a gap exists in one’s knowledge, prompting a desire to seek information to fill that gap. It often arises when a person encounters a problem or question they cannot resolve with their current understanding. Introduced by Elfreda Chatman in 1987, [ 14 ] information poverty is informed by the understanding that information is not equally accessible to all people. Information poverty does not describe a lack of information, but rather a worldview in which one's own experiences inside their own small world may create a distrust in the information provided by those outside their own lived experiences. [ 14 ] In Library and Information Science (LIS), a metatheory is described \"a set of assumptions that orient and direct theorizing about a given phenomenon\". [ 15 ] Library and information science researchers have adopted a number of different metatheories in their research. A common concern among LIS researchers, and a prominent discussion in the field, is the broad spectrum of theories that inform the study of information behavior, information users, or information use. This variation has been noted as a cause of concern because it makes individual studies difficult to compare or synthesize if they are not guided by the same theory. This sentiment has been expressed in studies of information behavior literature from the early 1980s [ 16 ] and more recent literature reviews have declared it necessary to refine their reviews to specific contexts or situations due to the sheer breadth of information behavior research available. [ 17 ] Below are descriptions of some, but not all, metatheories that have guided LIS research. A cognitive approach to understanding information behavior is grounded in psychology. It holds the assumption that a person's thinking influences how they seek, retrieve, and use information. Researchers that approach information behavior with the assumption that it is influenced by cognition, seek to understand what someone is thinking while they engage in information behavior and how those thoughts influence their behavior. [ 18 ] Wilson's attempt to understand information-seeking behavior by defining information need includes a cognitive approach. Wilson theorizes that information behavior is influenced by the cognitive need of an individual. By understanding the cognitive information need of an individual, we may gain insight into their information behavior. [ 2 ] Nigel Ford takes a cognitive approach to information-seeking, focusing on the intellectual processes of information-seeking. In 2004, Ford proposed an information-seeking model using a cognitive approach that focuses on how to improve information retrieval systems and serves to establish information-seeking and information behavior as concepts in and of themselves, rather than synonymous terms . [ 19 ] The constructionist approach to information behavior has roots in the humanities and social sciences. It relies on social constructionism , which assumes that a person's information behavior is influenced by their experiences in society. [ 18 ] In order to understand information behavior, constructionist researchers must first understand the social discourse that surrounds the behavior. The most popular thinker referenced in constructionist information behavior research is Michel Foucault , who famously rejected the concept of a universal human nature. The constructionist approach to information behavior research creates space for contextualizing the behavior based on the social experiences of the individual. One study that approaches information behavior research through the social constructionist approach is a study of the information behavior of a public library knitting group. [ 20 ] The authors use a collectivist theory to frame their research, which denies the universality of information behavior and focuses on \"understanding the ways that discourse communities collectively construct information needs, seeking, sources, and uses\". [ 20 ] The constructivist approach is born out of education and sociology in which, \"individuals are seen as actively constructing an understanding of their worlds, heavily influenced by the social world(s) in which they are operating\". [ 18 ] Constructivist approaches to information behavior research generally treat the individual's reality as constructed within their own mind rather than built by the society in which they live. [ 21 ] The constructivist metatheory makes space for the influence of society and culture with social constructivism, \"which argues that, while the mind constructs reality in its relationship to the world, this mental process is significantly informed by influences received from societal conventions, history and interaction with significant others\". [ 21 ] A common concern among LIS researchers, and a prominent discussion in the field, is the broad spectrum of theories that inform LIS research. This variation has been noted as a cause of concern because it makes individual studies difficult to compare if they are not guided by the same theory. Recent studies have shown that the impact of these theories and theoretical models is very limited. [ 22 ] LIS researchers have applied concepts and theories from many disciplines, including sociology, psychology, communication, organizational behavior, and computer science . [ 23 ] [ 24 ] The term was coined by Thomas D. Wilson in his 1981 paper, on the grounds that the current term, 'information needs' was unhelpful since 'need' could not be directly observed, while how people behaved in seeking information could be observed and investigated. [ 2 ] However, there is increasing work in the information-searching field that is relating behaviors to underlying needs. [ 25 ] In 2000, Wilson described information behavior as the totality of human behavior in relation to sources and channels of information, including both active and passive information-seeking, and information use. [ 5 ] He described information-seeking behavior as purposive seeking of information as a consequence of a need to satisfy some goal. Information-seeking behavior is the micro-level of behavior employed by the searcher in interacting with information systems of all kinds, be it between the seeker and the system, or the pure method of creating and following up on a search. Thomas Wilson proposed that information behavior covers all aspects of human information behavior, whether active or passive. Information-s eeking behavior is the act of actively seeking information in order to answer a specific query. Information-s earching behavior is the behavior which stems from the searcher interacting with the system in question. Information use behavior pertains to the searcher adopting the knowledge they sought. Elfreda Chatman developed the theory of life in the round, which she defines as a world of tolerated approximation. It acknowledges reality at its most routine, predictable enough that unless an initial problem should arise, there is no point in seeking information. [ 26 ] Chatman examined this principle within a small world: a world which imposes on its participants similar concerns and awareness of who is important; which ideas are relevant and whom to trust. Participants in this world are considered insiders. [ 26 ] Chatman focused her study on women at a maximum security prison. She learned that over time, prisoner's private views were assimilated to a communal acceptance of life in the round: a small world perceived in accordance with agreed upon standards and communal perspective. Members who live in the round will not cross the boundaries of their world to seek information unless it is critical; there is a collective expectation that information is relevant; or life lived in the round no longer functions. The world outside prison has secondary importance to inmates who are absent from this reality which is changing with time. [ 26 ] This compares the internet search methods of experienced information seekers (navigators) and inexperienced information seekers (explorers). Navigators revisit domains; follow sequential searches and have few deviations or regressions within their search patterns and interactions. Explorers visit many domains; submit many questions and their search trails branch frequently. [ 27 ] Brenda Dervin developed the concept of sensemaking. Sensemaking considers how we (attempt to) make sense of uncertain situations. [ 28 ] Her description of Sensemaking consisted of the definition of how we interpret information to use for our own information related decisions. Brenda Dervin described sensemaking as a method through which people make sense of their worlds in their own language. ASK was also developed by Nicholas J. Belkin. An anomalous state of knowledge is one in which the searcher recognises a gap in the state of knowledge. This, his or her further hypothesis, is influential in studying why people start to search. [ 29 ] McKenzie's model proposes that the information-seeking in everyday life of individuals occurs on a \"continuum of information practices... from actively seeking out a known source... to being given un-asked for advice.\" [ 30 ] This model crosses the threshold in information-seeking studies from information behavior research to information practices research. Information practices research creates space for understanding encounters with information that may not be a result of the individual's behavior. [ citation needed ] McKenzie's two-dimensional model includes four modes of information practices (active seeking, active scanning, non-directed monitoring, by proxy) over two phases of the information process (connecting and interacting). [ 30 ] Mode (below) In library and information science , Information search process (ISP) is a model proposed by Carol Kuhlthau in 1991 that represents a tighter focus on information-seeking behavior. Kuhlthau's framework was based on research into high school students, [ 31 ] but extended over time to include a diverse range of people, including those in the workplace. It examined the role of emotions, specifically uncertainty, in the information-seeking process, concluding that many searches are abandoned due to an overwhelmingly high level of uncertainty. [ 32 ] [ 33 ] [ 34 ] ISP is a 6-stage process, with each stage each encompassing 4 aspects: [ 35 ] [ 40 ] Kuhlthau's work is constructivist and explores information-seeking beyond the user's cognitive experience into their emotional experience while seeking information. She finds that the process of information-searching begins with feelings of uncertainty, navigates through feelings of anxiety, confusion, or doubt, and ultimately completes their information-seeking with feelings of relief or satisfaction, or disappointment. The consideration of an information-seeker's affect has been replicated more recently in Keilty and Leazer's study which focuses on physical affect and esthetics instead of emotional affect. [ 41 ] The usefulness of the model has been re-evaluated in 2008. [ 42 ] David Ellis investigated the behavior of researchers in the physical and social sciences, [ 43 ] and engineers and research scientists [ 44 ] through semi-structured interviews using a grounded theory approach, with a focus on describing the activities associated with information seeking rather than describing a process. Ellis' initial investigations produced six key activities within the information-seeking process: Later studies by Ellis (focusing on academic researchers in other disciplines) resulted in the addition of two more activities [ citation needed ] : Choo, Detlor and Turnbull elaborated on Ellis' model by applying it to information-searching on the web. Choo identified the key activities associated with Ellis in online searching episodes and connected them with four types of searching (undirected viewing, conditioned viewing, information search, and formal search). [ 45 ] Developed by Stuart Card , Ed H. Chi and Peter Pirolli , this model is derived from anthropological theories and is comparable to foraging for food. Information seekers use clues (or information scents) such as links, summaries and images to estimate how close they are to target information. A scent must be obvious as users often browse aimlessly or look for specific information. Information foraging is descriptive of why people search in particular ways rather than how they search. [ 46 ] Foster and Urquhart provide a rich understanding of their model for nonlinear information behavior. This model takes into consideration varying contexts and personalities when researching information behavior. The authors of this article are themselves cautious of this new model since it still requires more development . [ 47 ] Reijo Savolainen published his ELIS model in 1995. It is based on three basic concepts: way of life, life domain and information search in everyday life (ELIS). [ 48 ]",
    "links": [
      "Library and information science",
      "Information access",
      "Cultural studies",
      "Intellectual property",
      "Ed H. Chi",
      "Information society",
      "Categorization",
      "Human behavior",
      "Carol Kuhlthau",
      "Springer Publishing",
      "Semi-structured interview",
      "Doi (identifier)",
      "Metatheory",
      "Michel Foucault",
      "Taxonomy",
      "OCLC (identifier)",
      "Information architecture",
      "Elfreda Chatman",
      "Informatics",
      "S2CID (identifier)",
      "JSTOR (identifier)",
      "Journal of Documentation",
      "ISSN (identifier)",
      "Information Processing and Management",
      "Censorship",
      "Outline of information science",
      "Knowledge",
      "Information retrieval",
      "Knowledge organization",
      "Social constructionism",
      "Grounded theory",
      "Hdl (identifier)",
      "Information seeking",
      "ISBN (identifier)",
      "Stuart Card",
      "Jussi Karlgren",
      "Wilson's model of information behavior",
      "Intellectual freedom",
      "Science and technology studies",
      "Memory",
      "Bibliometrics",
      "Information foraging",
      "Brenda Dervin",
      "Peter Pirolli",
      "Privacy",
      "Computer science",
      "Steven M. Drucker",
      "Data modeling",
      "Thomas D. Wilson",
      "Ontology (information science)",
      "Constructivism (philosophy of science)",
      "Quantum information science",
      "Philosophy of information",
      "Preservation (library and archival science)",
      "Information management",
      "Computer data storage",
      "Information science",
      "Information technology",
      "Reference Services Review",
      "Sensemaking (information science)",
      "Library classification"
    ]
  },
  "Multi-document summarization": {
    "url": "https://en.wikipedia.org/wiki/Multi-document_summarization",
    "title": "Multi-document summarization",
    "content": "Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload . Multi-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together & outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should in theory contain the required information, hence limiting the need for accessing original files to cases when refinement is required. In practice, it is hard to summarize multiple documents with conflicting views and biases. In fact, it is almost impossible to achieve clear extractive summarization of documents with conflicting views. Abstractive summarization is the preferred venue in this case. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. The difficulties remain, if doing automatic extractive summaries of documents with conflicting views. The multi-document summarization task is more complex than summarizing a single document , even a long one. The difficulty arises from thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and concision. The Document Understanding Conferences, [ 1 ] conducted annually by NIST , have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge. An ideal multi-document summarization system not only shortens the source texts, but also presents information organized around the key aspects to represent diverse views. Success produces an overview of a given topic. Such text compilations should also follow basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows: The latter point deserves an additional note. Care is taken to ensure that the automatic overview shows: The multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available. As auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face copyright issues in relation to the fair use copyright concept.",
    "links": [
      "Latent Dirichlet allocation",
      "Stop word",
      "Doi (identifier)",
      "Relationship extraction",
      "Part-of-speech tagging",
      "Automatic summarization",
      "Interactive fiction",
      "WDQ (identifier)",
      "NIST",
      "AI-complete",
      "Word-sense disambiguation",
      "Syntactic parsing (computational linguistics)",
      "Corpus linguistics",
      "Latent semantic analysis",
      "Redundancy (information theory)",
      "Sentence extraction",
      "Bank of English",
      "Topic model",
      "Optical character recognition",
      "Machine translation",
      "Reuters",
      "Grammar checker",
      "Uniform Resource Locator",
      "Copyright",
      "Sentiment analysis",
      "Semantic analysis (machine learning)",
      "Large language model",
      "Neural machine translation",
      "Fair use",
      "Universal Dependencies",
      "Google Ngram Viewer",
      "CNN",
      "Formal semantics (natural language)",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Bag-of-words model",
      "Parsing",
      "Predictive text",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "Named-entity recognition",
      "Parallel text",
      "Word embedding",
      "Argument mining",
      "SpaCy",
      "Ontology learning",
      "Fox News",
      "DBpedia",
      "Computer-assisted translation",
      "Semantic role labeling",
      "Word2vec",
      "Computational linguistics",
      "Thesaurus (information retrieval)",
      "Information extraction",
      "Collocation extraction",
      "Pachinko allocation",
      "Natural Language Toolkit",
      "Lexical resource",
      "Word-sense induction",
      "Speech synthesis",
      "Language model",
      "Wikidata",
      "Shallow parsing",
      "Communication noise",
      "Speech segmentation",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Document-term matrix",
      "ArXiv (identifier)",
      "Small language model",
      "Speech corpus",
      "Natural language processing",
      "Pronunciation assessment",
      "Hallucination (artificial intelligence)",
      "Rule-based machine translation",
      "Stemming",
      "UBY",
      "Compound-term processing",
      "Text simplification",
      "GloVe",
      "Bigram",
      "Semantic parsing",
      "Text segmentation",
      "Google News",
      "Information overload",
      "Example-based machine translation",
      "Distant reading",
      "Statistical machine translation",
      "Trigram",
      "Concordancer",
      "News aggregators",
      "Text corpus",
      "WordNet",
      "Natural language understanding",
      "FastText",
      "BabelNet",
      "Journal of Artificial Intelligence Research",
      "Treebank",
      "BERT (language model)",
      "Semantic decomposition (natural language processing)",
      "Semantic similarity",
      "Spell checker",
      "Automatic identification and data capture",
      "Explicit semantic analysis",
      "Lexical analysis",
      "Text mining",
      "Concept mining",
      "Language resource",
      "Semantic network",
      "Linguistic Linked Open Data",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Transfer-based machine translation",
      "Wayback Machine",
      "Textual entailment",
      "Speech recognition",
      "Terminology extraction",
      "ISSN (identifier)",
      "Lemmatisation",
      "Text processing",
      "Question answering",
      "Chatbot",
      "Document classification",
      "Automated essay scoring",
      "Truecasing",
      "Long short-term memory",
      "FrameNet",
      "Dragomir R. Radev",
      "Transformer (deep learning architecture)",
      "Machine-readable dictionary",
      "Readability"
    ]
  },
  "Bill Maron": {
    "url": "https://en.wikipedia.org/wiki/Bill_Maron",
    "title": "Bill Maron",
    "content": "Melvin Earl \"Bill\" Maron (Jan 23, 1924 - September 28, 2016) was an American computer scientist and emeritis professor of University of California, Berkeley . [ 1 ] He studied mechanical engineering and physics at the University of Nebraska and received his Ph.D. in philosophy from the University of California in 1951. [ 2 ] Maron is best known for his work on probabilistic information retrieval which he published together with his friend and colleague Lary Kuhns. [ 3 ] [ 4 ] Quite remarkably, Maron also pioneered relational databases, proposing a system called the Relational Data File in 1967, on which Ted Codd based his Relational model of data. [ 5 ] This article about an American scientist in academia is a stub . You can help Wikipedia by expanding it .",
    "links": [
      "Relational model",
      "Ted Codd",
      "S2CID (identifier)",
      "University of Nebraska–Lincoln",
      "Legal information retrieval",
      "University of California, Berkeley",
      "Doi (identifier)",
      "ISSN (identifier)",
      "Information retrieval"
    ]
  },
  "XML retrieval": {
    "url": "https://en.wikipedia.org/wiki/XML_retrieval",
    "title": "XML retrieval",
    "content": "XML retrieval , or XML information retrieval , is the content-based retrieval of documents structured with XML (eXtensible Markup Language). As such it is used for computing relevance of XML documents. [ 1 ] Most XML retrieval approaches do so based on techniques from the information retrieval (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain structural hints . So-called \"content and structure\" (CAS) queries enable users to specify what structure the requested content can or must have. Taking advantage of the self-describing structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments. Ranking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request. [ 2 ] An overview of two potential approaches is available. [ 3 ] [ 4 ] The INitiative for the Evaluation of XML-Retrieval ( INEX ) was founded in 2002 and provides a platform for evaluating such algorithms . [ 2 ] Three different areas influence XML-Retrieval: [ 5 ] Query languages such as the W3C standard XQuery [ 6 ] supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents' schemas . [ 7 ] Classic database systems have adopted the possibility to store semi-structured data [ 5 ] and resulted in the development of XML databases . Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries. Classic information retrieval models such as the vector space model provide relevance ranking, but do not include document structure; only flat queries are supported. Also, they apply a static document concept, so retrieval units usually are entire documents. [ 7 ] They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document subtrees (index terms plus structure) as dimensions of the vector space. [ 8 ] For data-centric XML datasets, the unique and distinct keyword search method, namely, XDMA [ 9 ] for XML databases is designed and developed based on dual indexing and mutual summation.",
    "links": [
      "Data structure",
      "Relevance (information retrieval)",
      "W3C",
      "Doi (identifier)",
      "Semi-structured model",
      "Database",
      "Document retrieval",
      "S2CID (identifier)",
      "CiteSeerX (identifier)",
      "Vector space model",
      "Self-documenting",
      "Hint (SQL)",
      "Subtree",
      "Information retrieval",
      "Database schema",
      "ISBN (identifier)",
      "Information retrieval applications",
      "Query language",
      "XML database",
      "XQuery",
      "Algorithm",
      "XML"
    ]
  },
  "Case Western Reserve University": {
    "url": "https://en.wikipedia.org/wiki/Case_Western_Reserve_University",
    "title": "Case Western Reserve University",
    "content": "Case Western Reserve University ( CWRU ) is a private research university in Cleveland , Ohio, United States. It was federated in 1967 by a merger between Western Reserve University, founded in 1826 by the Presbyterian Church , and the Case Institute of Technology, founded in 1880. [ 9 ] [ 10 ] Case Western Reserve University comprises eight schools that offer more than 100 undergraduate programs and about 160 graduate and professional options across fields in STEM, medicine, arts, and the humanities. In 2024, the university enrolled 12,475 students (6,528 undergraduate plus 5,947 graduate and professional) from all 50 states and 106 countries and employed more than 1,182 full-time faculty members. The university's athletic teams, Case Western Reserve Spartans, play in NCAA Division III as a founding member of the University Athletic Association . Case Western Reserve University is a member of the Association of American Universities and is classified among \"R1: Doctoral Universities – Very high research activity\". [ 11 ] According to the National Science Foundation , in 2023 the university had research and development (R&D) expenditures of $553.7 million, ranking it 18th among private institutions and 59th in the nation. [ 12 ] Case alumni, scientists, and scholars have played significant roles in many scientific breakthroughs and discoveries. Case professor Albert A. Michelson became the first American to win a Nobel Prize in science, receiving the Nobel Prize in Physics . In total, seventeen Nobel laureates are associated with Case Western Reserve University. Western Reserve College, the college of the Connecticut Western Reserve , was founded in 1826 in Hudson, Ohio , as the Western Reserve College and Preparatory School by the Presbyterian Church . [ 9 ] [ 10 ] Western Reserve College, or \"Reserve\" as it was popularly called, was the first college in northern Ohio. [ 13 ] The school was called \"Yale of the West\"; its campus, now that of the Western Reserve Academy , imitated that of Yale. It had the same motto, \"Lux et Veritas\" (Light and Truth), the same entrance standards, and nearly the same curriculum. It was different from Yale in that it was a manual labor college , in which students were required to perform manual labor, seen as psychologically beneficial. [ 14 ] Western Reserve College's founders sought to instill in students an \"evangelical ethos\" and train Christian ministers for Ohio, where there was an acute shortage of them. The college was located in Hudson because the town made the largest financial offer to help in its construction. [ 15 ] : 422 That town, about 30 miles southeast of Cleveland, had been an antislavery center from the beginning: its founder, David Hudson , was against slavery, and founding trustee Owen Brown was a noted abolitionist who secured the location for the college. The abolitionist John Brown , who would lead the 1859 raid on Harpers Ferry , grew up in Hudson and was the son of co-founder Owen Brown. Hudson was a major stop on the Underground Railroad . With Presbyterian influences of its founding, the school's origins were strongly though briefly associated with the pre- Civil War abolitionist movement; [ 16 ] the abolition of slavery was the dominant topic on campus in 1831. The trustees were unhappy with the situation. The college's chaplain and Bible professor, Beriah Green , gave four sermons on the topic [ 17 ] and then resigned, expecting that he would be fired. President Charles Backus Storrs took a leave of absence for health, and soon died. One of the two remaining professors, Elizur Wright , soon left to head the American Anti-Slavery Society . [ 18 ] Western Reserve was the first college west of the Appalachian Mountains to enroll (1832) and graduate (1836) an African-American student, John Sykes Fayette . [ 19 ] Frederick Douglass gave the commencement speech in 1854. [ 20 ] In 1838, the Loomis Observatory was built by astronomer Elias Loomis , and today remains the second oldest observatory in the United States, and the oldest still in its original location. [ 21 ] In 1852, the Medical School became the second medical school in the United States to graduate a woman, Nancy Talbot Clark . Five more women graduated over the next four years, including Emily Blackwell and Marie Zakrzewska , giving Western Reserve the distinction of graduating six of the first eight female physicians in the United States. [ 22 ] By 1875, Cleveland had emerged as the dominant population and business center of the region, and the city wanted a prominent higher education institution. In 1882, with funding from Amasa Stone , Western Reserve College moved to Cleveland and changed its name to Adelbert College of Western Reserve University. Adelbert was the name of Stone's son. [ 23 ] In 1877, Leonard Case Jr. began laying the groundwork for the Case School of Applied Science by secretly donating valuable pieces of Cleveland real estate to a trust. He asked his confidential advisor, Henry Gilbert Abbey, to administer the trust and to keep it secret until after his death in 1880. On March 29, 1880, articles of incorporation were filed for the founding of the Case School of Applied Science . Classes began on September 15, 1881. [ 24 ] The school received its charter by the state of Ohio in 1882. For the first four years of the school's existence, it was located in the Case family's home on Rockwell Street in downtown Cleveland . Classes were held in the family house, while the chemistry and physics laboratories were on the second floor of the barn. Amasa Stone 's gift to relocate Western Reserve College to Cleveland also included a provision for the purchase of land in the University Circle area, adjacent to Western Reserve University, for the Case School of Applied Science. The school relocated to University Circle in 1885. In 1921 Albert Einstein came to the Case campus during his first visit to the United States, out of respect for the physics work performed there. Besides noting the research done in the Michelson–Morley experiment , Einstein also met with physics professor Dayton Miller to discuss his own research. [ 25 ] During World War II , Case School of Applied Science was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program which offered students a path to a Navy commission. [ 26 ] Over time, the Case School of Applied Science expanded to encompass broader subjects, adopting the name Case Institute of Technology in 1947 to reflect the institution's growth. [ 23 ] Led by polymer expert Eric Baer in 1963, the nation's first stand-alone Polymer Science and Engineering program was founded, to eventually become the Department of Macromolecular Science and Engineering. [ 27 ] Although the trustees of Case Institute of Technology and Western Reserve University did not formally federate their institutions until 1967, the institutions already shared buildings and staff when necessary and worked together often. One such example was seen in 1887, when Case physicist Albert Michelson and Reserve chemist Edward Morley collaborated on the famous Michelson–Morley experiment . There had been some discussion of a merger of the two institutions as early as 1890, but those talks dissolved quickly. In the 1920s, the Survey Commission on Higher Education in Cleveland took a strong stand in favor of federation and the community was behind the idea as well, but in the end all that came of the study was a decision by the two institutions to cooperate in founding Cleveland College, a special unit for part-time and adult students in downtown Cleveland . By the 1960s, Reserve President John Schoff Millis and Case President T. Keith Glennan shared the idea that federation would create a complete university, one better able to attain national distinction. Financed by the Carnegie Corporation , Cleveland Foundation , Greater Cleveland Associated Foundation , and several local donors, a study commission of national leaders in higher education and public policy was charged with exploring the idea of federation. The Heald Commission, so known for its chair, former Ford Foundation President Henry T. Heald , predicted in its final report that a federation could create one of the largest private universities in the nation. In 1967, Case Institute of Technology, a school with its emphasis on engineering and science, and Western Reserve University, a school with professional programs and liberal arts, came together to form Case Western Reserve University. [ 28 ] In 1968, the Department of Biomedical Engineering launched as a newly unified collaboration between the School of Engineering and School of Medicine as the first in the nation and as one of the first Biomedical Engineering programs in the world. [ 29 ] The following year in 1969, the first Biomedical Engineering MD/PhD program in the world began at Case Western Reserve. [ 30 ] The first computer engineering degree program in the United States was established in 1971 at Case Western Reserve. On August 18, 2003, the university unveiled a new logo and branding campaign that emphasized the \"Case\" portion of its name. [ 31 ] The decision to put emphasis on the \"Case\" portion of the name was motivated by issues related to name recognition of the existing CWRU acronym, especially outside of northeast Ohio. [ 31 ] In 2006, interim university president Gregory Eastwood convened a task group to study reactions to the campaign. The panel's report indicated that it had gone so poorly that, \"There appear to be serious concerns now about the university's ability to recruit and maintain high-quality faculty, fund-raising and leadership.\" Also, the logo was derided among the university's community and alumni and throughout northeastern Ohio; critics said it looked like \"...a fat man with a surfboard.\" [ 32 ] [ 33 ] On May 9, 2003, the 2003 Case Western Reserve University shooting occurred when Biswanath Halder entered the Peter B. Lewis Building of the Weatherhead School of Management where he killed graduate student Norman Wallace and wounded two professors. Halder took people in the building hostage, and they ran and barricaded themselves and hid during the seven hours that the gunman roamed the building, shooting indiscriminately. He was finally apprehended by a SWAT team. Halder was convicted on multiple felony counts and sentenced to life in prison; he lost a 2008 appeal. [ 34 ] [ 35 ] In March 2007, the Branding Task Group presented its recommendations; a key recommendation was to return a graphic identity that gave equal weight to both the \"Case\" and \"Western Reserve\" names. [ 36 ] As part of this, the creation of a new logo and wordmark was also recommended, with an implementation group to work with various stakeholders to develop a replacement logo and wordmark. [ 36 ] At a June 2nd meeting, the university's board of trustees approved a shift back to giving equal weight to \"Case\" and \"Western Reserve\". [ 37 ] In an open letter to the university community, interim president Eastwood admitted that \"the university had misplaced its own history and traditions\" with the 2003 branding changes. [ 38 ] Implementation of the new logo began July 1, 2007. [ 39 ] [ 40 ] The replacement logo, informally known as the \"sunburst\", would last until 2023. [ 41 ] [ 42 ] The \"Forward Thinking\" campaign was launched in 2011 by President Barbara Snyder and raised $1 billion in 30 months. The board of trustees unanimously agreed to expand the campaign to $1.5 billion, which reached its mark in 2017. [ 43 ] The campaign ultimately raised $1.82 billion. [ 44 ] A 2020 United States presidential debate , the first of two, was held at the Samson Pavilion of the Health Education Campus (HEC), shared by the Cleveland Clinic . [ 45 ] In February 2020, president Barbara Snyder was appointed the president of Association of American Universities (AAU). Later that year, former Tulane University president Scott Cowen was appointed interim president. On October 29, 2020, Eric W. Kaler , former University of Minnesota president, was appointed as the new Case Western Reserve University president, effective July 1, 2021. [ 46 ] On 2 June 2023, the 16 year old \"sunburst\" logo was replaced by a new logo which retained the sun element, but presented a more simple design, new fonts and brighter colors. [ 47 ] The new logo was met with mixed feelings from students, some praising the font choice and colors. [ 47 ] Others disliked the removal of the university's establishment year, 1826, present on the 2007-2023 logo, and the redesign of the sun image. [ 48 ] The editorial board of Case Western Reserve's student paper, The Observer , expressed overall dissatisfaction with the new logo, describing it as \"bland\" and \"an embarrassment and stains the reputation of success that built our historic institution.\" [ 48 ] Concerns were also expressed about the frequency of logo changes, as this was the third logo in 23 years. [ 47 ] Such frequent changes could harm the university's image and brand consistency and lead to a repeat of the 2003 logo situation reoccurring. [ 47 ] Case Western Reserve University's main campus is approximately 5 miles (8 km) east of Downtown Cleveland in the neighborhood known as University Circle , an area containing many educational, medical, and cultural institutions. [ 49 ] The Case Quadrangle, known also to students as the Engineering Quad, contains most engineering and science buildings, notably the John D. Rockefeller Physics Building. [ 50 ] The Case Quad also houses administration buildings, including Adelbert Hall . The Michelson–Morley experiment occurred here, commemorated by a marker and the Michelson-Morley Memorial Fountain . The southernmost edge consists of athletic areas— Adelbert Gymnasium , Van Horn Field and the Veale Convocation, Recreation and Athletic Center (commonly referred to as the Veale Center). The Veale Center houses the Horsburgh Gymnasium and the Veale Natatorium. The Flora Stone Mather Quadrangle is located north of Euclid Avenue between East Blvd., East 115th Street, and Juniper Road. The Flora Stone Mather College Historic District is more strictly defined by the area between East Blvd, Bellflower Road, and Ford Road north of Euclid Avenue. Named for the philanthropist wife of prominent industrialist Samuel Mather and sister-in-law of the famous statesman John Hay , the Mather Quad is home to Weatherhead School of Management , School of Law , Mandel School of Applied Social Sciences , and many departments of the College of Arts and Sciences . On and near campus, CircleLink is a free public shuttle service in University Circle and Little Italy. [ 51 ] For city public transit , rail and bus access are managed by the Greater Cleveland Regional Transit Authority (RTA). The two Red Line rapid train stations are Little Italy–University Circle and Cedar–University . Notably, the Red Line connects campus to Cleveland Hopkins Airport and Downtown Cleveland . The bus rapid transit (BRT) HealthLine runs down the center of campus along Euclid Ave . Numerous RTA bus routes run through campus. [ 52 ] The university in its present form consists of eight schools that offer more than 100 undergraduate programs and about 160 graduate and professional options. [ 53 ] CWRU also supports over 100 interdisciplinary academic and research centers in various fields. [ 54 ] Its graduate medical education includes residency and fellowship programs at University Hospitals Cleveland Medical Center (also known as University Hospitals Case Medical Center) and The MetroHealth System . [ 55 ] [ 56 ] The undergraduate student body hails from all 50 states and over 90 countries. [ 58 ] The six most popular majors are biomedical engineering , biology / biological sciences , nursing , mechanical engineering , and psychology . Since 2016, the top fields for graduating CWRU undergraduate students have been engineering, nursing, research and science, accounting and financial services, and information technology. [ 59 ] In 2023, the university received 39,039 applications. It extended offers of admission to 11,193 applicants, or 28.7%. 73% of admitted students were from outside Ohio and 13% from outside the United States. 1,544 accepted students chose to enroll, a yield rate of 13.8%. [ 57 ] Of the 43% of incoming students in 2023 who submitted SAT scores, the total interquartile range was 1440–1530; of the 23% of incoming students in 2023 who submitted ACT scores, the interquartile range of composite scores was 32–35. Of all matriculating students, the average high school GPA was 3.8. 71% of admitted students graduated in the top 10% of their high school class. [ 57 ] In U.S. News & World Report ' s 2025 rankings , Case Western Reserve was ranked as tied for 51st among national universities and 160th among global universities. [ 68 ] [ 69 ] The 2020 edition of The Wall Street Journal / Times Higher Education (WSJ/THE) rankings ranked Case Western Reserve as 52nd among US colleges and universities. [ 70 ] In 2018, Case Western Reserve was ranked 37th in the category American \"national universities\" and 146th in the category \"global universities\" by U.S. News & World Report . In 2019 U.S. News ranked it tied for 42nd and 152nd, respectively. Case Western Reserve was also ranked 32nd among U.S. universities—and 29th among private institutions—in the inaugural 2016 edition of The Wall Street Journal/Times Higher Education (WSJ/THE) rankings, but ranked tied for 39th among U.S. universities in 2019. [ 70 ] Case Western Reserve University's biochemistry program is jointly administered with the CWRU School of Medicine , and was ranked 14th nationally in the latest rankings by Blue Ridge Institute for Medical Research . [ 71 ] Case Western Reserve is noted (among other fields) for research in electrochemistry and electrochemical engineering . The Michelson–Morley interferometer experiment was conducted in 1887 in the basement of a campus dormitory by Albert A. Michelson of Case School of Applied Science and Edward W. Morley of Western Reserve University. Michelson became the first American to win a Nobel Prize in science. [ 72 ] Also in 2018, The Hollywood Reporter ranked CWRU's Department of Theater Master of Fine Arts program with the Cleveland Play House as 18th in the English-speaking world. In 2019, this ranking improved to 12th. [ 73 ] In 2014, Washington Monthly ranked Case Western Reserve University as the 9th best National University, [ 74 ] [ 75 ] but in the 2018 rankings, Case Western Reserve was ranked the 118th best National University. [ 76 ] In 2013, Washington Monthly ranked Case Western Reserve as the nation's 4th best National University for contributing to the public good. The publication's ranking was based upon a combination of factors including social mobility, research, and service. [ 77 ] In 2009, the school had ranked 15th. [ 78 ] Although Washington Monthly no longer ranks contributions to the public good as such, in its 2018 rankings of National Universities Case Western Reserve was ranked 180th in Social mobility and 118th in Service. [ 76 ] In 2013, Case Western Reserve was among the Top 25 LGBT-Friendly Colleges and Universities, according to Campus Pride. The recognition follows Case Western Reserve's first five-star ranking on the Campus Pride Index, a detailed survey of universities' policies, services and institutional support for LGBT individuals. [ 79 ] Case Western Reserve ranks 13th among private institutions (26th among all) in federal expenditures for science and engineering research and development, per the National Science Foundation . [ 80 ] Case Western Reserve University is a member of the Association of American Universities and is classified among \"R1: Doctoral Universities – Very high research activity\". [ 11 ] Following is a partial list of major contributions made by faculty, staff, and students at Case Western Reserve since 1887: [ citation needed ] Today, the university operates several facilities off campus for scientific research. One example of this is the Warner and Swasey Observatory at Kitt Peak National Observatory in Arizona . CWRU has contributed to the electrochemical sciences since the 1930s beginning with Frank Hovorka's studies of quinhydrone (quinone) and other electrodes. Subsequently, Ernest Yeager carried out pioneering studies on ultrasound electrodeposition and oxygen reduction reaction (ORR), which is directly relevant for H2-O2 fuel cells and batteries that use air electrodes such as zinc-air, iron-air, etc. The Yeager Center for Electrochemical Sciences (YCES), formerly the Case Center for Electrochemical Sciences, has provided annual workshops on electrochemical measurements since the late 1970s. The leadership in the Electrochemical Society have frequently included CWRU professors, and the university is home to six Fellows of the Electrochemical Society. Some notable achievements involve the work on ultrasound electrochemistry, oxygen reduction fundamentals, boron-doped diamond electrodes, in-situ electrochemical spectroscopy, polybenzimidazole (PBI) membranes for high-temperature fuel cells (HT-PEM), methanol fuel cells, iron-based flow batteries, metal deposition studies, dendrite modeling and electrochemical sensors. Noted laboratories at Case include the Electrochemical Engineering and Energy Laboratory (EEEL), the Electrochemical Materials Fabrication Laboratory (EMFL), the Case Electrochemical Capacitor Fabrication Facility and the ENERGY LAB. Larry Sears and Sally Zlotnick Sears think[box] is a public-access design and innovation center at Case Western Reserve University that allows students and other users to access prototyping equipment and other invention resources. The makerspace is located in the Richey Mixon building, a seven-story, 50,000 sq. ft. facility behind the campus athletic center. Over $35 million has been invested in space including in large part from a funding of $10 million from alumni Larry Sears and his wife Sally Zlotnick Sears. [ 94 ] [ 95 ] Larry Sears is an adjunct faculty member in the Department of Electrical Computer and Systems Engineering at CWRU and the founder of Hexagram, Inc. (now ACLARA Wireless Technologies). [ 96 ] Many projects and startup companies have come out of the makerspace . [ 97 ] The primary area for restaurants and shopping is the Uptown district along Euclid Ave adjacent to campus. Cleveland's Little Italy is within walking distance. A campus shuttle runs to Coventry Village , a shopping district in neighboring Cleveland Heights . Popular with students, Downtown Cleveland , Ohio City , Legacy Village , and Shaker Square are all a short driving distance or accessible by RTA . WRUW-FM (91.1 FM) is the campus radio station of Case Western Reserve University. WRUW broadcasts at a power of 15,000 watts and covers most of Northeast Ohio. Case Western Reserve is also home to 19 performing ensembles. [ 99 ] [ 100 ] [ 101 ] For performances, all students, ensembles, and a cappella groups use Harkness Chapel. The bands and orchestra also perform at Severance Hall (the on-campus home of the Cleveland Orchestra ) and CIM 's Kulas Hall. Case Western Reserve had the first ABET -accredited program in computer engineering . [ 102 ] In 1968, the university formed a private company, Chi Corporation, to provide computer time to both it and other customers. Initially this was on a Univac 1108 (replacing the preceding UNIVAC 1107 ), 36 bit, ones' complement machine. [ 103 ] The company was sold in 1977 to Robert G. Benson in Beachwood, Ohio becoming Ecocenters Corporation. Project Logos, under ARPA contract, was begun within the department on a DEC System-10 (later converted to TENEX ( BBN ) in conjunction with connection to the ARPANET ) to develop a computer-aided computer design system. This system consisted in a distributed, networked, graphics environment, a control and data flow designer and logic (both hardware and software) analyzer. An Imlac PDS-1 with lightpen interrupt was the main design workstation in 1973, communicating with the PDP-10 over a display communications protocol written by Don Huff as a Master Thesis and implemented on the Imlac by Ted Brenneman. Graphics and animation became another departmental focus with the acquisition of an Evans & Sutherland LDS-1 (Line Drawing System-1) , which was hosted by the DEC System-10, and later with the acquisition of the stand-alone LDS-2. Case Western Reserve was one of the earliest universities connected to the ARPANET , predecessor to the Internet . ARPANET went online in 1969; Case Western Reserve was connected in January 1971. [ 104 ] Case Western Reserve graduate Ken Biba published the Biba Integrity Model in 1977 and served on the ARPA Working Group that developed the Transmission Control Protocol (TCP) used on the Internet. It was the first university to have an all-fiber-optic network, in 1989. [ 105 ] At the inaugural meeting in October 1996, Case Western Reserve was one of the 34 charter university members of Internet2 . [ 106 ] The university was ranked No. 1 in Yahoo Internet Life's 1999 Most Wired College list. [ 107 ] There was a perception that this award was obtained through partially false or inaccurate information submitted for the survey, [ 108 ] and the university did not appear at all on the 2000 Most Wired College list (which included 100 institutions). The numbers reported were much lower than those submitted by Ray Neff in 1999. [ 109 ] [ 110 ] The university had previously placed No. 13 in the 1997 poll. [ 111 ] In August 2003, Case Western Reserve joined the Internet Streaming Media Alliance , then one of only two university members. [ 112 ] In September 2003, Case Western Reserve opened 1,230 public wireless access points on the Case Western Reserve campus and University Circle. [ 113 ] Case Western Reserve was one of the founding members of OneCleveland, formed in October 2003. [ 114 ] OneCleveland is an \"ultra broadband\" (gigabit speed) fiber optic network. This network is for the use of organizations in education, research, government, healthcare, arts, culture, and the nonprofit sector in Greater Cleveland. Case Western Reserve's Virtual Worlds gaming computer lab opened in 2005. The lab has a large network of Alienware PCs equipped with game development software such as the Torque Game Engine and Maya 3D modeling software. Additionally, it contains a number of specialized advanced computing rooms including a medical simulation room, a MIDI instrument music room, a 3D projection \"immersion room\", a virtual reality research room, and console room, which features video game systems such as Xbox 360 , PlayStation 3 , and Wii . [ 115 ] This laboratory can be used by any student in the Electrical Engineering and computer science department, and is heavily used for the Game Development (EECS 290) course. First-year students are grouped into one of four residential colleges that are overseen by first-year coordinators. The Mistletoe, Juniper, and Magnolia residential colleges were established when the \" First Year Experience \" system was introduced, and Cedar was created in the fall of 2005 to accommodate a large influx of new students. In the fall of 2007, Magnolia was integrated into Mistletoe, however, it was later re-separated in the fall of 2012. The areas of focus for each college are – Cedar: visual and performing arts; Mistletoe: service leadership; Juniper: multiculturalism and Magnolia: sustainability. [ 116 ] Nearly one-half of the campus undergraduates are said to be in a fraternity or sorority . There are dozens of Greek organizations on campus. The Office of Emergency Management prepares for various levels of emergencies on campus, such as chemical spills, severe weather, infectious diseases, and security threats. RAVE, a multi-platform emergency alerting system, is operated by Emergency Management for issuing emergency alerts and instructions for events on campus. The Office of Emergency Management also performs risk assessment to identify possible safety issues and aims to mitigate these issues. Additionally, CERT is managed through Emergency Management, enabling faculty and staff members to engage in emergency preparedness. The Office of Emergency Management works closely with other campus departments, such as Police and Security Services, University Health Services, and Environmental Health and Safety, as well as community resources including city, state, and federal emergency management agencies. [ 117 ] Case operates a police force of sworn officers as well as a security officers. [ 118 ] CWRU Police also works closely with RTA transit police, University Circle Police, Cleveland Police , East Cleveland Police, Cleveland Heights Police, University Hospitals Police Department, and other surrounding emergency services. Police and Security, with conjunction with the Emergency Management Office, conduct tabletop drills and full-scale exercises involving surrounding emergency services. [ 119 ] Case Western Reserve University Emergency Medical Services (CWRU EMS) is a student-run all volunteer ambulance service and a National Collegiate Emergency Medical Services Foundation member. Covering University Circle, CWRU EMS is run solely by undergraduates volunteers, who provides free basic life support level treatment and transport to local hospitals. [ 120 ] Crews receive medical direction from University Hospitals. [ 121 ] Starting in 1910, the Hudson Relay is an annual relay race event remembering and honoring the university relocation from Hudson, Ohio to Cleveland. Conceived by then-student, Monroe Curtis, [ 122 ] the relay race was run from the old college in Hudson, Ohio to the new university in University Circle. Since the mid-1980s, the race has been run entirely in the University Circle area. The race is a distance of 26 miles (42 km). It is held weekend before spring semester finals. Competing running teams are divided by graduating class. If a class wins the relay all four years, tradition dictates a reward of a champagne and steak dinner with the president of the university be awarded. Only six classes have won all four years—1982, 1990, 1994, 2006, 2011, and 2017. [ 123 ] [ 124 ] The winning classes of each year is carved on an original boulder located behind Adelbert Hall . Since 1976, the Film Society [ 125 ] of Case Western Reserve University has held a science fiction marathon . The film festival, the oldest of its type, boasts more than 34 hours of non-stop movies, cartoons, trailers, and shorts spanning many decades and subgenres, using both film and digital projection. The Film Society, which is student-run and open to the public, also shows movies on Friday and Saturday evenings throughout the school year. Case Western Reserve competes in 19 varsity sports—10 men's sports and 9 women's sports. All 19 varsity teams wear a commemorative patch on their uniforms honoring Case alumnus, M. Frank Rudy , inventor of the Nike air-sole. [ 126 ] The Spartans' primary athletic rival is the Carnegie Mellon Tartans . DiSanto Field is home to the football , men's soccer, women's soccer, and track and field teams. Case Western Reserve is a founding and current member of the University Athletic Association (UAA). The conference participates in the National Collegiate Athletic Association 's (NCAA) Division III . Case Institute of Technology and Western Reserve University were also founding members of the Presidents' Athletic Conference (PAC) in 1958. The university remained a member of the PAC after the merger of Case Institute of Technology and Western Reserve University and until 1983. In the fall of 1984, the university joined the North Coast Athletic Conference (NCAC) as a charter member. The 1998–99 school year marked the final season in which the Spartans were members of the NCAC. As the university had held joint conference membership affiliation with the UAA and the NCAC for over a decade. In 2014, the football team began competing as an associate member of the PAC, as only four out of the eight UAA member institutions sponsored football. [ 127 ] The Case Western Reserve football team reemerged in the mid-2000s under the direction of Head Coach Greg Debeljak . The 2007 team finished undefeated earning the school's first playoff appearance and first playoff victory, winning against the Widener Pride . Notable alumni include John Charles Cutler , former surgeon general who violated human rights and led to deaths in the Tuskegee Syphilis Study , Terre Haute prison experiments , and the syphilis experiments in Guatemala ; Anthony Russo and Joe Russo , Hollywood movie directors, Paul Buchheit , creator and lead developer of Gmail ; Craig Newmark , billionaire founder of Craigslist ; Peter Tippett , developer of the anti-virus software Vaccine, which Symantec purchased and turned into the popular Norton AntiVirus ; Francis E. Sweeney the main suspect from the Cleveland Torso Murders also was a Case Alumnus. Founders of Fortune 500 companies include Herbert Henry Dow , founder of Dow Chemical , Art Parker , founder of Parker Hannifin , and Edward Williams , co-founder of Sherwin-Williams . Other notable alumni include Larry Hurtado , New Testament scholar; Harvey Hilbert , a zen master, psychologist and expert on post-Vietnam stress syndrome; Peter Sterling , neuroscientist and co-founder of the concept of allostasis ; Ogiame Atuwatse III , Tsola Emiko the 21st Olu of Warri – a historic monarch of the Itsekiri people in Nigeria's Delta region, and Donald Knuth , a leading expert on computer algorithms and creator of the TeX typesetting system.",
    "links": [
      "Wesleyan University",
      "Indiana University Indianapolis",
      "The Wall Street Journal",
      "Flag of Cleveland",
      "Cleveland Institute of Art",
      "Great Lakes Science Center",
      "Mandel School of Applied Social Sciences",
      "Blood transfusion",
      "Greater Cleveland Associated Foundation",
      "Athletic nickname",
      "Leeds Beckett University",
      "USS Cod",
      "Columbus State Community College",
      "Higher education accreditation",
      "Union–Miles Park",
      "American middle class",
      "Gale Academic Onefile",
      "Civil rights movement",
      "Kitt Peak National Observatory",
      "Research university",
      "Peter Tippett",
      "University of Denver",
      "University of South Carolina Upstate",
      "Hiram College",
      "Hocking College",
      "Luminiferous aether",
      "Owens Community College",
      "Cleveland Museum of Natural History",
      "Playhouse Square",
      "University of Washington",
      "Eric Baer",
      "University of Miami",
      "Case School of Applied Science",
      "University of Texas at El Paso",
      "Cleveland Public Theatre",
      "University of North Carolina at Greensboro",
      "Underground Railroad",
      "University of Iowa",
      "John S. Millis",
      "Dhamakapella",
      "Morgan State University",
      "Texas A&M University",
      "California State University, Los Angeles",
      "Chatham University",
      "Biomedical Engineering",
      "Parma City School District",
      "Eric W. Kaler",
      "South Euclid–Lyndhurst City School District",
      "Allegheny College",
      "Purdue University Fort Wayne",
      "Western Reserve Academy",
      "World War II",
      "Peter B. Lewis",
      "University of Mount Union",
      "Cleveland Kurentovanje",
      "Agora Theatre and Ballroom",
      "Nobel Prize in Chemistry",
      "Bluffton University",
      "SciTech (magazine)",
      "Baylor University",
      "Notre Dame College",
      "Forbes",
      "Cleveland International Film Festival",
      "Bibcode (identifier)",
      "Wilberforce University",
      "Postgraduate education",
      "Northwest State Community College",
      "Beachwood, Ohio",
      "Saint Vincent College",
      "Rollins College",
      "Collinwood",
      "Coppin State University",
      "CiteSeerX (identifier)",
      "Central, Cleveland",
      "West Park, Cleveland",
      "Holy Name High School",
      "Purdue University",
      "Hopkins, Cleveland",
      "Imlac PDS-1",
      "Interquartile range",
      "Xbox 360",
      "Kent State University",
      "Emily Blackwell",
      "Goodtime III",
      "Villa Angela-St. Joseph High School",
      "ARPANET",
      "Tuskegee syphilis experiment",
      "Art Academy of Cincinnati",
      "Cincinnati State Technical and Community College",
      "Joy K. Ward",
      "America's Top Colleges",
      "Williams College",
      "Presbyterianism",
      "Lone Star College",
      "Lake Erie",
      "Saint Ignatius High School (Cleveland)",
      "University of Southern California",
      "Case Western Reserve Journal of International Law",
      "Non-Hispanic whites",
      "Dartmouth College",
      "Cleveland Hopkins International Airport",
      "Washington State University Tri-Cities",
      "Widener Pride",
      "Marietta College",
      "McGill University",
      "Fairleigh Dickinson University",
      "Albert A. Michelson",
      "Virginia Commonwealth University",
      "Nursing",
      "Paul C. Lauterbur",
      "Nobel Prize",
      "Bowling Green State University",
      "Carnegie Classification of Institutions of Higher Education",
      "Cleveland Crunch",
      "Federal Reserve Bank of Cleveland",
      "American Splendor",
      "Times Higher Education",
      "Georgetown University",
      "Rutgers University–Newark",
      "Sherwin-Williams Paints",
      "St. Vincent Charity Medical Center",
      "Pell grant",
      "Waynesburg University",
      "Children's Museum of Cleveland",
      "University of Maryland, Baltimore",
      "Greater Cleveland Aquarium",
      "STS-73",
      "Polykarp Kusch",
      "V-12 Navy College Training Program",
      "John Carroll University",
      "Emory Eagles",
      "Dow Chemical Company",
      "George Washington Crile",
      "University of Missouri",
      "International Space Station",
      "Cleveland Museum of Art",
      "Perry Monument (Cleveland)",
      "Marie Zakrzewska",
      "Fellowship (medicine)",
      "Cleveland Memorial Shoreway",
      "Prototype",
      "Westlake City School District",
      "Arizona State University",
      "Johns Hopkins University",
      "Loyola University Maryland",
      "Alfred G. Gilman",
      "University of Massachusetts Lowell",
      "University of Chicago",
      "Tiffin University",
      "Cuyahoga River",
      "Public Auditorium",
      "Wayback Machine",
      "Euclid Avenue (Cleveland)",
      "Little Italy, Cleveland",
      "Milton Sernett",
      "Duquesne University",
      "Case School of Dental Medicine",
      "Economy of Greater Cleveland",
      "NASA",
      "Wade Park, Cleveland",
      "Alienware",
      "Lorain County Community College",
      "Claude Beck",
      "Arizona",
      "Cleveland.com",
      "Public university",
      "Metropolitan Community College (Nebraska)",
      "Cleveland Heights–University Heights City School District",
      "Bethany College (West Virginia)",
      "African-American",
      "University of Connecticut",
      "United States Department of Education",
      "ISBN (identifier)",
      "Michigan State University",
      "Hudson, Ohio",
      "James W. Wagner",
      "Vanderbilt University",
      "Oxygen reduction reaction",
      "Paul Berg",
      "Guilford College",
      "Smith College",
      "John Charles Cutler",
      "Nobel Prize in Physiology or Medicine",
      "North Central State College",
      "East 4th Street (Cleveland)",
      "Loomis Observatory",
      "Martin Luther King Jr.",
      "Ferid Murad",
      "Gateway Sports and Entertainment Complex",
      "First Year Experience",
      "Edward C. Prescott",
      "Kamm's Corners",
      "Clayton State University",
      "Medical College of Wisconsin",
      "University of Michigan–Flint",
      "John Hay",
      "Belmont Technical College",
      "Northeast Ohio Medical University",
      "Rosa Parks",
      "Boston College",
      "Grove City College",
      "Glenn Research Center",
      "Ohio State University",
      "Pomona College",
      "Division III (NCAA)",
      "Manual labor college",
      "Cleveland Monsters",
      "Loyola University Chicago",
      "National Registry Emergency Medical Technician",
      "Case Western Reserve Spartans",
      "Western Reserve Historical Society",
      "Barnard College",
      "Quinhydrone",
      "Internet2",
      "Psychology",
      "Cleveland Trust Company Building",
      "Old Brooklyn",
      "The MetroHealth System",
      "S2CID (identifier)",
      "Peter Sterling (Neuroscientist)",
      "Community College of Philadelphia",
      "Northwestern University",
      "Wolstein Center",
      "Asiatown, Cleveland",
      "1904 Case football team",
      "List of Case Western Reserve University people",
      "Mount Vernon Nazarene University",
      "Warner and Swasey Observatory",
      "Mather House (Case Western Reserve University)",
      "Cleveland Browns",
      "University of Houston–Downtown",
      "National Collegiate Athletic Association",
      "Legacy Village",
      "Biological sciences",
      "GPA",
      "Brown University",
      "X-ray",
      "2003 Case Western Reserve University shooting",
      "St. Edward High School (Ohio)",
      "University of Rochester",
      "Special relativity",
      "Electrochemical Society",
      "College of Wooster",
      "Stark State College",
      "Tremont, Cleveland",
      "Washington & Jefferson Presidents",
      "University of Nebraska at Omaha",
      "Middlebury College",
      "Saint Louis University",
      "Soldiers' and Sailors' Monument (Cleveland)",
      "Dayton Miller",
      "West Side Market",
      "Case Western Reserve University School of Law",
      "Willard Park, Cleveland",
      "Electrochemical engineering",
      "Talespinner Children's Theatre",
      "University of Nevada-Las Vegas",
      "Italian American Museum of Cleveland",
      "American Anti-Slavery Society",
      "Barbara Snyder",
      "Quacquarelli Symonds",
      "Temple University",
      "Affluence in the United States",
      "University of Colorado Boulder",
      "Greater Cleveland",
      "Frank Gehry",
      "University of Houston",
      "Detroit–Superior Bridge",
      "Sherwin-Williams",
      "University of Rio Grande",
      "University of Toronto",
      "Cedar–University station",
      "United States Naval Academy",
      "Norton AntiVirus",
      "University of Oregon",
      "Tulane University",
      "Bellaire–Puritas, Cleveland",
      "List of mass media in Cleveland",
      "List of people from Cleveland",
      "Frank J. Lausche State Office Building",
      "Cleveland Public Library",
      "MIDI",
      "National Space Grant College and Fellowship Program",
      "Robert W. Morse",
      "Cornell University",
      "St. Clair–Superior",
      "Washington & Jefferson College",
      "Cuyahoga County, Ohio",
      "Massachusetts College of Art and Design",
      "Greg Debeljak",
      "U.S. Coast Guard Great Lakes District",
      "David H. Auston",
      "Frederick Reines",
      "Goodrich–Kirtland Park",
      "Washington University in St. Louis",
      "Mount St. Joseph University",
      "Eastern Gateway Community College",
      "Fairfax, Cleveland",
      "Case Western Reserve Spartans football",
      "Rice University",
      "Louis A. Toepfer",
      "ACT (test)",
      "Simon Ostrach",
      "Oakland University",
      "Sports in Cleveland",
      "Adelbert Hall",
      "Local Loop Unbundling",
      "Terre Haute prison experiments",
      "Transmission Control Protocol",
      "Little Italy–University Circle station",
      "University of Kansas",
      "University of South Florida",
      "Port of Cleveland",
      "Sinclair Community College",
      "Hamilton College",
      "Academic Bowl (college football)",
      "Solon City School District",
      "Dow Inc.",
      "Jacobs Pavilion",
      "Cudell, Cleveland",
      "Harvard University",
      "List of colleges and universities in Ohio",
      "Hough, Cleveland",
      "Nine-Twelve District",
      "CWRU Film Society Science Fiction Marathon",
      "Appalachian Mountains",
      "Larry Hurtado",
      "Asian Americans",
      "The Flats",
      "Michelson-Morley Memorial Fountain",
      "Cincinnati Christian University",
      "Rutgers University",
      "Electrochemistry",
      "American Civil War",
      "Cleveland State Vikings",
      "American lower class",
      "Yale University",
      "Terra State Community College",
      "George Washington University",
      "University of Central Oklahoma",
      "Purdue University Northwest",
      "Wii",
      "Times Higher Education World University Rankings",
      "University of Northwestern Ohio",
      "Albert Michelson",
      "Evans & Sutherland",
      "Davidson College",
      "Muskingum University",
      "Private university",
      "Mechanical engineering",
      "Cleveland Playhouse",
      "Great Lakes Theater",
      "University of Illinois Urbana-Champaign",
      "California State University, Dominguez Hills",
      "Signed number representations",
      "University of Arizona",
      "John Brown (abolitionist)",
      "University of Minnesota",
      "Robert Kearns",
      "Higher Learning Commission",
      "University of Florida",
      "Xavier University of Louisiana",
      "Colorado College",
      "Ohio Christian University",
      "UNIVAC 1107",
      "New Testament",
      "Chicago Maroons",
      "Case Comprehensive Cancer Center",
      "Physics",
      "Huntington Convention Center of Cleveland",
      "Edgewater, Cleveland",
      "Cleveland Metropolitan School District",
      "Northeast Ohio",
      "Near West Theatre",
      "Earl W. Sutherland Jr.",
      "Berea City School District",
      "University of California, Santa Cruz",
      "Make (magazine)",
      "David V. Ragone",
      "Princeton University",
      "Saint Joseph Academy (Cleveland, Ohio)",
      "University School",
      "2020 United States presidential debates",
      "Edward M. Hundert",
      "University of San Diego",
      "Provost (education)",
      "University Athletic Association",
      "Internet Streaming Media Alliance",
      "WRUW-FM",
      "Rockefeller Park",
      "Cleveland Masonic Temple",
      "East Cleveland City School District",
      "University of North Texas at Dallas",
      "College of Arts and Sciences (Case Western Reserve University)",
      "Anthony J. Celebrezze Federal Building",
      "Scripps College",
      "Youngstown State University",
      "Harvey Pekar",
      "The Mall (Cleveland)",
      "Emory University",
      "Magnificat High School",
      "George Walker Bush",
      "Dunham Tavern",
      "Bowdoin College",
      "Ohio Wesleyan University",
      "Commencement speech",
      "Colby College",
      "Euclid City School District",
      "Antioch College",
      "Undergraduate education",
      "University of Findlay",
      "University of the District of Columbia",
      "Washington Monthly",
      "Virtual reality",
      "Cleveland Heights",
      "Pace University",
      "UNIVAC 1108",
      "Amherst College",
      "Chancellor University",
      "RTA Rapid Transit",
      "Public transit",
      "JSTOR (identifier)",
      "Macalester College",
      "Risk assessment",
      "Capital University",
      "Brandeis University",
      "NCAA Division III",
      "Westminster College (Pennsylvania)",
      "Detroit–Shoreway",
      "Anthony Russo (movie director)",
      "Rock and Roll Hall of Fame",
      "PlayStation 3",
      "List of public art in Cleveland",
      "University of California, Irvine",
      "Wellesley College",
      "Cosmic ray",
      "Financial endowment",
      "George A. Olah",
      "Association of Independent Technological Universities",
      "List of presidents of Case Western Reserve University",
      "DARPA Urban Challenge",
      "STS-50",
      "Makerspace",
      "Baldwin Wallace University",
      "List of tallest buildings in Cleveland",
      "University of Pennsylvania",
      "Central Connecticut State University",
      "Carleton College",
      "University of Cincinnati",
      "Amasa Stone",
      "Health Education Campus",
      "Warrensville Heights City School District",
      "Rochester Institute of Technology",
      "Donald Knuth",
      "University of California, Santa Barbara",
      "University of Arkansas at Little Rock",
      "Washington Monthly college rankings",
      "Ohio Dominican University",
      "Brooklyn Centre",
      "Edward Morley",
      "Fraternities and sororities",
      "Flora Stone Mather College Historic District",
      "Cleveland State University",
      "Cleveland Cavaliers",
      "James A. Garfield Memorial",
      "Ford Foundation",
      "Oberlin College",
      "Paul Buchheit",
      "Stockyards, Cleveland",
      "Cleveland Guardians",
      "Brandeis Judges",
      "Cleveland Hungarian Museum",
      "University of North Carolina at Chapel Hill",
      "Miami University",
      "Autonomous car",
      "Howard M. Metzenbaum United States Courthouse",
      "Fred Gray (attorney)",
      "Mount Pleasant, Cleveland",
      "Florida Atlantic University",
      "NASA Public Service Medal",
      "LDS-1 (Line Drawing System-1)",
      "Lake Erie College",
      "Xavier University",
      "Saint Francis University",
      "Edward W. Morley",
      "Harvey Hilbert",
      "Community Emergency Response Team",
      "Otterbein University",
      "PMID (identifier)",
      "Hollywood (film industry)",
      "University of Maryland, College Park",
      "Public Square, Cleveland",
      "Garfield Heights City School District",
      "Elizur Wright",
      "Edward Williams (businessman)",
      "Broadway–Slavic Village",
      "Marvel Cinematic Universe",
      "Ohio University",
      "Tshilidzi Marwala",
      "University of California, Riverside",
      "Mir",
      "Rochester Yellowjackets",
      "Colgate University",
      "California State University, San Bernardino",
      "Cleveland City Council",
      "Sweetest Day",
      "Dallas College",
      "College of the Holy Cross",
      "History of Cleveland",
      "Washington University Bears",
      "Edison State Community College",
      "Beriah Green",
      "University of the Western Cape",
      "North Olmsted City School District",
      "Lake View Cemetery",
      "FWD (club)",
      "Defiance College",
      "DiSanto Field",
      "Stanford University",
      "Cozad–Bates House",
      "Economic diversity",
      "Case Western Reserve University School of Medicine",
      "HealthLine",
      "Carnegie Mellon University",
      "Kenyon College",
      "Huntington Bank Field",
      "Walsh University",
      "Cleveland Botanical Garden",
      "Minister (Christianity)",
      "Residential college",
      "Neighborhoods in Cleveland",
      "Indiana University Bloomington",
      "Museum of Contemporary Art Cleveland",
      "Downtown Cleveland",
      "Rocket Arena",
      "The New School",
      "California Institute of Technology",
      "Hope Memorial Bridge",
      "Padua Franciscan High School",
      "Cleveland Play House",
      "Erie Street Cemetery",
      "Ohio Northern University",
      "Allen Memorial Medical Library",
      "John Brown's Raid on Harpers Ferry",
      "Portland State University",
      "Cleveland Metroparks Zoo",
      "Internet",
      "Donald A. Glaser",
      "Florida International University",
      "Stockton University",
      "CWRU Biomedical Engineering",
      "Transportation in Cleveland",
      "Ukrainian Museum-Archives",
      "Hispanic and Latino Americans",
      "Craig Newmark",
      "MetroHealth",
      "Grinnell College",
      "Cleveland Orchestra",
      "Coventry Village",
      "QuestBridge",
      "African Americans",
      "The Hollywood Reporter",
      "University System of Ohio",
      "Academic Ranking of World Universities",
      "Spacelab",
      "NASA.gov",
      "Cuyahoga Community College",
      "Cleveland Thyagaraja Festival",
      "Olmsted Falls City School District",
      "Shaker Square",
      "Geneva College",
      "John D. Rockefeller",
      "Montclair State University",
      "Demographics of Cleveland",
      "Chancellor (education)",
      "George H. Hitchings",
      "Boston University",
      "U.S. News & World Report Best Global Universities Ranking",
      "Cleveland Clinic",
      "Nike, Inc.",
      "Beta decay",
      "Frederick K. Cox International Law Center",
      "University of Notre Dame",
      "University of Pittsburgh",
      "University of Michigan–Dearborn",
      "Cleveland Institute of Music",
      "Indiana University Northwest",
      "Craigslist",
      "Cleveland Foundation Centennial Lake Link Trail",
      "Marion Technical College",
      "Terminal Tower",
      "Weatherhead School of Management",
      "Gmail",
      "Frederick Douglass",
      "Cleveland Division of Fire",
      "Typhoid",
      "Gordon Park, Cleveland",
      "Warehouse District, Cleveland",
      "Clark–Fulton",
      "Particle detector",
      "U.S. News & World Report",
      "Counterfactuals",
      "Mayor of Cleveland",
      "Cleveland Division of Police",
      "Case Western Reserve University - Biomedical Engineering",
      "Northeastern Illinois University",
      "School colors",
      "Central Ohio Technical College",
      "Augsburg University",
      "Civil War History",
      "Carnegie Mellon Tartans",
      "Carl B. Stokes United States Courthouse",
      "David Hudson (pioneer)",
      "Windscreen wiper",
      "Washington and Lee University",
      "Shawnee State University",
      "Doi (identifier)",
      "Bus rapid transit",
      "University Hospitals Cleveland Medical Center",
      "Greater Cleveland Regional Transit Authority",
      "SS William G. Mather (1925)",
      "Samuel Mather",
      "Denison University",
      "Agnar Pytte",
      "Eric Kaler",
      "Nobel Memorial Prize in Economic Sciences",
      "National Register of Historic Places listings in Cleveland",
      "Cleveland Cinematheque",
      "Carnegie Corporation",
      "Wittenberg University",
      "Karamu House",
      "Abolitionist",
      "Case School of Engineering",
      "Abolitionism in the United States",
      "Saint Paul College",
      "Richard Thaler",
      "City Club of Cleveland",
      "Lakeland Community College",
      "Cleveland Charge",
      "Beaumont School (Ohio)",
      "Nobel Prize in Physics",
      "University of Texas at Dallas",
      "Benedictine High School (Ohio)",
      "U.S. News & World Report Best Colleges Ranking",
      "Rainbow Babies & Children's Hospital",
      "University of Michigan",
      "Ohio",
      "Accreditation Board for Engineering and Technology",
      "University of Kansas Medical Center",
      "Lourdes University",
      "Mayfield City School District",
      "Whiskey Island (Cleveland)",
      "Observatory",
      "National Science Foundation",
      "Whegs",
      "International Women's Air & Space Museum",
      "Ohio City, Cleveland",
      "Maltz Performing Arts Center",
      "Severance Hall",
      "Malone University",
      "Shaker Heights City School District",
      "Lee–Miles",
      "Syphilis experiments in Guatemala",
      "Hebrew Union College-Jewish Institute of Religion",
      "Tufts University",
      "Cleveland EMS",
      "Cedarville University",
      "A Christmas Story House",
      "University at Buffalo",
      "Medgar Evers College",
      "Rutgers University–Camden",
      "Case Western Reserve University Department of Biomedical Engineering",
      "Bedford City School District",
      "Tuskegee Syphilis Study",
      "Allostasis",
      "Leonard Case Jr.",
      "University of Dayton",
      "University Circle",
      "Wilmington College (Ohio)",
      "Antioch University",
      "University of Utah",
      "SAT",
      "University of California, San Diego",
      "Yahoo!",
      "Indiana University South Bend",
      "Cleveland Institute of Electronics",
      "Nicknames of Cleveland",
      "Towson University",
      "Michelson–Morley experiment",
      "Interface Message Processor",
      "Herbert Henry Dow",
      "Frances Payne Bolton School of Nursing",
      "American University",
      "Urbana University",
      "Buckeye–Shaker",
      "Timeline of Cleveland",
      "University of Maryland Baltimore County",
      "NYU Violets",
      "University of Missouri–Kansas City",
      "Columbia University",
      "North Coast Harbor",
      "Wright State University",
      "Cleveland Foundation",
      "Scott Cowen",
      "Hunter College",
      "Nancy Talbot Clark",
      "Bibliography of Cleveland",
      "Swarthmore College",
      "Cleveland WNBA team",
      "Civic Center (Cleveland)",
      "Biology",
      "Peter Agre",
      "Heidelberg University (Ohio)",
      "Research and development",
      "Matriculation",
      "Pennsylvania State University",
      "Red Line (Cleveland)",
      "Thiel College",
      "Ursuline College",
      "Universities Research Association",
      "University of Central Florida",
      "Coronary artery disease",
      "Syracuse University Press",
      "Glenville, Cleveland",
      "Neutrino",
      "History of Education Quarterly",
      "University of North Carolina at Charlotte",
      "I-X Center",
      "Stony Brook University",
      "Ashland University",
      "John Sykes Fayette",
      "Multiracial Americans",
      "James A. Rhodes State College",
      "Zane State College",
      "Metropolitan State University",
      "Buckeye–Woodhill",
      "List of museums in Cleveland",
      "Progressive Field",
      "M. Frank Rudy",
      "Washington State Community College",
      "University of Texas Rio Grande Valley",
      "Space Shuttle",
      "Weber State University",
      "Northeastern University",
      "Grays Armory",
      "Computer engineering",
      "University of California, Los Angeles",
      "Kinsman, Cleveland",
      "DePaul University",
      "University of Toledo",
      "Georgia Tech",
      "California State University, Sacramento",
      "University of Colorado Denver",
      "Beachwood City Schools",
      "Cleveland Ballet (founded 2014)",
      "University of Massachusetts Boston",
      "California State University, Northridge",
      "Massachusetts Institute of Technology",
      "Drexel University",
      "Owen Brown (abolitionist, born 1771)",
      "Residency (medicine)",
      "Parker Hannifin",
      "Buffalo State University",
      "Duke University",
      "T. Keith Glennan",
      "Skidmore College",
      "Haverford College",
      "Charles Backus Storrs",
      "Saint Francis Red Flash",
      "Dittrick Museum of Medical History",
      "Kettering College of Medical Arts",
      "Euclid–Green",
      "University of Tennessee at Chattanooga",
      "University of Virginia",
      "Maya (software)",
      "Torque Game Engine",
      "Biba Model",
      "National Collegiate Emergency Medical Services Foundation",
      "Cuyahoga Valley, Cleveland",
      "Coalition of Urban and Metropolitan Universities",
      "University of Missouri–St. Louis",
      "Arthur L. Parker",
      "Information asymmetry",
      "Startup company",
      "California State University, San Marcos",
      "BBN Technologies",
      "Clark State College",
      "QS World University Rankings",
      "Albert Einstein",
      "University of Wisconsin–Madison",
      "Franciscan University of Steubenville",
      "Joe Russo (director)",
      "Orange City School District",
      "Vassar College",
      "Cleveland City Hall",
      "North Coast Athletic Conference",
      "New York University",
      "Syracuse University",
      "Ogiame Atuwatse III",
      "Cleveland",
      "Elias Loomis",
      "Blue Ridge Institute for Medical Research",
      "Southern State Community College",
      "Association of American Universities",
      "Georgia Institute of Technology",
      "University of California, Berkeley",
      "Foreign national",
      "University of Baltimore",
      "Wagner College",
      "Kean University",
      "Biochemistry",
      "Claremont McKenna College",
      "Campus District",
      "College and university rankings in the United States",
      "Bay Village City School District",
      "Yield (college admissions)",
      "University of Louisville",
      "Fortune 500",
      "Connecticut Western Reserve",
      "University of California, Davis",
      "Central State University",
      "Cleveland Burke Lakefront Airport",
      "Presidents' Athletic Conference",
      "University of Texas at Austin",
      "West Chester University",
      "Cleveland Arcade",
      "Cleveland Metroparks",
      "University president",
      "Gilmour Academy",
      "TeX",
      "Henry Townley Heald",
      "Cleveland Hopkins Airport",
      "San Diego State University",
      "Cleveland Feast of the Assumption Festival",
      "TENEX (operating system)",
      "Dayton C. Miller",
      "NortonLifeLock",
      "Cleveland Cultural Gardens",
      "West Boulevard",
      "University of Akron",
      "Freshman",
      "Jefferson, Cleveland"
    ]
  },
  "Computing": {
    "url": "https://en.wikipedia.org/wiki/Computing",
    "title": "Computing",
    "content": "Computing is any goal-oriented activity requiring, benefiting from, or creating computing machinery . [ 1 ] It includes the study and experimentation of algorithmic processes, and the development of both hardware and software . Computing has scientific, engineering, mathematical, technological, and social aspects. Major computing disciplines include computer engineering , computer science , cybersecurity , data science , information systems , information technology , and software engineering . [ 2 ] The term computing is also synonymous with counting and calculating . In earlier times, it was used in reference to the action performed by mechanical computing machines , and before that, to human computers . [ 3 ] The history of computing is longer than the history of computing hardware and includes the history of methods intended for pen and paper (or for chalk and slate) with or without the aid of tables. Computing is intimately tied to the representation of numbers, though mathematical concepts necessary for computing existed before numeral systems . The earliest known tool for use in computation is the abacus , and it is thought to have been invented in Babylon circa between 2700 and 2300 BC. Abaci, of a more modern design, are still used as calculation tools today. The first recorded proposal for using digital electronics in computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams . [ 4 ] Claude Shannon 's 1938 paper \" A Symbolic Analysis of Relay and Switching Circuits \" then introduced the idea of using electronics for Boolean algebraic operations. The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain , while working under William Shockley at Bell Labs , built the first working transistor , the point-contact transistor , in 1947. [ 5 ] [ 6 ] In 1953, the University of Manchester built the first transistorized computer , the Manchester Baby . [ 7 ] However, early junction transistors were relatively bulky devices that were difficult to mass-produce, which limited them to a number of specialised applications. [ 8 ] In 1957, Frosch and Derick were able to manufacture the first silicon dioxide field effect transistors at Bell Labs, the first transistors in which drain and source were adjacent at the surface. [ 9 ] Subsequently, a team demonstrated a working MOSFET at Bell Labs 1960. [ 10 ] [ 11 ] The MOSFET made it possible to build high-density integrated circuits , [ 12 ] [ 13 ] leading to what is known as the computer revolution [ 14 ] or microcomputer revolution . [ 15 ] A computer is a machine that manipulates data according to a set of instructions called a computer program . [ 16 ] The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop a sequence of steps known as an algorithm . [ 17 ] Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the CPU type. [ 18 ] The execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions. Computer hardware includes the physical parts of a computer, including the central processing unit , memory , and input/output . [ 19 ] Computational logic and computer architecture are key topics in the field of computer hardware. [ 20 ] [ 21 ] Computer software, or just software , is a collection of computer programs and related data, which provides instructions to a computer. Software refers to one or more computer programs and data held in the storage of the computer. It is a set of programs, procedures, algorithms, as well as its documentation concerned with the operation of a data processing system. [ citation needed ] Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term hardware (meaning physical devices). In contrast to hardware, software is intangible. [ 22 ] Software is also sometimes used in a more narrow sense, meaning application software only. System software, or systems software, is computer software designed to operate and control computer hardware, and to provide a platform for running application software. System software includes operating systems , utility software , device drivers , window systems , and firmware . Frequently used development tools such as compilers , linkers , and debuggers are classified as system software. [ 23 ] System software and middleware manage and integrate a computer's capabilities, but typically do not directly apply them in the performance of tasks that benefit the user, unlike application software. Application software, also known as an application or an app , is computer software designed to help the user perform specific tasks. Examples include enterprise software , accounting software , office suites , graphics software , and media players . Many application programs deal principally with documents . [ 24 ] Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install additional applications. The system software manages the hardware and serves the application, which in turn serves the user. Application software applies the power of a particular computing platform or system software to a particular purpose. Some apps, such as Microsoft Office , are developed in multiple versions for several different platforms; others have narrower requirements and are generally referred to by the platform they run on. For example, a geography application for Windows or an Android application for education or Linux gaming . Applications that run only on one platform and increase the desirability of that platform due to the popularity of the application, known as killer applications . [ 25 ] A computer network, often simply referred to as a network, is a collection of hardware components and computers interconnected by communication channels that allow the sharing of resources and information. [ 26 ] When at least one process in one device is able to send or receive data to or from at least one process residing in a remote device, the two devices are said to be in a network. Networks may be classified according to a wide variety of characteristics such as the medium used to transport the data, communications protocol used, scale, topology , and organizational scope. Communications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for network programming . One well-known communications protocol is Ethernet , a hardware and link layer standard that is ubiquitous in local area networks . Another common protocol is the Internet Protocol Suite , which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, host-to-host data transfer, and application-specific data transmission formats. [ 27 ] Computer networking is sometimes considered a sub-discipline of electrical engineering , telecommunications, computer science , information technology, or computer engineering , since it relies upon the theoretical and practical application of these disciplines. [ 28 ] The Internet is a global system of interconnected computer networks that use the standard Internet Protocol Suite (TCP/IP) to serve billions of users. This includes millions of private, public, academic, business, and government networks, ranging in scope from local to global. These networks are linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web and the infrastructure to support email. [ 29 ] Computer programming is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a programming language , which is an artificial language that is often more restrictive than natural languages , but easily translated by the computer. Programming is used to invoke some desired behavior (customization) from the machine. [ 30 ] Writing high-quality source code requires knowledge of both the computer science domain and the domain in which the application will be used. The highest-quality software is thus often developed by a team of domain experts, each a specialist in some area of development. [ 31 ] However, the term programmer may apply to a range of program quality, from hacker to open source contributor to professional. It is also possible for a single programmer to do most or all of the computer programming needed to generate the proof of concept to launch a new killer application . [ 32 ] A programmer, computer programmer, or coder is a person who writes computer software. The term computer programmer can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. [ 33 ] A programmer's primary computer language ( C , C++ , Java , Lisp , Python , etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with Web . The term programmer can be used to refer to a software developer , software engineer, computer scientist , or software analyst . However, members of these professions typically possess other software engineering skills, beyond programming. [ 34 ] The computer industry is made up of businesses involved in developing computer software, designing computer hardware and computer networking infrastructures, manufacturing computer components, and providing information technology services, including system administration and maintenance. [ 35 ] The software industry includes businesses engaged in development , maintenance , and publication of software. The industry also includes software services , such as training , documentation , and consulting. [ citation needed ] Computer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. [ 36 ] Computer engineers usually have training in electronic engineering (or electrical engineering ), software design , and hardware-software integration, rather than just software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors , personal computers, and supercomputers , to circuit design . This field of engineering includes not only the design of hardware within its own domain, but also the interactions between hardware and the context in which it operates. [ 37 ] Software engineering is the application of a systematic, disciplined, and quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches. That is, the application of engineering to software. [ 38 ] [ 39 ] [ 40 ] It is the act of using insights to conceive, model and scale a solution to a problem. The first reference to the term is the 1968 NATO Software Engineering Conference , and was intended to provoke thought regarding the perceived software crisis at the time. [ 41 ] [ 42 ] [ 43 ] Software development , a widely used and more generic term, does not necessarily subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the Software Engineering Body of Knowledge (SWEBOK). The SWEBOK has become an internationally accepted standard in ISO/IEC TR 19759:2015. [ 44 ] Computer science or computing science (abbreviated CS or Comp Sci) is the scientific and practical approach to computation and its applications. A computer scientist specializes in the theory of computation and the design of computational systems. [ 45 ] Its subfields can be divided into practical techniques for its implementation and application in computer systems , and purely theoretical areas. Some, such as computational complexity theory , which studies fundamental properties of computational problems , are highly abstract, while others, such as computer graphics , emphasize real-world applications. Others focus on the challenges in implementing computations. For example, programming language theory studies approaches to the description of computations, while the study of computer programming investigates the use of programming languages and complex systems . The field of human–computer interaction focuses on the challenges in making computers and computations useful, usable, and universally accessible to humans. [ 46 ] The field of cybersecurity pertains to the protection of computer systems and networks. This includes information and data privacy , preventing disruption of IT services and prevention of theft of and damage to hardware, software, and data. [ 47 ] Data science is a field that uses scientific and computing tools to extract information and insights from data, driven by the increasing volume and availability of data. [ 48 ] Data mining , big data , statistics, machine learning and deep learning are all interwoven with data science. [ 49 ] Information systems (IS) is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute data . [ 50 ] [ 51 ] [ 52 ] The ACM 's Computing Careers describes IS as: \"A majority of IS [degree] programs are located in business schools; however, they may have different names such as management information systems, computer information systems, or business information systems. All IS degrees combine business and computing topics, but the emphasis between technical and organizational issues varies among programs. For example, programs differ substantially in the amount of programming required.\" [ 53 ] The study of IS bridges business and computer science , using the theoretical foundations of information and computation to study various business models and related algorithmic processes within a computer science discipline. [ 54 ] [ 55 ] [ 56 ] The field of Computer Information Systems (CIS) studies computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society [ 57 ] [ 58 ] while IS emphasizes functionality over design. [ 59 ] Information technology (IT) is the application of computers and telecommunications equipment to store, retrieve, transmit, and manipulate data, [ 60 ] often in the context of a business or other enterprise. [ 61 ] The term is commonly used as a synonym for computers and computer networks, but also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, including computer hardware, software, electronics , semiconductors , internet, telecom equipment , e-commerce , and computer services . [ 62 ] [ 63 ] DNA-based computing and quantum computing are areas of active research for both computing hardware and software, such as the development of quantum algorithms . Potential infrastructure for future technologies includes DNA origami on photolithography [ 64 ] and quantum antennae for transferring information between ion traps. [ 65 ] By 2011, researchers had entangled 14 qubits . [ 66 ] [ 67 ] Fast digital circuits , including those based on Josephson junctions and rapid single flux quantum technology, are becoming more nearly realizable with the discovery of nanoscale superconductors . [ 68 ] Fiber-optic and photonic (optical) devices, which already have been used to transport data over long distances, are starting to be used by data centers, along with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. [ 69 ] IBM has created an integrated circuit with both electronic and optical information processing in one chip. This is denoted CMOS-integrated nanophotonics (CINP). [ 70 ] One benefit of optical interconnects is that motherboards, which formerly required a certain kind of system on a chip (SoC), can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs. [ 71 ] Another field of research is spintronics . Spintronics can provide computing power and storage, without heat buildup. [ 72 ] Some research is being done on hybrid chips, which combine photonics and spintronics. [ 73 ] [ 74 ] There is also research ongoing on combining plasmonics , photonics, and electronics. [ 75 ] Cloud computing is a model that allows for the use of computing resources, such as servers or applications, without the need for interaction between the owner of these resources and the end user. It is typically offered as a service, making it an example of software as a service , platform as a service , and infrastructure as a service , depending on the functionality offered. Key characteristics include on-demand access, broad network access, and the capability of rapid scaling. [ 76 ] It allows individual users or small business to benefit from economies of scale . One area of interest in this field is its potential to support energy efficiency. Allowing thousands of instances of computation to occur on one single machine instead of thousands of individual machines could help save energy. It could also ease the transition to renewable energy source, since it would suffice to power one server farm with renewable energy, rather than millions of homes and offices. [ 77 ] However, this centralized computing model poses several challenges, especially in security and privacy. Current legislation does not sufficiently protect users from companies mishandling their data on company servers. This suggests potential for further legislative regulations on cloud computing and tech companies. [ 78 ] Quantum computing is an area of research that brings together the disciplines of computer science, information theory, and quantum physics. While the idea of information as part of physics is relatively new, there appears to be a strong tie between information theory and quantum mechanics. [ 79 ] Whereas traditional computing operates on a binary system of ones and zeros, quantum computing uses qubits . Qubits are capable of being in a superposition, i.e. in both states of one and zero, simultaneously. Thus, the value of the qubit is not between 1 and 0, but changes depending on when it is measured. This trait of qubits is known as quantum entanglement , and is the core idea of quantum computing that allows quantum computers to do large scale computations. [ 80 ] Quantum computing is often used for scientific research in cases where traditional computers do not have the computing power to do the necessary calculations, such in molecular modeling . Large molecules and their reactions are far too complex for traditional computers to calculate, but the computational power of quantum computers could provide a tool to perform such calculations. [ 81 ]",
    "links": [
      "Transistor",
      "Very-large-scale integration",
      "Video game",
      "Proof of concept",
      "Artificial intelligence",
      "Networking hardware",
      "Software Engineering Body of Knowledge",
      "Central processing unit",
      "Rendering (computer graphics)",
      "Multiprocessing",
      "Data (computing)",
      "Office suite",
      "Python (programming language)",
      "Programming team",
      "Interaction design",
      "Computational geometry",
      "Industrial process control",
      "Document file format",
      "Software framework",
      "Machine instructions",
      "Fault tolerance",
      "System on a chip",
      "Digital marketing",
      "Printed circuit board",
      "Document management system",
      "Profession",
      "Liquid computing",
      "John Wiley & Sons",
      "Information technology",
      "Integrated development environment",
      "Compute!",
      "Confidential computing",
      "Wireless sensor network",
      "Distributed computing",
      "Software engineering",
      "Enthusiast computing",
      "E-commerce",
      "Computational biology",
      "Point-contact transistor",
      "Knowledge representation and reasoning",
      "Computer software",
      "Data-centric computing",
      "Bibcode (identifier)",
      "Mechanical computer",
      "Hypertext",
      "Database",
      "CiteSeerX (identifier)",
      "3D visualization",
      "Reinforcement learning",
      "Computer History Museum",
      "Concurrency (computer science)",
      "Hardware security",
      "Supercomputer",
      "Augmented reality",
      "Unsupervised learning",
      "Computing (magazine)",
      "Form factor (design)",
      "Electronic design automation",
      "Microsoft Office",
      "Network architecture",
      "Control theory",
      "ArXiv (identifier)",
      "System software",
      "Cyber-physical system",
      "Modeling language",
      "Manchester Baby",
      "Computer services",
      "Hacker (hobbyist)",
      "Babylon",
      "Java (programming language)",
      "Formal methods",
      "Computer security",
      "Network scheduler",
      "Spatial computing",
      "Computational mathematics",
      "University of Manchester",
      "Computer networking",
      "Electronics",
      "Microprocessor",
      "Graphics software",
      "Algorithm",
      "Training",
      "Electronic voting",
      "Mathematical software",
      "Friedrich L. Bauer",
      "MIT",
      "Algorithmic efficiency",
      "Open-source software",
      "Deep learning",
      "Wayback Machine",
      "Software",
      "Qubit",
      "List of unsolved problems in computer science",
      "Computational complexity",
      "ISSN (identifier)",
      "Integrated circuit",
      "Human–computer interaction",
      "Quantum computing",
      "Mobile computing",
      "Input/output",
      "ISBN (identifier)",
      "Claude Shannon",
      "Software development process",
      "Requirements analysis",
      "Mathematical optimization",
      "Intrusion detection system",
      "PMC (identifier)",
      "Statistics",
      "Quantum entanglement",
      "Information system",
      "History of computing",
      "Linker (computing)",
      "Killer application",
      "S2CID (identifier)",
      "Data visualization",
      "Software developer",
      "Computer",
      "Programming language theory",
      "System administration",
      "Computer simulation",
      "Academic discipline",
      "Device driver",
      "Computer memory",
      "Artificial language",
      "Abacus",
      "Software maintenance",
      "C (programming language)",
      "Computer animation",
      "Lehmer sieve",
      "Media player (application software)",
      "Thermodynamic computing",
      "Cryptography",
      "Concurrent computing",
      "Formal semantics of programming languages",
      "Compiler",
      "Software engineer",
      "Image compression",
      "Complex systems",
      "Educational technology",
      "Peter Naur",
      "Domain-specific language",
      "Lisp (programming language)",
      "List of computer size categories",
      "Software quality",
      "Software repository",
      "Internet of things",
      "Data communication",
      "C. E. Wynn-Williams",
      "Denial-of-service attack",
      "Software configuration management",
      "Data mining",
      "Cambridge University Press",
      "Operating system",
      "Urban computing",
      "Interpreter (computing)",
      "Software design",
      "DNA computing",
      "Social computing",
      "Middleware",
      "Visualization (graphics)",
      "Creative computing",
      "Scientific",
      "A Symbolic Analysis of Relay and Switching Circuits",
      "Accounting software",
      "Natural language",
      "Information theory",
      "Control flow",
      "Supervised learning",
      "Information security",
      "Machine learning",
      "Electronic data processing",
      "Numerical analysis",
      "Model of computation",
      "Glossary of computer terms",
      "American Chemical Society",
      "Real-time computing",
      "Junction transistor",
      "Local area network",
      "Enterprise information system",
      "Association for Computing Machinery",
      "Automated planning and scheduling",
      "Hardware acceleration",
      "Multithreading (computer architecture)",
      "Analysis of algorithms",
      "ACM Computing Classification System",
      "Proceedings of the Royal Society A",
      "Information privacy",
      "Bell Labs",
      "Software deployment",
      "CPU",
      "Computer algebra",
      "Human computers",
      "List of computer term etymologies",
      "Ethernet",
      "Probability",
      "Application software",
      "Dependability",
      "Field-effect transistor",
      "NATO Software Engineering Conferences",
      "Theory of computation",
      "C++",
      "Computational thinking",
      "Computer graphics (computer science)",
      "Outline of computer science",
      "Semiconductor",
      "Compiler construction",
      "Computer programming",
      "Linux gaming",
      "History of computing hardware",
      "Virtual reality",
      "Computational social science",
      "Julius Edgar Lilienfeld",
      "Software crisis",
      "Function (engineering)",
      "JSTOR (identifier)",
      "DNA origami",
      "Nanometer",
      "Outline of computing",
      "Information retrieval",
      "Outline of computers",
      "Semantics (computer science)",
      "Digital circuit",
      "Walter Brattain",
      "Computer scientist",
      "Calculating",
      "Discrete mathematics",
      "Big data",
      "Word processor",
      "Jerry G. Fossum",
      "Process (computing)",
      "Information infrastructure",
      "Green computing",
      "Logic in computer science",
      "Computer vision",
      "Quantum algorithm",
      "Stochastic computing",
      "Algorithm design",
      "Geographic information system",
      "Communications protocol",
      "Spintronics",
      "Platform as a service",
      "Nanoscale superconductor",
      "PMID (identifier)",
      "Peripheral",
      "Enterprise software",
      "Multimedia database",
      "Randomized algorithm",
      "Software construction",
      "Electronic publishing",
      "Data science",
      "Counting",
      "World Wide Web",
      "MOSFET",
      "Service (economics)",
      "Boolean algebra",
      "Instruction set architecture",
      "Operations research",
      "Programming tool",
      "Internet",
      "Computer science",
      "Photograph manipulation",
      "Cybersecurity",
      "Virtual machine",
      "Parallel computing",
      "Network security",
      "Mathematical analysis",
      "Social software",
      "Multi-task learning",
      "Source code",
      "Health informatics",
      "Timeline of computing",
      "Communication protocol",
      "Computing platform",
      "Hacker (programmer subculture)",
      "Computer network",
      "Computer industry",
      "Computation",
      "Computational problem",
      "Ubiquitous computing",
      "Computer accessibility",
      "Instruction (computer science)",
      "Josephson junction",
      "Distributed artificial intelligence",
      "Graphics processing unit",
      "Molecular modeling",
      "Synonymous",
      "Rapid single flux quantum",
      "Computer Industry",
      "Human-centered computing",
      "Plasmonics",
      "Software as a service",
      "Solid modeling",
      "Doi (identifier)",
      "Terminology",
      "Security hacker",
      "Computational physics",
      "Programming paradigm",
      "Software documentation",
      "Software analyst",
      "Computational science",
      "Scientific computing",
      "Debugging",
      "Windows",
      "Software publisher",
      "Cross-validation (statistics)",
      "Link layer",
      "Computability theory",
      "Processor (computing)",
      "Computational logic",
      "Cyberwarfare",
      "Computational engineering",
      "Programming language",
      "Circuit design",
      "Numeral system",
      "Telecommunications equipment",
      "Software industry",
      "Electrical engineering",
      "Information systems",
      "Software development",
      "Network service",
      "Computer graphics",
      "Automata theory",
      "Nature Nanotechnology",
      "John Bardeen",
      "Android (operating system)",
      "Embedded system",
      "Computer hardware",
      "Computer architecture",
      "Computational chemistry",
      "Library (computing)",
      "Computer data storage",
      "Educational software",
      "William Shockley",
      "Application security",
      "Natural language processing",
      "Computer engineering",
      "Very large-scale integration",
      "Decision support system",
      "Brian Randell",
      "Computer system",
      "Electronic engineering",
      "Formal language",
      "Internet Protocol Suite",
      "Neural network (biology)",
      "Computer program",
      "Product bundling",
      "Economies of scale",
      "Transistorized computer",
      "Firmware",
      "Infrastructure as a service",
      "Unconventional computing",
      "IEEE Computer Society",
      "Microcomputer revolution",
      "Programmer",
      "Utility software",
      "Computer revolution",
      "Digital library",
      "Philosophy of artificial intelligence",
      "Computer network programming",
      "Security service (telecommunication)",
      "Digital art",
      "Photonics",
      "ENIAC",
      "Network performance",
      "Network topology",
      "Theoretical computer science",
      "Computational complexity theory",
      "Window system"
    ]
  },
  "SMART Information Retrieval System": {
    "url": "https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System",
    "title": "SMART Information Retrieval System",
    "content": "The SMART (System for the Mechanical Analysis and Retrieval of Text) Information Retrieval System is an information retrieval system developed at Cornell University in the 1960s. [ 1 ] Many important concepts in information retrieval were developed as part of research on the SMART system, including the vector space model , relevance feedback , and Rocchio classification . Gerard Salton led the group that developed SMART. Other contributors included Mike Lesk . The SMART system also provides a set of corpora, queries and reference rankings, taken from different subjects, notably To the legacy of the SMART system belongs the so-called SMART triple notation, a mnemonic scheme for denoting tf-idf weighting variants in the vector space model. The mnemonic for representing a combination of weights takes the form ddd.qqq , where the first three letters represents the term weighting of the collection document vector and the second three letters represents the term weighting for the query document vector. For example, ltc.lnn represents the ltc weighting applied to a collection document and the lnn weighting applied to a query document. The following tables establish the SMART notation: [ 2 ] The gray letters in the first, fifth, and ninth columns are the scheme used by Salton and Buckley in their 1988 paper. [ 4 ] The bold letters in the second, sixth, and tenth columns are the scheme used in experiments reported thereafter. This software-engineering -related article is a stub . You can help Wikipedia by expanding it .",
    "links": [
      "Relevance feedback",
      "Time (magazine)",
      "Cornell University",
      "Computer science",
      "Nearest centroid classifier",
      "Software engineering",
      "Cranfield experiments",
      "Mike Lesk",
      "Association for Information Science and Technology",
      "Forensic science",
      "MEDLINE",
      "Doi (identifier)",
      "Vector space model",
      "Gerard Salton",
      "Information retrieval",
      "Tf-idf"
    ]
  },
  "Divergence-from-randomness model": {
    "url": "https://en.wikipedia.org/wiki/Divergence-from-randomness_model",
    "title": "Divergence-from-randomness model",
    "content": "In the field of information retrieval , divergence from randomness ( DFR ), is a generalization of one of the very first models, Harter's 2-Poisson indexing-model . [ 1 ] It is one type of probabilistic model . It is used to test the amount of information carried in documents . The 2-Poisson model is based on the hypothesis that the level of documents is related to a set of documents that contains words that occur in relatively greater extent than in the rest of the documents. It is not a 'model', but a framework for weighting terms using probabilistic methods , and it has a special relationship for term weighting based on the notion of elite Term weights are being treated as the standard of whether a specific word is in that set or not. Term weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution . Divergence from randomness models set up by instantiating the three main components of the framework: first selecting a basic randomness model, then applying the first normalization and at last normalizing the term frequencies . The basic models are from the following tables. The divergence from randomness is based on this idea: \"The more the divergence of the within-document term-frequency from its frequency within the collection, the more the information carried by the word t in document d. In other words, the term-weight is inversely related to the probability of term-frequency within the document d obtained by a model M of randomness.\" [ 1 ] weight ( t | d ) = k Prob M ( t ∈ d | Collection ) {\\displaystyle {\\text{weight}}(t|d)=k{\\text{Prob}}_{M}(t\\in d|{\\text{Collection}})} (Formula 1) It is possible that we use different URN models to choose the appropriate model M of randomness. In Information Retrieval, there are documents instead of URNs, and terms instead of colors. There are several ways to choose M, each of these has a basic divergence from randomness model to support it. [ 1 ] When a specific rare term cannot be found in a document, then in that document the term has approximately zero probability of being informative. On the other hand, if a rare term occurs frequently in a document, therefore it can have a very high, near 100% probability to be informative for the topic that mentioned by the document. Applying to Ponte and Croft's language model can also provide further data. A risk component is considered in the DFR. Logically speaking, if the term-frequency in the document is relatively high, then inversely the risk for the term of not being informative is relatively small. If Formula 1 gives a high value, then there is a minimal risk that it has the negative effect of showing small information gain. As a result, the weight of Formula one is organized 1 to only consider the portion of which is the amount of information gained with the term. The more the term occurs in the elite set, the less term-frequency is due to randomness, and thus the smaller the associated risk is. We use two models to compute the information-gain with a term within a document: the Laplace L model and the ratio of two Bernoulli 's processes B. [ 2 ] Before using the within-document frequency tf of a term, the document-length dl is normalized to a standard length sl. Therefore, the term-frequencies tf are recalculated with the respect to the standard document-length, that is: tfn represents the normalized term frequency. Another version of the normalization formula is the following: Normalization 2 is usually considered to be more flexible, since there is no fixed value for c. Utility-Theoretic Indexing developed by Cooper and Maron is a theory of indexing based on utility theory. To reflect the value for documents that is expected by the users, index terms are assigned to documents. Also, Utility-Theoretic Indexing is related an \"event space\" in the statistical word. There are several basic spaces Ω in the Information Retrieval. A really simple basic space Ω can be the set V of terms t, which is called the vocabulary of the document collection. Due to Ω=V is the set of all mutually exclusive events, Ω can also be the certain event with probability: [ 3 ] Thus P, the probability distribution , assigns probabilities to all sets of terms for the vocabulary. Notice that the basic problem of Information Retrieval is to find an estimate for P(t). Estimates are computed on the basis of sampling and the experimental text collection furnishes the samples needed for the estimation. Now we run into the main concern which is how do we treat two arbitrary but heterogeneous pieces of texts appropriately. Paragons like a chapter in a Science Magazine and an article from a sports newspaper as the other. They can be considered as two different samples since those aiming at different population. The relationship of the document with the experiments is made by the way in which the sample space is chosen. In IR, term experiment, or trial, is used here with a technical meaning rather than a common sense. For example, a document could be an experiment which means the document is a sequence of outcomes t∈V, or just a sample of a population. We will talk about the event of observing a number Xt =tf of occurrences of a given word t in a sequence of experiments. In order to introduce this event space, we should introduce the product of the probability spaces associated with the experiments of the sequence. We could introduce our sample space to associate a point with possible configurations of the outcomes. The one-to-one correspondence for sample space can be defined as: Where ld is the number of trials of the experiment or in this example, the length of a document. We can assume that each outcome may or may not depend on the outcomes of the previous experiments. If the experiments are designed so that an outcome is influencing the next outcomes, then the probability distribution on V is different at each trial. But, more commonly, in order to establish the simpler case when the probability space is invariant in IR, the term independence assumption is often made. Therefore, all possible configurations ofΩ=Vld are considered equiprobable. Considering this assumption, we can consider each document a Bernoulli process . The probability spaces of the product are invariant and the probability of a given sequence is the product of the probabilities at each trial. Consequently, if p=P(t) is the prior probability that the outcome is t and the number of experiments is ld we obtain the probability of X t = t f {\\displaystyle X_{t}=tf} is equal to: Which is the sum of the probability of all possible configurations having tf outcomes out of ld. P(Xt=tf|p) is a probability distribution because Already considering the hypothesis of having a single sample, we need to consider that we have several samples, for example, a collection D of documents. The situation of having a collection of N documents is abstractly equivalent to the scheme of placing a certain number Tot of V colored types of balls in a collection of N cells. For each term t∈V a possible configuration of ball placement satisfies the equations: And the condition Where Ft is the number of balls of the same color t to be distributed in the N cells. We have thus changed the basic space. The outcome of our experiment will be the documents d in which the ball will be placed. Also, we will have a lot of possible configurations consistent with the number of colored balls. The divergence from Randomness Model is based on the Bernoulli model and its limiting forms, the hypergeometric distribution, Bose–Einstein statistics and its limiting forms, the compound of the binomial distribution with the beta distribution , and the fat-tailed distribution. Divergence from randomness model shows a unifying framework that has the potential constructing a lot of different effective models of IR. Proximity can be handled within divergence from randomness to consider the number of occurrences of a pair of query terms within a window of pre-defined size. To specify, the DFR Dependence Score Modifier DSM implements both the pBiL and pBiL2 models, which calculate the randomness divided by the document's length, rather than the statistics of the pair in the corpus the pair in the corpus. Let t be a term and c be a collection. Let the term occur in tfc=nL(t,c)=200 locations, and in df(t,c)=nL(t,c)=100 documents. The expected average term frequency is avgtf(t,c)=200/100=2; this is the average over the documents in which the term occurs. Let N.D(c)=1000 be the total amounts of documents. The term's occurrence is 10% in the documents: P.D(t|c)=100/1000. The expected average term frequency is 200/1000=1/5, and this is the average over all documents. The term frequency is shown as Kt =0,...,6. The following table show the column nD is the number of Documents that contains kt occurrence of t, shown as nD(t,c,kt). Another column nL is the number of Locations at which the term occurs follows by this equation: nL=kt*nD. The columns to the right show the observed and Poisson probabilities. P obs,elite(Kt) is the observed probability over all documents. P Poisson, all, lambda(Kt) is the Poisson probability, where lambda(t,c)=nL(t,c)/N D(c)=0.20 is the Poisson parameter. The table illustrates how the observed probability is different from the Poisson probability. P Poisson(1) is greater than P obs(1), whereas for kt>1.the observed probabilities are greater than the Poisson probabilities. There is more mass in the tail of the observed distribution than the Poisson distribution assumes. Moreover, the columns to the right illustrate the usage of the elite documents instead of all documents. Here, the single event probability is based on the locations of elite documents only.",
    "links": [
      "Randomness",
      "Prior probability",
      "Science Magazine",
      "Probability space",
      "Beta distribution",
      "Probabilistic method",
      "Bernoulli process",
      "Weighting",
      "Probabilistic",
      "Poisson distribution",
      "Hypergeometric distribution",
      "Information retrieval",
      "Term Frequency",
      "Heterogeneous",
      "Divergence (statistics)",
      "Negative relationship",
      "Binomial distribution",
      "Random walk",
      "Fat-tailed distribution",
      "Computing",
      "Okapi BM25",
      "Relation (mathematics)",
      "Generalization",
      "Elite",
      "Language model",
      "Ω",
      "Model",
      "Uniform Resource Name",
      "Document",
      "Stochastic process",
      "Query expansion",
      "Bernoulli's principle",
      "Bose–Einstein statistics",
      "Sample space",
      "Information",
      "Hypothesis",
      "Distribution (mathematics)"
    ]
  },
  "Text corpora": {
    "url": "https://en.wikipedia.org/wiki/Text_corpora",
    "title": "Text corpora",
    "content": "In linguistics and natural language processing , a corpus ( pl. : corpora ) or text corpus is a dataset, consisting of natively digital and older, digitalized, language resources , either annotated or unannotated. Annotated, they have been used in corpus linguistics for statistical hypothesis testing , checking occurrences or validating linguistic rules within a specific language territory. A corpus may contain texts in a single language ( monolingual corpus ) or text data in multiple languages ( multilingual corpus ). In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as annotation . An example of annotating a corpus is part-of-speech tagging , or POS-tagging , in which information about each word's part of speech (verb, noun, adjective, etc.) is added to the corpus in the form of tags . Another example is indicating the lemma (base) form of each word. When the language of the corpus is not a working language of the researchers who use it, interlinear glossing is used to make the annotation bilingual. Some corpora have further structured levels of analysis applied. In particular, smaller corpora may be fully parsed . Such corpora are usually called Treebanks or Parsed Corpora . The difficulty of ensuring that the entire corpus is completely and consistently annotated means that these corpora are usually smaller, containing around one to three million words. Other levels of linguistic structured analysis are possible, including annotations for morphology , semantics and pragmatics . Corpora are the main knowledge base in corpus linguistics . Other notable areas of application include:",
    "links": [
      "Latent Dirichlet allocation",
      "Stop word",
      "Doi (identifier)",
      "Part-of-speech tagging",
      "Automatic summarization",
      "Interactive fiction",
      "AI-complete",
      "Word-sense disambiguation",
      "S2CID (identifier)",
      "Corpus linguistics",
      "Syntactic parsing (computational linguistics)",
      "Latent semantic analysis",
      "Bank of English",
      "Sentence extraction",
      "Topic model",
      "Optical character recognition",
      "Morphology (linguistics)",
      "Statistical hypothesis testing",
      "Machine translation",
      "Grammar checker",
      "Sentiment analysis",
      "Semantic analysis (machine learning)",
      "Large language model",
      "Neural machine translation",
      "Zipf's law",
      "Semantics",
      "Decipherment",
      "Universal Dependencies",
      "Google Ngram Viewer",
      "Formal semantics (natural language)",
      "Parallel corpora",
      "Natural-language user interface",
      "Computer-assisted reviewing",
      "Lemma (morphology)",
      "Concordance (publishing)",
      "Bag-of-words model",
      "Parsing",
      "Predictive text",
      "Sentence boundary disambiguation",
      "Seq2seq",
      "Distributional semantics",
      "N-gram",
      "Named-entity recognition",
      "Frequency list",
      "Parallel text",
      "Word embedding",
      "Argument mining",
      "SpaCy",
      "Ontology learning",
      "Linguistic Data Consortium",
      "DBpedia",
      "Computer-assisted translation",
      "Biblical scholarship",
      "Semantic role labeling",
      "Word2vec",
      "Computational linguistics",
      "Thesaurus (information retrieval)",
      "Information extraction",
      "Collocation extraction",
      "Pachinko allocation",
      "Natural Language Toolkit",
      "Lexical resource",
      "Word-sense induction",
      "Speech synthesis",
      "Language model",
      "Wikidata",
      "Shallow parsing",
      "Language technology",
      "Annotation",
      "Speech segmentation",
      "Voice user interface",
      "Simple Knowledge Organization System",
      "Virtual assistant",
      "Document-term matrix",
      "Translation memory",
      "ArXiv (identifier)",
      "Interlinear gloss",
      "Small language model",
      "Speech corpus",
      "Natural language processing",
      "Pronunciation assessment",
      "Hallucination (artificial intelligence)",
      "Rule-based machine translation",
      "Pragmatics",
      "Stemming",
      "Language teaching",
      "UBY",
      "Compound-term processing",
      "Text simplification",
      "GloVe",
      "Bigram",
      "Semantic parsing",
      "Text segmentation",
      "Example-based machine translation",
      "Distant reading",
      "Multi-document summarization",
      "Statistical machine translation",
      "Trigram",
      "Concordancer",
      "List of text corpora",
      "WordNet",
      "Hidden Markov model",
      "Natural language understanding",
      "FastText",
      "BabelNet",
      "Philology",
      "Treebank",
      "Distributional–relational database",
      "BERT (language model)",
      "Historical document",
      "Semantic decomposition (natural language processing)",
      "Semantic similarity",
      "Spell checker",
      "Automatic identification and data capture",
      "Explicit semantic analysis",
      "Lexical analysis",
      "Text mining",
      "Concept mining",
      "Language resource",
      "Semantic network",
      "Linguistics",
      "Amarna letters",
      "Linguistic Linked Open Data",
      "Deep linguistic processing",
      "PropBank",
      "Natural language generation",
      "Transfer-based machine translation",
      "Wayback Machine",
      "Foreign language writing aid",
      "Textual entailment",
      "Speech recognition",
      "Terminology extraction",
      "ISSN (identifier)",
      "Lemmatisation",
      "Text processing",
      "Culturomics",
      "ISBN (identifier)",
      "Question answering",
      "1350 BC",
      "Chatbot",
      "Document classification",
      "Automated essay scoring",
      "Truecasing",
      "Long short-term memory",
      "FrameNet",
      "Kültepe",
      "Transformer (deep learning architecture)",
      "Machine-readable dictionary"
    ]
  }
}